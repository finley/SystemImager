diff -urN linux/Documentation/Configure.help /tmp/linux/Documentation/Configure.help
--- linux/Documentation/Configure.help	Sun Dec 10 17:49:41 2000
+++ /tmp/linux/Documentation/Configure.help	Fri Feb  2 19:06:00 2001
@@ -1015,6 +1015,13 @@
 
   If unsure, say N.
 
+Autodetect RAID partitions
+CONFIG_AUTODETECT_RAID
+  This feature lets the kernel detect RAID partitions on bootup.
+  An autodetect RAID partition is a normal partition with partition
+  type 0xfd. Use this if you want to boot RAID devices, or want to
+  run them automatically.
+
 Linear (append) mode
 CONFIG_MD_LINEAR
   If you say Y here, then your multiple devices driver will be able to
@@ -1094,6 +1101,21 @@
 
   If unsure, say Y.
 
+Translucent Block Device Support (EXPERIMENTAL)
+CONFIG_MD_TRANSLUCENT
+  DO NOT USE THIS STUFF YET!
+
+  currently there is only a placeholder there as the implementation
+  is not yet usable.
+
+Hierarchical Storage Management support (EXPERIMENTAL)
+CONFIG_MD_HSM
+  DO NOT USE THIS STUFF YET!
+
+  i have released this so people can comment on the architecture,
+  but user-space tools are still unusable so there is nothing much
+  you can do with this.
+
 Boot support (linear, striped)
 CONFIG_MD_BOOT
   To boot with an initial linear or striped md device you have to
@@ -8142,6 +8164,28 @@
   say M here and read Documentation/modules.txt. The module will be
   called minix.o. Note that the filesystem of your root partition (the
   one containing the directory /) cannot be compiled as a module.
+
+Reiserfs support
+CONFIG_REISERFS_FS
+  New, fast, space saving filesystem, based on a balanced tree algorithm.
+  Uses journaling, and includes a filesystem resizer.
+  You can use reiserfs in all cases where you use the ext2fs file system.
+  It has fewer worst case performance situations than other file systems.
+  It is more easily extended to have features currently found in database
+  and keyword search systems than block allocation based filesystems are.
+  Have fun.
+  
+Enable Reiserfs consistency checks
+CONFIG_REISERFS_CHECK
+  If you set this to yes, then ReiserFS will perform every check it
+  can possibly imagine of its internal consistency throughout its
+  operation.  It will also go substantially slower.  More than once we
+  have forgotten that this was on, and then gone despondent over the
+  latest benchmarks.:-) Use of this option allows our team to go all
+  out in checking for consistency when debugging without fear of its
+  effect on end users.  If you are on the verge of sending in a bug
+  report, say yes and you might get a useful error message.  Almost
+  everyone should say no.
 
 Second extended fs support
 CONFIG_EXT2_FS
diff -urN linux/Documentation/kernel-docs.txt /tmp/linux/Documentation/kernel-docs.txt
--- linux/Documentation/kernel-docs.txt	Wed Jun  7 15:26:42 2000
+++ /tmp/linux/Documentation/kernel-docs.txt	Wed Dec 31 17:00:00 1969
@@ -1,606 +0,0 @@
-
-       Index of Documentation for People Interested in Writing and/or
-                                      
-                      Understanding the Linux Kernel.
-                                      
-              Juan-Mariano de Goyeneche < jmseyas@dit.upm.es>
-                                      
-/*
- * The latest version of this document may be found at:
- *   http://www.dit.upm.es/~jmseyas/linux/kernel/hackers-docs.html
- */
-
-   The need for a document like this one became apparent in the
-   linux-kernel mailing list as the same questions, asking for pointers
-   to information, appeared again and again.
-   
-   Fortunately, as more and more people get to GNU/Linux, more and more
-   get interested in the Kernel. But reading the sources is not always
-   enough. It is easy to understand the code, but miss the concepts, the
-   philosophy and design decisions behind this code.
-   
-   Unfortunately, not many documents are available for beginners to
-   start. And, even if they exist, there was no "well-known" place which
-   kept track of them. These lines try to cover this lack. All documents
-   available on line known by the author are listed, while some reference
-   books are also mentioned.
-   
-   PLEASE, if you know any paper not listed here or write a new document,
-   send me an e-mail, and I'll include a reference to it here. Any
-   corrections, ideas or comments are also welcomed.
-   
-   The papers that follow are listed in no particular order. All are
-   cataloged with the following fields: the document's "Title", the
-   "Author"/s, the "URL" where they can be found, some "Keywords" helpful
-   when searching for specific topics, and a brief "Description" of the
-   Document.
-   
-   Enjoy!
-   
-     ON-LINE DOCS:
-       
-     * Title: "The Linux Kernel"
-       Author: David A. Rusling.
-       URL: http://sunsite.unc.edu/linux/LDP/tlk/tlk.html
-       Keywords: everything!, book.
-       Description: On line, 200 pages book describing most aspects of
-       the Linux Kernel. Probably, the first reference for beginners.
-       Lots of illustrations explaining data structures use and
-       relationships in the purest Richard W. Stevens' style. Contents:
-       "1.-Hardware Basics, 2.-Software Basics, 3.-Memory Management,
-       4.-Processes, 5.-Interprocess Communication Mechanisms, 6.-PCI,
-       7.-Interrupts and Interrupt Handling, 8.-Device Drivers, 9.-The
-       File system, 10.-Networks, 11.-Kernel Mechanisms, 12.-Modules,
-       13.-The Linux Kernel Sources, A.-Linux Data Structures, B.-The
-       Alpha AXP Processor, C.-Useful Web and FTP Sites, D.-The GNU
-       General Public License, Glossary". In short: a must have.
-       
-     * Title: "The Linux Kernel Hackers' Guide"
-       Author: Michael K.Johnson and others.
-       URL: http://khg.redhat.com/HyperNews/get/khg.html
-       Keywords: everything!
-       Description: No more Postscript book-like version. Only HTML now.
-       Many people have contributed. The interface is similar to web
-       available mailing lists archives. You can find some articles and
-       then some mails asking questions about them and/or complementing
-       previous contributions. A little bit anarchic in this aspect, but
-       with some valuable information in some cases.
-       
-     * Title: "Conceptual Architecture of the Linux Kernel"
-       Author: Ivan T. Bowman.
-       URL: http://plg.uwaterloo.ca/~itbowman/papers/CS746G-a1.html
-       Keywords: conceptual software arquitecture, extracted design,
-       reverse engineering, system structure.
-       Description: Conceptual software arquitecture of the Linux kernel,
-       automatically extracted from the source code. Very detailed. Good
-       figures. Gives good overall kernel understanding.
-       
-     * Title: "Concrete Architecture of the Linux Kernel"
-       Author: Ivan T. Bowman, Saheem Siddiqi, and Meyer C. Tanuan.
-       URL: http://plg.uwaterloo.ca/~itbowman/papers/CS746G-a2.html
-       Keywords: concrete arquitecture, extracted design, reverse
-       engineering, system structure, dependencies.
-       Description: Concrete arquitecture of the Linux kernel,
-       automatically extracted from the source code. Very detailed. Good
-       figures. Gives good overall kernel understanding. This papers
-       focus on lower details than its predecessor (files, variables...).
-       
-     * Title: "Linux as a Case Study: Its Extracted Software
-       Architecture"
-       Author: Ivan T. Bowman, Richard C. Holt and Neil V. Brewster.
-       URL: http://plg.uwaterloo.ca/~itbowman/papers/linuxcase.html
-       Keywords: software architecture, architecture recovery,
-       redocumentation.
-       Description: Paper appeared at ICSE'99, Los Angeles, May 16-22,
-       1999. A mixture of the previous two documents from the same
-       author.
-       
-     * Title: "Overview of the Virtual File System"
-       Author: Richard Gooch.
-       URL: http://www.atnf.csiro.au/~rgooch/linux/vfs.txt
-       Keywords: VFS, File System, mounting filesystems, opening files,
-       dentries,
-       dcache. Description: Brief introduction to the Linux Virtual File
-       System. What is it, how it works, operations taken when opening a
-       file or mounting a file system and description of important data
-       structures explaining the purpose of each of their entries.
-       
-     * Title: "The Linux RAID-1, 4, 5 Code"
-       Author: Ingo Molnar, Gadi Oxman and Miguel de Icaza.
-       URL: http://www.ssc.com/lj/issue44/2391.html
-       Keywords: RAID, MD driver.
-       Description: Linux Journal Kernel Korner article. Here is it's
-       abstract: "A description of the implementation of the RAID-1,
-       RAID-4 and RAID-5 personalities of the MD device driver in the
-       Linux kernel, providing users with high performance and reliable,
-       secondary-storage capability using software".
-       
-     * Title: "Dynamic Kernels: Modularized Device Drivers"
-       Author: Alessandro Rubini.
-       URL: http://www.ssc.com/lj/issue23/1219.html
-       Keywords: device driver, module, loading/unloading modules,
-       allocating resources.
-       Description: Linux Journal Kernel Korner article. Here is it's
-       abstract: "This is the first of a series of four articles
-       co-authored by Alessandro Rubini and Georg Zezchwitz which present
-       a practical approach to writing Linux device drivers as kernel
-       loadable modules. This installment presents an introduction to the
-       topic, preparing the reader to understand next month's
-       installment".
-       
-     * Title: "Dynamic Kernels: Discovery"
-       Author: Alessandro Rubini.
-       URL: http://www.ssc.com/lj/issue24/kk24.html
-       Keywords: character driver, init_module, clean_up module,
-       autodetection,
-       mayor number, minor number, file operations, open(), close().
-       Description: Linux Journal Kernel Korner article. Here is it's
-       abstract: "This article, the second of four, introduces part of
-       the actual code to create custom module implementing a character
-       device driver. It describes the code for module initialization and
-       cleanup, as well as the open() and close() system calls".
-       
-     * Title: "The Devil's in the Details"
-       Author: Georg v. Zezschwitz and Alessandro Rubini.
-       URL: http://www.ssc.com/lj/issue25/kk25.html
-       Keywords: read(), write(), select(), ioctl(), blocking/non
-       blocking mode, interrupt handler.
-       Description: Linux Journal Kernel Korner article. Here is it's
-       abstract: "This article, the third of four on writing character
-       device drivers, introduces concepts of reading, writing, and using
-       ioctl-calls".
-       
-     * Title: "Dissecting Interrupts and Browsing DMA"
-       Author: Alessandro Rubini and Georg v. Zezschwitz.
-       URL: http://www.ssc.com/lj/issue26/interrupt.html
-       Keywords: interrupts, irqs, DMA, bottom halves, task queues.
-       Description: Linux Journal Kernel Korner article. Here is it's
-       abstract: "This is the fourth in a series of articles about
-       writing character device drivers as loadable kernel modules. This
-       month, we further investigate the field of interrupt handling.
-       Though it is conceptually simple, practical limitations and
-       constraints make this an ``interesting'' part of device driver
-       writing, and several different facilities have been provided for
-       different situations. We also investigate the complex topic of
-       DMA".
-       
-     * Title: "Device Drivers Concluded"
-       Author: Georg v. Zezschwitz.
-       URL: http://www2.linuxjournal.com/lj-issues/issue28/1287.html
-       Keywords: address spaces, pages, pagination, page management,
-       demand loading, swapping, memory protection, memory mapping, mmap,
-       virtual memory areas (VMAs), vremap, PCI.
-       Description: Finally, the above turned out into a five articles
-       series. This latest one's introduction reads: "This is the last of
-       five articles about character device drivers. In this final
-       section, Georg deals with memory mapping devices, beginning with
-       an overall description of the Linux memory management concepts".
-       
-     * Title: "Network Buffers And Memory Management"
-       Author: Alan Cox.
-       URL: http://www.ssc.com/lj/issue30/kk30.html
-       Keywords: sk_buffs, network devices, protocol/link layer
-       variables, network devices flags, transmit, receive,
-       configuration, multicast.
-       Description: Linux Journal Kernel Korner. Here is the abstract:
-       "Writing a network device driver for Linux is fundamentally
-       simple---most of the complexity (other than talking to the
-       hardware) involves managing network packets in memory".
-       
-     * Title: "An Introduction to the Linux 1.3.x Networking Code"
-       Author: Vipul Gupta.
-       URL: http://anchor.cs.binghamton.edu/courses/cs628/linux-net.html
-       Keywords: files, sk_buffs.
-       Description: A short description of files under the net/
-       directory. Each file has a one or two lines paragraph description.
-       sk_buffs explained, too, with some beautiful pictures. A little
-       bit outdated.
-       
-     * Title: "Linux ioctl() Primer"
-       Author: Vipul Gupta.
-       URL: http://anchor.cs.binghamton.edu/courses/cs628/ioctl.html
-       Keywords: ioctl, socket.
-       Description: Little description and examples on the use and
-       implementation of the ioctl() system call. A little bit biased
-       towards sockets.
-       
-     * Title: "Writing Linux Device Drivers"
-       Author: Michael K. Johnson.
-       URL: http://www.redhat.com/~johnsonm/devices.html
-       Keywords: files, VFS, file operations, kernel interface, character
-       vs block devices, I/O access, hardware interrupts, DMA, access to
-       user memory, memory allocation, timers.
-       Description: Introductory 50-minutes (sic) tutorial on writing
-       device drivers. 12 pages written by the same author of the "Kernel
-       Hackers' Guide" which give a very good overview of the topic.
-       
-     * Title: "The Venus kernel interface"
-       Author: Peter J. Braam.
-       URL:
-       http://www.coda.cs.cmu.edu/doc/html/kernel-venus-protocol.html
-       Keywords: coda, filesystem, venus, cache manager.
-       Description: "This document describes the communication between
-       Venus and kernel level file system code needed for the operation
-       of the Coda filesystem. This version document is meant to describe
-       the current interface (version 1.0) as well as improvements we
-       envisage".
-       
-     * Title: "Programming PCI-Devices under Linux"
-       Author: Claus Schroeter.
-       URL:
-       ftp://ftp.llp.fu-berlin.de/pub/linux/LINUX-LAB/whitepapers/pcip.ps
-       .gz
-       Keywords: PCI, device, busmastering.
-       Description: 6 pages tutorial on PCI programming under Linux.
-       Gives the basic concepts on the architecture of the PCI subsystem,
-       as long as basic functions and macros to read/write the devices
-       and perform busmastering.
-       
-     * Title: "Writing Character Device Driver for Linux"
-       Author: R. Baruch and C. Schroeter.
-       URL:
-       ftp://ftp.llp.fu-berlin.de/pub/linux/LINUX-LAB/whitepapers/drivers
-       .ps.gz
-       Keywords: character device drivers, I/O, signals, DMA, accesing
-       ports in user space, kernel environment.
-       Description: 68 pages paper on writing character drivers. A little
-       bit old (1.993, 1.994) although still useful.
-       
-     * Title: "Design and Implementation of the Second Extended
-       Filesystem"
-       Author: Rémy Card, Theodore Ts'o, Stephen Tweedie.
-       URL: http://web.mit.edu/tytso/www/linux/ext2intro.html
-       Keywords: ext2, linux fs history, inode, directory, link, devices,
-       VFS, physical structure, performance, benchmarks, ext2fs library,
-       ext2fs tools, e2fsck.
-       Description: Paper written by three of the top ext2 hackers.
-       Covers Linux filesystems history, ext2 motivation, ext2 features,
-       design, physical structure on disk, performance, benchmarks,
-       e2fsck's passes description... A must read!
-       Notes: This paper was first published in the Proceedings of the
-       First Dutch International Symposium on Linux, ISBN 90-367-0385-9.
-       
-     * Title: "The Second Extended Filesystem"
-       Author: Matthew Wilcox.
-       URL: http://pocket.fluff.org/~mrw/linux/ext2.txt
-       Keywords: ext2, filesystem.
-       Description: Description of ext2's blocks, directories, inodes...
-       Notes: Seems to be DOWN. Anyone knows another link for it?
-       
-     * Title: "Analysis of the Ext2fs structure"
-       Author: Louis-Dominique Dubeau.
-       URL: http://step.polymtl.ca/~ldd/ext2fs/ext2fs_toc.html
-       Keywords: ext2, filesystem, ext2fs.
-       Description: Description of ext2's blocks, directories, inodes,
-       bitmaps, invariants ...
-       
-     * Title: "Journaling the Linux ext2fs Filesystem"
-       Author: Stephen C. Tweedie.
-       URL:
-       ftp://ftp.uk.linux.org:/pub/linux/sct/fs/jfs/journal-design.ps.gz
-       Keywords: ext3, journalist.
-       Description: Excellent 8-pages paper explaining the journaling
-       capabilities added to ext2 by the author, showing different
-       problems faced and the alternatives chosen.
-       
-     * Title: "Kernel API changes from 2.0 to 2.2"
-       Author: Richard Gooch.
-       URL:
-       http://www.atnf.csiro.au/~rgooch/linux/docs/porting-to-2.2.html
-       Keywords: 2.2, changes.
-       Description: Kernel functions/structures/variables which changed
-       from 2.0.x to 2.2.x.
-       
-     * Title: "Kernel API changes from 2.2 to 2.3"
-       Author: Richard Gooch.
-       URL:
-       http://www.atnf.csiro.au/~rgooch/linux/docs/porting-to-2.3.html
-       Keywords: 2.3, changes.
-       Description: Kernel functions/structures/variables which changed
-       from 2.2.x to 2.3.x.
-       
-     * Title: "Linux Kernel Module Programming Guide"
-       Author: Ori Pomerantz.
-       URL: http://www.linuxdoc.org/LDP/lkmpg/mpg.html
-       Keywords: modules, GPL book, /proc, ioctls, system calls,
-       interrupt handlers .
-       Description: Very nice 92 pages GPL book on the topic of modules
-       programming. Lots of examples.
-       
-     * Title: "Device File System (devfs) Overview"
-       Author: Richard Gooch.
-       URL: http://www.atnf.csiro.au/~rgooch/linux/docs/devfs.txt
-       Keywords: filesystem, /dev, devfs, dynamic devices, major/minor
-       allocation, device management.
-       Description: Document describing Richard Gooch's controversial
-       devfs, which allows for dynamic devices, only shows present
-       devices in /dev, gets rid of major/minor numbers allocation
-       problems, and allows for hundreds of identical devices (which some
-       USB systems might demand soon).
-       
-     * Title: "I/O Event Handling Under Linux"
-       Author: Richard Gooch.
-       URL: http://www.atnf.csiro.au/~rgooch/linux/docs/io-events.html
-       Keywords: IO, I/O, select(2), poll(2), FDs, aio_read(2), readiness
-       event queues.
-       Description: From the Introduction: "I/O Event handling is about
-       how your Operating System allows you to manage a large number of
-       open files (file descriptors in UNIX/POSIX, or FDs) in your
-       application. You want the OS to notify you when FDs become active
-       (have data ready to be read or are ready for writing). Ideally you
-       want a mechanism that is scalable. This means a large number of
-       inactive FDs cost very little in memory and CPU time to manage".
-       
-     * Title: "The Kernel Hacking HOWTO"
-       Author: Various Talented People, and Rusty.
-       URL: http://www.samba.org/~netfilter/kernel-hacking-HOWTO.html
-       Keywords: HOWTO, kernel contexts, deadlock, locking, modules,
-       symbols, return conventions.
-       Description: From the Introduction: "Please understand that I
-       never wanted to write this document, being grossly underqualified,
-       but I always wanted to read it, and this was the only way. I
-       simply explain some best practices, and give reading entry-points
-       into the kernel sources. I avoid implementation details: that's
-       what the code is for, and I ignore whole tracts of useful
-       routines. This document assumes familiarity with C, and an
-       understanding of what the kernel is, and how it is used. It was
-       originally written for the 2.3 kernels, but nearly all of it
-       applies to 2.2 too; 2.0 is slightly different. ".
-       
-     * Title: "ALSA 0.5.0 Developer documentation"
-       Author: Stephan 'Jumpy' Bartels .
-       URL: http://www.math.TU-Berlin.de/~sbartels/alsa/
-       Keywords: ALSA, sound, soundcard, driver, lowlevel, hardware.
-       Description: Advanced Linux Sound Architecture for developers,
-       both at kernel and user-level sides. Work in progress. ALSA is
-       supposed to be Linux's next generation sound architecture.
-       
-     * Title: "Programming Guide for Linux USB Device Drivers"
-       Author: Detlef Fliegl.
-       URL: http://usb.in.tum.de/usbdoc/
-       Keywords: USB, universal serial bus.
-       Description: A must-read. From the Preface: "This document should
-       give detailed information about the current state of the USB
-       subsystem and its API for USB device drivers. The first section
-       will deal with the basics of USB devices. You will learn about
-       different types of devices and their properties. Going into detail
-       you will see how USB devices communicate on the bus. The second
-       section gives an overview of the Linux USB subsystem [2] and the
-       device driver framework. Then the API and its data structures will
-       be explained step by step. The last section of this document
-       contains a reference of all API calls and their return codes".
-       Notes: Beware: the main page states: "This document may not be
-       published, printed or used in excerpts without explicit permission
-       of the author". Fortunately, it may still be read...
-       
-     * Title: "Tour Of the Linux Kernel Source"
-       Author: Vijo Cherian.
-       URL: http://www.geocities.com/vijoc/tolks/tolks.html
-       Keywords: .
-       Description: A classic of this page! Was lost for a while and is
-       back again. Thanks Vijo! TOLKS: the name says it all. A tour of
-       the sources, describing directories, files, variables, data
-       structures... It covers general stuff, device drivers,
-       filesystems, IPC and Networking Code.
-       
-     * Title: "Linux Kernel Mailing List Glossary"
-       Author: John Levon.
-       URL: http://www.movement.uklinux.net/glossary.html
-       Keywords: glossary, terms, linux-kernel.
-       Description: From the introduction: "This glossary is intended as
-       a brief description of some of the acronyms and terms you may hear
-       during discussion of the Linux kernel".
-       
-     * Title: "Linux Kernel Locking HOWTO"
-       Author: Various Talented People, and Rusty.
-       URL:
-       http://netfilter.kernelnotes.org/unreliable-guides/kernel-locking-
-       HOWTO.html
-       Keywords: locks, locking, spinlock, semaphore, atomic, race
-       condition, bottom halves, tasklets, softirqs.
-       Description: The title says it all: document describing the
-       locking system in the Linux Kernel either in uniprocessor or SMP
-       systems.
-       Notes: "It was originally written for the later (>2.3.47) 2.3
-       kernels, but most of it applies to 2.2 too; 2.0 is slightly
-       different". Freely redistributable under the conditions of the GNU
-       General Public License.
-       
-     BOOKS: (Not on-line)
-   
-     * Title: "Linux Device Drivers"
-       Author: Alessandro Rubini.
-       Publisher: O'Reilly &Associates.
-       Date: 1998.
-       ISBN: 1-56592-292-1
-       
-     * Title: "Linux Kernel Internals"
-       Author: Michael Beck.
-       Publisher: Addison-Wesley.
-       Date: 1997.
-       ISBN: 0-201-33143-8 (second edition)
-       
-     * Title: "The Design of the UNIX Operating System"
-       Author: Maurice J. Bach.
-       Publisher: Prentice Hall.
-       Date: 1986.
-       Pages: 471.
-       ISBN: 0-13-201757-1
-       
-     * Title: "The Design and Implementation of the 4.3 BSD UNIX
-       Operating System"
-       Author: Samuel J. Leffler, Marshall Kirk McKusick, Michael J.
-       Karels, John S. Quarterman.
-       Publisher: Addison-Wesley.
-       Date: 1989 (reprinted with corrections on October, 1990).
-       ISBN: 0-201-06196-1
-       
-     * Title: "The Design and Implementation of the 4.4 BSD UNIX
-       Operating System"
-       Author: Marshall Kirk McKusick, Keith Bostic, Michael J. Karels,
-       John S. Quarterman.
-       Publisher: Addison-Wesley.
-       Date: 1996.
-       ISBN: 0-201-54979-4
-       
-     * Title: "Programmation Linux 2.0 API systeme et fonctionnement du
-       noyau"
-       Author: Remy Card, Eric Dumas, Franck Mevel.
-       Publisher: Eyrolles.
-       Date: 1997.
-       Pages: 520.
-       ISBN: 2-212-08932-5
-       Notes: French.
-       
-     * Title: "The Linux Kernel Book"
-       Author: Remy Card, Eric Dumas, Franck Mevel.
-       Publisher: John Wiley & Sons.
-       Date: 1998.
-       ISBN: 0-471-98141-9
-       Notes: English translation.
-       
-     * Title: "Linux 2.0"
-       Author: Remy Card, Eric Dumas, Franck Mevel.
-       Publisher: Gestión 2000.
-       Date: 1997.
-       Pages: 501.
-       ISBN: 8-480-88208-5
-       Notes: Spanish translation.
-       
-     * Title: "Unix internals -- the new frontiers"
-       Author: Uresh Vahalia.
-       Publisher: Prentice Hall.
-       Date: 1996.
-       Pages: 600.
-       ISBN: 0-13-101908-2
-       
-     * Title: "Linux Core Kernel Commentary. Guide to Insider's Knowledge
-       on the Core Kernel od the Linux Code"
-       Author: Scott Maxwell.
-       Publisher: Coriolis.
-       Date: 1999.
-       Pages: 592.
-       ISBN: 1-57610-469-9
-       Notes: CD-ROM included. Line by line commentary of the kernel
-       code.
-       
-     * Title: "Linux IP Stacks Commentary"
-       Author: Stephen Satchell and HBJ Clifford.
-       Publisher: Coriolis.
-       Date: 2000.
-       Pages: ???.
-       ISBN: 1-57610-470-2
-       Notes: Line by line source code commentary book.
-       
-     * Title: "Programming for the real world - POSIX.4"
-       Author: Bill O. Gallmeister.
-       Publisher: O'Reilly & Associates, Inc..
-       Date: 1995.
-       Pages: ???.
-       ISBN: I-56592-074-0
-       Notes: Though not being directly about Linux, Linux aims to be
-       POSIX. Good reference.
-       
-     MISCELLANEOUS:
-   
-     * Name: "Linux Source Driver"
-       URL: http://lsd.linux.cz
-       Keywords: Browsing source code.
-       Description: "Linux Source Driver (LSD) is an application, which
-       can make browsing source codes of Linux kernel easier than you can
-       imagine. You can select between multiple versions of kernel (e.g.
-       0.01, 1.0.0, 2.0.33, 2.0.34pre13, 2.0.0, 2.1.101 etc.). With LSD
-       you can search Linux kernel (fulltext, macros, types, functions
-       and variables) and LSD can generate patches for you on the fly
-       (files, directories or kernel)".
-       
-     * Name: "Cross-Referencing Linux"
-       URL: http://lxr.linux.no/source/
-       Keywords: Browsing source code.
-       Description: Another web-based Linux kernel source code browser.
-       Lots of cross references to variables and functions. You can see
-       where they are defined and where they are used.
-       
-     * Name: "Linux Weekly News"
-       URL: http://lwn.net
-       Keywords: latest kernel news.
-       Description: The title says it all. There's a fixed kernel section
-       summarizing developers' work, bug fixes, new features and versions
-       produced during the week. Published every Thursday.
-       
-     * Name: "Kernel Traffic"
-       URL: http://kt.linuxcare.com
-       Keywords: linux-kernel mailing list, weekly kernel news.
-       Description: Weekly newsletter covering the most relevant
-       discussions of the linux-kernel mailing list.
-       
-     * Name: "CuTTiNG.eDGe.LiNuX"
-       URL: http://edge.kernelnotes.org
-       Keywords: changelist.
-       Description: Site which provides the changelist for every kernel
-       release. What's new, what's better, what's changed. Myrdraal reads
-       the patches and describes them. Pointers to the patches are there,
-       too.
-       
-     * Name: "New linux-kernel Mailing List FAQ"
-       URL: Original site:
-       http://www.altern.org/andrebalsa/doc/lkml-faq.html
-       URL: U.S. mirror site:
-       http://www.ececs.uc.edu/~rreilova/linux/lkml-faq.html
-       Keywords: linux-kernel mailing list FAQ.
-       Description: linux-kernel is a mailing list for developers to
-       communicate. This FAQ builds on the previous linux-kernel mailing
-       list FAQ maintained by Frohwalt Egerer, who no longer maintains
-       it. Read it to see how to join the mailing list. Dozens of
-       interesting questions regarding the list, Linux, developers (who
-       is ...?), terms (what is...?) are answered here too. Just read it.
-       
-     * Name: "Linux Virtual File System"
-       Author: Peter J. Braam.
-       URL: http://www.coda.cs.cmu.edu/doc/talks/linuxvfs
-       Keywords: slides, VFS, inode, superblock, dentry, dcache.
-       Description: Set of slides, presumably from a presentation on the
-       Linux VFS layer. Covers version 2.1.x, with dentries and the
-       dcache.
-       
-     * Name: "Gary's Enciclopedia - The Linux Kernel"
-       Author: Gary (I suppose...).
-       URL: http://members.aa.net/~swear/pedia/kernel.html
-       Keywords: links, not found here?.
-       Description: Gary's Enciclopedia exists to allow the rapid finding
-       of documentation and other information of interest to GNU/Linux
-       users. It has about 4000 links to external pages in 150 major
-       categories. This link is for kernel-specific links, documents,
-       sites... Look there if you could not find here whar you were
-       looking for.
-       
-     * Name: "The home page of Linux-MM"
-       Author: The Linux-MM team.
-       URL: http://www.linux.eu.org/Linux-MM/
-       Keywords: memory management, Linux-MM, mm patches, TODO, docs,
-       mailing list.
-       Description: Site devoted to Linux Memory Mangement development.
-       Memory related patches, HOWTOs, links, mm developers... Don't miss
-       it if you are interested in memory management development!
-       
-     * Name: "Kernel Newbies IRC Channel"
-       URL: http://www.surriel.com/kernelnewbies.shtml
-       Keywords: IRC, newbies, channel, asking doubts.
-       Description: #kernelnewbies on irc.openprojects.net. From the web
-       page: "#kernelnewbies is an IRC network dedicated to the 'newbie'
-       kernel hacker. The audience mostly consists of people who are
-       learning about the kernel, working on kernel projects or
-       professional kernel hackers that want to help less seasoned kernel
-       people. [...] #kernelnewbies is on the Open Projects IRC Network,
-       try irc.openprojects.net or irc.<country>.openprojects.net as your
-       server and then /join #kernelnewbies".
-       
-     * Name: "linux-kernel mailing list archives and search engines"
-       URL: http://www.uwsg.indiana.edu/hypermail/linux/kernel/index.html
-       URL: http://www.kernelnotes.org/lnxlists/linux-kernel/
-       Keywords: linux-kernel, archives, search.
-       Description: Some of the linux-kernel mailing list archivers. If
-       you have a better/another one, please let me know.
-     _________________________________________________________________
-   
-   Document last updated on Mon Apr 17 18:07:07 CEST 2000
diff -urN linux/Documentation/proc.txt /tmp/linux/Documentation/proc.txt
--- linux/Documentation/proc.txt	Mon Sep  4 11:39:16 2000
+++ /tmp/linux/Documentation/proc.txt	Wed Dec 31 17:00:00 1969
@@ -1,1370 +0,0 @@
-
------------------------------------------------------------------------
-
-                T H E  /proc   F I L E S Y S T E M
-
------------------------------------------------------------------------
-/proc/sys      Terrehon Bowden <terrehon@wpi.com>       January 27 1999
-               Bodo Bauer <bb@ricochet.net>                
------------------------------------------------------------------------
-Version 1.1                                         Kernel version 2.2
------------------------------------------------------------------------
-Contents
-
-1   Introduction/Credits
-
-1.1  Legal Issues
-
-2   The /proc file system
-
-2.1  Process specific subdirectories
-2.2  Kernel data
-2.3  IDE devices in /proc/ide
-2.4  Networking info in /proc/net
-2.5  SCSI info
-2.6  Parallel port info in /proc/parport
-2.7  TTY info in /proc/tty
-
-3   Reading and modifying kernel parameters
-
-3.1  /proc/sys/debug and /proc/sys/proc
-3.2  /proc/fs - File system data
-3.3  /proc/fs/binfmt_misc - Miscellaneous binary formats
-3.4  /proc/sys/kernel - General kernel parameters
-3.5  /proc/sys/vm - The virtual memory subsystem
-3.6  /proc/sys/dev - Device specific parameters
-3.7  /proc/sys/sunrpc - Remote procedure calls
-3.8  /proc/sys/net - Networking stuff
-3.9  /proc/sys/net/ipv4 - IPV4 settings=20
-3.10 Appletalk
-3.11 IPX
-
------------------------------------------------------------------------
-
-1   Introduction/Credits
-
-This documentation is part of a soon to be released book published by
-IDG Books on the SuSE Linux distribution. As there is no complete
-documentation for the /proc file system and we've used many freely
-available sources to write this chapter, it seems only fair to give
-the work back to the Linux community. This work is based on the
-2.1.132 and 2.2.0-pre-kernel versions. I'm afraid it's still far from
-complete, but we hope it will be useful. As far as we know, it is the
-first 'all-in-one' document about the /proc file system. It is
-focused on the Intel x86 hardware, so if you are looking for PPC, ARM,
-SPARC, APX, etc., features, you probably won't find what you are
-looking for. It also only covers IPv4 networking, not IPv6 nor other
-protocols - sorry.
-
-We'd like to thank Alan Cox, Rik van Riel, and Alexey Kuznetsov.  We'd
-also like to extend a special thank you to Andi Kleen for
-documentation, which we relied on heavily to create this document, as
-well as the additional information he provided. Thanks to everybody
-else who contributed source or docs to the Linux kernel and helped
-create a great piece of software... :)
-
-If you have any comments, corrections or additions, please don't
-hesitate to contact Bodo Bauer at bb@ricochet.net. We'll be happy to
-add them to this document.
-
-The latest version of this document is available online at
-http://www.suse.com/~bb/Docs/proc.html in HTML, ASCII, and as 
-Postscript file. 
-
-1.1  Legal Stuff
-
-We don't guarantee the correctness of this document, and if you come
-to us complaining about how you screwed up your system because of
-incorrect documentation, we won't feel responsible...
-
------------------------------------------------------------------------
-
-2   The /proc file system
-
-The proc file system acts as an interface to internal data structures
-in the kernel. It can be used to obtain information about the system
-and to change certain kernel parameters at runtime.  It contains
-(among other things) one subdirectory for each process running on the
-system which is named after the process id (PID) of the process. The
-link self points to the process reading the file system.
-
-2.1  Process specific subdirectories
-
-Each process subdirectory has the in table 1.1 listed entries.
-
-      _________________________________________________
-      cmdline Command line arguments
-      environ Values of environment variables
-      fd    Directory, which contains all file descriptors
-      mem   Memory held by this process
-      stat   Process status
-      status  Process status in human readable form
-      cwd   Link to the current working directory
-      exe   Link to the executable of this process
-      maps   Memory maps
-      root   Link to the root directory of this process
-      statm  Process memory status information
-     _________________________________________________
-      Table 1.1: Process specific entries in /proc
-
-For example, to get the status information of a process, all you have
-to do is read the file /proc/PID/status:
-
-> cat /proc/self/status
-Name:   cat
-State:  R (running)
-Pid:    5633
-PPid:   5609
-Uid:    501     501     501     501
-Gid:    100     100     100     100
-Groups: 100 16 
-VmSize:      804 kB
-VmLck:         0 kB
-VmRSS:       344 kB
-VmData:       68 kB
-VmStk:        20 kB
-VmExe:        12 kB
-VmLib:       660 kB
-SigPnd: 0000000000000000
-SigBlk: 0000000000000000
-SigIgn: 0000000000000000
-SigCgt: 0000000000000000
-CapInh: 00000000fffffeff
-CapPrm: 0000000000000000
-CapEff: 0000000000000000
-
-This shows you almost the same information as you would get if you
-viewed it with the ps command. In fact, ps uses the proc file system
-to obtain its information.
-
-The statm file contains more detailed information about the process
-memory usage. It contains seven values with the following meanings:
-
-size       total program size
-resident   size of in memory portions
-shared     number of the pages that are shared
-trs        number of pages that are 'code'
-drs        number of pages of data/stack
-lrs        number of pages of library
-dt	   number of dirty pages
-
-The ratio text/data/library is approximate only by heuristics.
-
-2.2  Kernel data
-
-Similar to the process entries, these are files which give information
-about the running kernel. The files used to obtain this information
-are contained in /proc and are listed in table 1.2. Not all of these
-will be present in your system. It depends on the kernel configuration
-and the loaded modules, which files are there, and which are missing.
-
-      ________________________________________________
-      apm           Advanced power management info
-      cmdline       Kernel command line
-      cpuinfo       Info about the CPU
-      devices       Available devices (block and character)
-      dma           Used DMS channels
-      filesystems   Supported filesystems
-      interrupts    Interrupt usage
-      ioports       I/O port usage
-      kcore         Kernel core image
-      kmsg          Kernel messages
-      ksyms         Kernel symbol table
-      loadavg       Load average
-      locks         Kernel locks
-      meminfo       Memory info
-      misc          Miscellaneous
-      modules       List of loaded modules
-      mounts        Mounted filesystems
-      partitions    Table of partitions known to the system
-      rtc           Real time clock
-      slabinfo      Slab pool info
-      stat          Overall statistics
-      swaps         Swap space utilization
-      uptime        System uptime
-      version       Kernel version
-      ________________________________________________
-           Table 1.2: Kernel info in /proc
-
-You can, for example, check which interrupts are currently in use and
-what they are used for by looking in the file /proc/interrupts:
-
-> cat /proc/interrupts
-           CPU0       
-  0:    8728810          XT-PIC  timer
-  1:        895          XT-PIC  keyboard
-  2:          0          XT-PIC  cascade
-  3:     531695          XT-PIC  aha152x
-  4:    2014133          XT-PIC  serial
-  5:      44401          XT-PIC  pcnet_cs
-  8:          2          XT-PIC  rtc
- 11:          8          XT-PIC  i82365
- 12:     182918          XT-PIC  PS/2 Mouse
- 13:          1          XT-PIC  fpu
- 14:    1232265          XT-PIC  ide0
- 15:          7          XT-PIC  ide1
-NMI:          0
-
-There three more important subdirectories in /proc: net, scsi and
-sys. The general rule is that the contents, or even the existence of
-these directories, depends on your kernel configuration. If SCSI is
-not enabled, the directory scsi may not exist. The same is true with
-the net, which is only there when networking support is present in the
-running kernel.
-
-The slabinfo file gives information about memory usage on the slab
-level.  Linux uses slab pools for memory management above page level
-in version 2.2. Commonly used objects have their own slab pool (like
-network buffers, directory cache, etc.).
-
-2.3  IDE devices in /proc/ide
-
-This subdirectory contains information about all IDE devices that the
-kernel is aware of.  There is one subdirectory for each device
-(i.e. hard disk) containing the following files:
-
-       cache             The cache
-       capacity          Capacity of the medium
-       driver            Driver and version
-       geometry          Physical and logical geometry
-       identify          Device identify block
-       media             Media type
-       model             Device identifier
-       settings          Device setup
-       smart_thresholds  IDE disk management thresholds
-       smart_values      IDE disk management values
-
-2.4  Networking info in /proc/net
-
-This directory follows the usual pattern. Table 1.3 lists the files
-and their meaning.
-
-     ____________________________________________________
-     arp             Kernel ARP table
-     dev             network devices with statistics
-     dev_mcast       Lists the Layer2 multicast groups a
-                     device is listening to (interface index,
-                     label, number of references, number of
-                     bound addresses).
-     dev_stat        network device status
-     ip_fwchains     Firewall chain linkage
-     ip_fwnames      Firewall chains
-     ip_masq         Directory containing the masquerading
-                     tables.
-     ip_masquerade   Major masquerading table
-     netstat         Network statistics
-     raw             Raw device statistics
-     route           Kernel routing table
-     rpc             Directory containing rpc info
-     rt_cache        Routing cache
-     snmp            SNMP data
-     sockstat        Socket statistics
-     tcp             TCP sockets
-     tr_rif          Token ring RIF routing table
-     udp             UDP sockets
-     unix            UNIX domain sockets
-     wireless        Wireless interface data (Wavelan etc)
-     igmp            IP multicast addresses, which this host joined
-     psched          Global packet scheduler parameters.
-     netlink         List of PF_NETLINK sockets.
-     ip_mr_vifs      List of multicast virtual interfaces.
-     ip_mr_cache     List of multicast routing cache.
-     udp6            UDP sockets (IPv6)
-     tcp6            TCP sockets (IPv6)
-     raw6            Raw device statistics (IPv6)
-     igmp6	     IP multicast addresses, which this host joineed (IPv6)
-     if_inet6        List of IPv6 interface addresses.
-     ipv6_route      Kernel routing table for IPv6
-     rt6_stats       global IPv6 routing tables statistics.
-     sockstat6       Socket statistics (IPv6)
-     snmp6           Snmp data (IPv6)
-     ____________________________________________________
-         Table 1.3: Network info in /proc/net
-
-You can use this information to see which network devices are
-available in your system and how much traffic was routed over those
-devices:
-
-> cat /proc/net/dev
-Inter-|Receive                                                   |[...
- face |bytes    packets errs drop fifo frame compressed multicast|[...
-    lo:  908188   5596     0    0    0     0          0         0 [...        
-  ppp0:15475140  20721   410    0    0   410          0         0 [... 
-  eth0:  614530   7085     0    0    0     0          0         1 [...
-
-...] Transmit
-...] bytes    packets errs drop fifo colls carrier compressed
-...]  908188     5596    0    0    0     0       0          0
-...] 1375103    17405    0    0    0     0       0          0
-...] 1703981     5535    0    0    0     3       0          0
-
-2.5  SCSI info
-
-If you have a SCSI host adapter in your system, you'll find a
-subdirectory named after the driver for this adapter in /proc/scsi. 
-You'll also see a list of all recognized SCSI devices in /proc/scsi:
-
->cat /proc/scsi/scsi
-Attached devices: 
-Host: scsi0 Channel: 00 Id: 00 Lun: 00
-  Vendor: QUANTUM  Model: XP34550W         Rev: LXY4
-  Type:   Direct-Access                    ANSI SCSI revision: 02
-Host: scsi0 Channel: 00 Id: 01 Lun: 00
-  Vendor: SEAGATE  Model: ST34501W         Rev: 0018
-  Type:   Direct-Access                    ANSI SCSI revision: 02
-Host: scsi0 Channel: 00 Id: 02 Lun: 00
-  Vendor: SEAGATE  Model: ST34501W         Rev: 0017
-  Type:   Direct-Access                    ANSI SCSI revision: 02
-Host: scsi0 Channel: 00 Id: 04 Lun: 00
-  Vendor: ARCHIVE  Model: Python 04106-XXX Rev: 703b
-  Type:   Sequential-Access                ANSI SCSI revision: 02
-
-The directory named after the driver has one file for each adapter
-found in the system. These files contain information about
-the controller, including the used IRQ and the IO address range:
-
->cat /proc/scsi/ncr53c8xx/0
-General information:
- Chip NCR53C875, device id 0xf, revision id 0x4
- IO port address 0xec00, IRQ number 11
- Synchronous period factor 12, max commands per lun 4
-
-2.6  Parallel port info in /proc/parport
-
-The directory /proc/parport contains information about the parallel
-ports of your system. It has one subdirectory for each port, named
-after the port number (0,1,2,...).
-
-This directory contains four files:
-
-    autoprobe   Autoprobe results of this port
-    devices     Connected device modules
-    hardware    Hardware info (port type, io-port, DMA, IRQ, etc.)
-    irq         Used interrupt, if any
-
-2.7  TTY info in /proc/tty
-
-Information about the available and the actually used tty's can be
-found in /proc/tty. You'll find entries for drivers and line
-disciplines in this directory, as shown in the table below:
-
-     drivers       List of drivers and their usage
-     ldiscs        Registered line disciplines
-     driver/serial Usage statistic and status of single tty lines
-
-To see which tty's are currently in use, you can simply look into the
-file /proc/tty/drivers:
-
->cat /proc/tty/drivers
-pty_slave            /dev/pts      136   0-255 pty:slave
-pty_master           /dev/ptm      128   0-255 pty:master
-pty_slave            /dev/ttyp       3   0-255 pty:slave
-pty_master           /dev/pty        2   0-255 pty:master
-serial               /dev/cua        5   64-67 serial:callout
-serial               /dev/ttyS       4   64-67 serial
-/dev/tty0            /dev/tty0       4       0 system:vtmaster
-/dev/ptmx            /dev/ptmx       5       2 system
-/dev/console         /dev/console    5       1 system:console
-/dev/tty             /dev/tty        5       0 system:/dev/tty
-unknown              /dev/tty        4    1-63 console
-
------------------------------------------------------------------------
-
-3   Reading and modifying kernel parameters
-
-A very interesting part of /proc is the directory /proc/sys. This not
-only provides information, it also allows you to change parameters
-within the kernel. Be very careful when trying this. You can optimize
-your system, but you also can crash it. Never play around with kernel
-parameters on a production system. Set up a development machine and
-test to make sure that everything works the way you want it to.  You
-may have no alternative but to reboot the machine once an error has
-been made.
-
-To change a value, simply echo the new value into the file. An example
-is given below in the section on the file system data. You need to be
-root to do this. You can create your own boot script to get this done
-every time your system boots. 
-
-The files in /proc/sys can be used to tune and monitor miscellaneous
-and general things in the operation of the Linux kernel.  Since some
-of the files can inadvertently disrupt your system, it is advisable to
-read both documentation and source before actually making
-adjustments. In any case, be very careful when writing to any of these
-files. The entries in /proc may change slightly between the 2.1.* and
-the 2.2 kernel, so review the kernel documentation if there is any
-doubt. You'll find the documentation in the directory
-/usr/src/linux/Documentation/sys. This chapter is heavily based on the
-documentation included in the pre 2.2 kernels. Thanks to Rick van Riel
-for providing this information.
-
-3.1  /proc/sys/debug and /proc/sys/proc
-
-These two subdirectories are empty.
-
-3.2  /proc/fs - File system data
-
-This subdirectory contains specific file system, file handle, inode,
-dentry and quota information.
-
-Currently, these files are in /proc/sys/fs:
-
-dentry-state
-   Status of the directory cache. Since directory entries are
-   dynamically allocated and deallocated, this file gives information
-   about the current status. It holds six values, in which the last
-   two are not used and are always zero. The other four mean:
-
-       nr_dentry   Seems to be zero all the time
-       nr_unused   Number of unused cache entries
-       age_limit   Age in seconds after the entry may be
-                   reclaimed, when memory is short
-       want_pages  internal
-
-dquot-nr and dquot-max
-   The file dquot-max shows the maximum number of cached disk quota
-   entries.
-
-   The file dquot-nr shows the number of allocated disk quota
-   entries and the number of free disk quota entries.
-
-   If the number of free cached disk quotas is very low and you have
-   a large number of simultaneous system users, you might want
-   to raise the limit.
-
-file-nr and file-max
-   The kernel allocates file handles dynamically, but as yet
-   doesn't free them again.
-
-   The value in file-max denotes the maximum number of file handles
-   that the Linux kernel will allocate. When you get a lot of error
-   messages about running out of file handles, you might want to raise
-   this limit. The default value is 4096. To change it, just write the
-   new number into the file:
-
-   # cat /proc/sys/fs/file-max
-   4096
-   # echo 8192 > /proc/sys/fs/file-max
-   # cat /proc/sys/fs/file-max
-   8192
-
-   This method of revision is useful for all customizable parameters
-   of the kernel - simply echo the new value to the corresponding
-   file.
-  
-   The three values in file-nr denote the number of allocated file
-   handles, the number of used file handles, and the maximum number of
-   file handles. When the allocated file handles come close to the
-   maximum, but the number of actually used ones is far behind, you've
-   encountered a peak in your usage of file handles and you don't need
-   to increase the maximum.
-
-inode-state, inode-nr and inode-max
-   As with file handles, the kernel allocates the inode structures
-   dynamically, but can't free them yet.
-
-   The value in inode-max denotes the maximum number of inode
-   handlers. This value should be 3 to 4 times larger than the value
-   in file-max, since stdin, stdout, and network sockets also need an
-   inode struct to handle them. If you regularly run out of inodes,
-   you should increase this value.
-
-   The file inode-nr contains the first two items from inode-state, so
-   we'll skip to that file...
-
-   inode-state contains three actual numbers and four dummy values. The 
-   actual numbers are (in order of appearance) nr_inodes, nr_free_inodes,
-   and preshrink.
-
-   nr_inodes 
-     Denotes the number of inodes the system has allocated. This can
-     be slightly more than inode-max because Linux allocates them one
-     pageful at a time.
-
-   nr_free_inodes 
-     Represents the number of free inodes and pre shrink is nonzero
-     when the nr_inodes > inode-max and the system needs to prune the
-     inode list instead of allocating more.
-
-super-nr and super-max
-   Again, super block structures are allocated by the kernel,
-   but not freed. The file super-max contains the maximum number of
-   super block handlers, where super-nr shows the number of
-   currently allocated ones.
-
-   Every mounted file system needs a super block, so if you plan to
-   mount lots of file systems, you may want to increase these
-   numbers.
-
-3.3  /proc/fs/binfmt_misc - Miscellaneous binary formats
-
-Besides these files, there is the subdirectory
-/proc/sys/fs/binfmt_misc. This handles the kernel support for
-miscellaneous binary formats.
-
-Binfmt_misc provides the ability to register additional binary formats
-to the Kernel without compiling an additional module/kernel. Therefore
-binfmt_misc needs to know magic numbers at the beginning or the
-filename extension of the binary.
-
-It works by maintaining a linked list of structs, that contain a
-description of a binary format, including a magic with size (or the
-filename extension), offset and mask, and the interpreter name. On
-request it invokes the given interpreter with the original program as
-argument, as binfmt_java and binfmt_em86 and binfmt_mz do.
-Since binfmt_misc does not define any default binary-formats, you have to
-register an additional binary-format.
-
-There are two general files in binfmt_misc and one file per registered
-format. The two general files are register and status.
-
-Registering a new binary format
-
-echo :name:type:offset:magic:mask:interpreter: > /proc/sys/fs/binfmt_misc/register 
-
-with appropriate name (the name for the /proc-dir entry), offset
-(defaults to 0, if omitted), magic and mask (which can be omitted,
-defaults to all 0xff) and last but not least, the interpreter that is
-to be invoked (for example and testing '/bin/echo'). Type can be M for
-usual magic matching or E for filename extension matching (give
-extension in place of magic).
-
-To check or reset the status of the binary format handler:
-
-If you do a cat on the file /proc/sys/fs/binfmt_misc/status, you will
-get the current status (enabled/disabled) of binfmt_misc. Change the
-status by echoing 0 (disables) or 1 (enables) or -1 (caution: this
-clears all previously registered binary formats) to status. For
-example echo 0 > status to disable binfmt_misc (temporarily).
-
-Status of a single handler
-
-Each registered handler has an entry in /proc/sys/fs/binfmt_misc.
-These files perform the same function as status, but their scope is
-limited to the actual binary format. By cating this file, you also
-receive all related information about the interpreter/magic of the
-binfmt.
-
-Example usage of binfmt_misc (emulate binfmt_java)
-
-cd /proc/sys/fs/binfmt_misc
-echo ':Java:M::\xca\xfe\xba\xbe::/usr/local/java/bin/javawrapper:' > register
-echo ':HTML:E::html::/usr/local/java/bin/appletviewer:' > register
-echo ':Applet:M::<!--applet::/usr/local/java/bin/appletviewer:' > register
-echo ':DEXE:M::\x0eDEX::/usr/bin/dosexec:' > register
-
-These three lines add support for Java executables and Java applets
-(like binfmt_java, additionally recognizing the .html extension with
-no need to put <!--applet> to every applet file). You have to install
-the JDK and the shell-script /usr/local/java/bin/javawrapper too. It
-works around the brokenness of the Java filename handling. To add a
-Java binary, just create a link to the class-file somewhere in the
-path.
-
-3.4  /proc/sys/kernel - general kernel parameters
-
-This directory reflects general kernel behaviors. As I've said before,
-the contents are depend on your configuration. I'll list the most
-important files, along with descriptions of what they mean and how to
-use them.
-
-acct
-   The file contains three values; highwater, lowwater, and
-   frequency.
-
-   It exists only when BSD-style process accounting is enabled. These
-   values control its behavior. If the free space on the file system
-   where the log lives goes below lowwater%, accounting suspends. If
-   it goes above highwater%, accounting resumes. Frequency determines
-   how often you check the amount of free space (value is in
-   seconds). Default settings are: 4, 2, and 30. That is, suspend
-   accounting if there left <= 2% free; resume it if we have a value
-   >=3%; consider information about the amount of free space valid
-   for 30 seconds
-
-ctrl-alt-del
-   When the value in this file is 0, ctrl-alt-del is trapped and sent
-   to the init(1) program to handle a graceful restart. However, when
-   the value is > 0, Linux's reaction to this key combination will be
-   an immediate reboot, without syncing its dirty buffers.
-
-   Note: when a program (like dosemu) has the keyboard in raw mode,
-   the ctrl-alt-del is intercepted by the program before it ever
-   reaches the kernel tty layer, and it is up to the program to decide
-   what to do with it.
-
-domainname and hostname
-   These files can be controlled to set the NIS domainname and
-   hostname of your box. For the classic darkstar.frop.org a simple:
-
-   # echo "darkstar" > /proc/sys/kernel/hostname
-   # echo "frop.org" > /proc/sys/kernel/domainname
-
-   would suffice to set your hostname and NIS domainname.
-
-osrelease, ostype and version
-
-   The names make it pretty obvious what these fields contain:
-
-   >cat /proc/sys/kernel/osrelease
-   2.1.131
-   >cat /proc/sys/kernel/ostype
-   Linux
-   >cat /proc/sys/kernel/version
-   #8 Mon Jan 25 19:45:02 PST 1999
- 
-   The files osrelease and ostype should be clear enough. Version
-   needs a little more clarification however. The #8 means that this
-   is the 8th kernel built from this source base and the date behind
-   it indicates the time the kernel was built. The only way to tune
-   these values is to rebuild the kernel.
-
-panic 
-   The value in this file represents the number of seconds the kernel
-   waits before rebooting on a panic. When you use the software
-   watchdog, the recommended setting is 60. If set to 0, the auto
-   reboot after a kernel panic is disabled, this is the default
-   setting.
-
-printk
-   The four values in printk denote console_loglevel,
-   default_message_loglevel, minimum_console_level, and
-   default_console_loglevel respectively.
-
-   These values influence printk() behavior when printing or logging
-   error messages, which come from inside the kernel. See syslog(2)
-   for more information on the different log levels.
-
-   console_loglevel
-     Messages with a higher priority than this will be printed to
-     the console.
-
-   default_message_level
-     Messages without an explicit priority will be printed with
-     this priority.
-
-   minimum_console_loglevel
-     Minimum (highest) value to which the console_loglevel can be set.
-
-   default_console_loglevel
-     Default value for console_loglevel.
-
-sg-big-buff
-   This file shows the size of the generic SCSI (sg) buffer. At this
-   point, you can't tune it yet, but you can change it at compile time
-   by editing include/scsi/sg.h and changing the value of
-   SG_BIG_BUFF.
-
-   If you use a scanner with SANE (Scanner Access now easy) you
-   might want to set this to a higher value. Look into the SANE
-   documentation on this issue.
-
-modprobe
-   The location where the modprobe binary is located. The kernel
-   uses this program to load modules on demand.
-
-3.5  /proc/sys/vm - The virtual memory subsystem
-
-The files in this directory can be used to tune the operation of the
-virtual memory (VM) subsystem of the Linux kernel. In addition, one of
-the files (bdflush) has a little influence on disk usage.
-
-bdflush
-   This file controls the operation of the bdflush kernel daemon. It
-   currently contains 9 integer values, 6 of which are actually used
-   by the kernel:
-
-    nfract      Percentage of buffer cache dirty to
-                activate bdflush
-    ndirty      Maximum number of dirty blocks to
-                write out per-wake-cycle
-    nrefill     Number of clean buffers to try to obtain
-                each time we call refill
-    nref_dirt   Dirty buffer threshold for activating bdflush
-                when trying to refill buffers.
-    dummy       unused
-    age_buffer  Time for normal buffer to age before you flush it
-    age_super   Time for superblock to age before you flush it
-    dummy       unused
-    dummy       unused
-
-   nfract
-     This parameter governs the maximum number of dirty buffers
-     in the buffer cache. Dirty means that the contents of the
-     buffer still have to be written to disk (as opposed to a
-     clean buffer, which can just be forgotten about). Setting
-     this to a high value means that Linux can delay disk writes
-     for a long time, but it also means that it will have to do a
-     lot of I/O at once when memory becomes short. A low value
-     will spread out disk I/O more evenly.
-
-   ndirty
-     Ndirty gives the maximum number of dirty buffers that
-     bdflush can write to the disk at one time. A high value will
-     mean delayed, bursty I/O, while a small value can lead to
-     memory shortage when bdflush isn't woken up often enough.
-
-   nrefill
-     This the number of buffers that bdflush will add to the list
-     of free buffers when refill_freelist() is called. It is
-     necessary to allocate free buffers beforehand, since the
-     buffers are often different sizes than the memory pages
-     and some bookkeeping needs to be done beforehand. The
-     higher the number, the more memory will be wasted and the
-     less often refill_freelist() will need to run.
-
-   nref_dirt
-     When refill_freelist() comes across more than nref_dirt
-     dirty buffers, it will wake up bdflush.
-
-   age_buffer and age_super
-     Finally, the age_buffer and age_super parameters govern the
-     maximum time Linux waits before writing out a dirty buffer
-     to disk. The value is expressed in jiffies (clockticks), the
-     number of jiffies per second is 100. Age_buffer is the
-     maximum age for data blocks, while age_super is for
-     filesystems meta data.
-
-buffermem
-   The three values in this file control how much memory should be
-   used for buffer memory. The percentage is calculated as a
-   percentage of total system memory.
-
-   The values are:
-
-   min_percent
-     This is the minimum percentage of memory that should be
-     spent on buffer memory.
-
-   borrow_percent
-     When Linux is short on memory, and the buffer cache uses more
-     than it has been allotted, the memory mangement (MM) subsystem
-     will prune the buffer cache more heavily than other memory to
-     compensate.
-
-   max_percent
-     This is the maximum amount of memory that can be used for
-     buffer memory.
-
-freepages
-   This file contains three values: min, low and high:
-
-   min
-     When the number of free pages in the system reaches this number,
-     only the kernel can allocate more memory.
-
-   low
-     If the number of free pages gets below this point, the kernel
-     starts swapping aggressively.
-
-   high
-     The kernel tries to keep up to this amount of memory free; if
-     memory comes below this point, the kernel gently starts swapping
-     in the hopes that it never has to do really aggressive swapping.
-
-kswapd
-   Kswapd is the kernel swap out daemon. That is, kswapd is that piece
-   of the kernel that frees memory when it gets fragmented or
-   full. Since every system is different, you'll probably want some
-   control over this piece of the system.
-
-   The file contains three numbers:
-
-   tries_base
-     The maximum number of pages kswapd tries to free in one round is
-     calculated from this number. Usually this number will be divided
-     by 4 or 8 (see mm/vmscan.c), so it isn't as big as it looks.
-
-     When you need to increase the bandwidth to/from swap, you'll want
-     to increase this number.
-
-   tries_min
-     This is the minimum number of times kswapd tries to free a page
-     each time it is called. Basically it's just there to make sure
-     that kswapd frees some pages even when it's being called with
-     minimum priority.
-
-
-  swap_cluster
-     This is probably the greatest influence on system
-     performance. swap_cluster is the number of pages kswapd writes in
-     one turn. You'll want this value to be large so that kswapd does
-     its I/O in large chunks and the disk doesn't have to seek as
-     often., but you don't want it to be too large since that would
-     flood the request queue.
-
-overcommit_memory
-   This file contains one value. The following algorithm is used to
-   decide if there's enough memory: if the value of overcommit_memory
-   is positive, then there's always enough memory. This is a useful
-   feature, since programs often malloc() huge amounts of memory 'just
-   in case', while they only use a small part of it. Leaving this
-   value at 0 will lead to the failure of such a huge malloc(), when
-   in fact the system has enough memory for the program to run.
-
-   On the other hand, enabling this feature can cause you to run out
-   of memory and thrash the system to death, so large and/or important
-   servers will want to set this value to 0.
-
-pagecache
-   This file does exactly the same as buffermem, only this file
-   controls the amount of memory allowed for memory mapping and
-   generic caching of files.
-
-   You don't want the minimum level to be too low, otherwise your
-   system might thrash when memory is tight or fragmentation is
-   high.
-
-pagetable_cache
-   The kernel keeps a number of page tables in a per-processor cache
-   (this helps a lot on SMP systems). The cache size for each
-   processor will be between the low and the high value.
-
-   On a low-memory, single CPU system, you can safely set these values
-   to 0 so you don't waste memory. It is used on SMP systems so that
-   the system can perform fast pagetable allocations without having to
-   aquire the kernel memory lock.
-
-   For large systems, the settings are probably fine. For normal
-   systems they won't hurt a bit. For small systems (<16MB ram) it
-   might be advantageous to set both values to 0.
-
-swapctl
-   This file contains no less than 8 variables. All of these values
-   are used by kswapd.
-
-   The first four variables sc_max_page_age, sc_page_advance,
-   sc_page_decline and sc_page_initial_age are used to keep track of
-   Linux's page aging. Page aging is a bookkeeping method to track
-   which pages of memory are often used, and which pages can be
-   swapped out without consequences.
-
-   When a page is swapped in, it starts at sc_page_initial_age
-   (default 3) and when the page is scanned by kswapd, its age is
-   adjusted according to the following scheme:
-
-    o If the page was used since the last time we scanned, its age
-      is increased by sc_page_advance (default 3) up to a
-      maximum of sc_max_page_age (default 20).
-
-    o Else (meaning it wasn't used) its age is decreased by
-      sc_page_decline (default 1).
-
-   When a page reaches age 0, it's ready to be swapped out.
-
-   The next four variables sc_age_cluster_fract, sc_age_cluster_min,
-   sc_pageout_weight and sc_bufferout_weight, can be used to control
-   kswapd's aggressiveness in swapping out pages.
-
-   Sc_age_cluster_fract is used to calculate how many pages from a
-   process are to be scanned by kswapd. The formula used is
-
-           sc_age_cluster_fract
-           -------------------- * resident set size
-              1024   =20
-
-   So if you want kswapd to scan the whole process,
-   sc_age_cluster_fract needs to have a value of 1024. The minimum
-   number of pages kswapd will scan is represented by
-   sc_age_cluster_min, this is done so kswapd will also scan small
-   processes.
-
-   The values of sc_pageout_weight and sc_bufferout_weight are used
-   to control how many tries kswapd will make in order to swap out
-   one page/buffer. These values can be used to fine-tune the ratio
-   between user pages and buffer/cache memory. When you find that
-   your Linux system is swapping out too many process pages in order
-   to satisfy buffer memory demands, you might want to either
-   increase sc_bufferout_weight, or decrease the value of
-   sc_pageout_weight.
-
-3.6  /proc/sys/dev - Device specific parameters
-
-Currently there is only support for CDROM drives, and for those, there
-is only one read only file containing information about the CD-ROM
-drives attached to the system:
-
->cat /proc/sys/dev/cdrom/info
-CD-ROM information
-
-drive name:           sr0  hdc
-drive speed:           0    6
-drive # of slots:      1    0
-Can close tray:        1    1
-Can open tray:         1    1
-Can lock tray:         1    1
-Can change speed:      1    1
-Can select disk:       0    1
-Can read multisession: 1    1
-Can read MCN:          1    1
-Reports media changed: 1    1
-Can play audio:        1    1
-
-You see two drives, sr0 and hdc, and their lists of features.
-
-3.7  /proc/sys/sunrpc - Remote procedure calls
-
-This directory contains four files, which enable or disable debugging
-for the RPC functions NFS, NFS-daemon, RPC and NLM. The default values
-are 0. They can be set to one, to turn debugging on.  (The default
-value is 0 for each)
-
-3.8  /proc/sys/net - Networking stuff
-
-The interface to the networking parts of the kernel is located in
-/proc/sys/net. The table below shows all possible subdirectories. You
-may see only some of them, depending on the configuration of your
-kernel:
-
-+-------------------------------------------------------------+
-| core     General parameter   |appletalk  Appletalk protocol |
-| unix     Unix domain sockets |netrom     NET/ROM            |
-| 802      E802 protocol       |ax25       AX25               |
-| ethernet Ethernet protocol   |rose       X.25 PLP layer     |
-| ipv4     IP version 4        |x25        X.25 protocol      |
-| ipx      IPX                 |token-ring IBM token ring     |
-| bridge   Bridging            |decnet     DEC net            |
-| ipv6     IP version 6        |                              |
-+-------------------------------------------------------------+
-
-We will concentrate on IP networking here. As AX15, X.25, and DEC Net
-are only minor players in the Linux world, we'll skip them in this
-chapter. You'll find some short info to Appletalk and IPX further down
-in section 3.10 and 3.11. Please look in the online documentation and
-the kernel source to get a detailed view of the parameters for those
-protocols. In this section we'll discuss the subdirectories printed in
-bold letters in the table above. As default values are suitable for
-most needs, there is no need to change these values.
-
-/proc/sys/net/core - Network core options
-
-rmem_default
-   The default setting of the socket receive buffer in bytes.
-
-rmem_max
-   The maximum receive socket buffer size in bytes.
-
-wmem_default
-   The default setting (in bytes) of the socket send buffer.
-
-wmem_max
-   The maximum send socket buffer size in bytes.
-
-message_burst and message_cost
-   These parameters are used to limit the warning messages written to
-   the kernel log from the networking code. They enforce a rate limit
-   to make a denial-of-service attack impossible. The higher the
-   message_cost factor is, the less messages will be
-   written. Message_burst controls when messages will be dropped. The
-   default settings limit warning messages to one every five seconds.
-
-netdev_max_backlog
-   Maximal number of packets, queued on INPUT side, when the interface
-   receives packets faster than kernel can process them.
-
-optmem_max
-   Maximum ancillary buffer size allowed per socket. Ancillary data is
-   a sequence of struct cmsghdr structures with appended data.
-
-/proc/sys/net/unix - Parameters for UNIX domain sockets
-
-There are only two files in this subdirectory. They control the delays
-for deleting and destroying socket descriptors.
-
-3.9  /proc/sys/net/ipv4 - IPV4 settings
-
-IP version 4 is still the most used protocol in Unix networking. It
-will be replaced by IP version 6 in the next couple of years, but for
-the moment it's the de facto standard for the internet and is used in
-most networking environments around the world. Because of the
-importance of this protocol, we'll have a deeper look into the subtree
-controlling the behavior of the IPv4 subsystem of the Linux kernel.
-
-Let's start with the entries in /proc/sys/net/ipv4 itself.
-
-ICMP settings
-
-icmp_echo_ignore_all and icmp_echo_ignore_broadcasts
-   Turn on (1) or off (0), if the kernel should ignore all ICMP ECHO
-   requests, or just those to broadcast and multicast addresses.
-
-   Please note that if you accept ICMP echo requests with a
-   broadcast/multicast destination address your network may be used
-   as an exploder for denial of service packet flooding attacks to
-   other hosts.
-
-icmp_destunreach_rate, icmp_echoreply_rate,
-icmp_paramprob_rate and icmp_timeexeed_rate
-   Sets limits for sending ICMP packets to specific targets. A value of
-   zero disables all limiting. Any positive value sets the maximum
-   package rate in hundredths of a second (on Intel systems).
-
-IP settings
-
-ip_autoconfig
-   This file contains one, if the host got its IP configuration by
-   RARP, BOOTP, DHCP or a similar mechanism. Otherwise it is zero.
-
-ip_default_ttl
-   TTL (Time To Live) for IPv4 interfaces. This is simply the
-   maximum number of hops a packet may travel.
-
-ip_dynaddr
-   Enable dynamic socket address rewriting on interface address change. This
-   is useful for dialup interface with changing IP addresses.
-
-ip_forward
-   Enable or disable forwarding of IP packages between interfaces. A
-   change of this value resets all other parameters to their default
-   values. They differ if the kernel is configured as host or router.
-
-ip_local_port_range
-   Range of ports used by TCP and UDP to choose the local
-   port. Contains two numbers, the first number is the lowest port,
-   the second number the highest local port. Default is 1024-4999.
-   Should be changed to 32768-61000 for high-usage systems.
-
-ip_no_pmtu_disc
-   Global switch to turn path MTU discovery off. It can also be set
-   on a per socket basis by the applications or on a per route
-   basis.
-
-ip_masq_debug
-   Enable/disable debugging of IP masquerading.
-
-
-IP fragmentation settings
-
-ip_always_defrag
-   Replaces the former Kernel-Configuration option:
-	CONFIG_IP_ALWAYS_DEFRAG
-   All incoming fragments (parts of IP packets
-   that arose when some host between origin and destination decided
-   that the packets were too large and cut them into pieces) will be
-   reassembled (defragmented) before being processed, even if they are
-   about to be forwarded.
-
-   Only say Y here if running either a firewall that is the sole link
-   to your network or a transparent proxy; never ever say Y here for a
-   normal router or host.
-
-   This is automagically enabled when enabling masquerading.
-
-ipfrag_high_trash and ipfrag_low_trash
-   Maximum memory used to reassemble IP fragments. When
-   ipfrag_high_thresh bytes of memory is allocated for this purpose,
-   the fragment handler will toss packets until ipfrag_low_thresh is
-   reached.
-
-
-ipfrag_time
-   Time in seconds to keep an IP fragment in memory.
-
-TCP settings
-
-tcp_retrans_collapse
-   Bug-to-bug compatibility with some broken printers. On retransmit
-   try to send bigger packets to work around bugs in certain TCP
-   stacks. Can be turned off by setting it to zero.
-
-tcp_keepalive_probes
-   Number of keep alive probes TCP sends out, until it decides that the
-   connection is broken.
-
-tcp_keepalive_time
-   How often TCP sends out keep alive messages, when keep alive is
-   enabled. The default is 2 hours.
-
-tcp_syn_retries
-   Number of times initial SYNs for a TCP connection attempt will be
-   retransmitted. Should not be higher than 255. This is only the
-   timeout for outgoing connections, for incoming connections the
-   number of retransmits is defined by tcp_retries1.
-
-tcp_sack
-   Enable select acknowledgments after RFC2018.
-
-tcp_timestamps
-   Enable timestamps as defined in RFC1323.
-
-tcp_stdurg
-   Enable the strict RFC793 interpretation of the TCP urgent pointer
-   field. The default is to use the BSD compatible interpretation
-   of the urgent pointer pointing to the first byte after the urgent
-   data. The RFC793 interpretation is to have it point to the last
-   byte of urgent data. Enabling this option may lead to
-   interoperatibility problems. Disabled by default.
-
-tcp_syncookies
-   Only valid when the kernel was compiled with
-   CONFIG_SYNCOOKIES. Send out syncookies when the syn backlog queue
-   of a socket overflows. This is to prevent against the common 'syn
-   flood attack'. Disabled by default.
-
-   Note that the concept of a socket backlog is abandoned, this
-   means the peer may not receive reliable error messages from an
-   over loaded server with syncookies enabled.
-
-tcp_window_scaling
-   Enable window scaling as defined in RFC1323.
-
-tcp_fin_timeout
-   How many seconds to wait for a final FIN before the socket is
-   always closed. This is strictly a violation of the TCP
-   specification, but required to prevent denial-of-service attacks.
-
-tcp_max_ka_probes
-   How many keepalive probes are sent per slow timer run. Shouldn't be
-   set too high to prevent bursts.
-
-tcp_max_syn_backlog
-   Length of the per socket backlog queue. Since Linux 2.2 the backlog
-   specified in listen(2) only specifies the length of the backlog
-   queue of already established sockets. When more connection requests
-   arrive Linux starts to drop packets. When syncookies are enabled
-   the packets are still answered and the maximum queue is effectively
-   ignored.
-
-tcp_retries1
-   Defines how often an answer to a TCP connection request is
-   retransmitted before giving up.
-
-tcp_retries2
-   Defines how often a TCP packet is retransmitted before giving up.
-
-Interface specific settings
-
-In the directory /proc/sys/net/ipv4/conf you'll find one subdirectory
-for each interface the system knows about and one directory calls
-all. Changes in the all subdirectory affect all interfaces, where
-changes in the other subdirectories affect only one interface.
-
-All directories have the same entries:
-
-accept_redirects
-   This switch decides if the kernel accepts ICMP redirect messages
-   or not. The default is 'yes', if the kernel is configured for a
-   regular host; and 'no' for a router configuration.
-
-accept_source_route
-   Should source routed packages be accepted or declined. The
-   default is dependent on the kernel configuration. It's 'yes' for
-   routers and 'no' for hosts.
-
-bootp_relay
-   Accept packets with source address 0.b.c.d destined not to this
-   host as local ones. It is supposed that BOOTP relay daemon will
-   catch and forward such packets.
-
-   The default is 'no', as this feature is not implemented yet
-   (kernel version 2.2.0-pre?).
-
-forwarding
-   Enable or disable IP forwarding on this interface.
-
-log_martians
-   Log packets with source addresses with no known route to kernel log.
-
-mc_forwarding
-   Do multicast routing. The kernel needs to be compiled with
-   CONFIG_MROUTE and a multicast routing daemon is required.
-
-proxy_arp
-   Do (1) or don't (0) do proxy ARP.
-
-rp_filter
-   Integer value deciding if source validation should be made. 
-   1 means yes, 0 means no. Disabled by default, but 
-   local/broadcast address spoofing is always on.
-
-   If you set this to 1 on a router that is the only connection
-   for a network to the net , it evidently prevents spoofing attacks
-   against your internal networks (external addresses can still be
-   spoofed), without the need for additional firewall rules.
-
-secure_redirects
-   Accept ICMP redirect messages only for gateways, listed in
-   default gateway list. Enabled by default.
-
-hidden
-   Hide addresses attached to this device from another devices.
-   Such addresses will never be selected by source address autoselection
-   mechanism, host does not answer broadcast ARP requests for them,
-   does not announce it as source address of ARP requests,	but they
-   are still reachable via IP. This flag is activated only if it is
-   enabled both in specific device section and in "all" section.
-
-shared_media
-   If it is not set the kernel does not assume that different subnets
-   on this device can communicate directly. Default setting is 'yes'.  
-
-send_redirects
-   Determines if or if not to send ICMP redirects to other hosts.
-
-
-Routing settings
-
-The directory /proc/sys/net/ipv4/route contains several file to
-control routing issues.
-
-error_burst and error_cost
-   These parameters are used to limit the warning messages written to
-   the kernel log from the routing code. The higher the error_cost
-   factor is, the fewer messages will be written. Error_burst controls
-   when messages will be dropped. The default settings limit warning
-   messages to one every five seconds.
-
-flush
-   Writing to this file results in a flush of the routing cache.
-
-gc_elastic, gc_interval, gc_min_interval, gc_tresh, gc_timeout
-   Values to control the frequency and behavior of the garbage
-   collection algorithm for the routing cache.
-
-max_size
-   Maximum size of the routing cache. Old entries will be purged
-   once the cache has this size.
-
-max_delay, min_delay
-   Delays for flushing the routing cache.
-
-redirect_load, redirect_number
-   Factors which determine if more ICPM redirects should be sent to
-   a specific host. No redirects will be sent once the load limit or
-   the maximum number of redirects has been reached.
-
-redirect_silence
-
-   Timeout for redirects. After this period redirects will be sent
-   again, even if this has been stopped, because the load or number
-   limit has been reached.
-
-Network Neighbor handling
-
-Settings about how to handle connections with direct neighbors (nodes
-attached to the same link) can be found in the directory
-/proc/sys/net/ipv4/neigh.
-
-As we saw it in the conf directory, there is a default subdirectory
-which holds the default values, and one directory for each
-interface. The contents of the directories are identical, with the
-single exception that the default settings contain additional options
-to set garbage collection parameters.
-
-In the interface directories you'll find the following entries:
-
-base_reachable_time
-   A base value used for computing the random reachable time value
-   as specified in RFC2461.
-
-retrans_time
-   The time, expressed in jiffies (1/100 sec), between retransmitted
-   Neighbor Solicitation messages. Used for address resolution and to
-   determine if a neighbor is unreachable.
-
-unres_qlen
-   Maximum queue length for a pending arp request - how many packets
-   are accepted from other layers while the arp address is still
-   resolved.
-
-anycast_delay
-   Maximum for random delay of answers to neighbor solicitation
-   messages in jiffies (1/100 sec). Not yet implemented (Linux does
-   not have anycast support yet).
-
-ucast_solicit
-   Maximum number of retries for unicast solicitation.
-
-mcast_solicit
-   Maximum number of retries for multicast solicitation.
-
-delay_first_probe_time
-   Delay for the first time probe if the neighbor is reachable. (see
-   gc_stale_time).
-
-locktime
-   An ARP/neighbor entry is only replaced with a new one if the old
-   is at least locktime old. This prevents ARP cache thrashing.
-
-proxy_delay
-   Maximum time (real time is random [0..proxytime]) before
-   answering to an arp request for which we have an proxy arp entry.
-   In some cases, this is used to prevent network flooding.
-
-proxy_qlen
-   Maximum queue length of the delayed proxy arp timer (see
-   proxy_delay).
-
-app_solcit
-   Determines the number of requests to send to the user level arp
-   daemon. 0 to turn off.
-
-gc_stale_time
-   Determines how often to check for stale ARP entries. After an ARP
-   entry is stale it will be resolved again (useful when an IP address
-   migrates to another machine). When ucast_solicit is > 0 it first
-   tries to send an ARP packet directly to the known host, when that
-   fails and mcast_solicit is > 0, an ARP request is broadcasted.
-
-3.10  Appletalk
-
-The /proc/sys/net/appletalk directory holds the Appletalk
-configuration data when Appletalk is loaded. The configurable
-parameters are:
-
-aarp-expiry-time
-   The amount of time we keep an AARP entry before expiring
-   it. Used to age out old hosts.
-
-aarp-resolve-time
-   The amount of time we will spend trying to resolve an Appletalk
-   address.
-
-aarp-retransmit-limit
-   The number of times we will retransmit a query before giving up.
-
-aarp-tick-time
-   Controls the rate at which expiries are checked.
-
-
-The directory /proc/net/appletalk holds the list of active appletalk
-sockets on a machine.
-
-The fields indicate the DDP type, the local address (in network:node
-format) the remote address, the size of the transmit pending queue,
-the size of the received queue (bytes waiting for applications to
-read) the state and the uid owning the socket.
-
-/proc/net/atalk_iface lists all the interfaces configured for
-appletalk.It shows the name of the interface, its appletalk address,
-the network range on that ad- dress (or network number for phase 1
-networks), and the status of the interface.
-
-/proc/net/atalk_route lists each known network route. It lists the
-target (network) that the route leads to, the router (may be directly
-connected), the route flags, and the device the route is via.
-
-3.11  IPX
-
-The IPX protocol has no tunable values in /proc/sys/net.
-
-The IPX protocol does, however, provide /proc/net/ipx. This lists each
-IPX socket giving the local and remote addresses in Novell format
-(that is network:node:port). In accordance with the strange Novell
-tradition, everything but the port is in hex. Not_Connected is
-displayed for sockets that are not tied to a specific remote
-address. The Tx and Rx queue sizes indicate the number of bytes
-pending for transmit and receive. The state indicates the state the
-socket is in and the uid is the owning uid of the socket.
-
-The /proc/net/ipx_interface file lists all IPX interfaces. For each
-interface it gives the network number, the node number, and indicates
-if the network is the primary network. It also indicates which device it is bound to (or
-Internal for internal networks) and the Frame Type if
-appropriate. Linux supports 802.3, 802.2, 802.2 SNAP and DIX (Blue
-Book) ethernet framing for IPX.
-
-The /proc/net/ipx_route table holds a list of IPX routes. For each
-route it gives the destination network, the router node (or Directly)
-and the network address of the router (or Connected) for internal
-networks.
diff -urN linux/Makefile /tmp/linux/Makefile
--- linux/Makefile	Sun Dec 10 17:49:41 2000
+++ /tmp/linux/Makefile	Wed May 16 12:01:49 2001
@@ -1,7 +1,7 @@
 VERSION = 2
 PATCHLEVEL = 2
 SUBLEVEL = 18
-EXTRAVERSION =
+EXTRAVERSION = +reiserfs+raid+aic7xxx+VM+bef_2001.05.16
 
 ARCH := $(shell uname -m | sed -e s/i.86/i386/ -e s/sun4u/sparc64/ -e s/arm.*/arm/ -e s/sa110/arm/)
 
diff -urN linux/arch/i386/defconfig /tmp/linux/arch/i386/defconfig
--- linux/arch/i386/defconfig	Sun Dec 10 17:49:41 2000
+++ /tmp/linux/arch/i386/defconfig	Fri Feb  2 19:06:00 2001
@@ -93,7 +93,15 @@
 #
 # CONFIG_BLK_DEV_LOOP is not set
 # CONFIG_BLK_DEV_NBD is not set
-# CONFIG_BLK_DEV_MD is not set
+CONFIG_BLK_DEV_MD=y
+CONFIG_AUTODETECT_RAID=y
+CONFIG_MD_TRANSLUCENT=y
+CONFIG_MD_LINEAR=y
+CONFIG_MD_STRIPED=y
+CONFIG_MD_MIRRORING=y
+CONFIG_MD_RAID5=y
+CONFIG_MD_BOOT=y
+CONFIG_BLK_DEV_HSM=y
 # CONFIG_BLK_DEV_RAM is not set
 # CONFIG_BLK_DEV_XD is not set
 # CONFIG_BLK_DEV_DAC960 is not set
diff -urN linux/arch/sparc/config.in /tmp/linux/arch/sparc/config.in
--- linux/arch/sparc/config.in	Sun Dec 10 17:49:41 2000
+++ /tmp/linux/arch/sparc/config.in	Fri Feb  2 19:06:00 2001
@@ -88,10 +88,16 @@
 
 bool 'Multiple devices driver support' CONFIG_BLK_DEV_MD
 if [ "$CONFIG_BLK_DEV_MD" = "y" ]; then
+  bool 'Autodetect RAID partitions' CONFIG_AUTODETECT_RAID
   tristate '   Linear (append) mode' CONFIG_MD_LINEAR
   tristate '   RAID-0 (striping) mode' CONFIG_MD_STRIPED
   tristate '   RAID-1 (mirroring) mode' CONFIG_MD_MIRRORING
   tristate '   RAID-4/RAID-5 mode' CONFIG_MD_RAID5
+  tristate '   Translucent mode' CONFIG_MD_TRANSLUCENT
+  tristate '   Hierarchical Storage Management support' CONFIG_MD_HSM
+fi
+if [ "$CONFIG_MD_LINEAR" = "y" -o "$CONFIG_MD_STRIPED" = "y" ]; then
+  bool '      Boot support (linear, striped)' CONFIG_MD_BOOT
 fi
 
 tristate 'RAM disk support' CONFIG_BLK_DEV_RAM
diff -urN linux/arch/sparc/defconfig /tmp/linux/arch/sparc/defconfig
--- linux/arch/sparc/defconfig	Sun Dec 10 17:49:41 2000
+++ /tmp/linux/arch/sparc/defconfig	Fri Feb  2 19:06:00 2001
@@ -89,10 +89,13 @@
 #
 CONFIG_BLK_DEV_FD=y
 CONFIG_BLK_DEV_MD=y
+# CONFIG_AUTODETECT_RAID is not set
 CONFIG_MD_LINEAR=m
 CONFIG_MD_STRIPED=m
 CONFIG_MD_MIRRORING=m
 CONFIG_MD_RAID5=m
+# CONFIG_MD_TRANSLUCENT is not set
+# CONFIG_MD_HSM is not set
 CONFIG_BLK_DEV_RAM=y
 CONFIG_BLK_DEV_RAM_SIZE=4096
 CONFIG_BLK_DEV_INITRD=y
diff -urN linux/arch/sparc64/config.in /tmp/linux/arch/sparc64/config.in
--- linux/arch/sparc64/config.in	Sun Dec 10 17:49:41 2000
+++ /tmp/linux/arch/sparc64/config.in	Fri Feb  2 19:06:00 2001
@@ -102,10 +102,16 @@
 
 bool 'Multiple devices driver support' CONFIG_BLK_DEV_MD
 if [ "$CONFIG_BLK_DEV_MD" = "y" ]; then
+  bool 'Autodetect RAID partitions' CONFIG_AUTODETECT_RAID
   tristate '   Linear (append) mode' CONFIG_MD_LINEAR
   tristate '   RAID-0 (striping) mode' CONFIG_MD_STRIPED
   tristate '   RAID-1 (mirroring) mode' CONFIG_MD_MIRRORING
   tristate '   RAID-4/RAID-5 mode' CONFIG_MD_RAID5
+  tristate '   Translucent mode' CONFIG_MD_TRANSLUCENT
+  tristate '   Hierarchical Storage Management support' CONFIG_MD_HSM
+fi
+if [ "$CONFIG_MD_LINEAR" = "y" -o "$CONFIG_MD_STRIPED" = "y" ]; then
+  bool '      Boot support (linear, striped)' CONFIG_MD_BOOT
 fi
 
 tristate 'RAM disk support' CONFIG_BLK_DEV_RAM
diff -urN linux/arch/sparc64/defconfig /tmp/linux/arch/sparc64/defconfig
--- linux/arch/sparc64/defconfig	Sun Dec 10 17:49:41 2000
+++ /tmp/linux/arch/sparc64/defconfig	Fri Feb  2 19:06:00 2001
@@ -106,10 +106,13 @@
 #
 CONFIG_BLK_DEV_FD=y
 CONFIG_BLK_DEV_MD=y
+# CONFIG_AUTODETECT_RAID is not set
 CONFIG_MD_LINEAR=m
 CONFIG_MD_STRIPED=m
 CONFIG_MD_MIRRORING=m
 CONFIG_MD_RAID5=m
+# CONFIG_MD_TRANSLUCENT is not set
+# CONFIG_MD_HSM is not set
 CONFIG_BLK_DEV_RAM=y
 CONFIG_BLK_DEV_RAM_SIZE=4096
 CONFIG_BLK_DEV_INITRD=y
diff -urN linux/drivers/block/Config.in /tmp/linux/drivers/block/Config.in
--- linux/drivers/block/Config.in	Sun Dec 10 17:49:41 2000
+++ /tmp/linux/drivers/block/Config.in	Fri Feb  2 19:06:00 2001
@@ -103,10 +103,13 @@
 fi
 bool 'Multiple devices driver support' CONFIG_BLK_DEV_MD
 if [ "$CONFIG_BLK_DEV_MD" = "y" ]; then
+  bool 'Autodetect RAID partitions' CONFIG_AUTODETECT_RAID
   tristate '   Linear (append) mode' CONFIG_MD_LINEAR
   tristate '   RAID-0 (striping) mode' CONFIG_MD_STRIPED
   tristate '   RAID-1 (mirroring) mode' CONFIG_MD_MIRRORING
   tristate '   RAID-4/RAID-5 mode' CONFIG_MD_RAID5
+  tristate '   Translucent mode' CONFIG_MD_TRANSLUCENT
+  tristate '   Hierarchical Storage Management support' CONFIG_MD_HSM
 fi
 if [ "$CONFIG_MD_LINEAR" = "y" -o "$CONFIG_MD_STRIPED" = "y" ]; then
   bool '      Boot support (linear, striped)' CONFIG_MD_BOOT
diff -urN linux/drivers/block/Makefile /tmp/linux/drivers/block/Makefile
--- linux/drivers/block/Makefile	Sun Dec 10 17:49:41 2000
+++ /tmp/linux/drivers/block/Makefile	Fri Feb  2 19:06:00 2001
@@ -294,10 +294,28 @@
 endif
 
 ifeq ($(CONFIG_MD_RAID5),y)
+LX_OBJS += xor.o
 L_OBJS += raid5.o
 else
   ifeq ($(CONFIG_MD_RAID5),m)
+  LX_OBJS += xor.o
   M_OBJS += raid5.o
+  endif
+endif
+
+ifeq ($(CONFIG_MD_TRANSLUCENT),y)
+L_OBJS += translucent.o
+else
+  ifeq ($(CONFIG_MD_TRANSLUCENT),m)
+  M_OBJS += translucent.o
+  endif
+endif
+
+ifeq ($(CONFIG_MD_HSM),y)
+L_OBJS += hsm.o
+else
+  ifeq ($(CONFIG_MD_HSM),m)
+  M_OBJS += hsm.o
   endif
 endif
 
diff -urN linux/drivers/block/genhd.c /tmp/linux/drivers/block/genhd.c
--- linux/drivers/block/genhd.c	Sun Dec 10 17:49:41 2000
+++ /tmp/linux/drivers/block/genhd.c	Fri Feb  2 19:06:00 2001
@@ -28,6 +28,7 @@
 #include <linux/string.h>
 #include <linux/blk.h>
 #include <linux/init.h>
+#include <linux/raid/md.h>
 
 #ifdef CONFIG_ARCH_S390
 #include <asm/dasd.h>
@@ -1729,6 +1730,9 @@
 	else
 #endif
 	rd_load();
+#endif
+#ifdef CONFIG_BLK_DEV_MD
+	autodetect_raid();
 #endif
 #ifdef CONFIG_MD_BOOT
         md_setup_drive();
diff -urN linux/drivers/block/hsm.c /tmp/linux/drivers/block/hsm.c
--- linux/drivers/block/hsm.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/block/hsm.c	Fri Feb  2 19:06:00 2001
@@ -0,0 +1,840 @@
+/*
+   hsm.c : HSM RAID driver for Linux
+              Copyright (C) 1998 Ingo Molnar
+
+   HSM mode management functions.
+
+   This program is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 2, or (at your option)
+   any later version.
+   
+   You should have received a copy of the GNU General Public License
+   (for example /usr/src/linux/COPYING); if not, write to the Free
+   Software Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.  
+*/
+
+#include <linux/module.h>
+
+#include <linux/raid/md.h>
+#include <linux/malloc.h>
+
+#include <linux/raid/hsm.h>
+#include <linux/blk.h>
+
+#define MAJOR_NR MD_MAJOR
+#define MD_DRIVER
+#define MD_PERSONALITY
+
+
+#define DEBUG_HSM 1
+
+#if DEBUG_HSM
+#define dprintk(x,y...) printk(x,##y)
+#else
+#define dprintk(x,y...) do { } while (0)
+#endif
+
+void print_bh(struct buffer_head *bh)
+{
+	dprintk("bh %p: %lx %lx %x %x %lx %p %lx %p %x %p %x %lx\n", bh, 
+		bh->b_blocknr, bh->b_size, bh->b_dev, bh->b_rdev,
+		bh->b_rsector, bh->b_this_page, bh->b_state,
+		bh->b_next_free, bh->b_count, bh->b_data,
+		bh->b_list, bh->b_flushtime
+	);
+}
+
+static int check_bg (pv_t *pv, pv_block_group_t * bg)
+{
+	int i, free = 0;
+
+	dprintk("checking bg ...\n");
+
+	for (i = 0; i < pv->pv_sb->pv_bg_size-1; i++) {
+		if (pv_pptr_free(bg->blocks + i)) {
+			free++;
+			if (test_bit(i, bg->used_bitmap)) {
+				printk("hm, bit %d set?\n", i);
+			}
+		} else {
+			if (!test_bit(i, bg->used_bitmap)) {
+				printk("hm, bit %d not set?\n", i);
+			}
+		}
+	}
+	dprintk("%d free blocks in bg ...\n", free);
+	return free;
+}
+
+static void get_bg (pv_t *pv, pv_bg_desc_t *desc, int nr)
+{
+	unsigned int bg_pos = nr * pv->pv_sb->pv_bg_size + 2;
+	struct buffer_head *bh;
+
+	dprintk("... getting BG at %u ...\n", bg_pos);
+
+        bh = bread (pv->dev, bg_pos, HSM_BLOCKSIZE);
+	if (!bh) {
+		MD_BUG();
+		return;
+	}
+	desc->bg = (pv_block_group_t *) bh->b_data;
+	desc->free_blocks = check_bg(pv, desc->bg);
+}
+
+static int find_free_block (lv_t *lv, pv_t *pv, pv_bg_desc_t *desc, int nr,
+				unsigned int lblock, lv_lptr_t * index)
+{
+	int i;
+
+	for (i = 0; i < pv->pv_sb->pv_bg_size-1; i++) {
+		pv_pptr_t * bptr = desc->bg->blocks + i;
+		if (pv_pptr_free(bptr)) {
+			unsigned int bg_pos = nr * pv->pv_sb->pv_bg_size + 2;
+
+			if (test_bit(i, desc->bg->used_bitmap)) {
+				MD_BUG();
+				continue;
+			}
+			bptr->u.used.owner.log_id = lv->log_id;
+			bptr->u.used.owner.log_index = lblock;
+			index->data.phys_nr = pv->phys_nr;
+			index->data.phys_block = bg_pos + i + 1;
+			set_bit(i, desc->bg->used_bitmap);
+			desc->free_blocks--;
+			dprintk(".....free blocks left in bg %p: %d\n",
+					desc->bg, desc->free_blocks);
+			return 0;
+		}
+	}
+	return -ENOSPC;
+}
+
+static int __get_free_block (lv_t *lv, pv_t *pv,
+					unsigned int lblock, lv_lptr_t * index)
+{
+	int i;
+
+	dprintk("trying to get free block for lblock %d ...\n", lblock);
+
+	for (i = 0; i < pv->pv_sb->pv_block_groups; i++) {
+		pv_bg_desc_t *desc = pv->bg_array + i;
+
+		dprintk("looking at desc #%d (%p)...\n", i, desc->bg);
+		if (!desc->bg)
+			get_bg(pv, desc, i);
+
+		if (desc->bg && desc->free_blocks)
+			return find_free_block(lv, pv, desc, i,
+							lblock, index);
+	}
+	dprintk("hsm: pv %s full!\n", partition_name(pv->dev));
+	return -ENOSPC;
+}
+
+static int get_free_block (lv_t *lv, unsigned int lblock, lv_lptr_t * index)
+{
+	int err;
+
+	if (!lv->free_indices)
+		return -ENOSPC;
+
+ 	/* fix me */
+	err = __get_free_block(lv, lv->vg->pv_array + 0, lblock, index);
+
+	if (err || !index->data.phys_block) {
+		MD_BUG();
+		return -ENOSPC;
+	}
+
+	lv->free_indices--;
+
+	return 0;
+}
+
+/*
+ * fix me: wordsize assumptions ...
+ */
+#define INDEX_BITS 8
+#define INDEX_DEPTH (32/INDEX_BITS)
+#define INDEX_MASK ((1<<INDEX_BITS) - 1)
+
+static void print_index_list (lv_t *lv, lv_lptr_t *index)
+{
+	lv_lptr_t *tmp;
+	int i;
+
+	dprintk("... block <%u,%u,%x> [.", index->data.phys_nr,
+		index->data.phys_block, index->cpu_addr);
+
+	tmp = index_child(index);
+	for (i = 0; i < HSM_LPTRS_PER_BLOCK; i++) {
+		if (index_block(lv, tmp))
+			dprintk("(%d->%d)", i, index_block(lv, tmp));
+		tmp++;
+	}
+	dprintk(".]\n");
+}
+
+static int read_index_group (lv_t *lv, lv_lptr_t *index)
+{
+	lv_lptr_t *index_group, *tmp;
+	struct buffer_head *bh;
+	int i;
+
+	dprintk("reading index group <%s:%d>\n",
+		partition_name(index_dev(lv, index)), index_block(lv, index));
+
+	bh = bread(index_dev(lv, index), index_block(lv, index), HSM_BLOCKSIZE);
+	if (!bh) {
+		MD_BUG();
+		return -EIO;
+	}
+	if (!buffer_uptodate(bh))
+		MD_BUG();
+
+	index_group = (lv_lptr_t *) bh->b_data;
+	tmp = index_group;
+	for (i = 0; i < HSM_LPTRS_PER_BLOCK; i++) {
+		if (index_block(lv, tmp)) {
+			dprintk("index group has BLOCK %d, non-present.\n", i);
+			tmp->cpu_addr = 0;
+		}
+		tmp++;
+	}
+	index->cpu_addr = ptr_to_cpuaddr(index_group);
+
+	dprintk("have read index group %p at block %d.\n",
+				index_group, index_block(lv, index));
+	print_index_list(lv, index);
+
+	return 0;
+}
+
+static int alloc_index_group (lv_t *lv, unsigned int lblock, lv_lptr_t * index)
+{
+	struct buffer_head *bh;
+	lv_lptr_t * index_group;
+	
+	if (get_free_block(lv, lblock, index))
+		return -ENOSPC;
+
+	dprintk("creating block for index group <%s:%d>\n",
+		partition_name(index_dev(lv, index)), index_block(lv, index));
+
+	bh = getblk(index_dev(lv, index),
+			 index_block(lv, index), HSM_BLOCKSIZE);
+
+	index_group = (lv_lptr_t *) bh->b_data;
+	md_clear_page(index_group);
+	mark_buffer_uptodate(bh, 1);
+
+	index->cpu_addr = ptr_to_cpuaddr(index_group);
+
+	dprintk("allocated index group %p at block %d.\n",
+				index_group, index_block(lv, index));
+	return 0;
+}
+
+static lv_lptr_t * alloc_fixed_index (lv_t *lv, unsigned int lblock)
+{
+	lv_lptr_t * index = index_child(&lv->root_index);
+	int idx, l;
+
+	for (l = INDEX_DEPTH-1; l >= 0; l--) {
+		idx = (lblock >> (INDEX_BITS*l)) & INDEX_MASK;
+		index += idx;
+		if (!l)
+			break;
+		if (!index_present(index)) {
+			dprintk("no group, level %u, pos %u\n", l, idx);
+			if (alloc_index_group(lv, lblock, index))
+				return NULL;
+		}
+		index = index_child(index);
+	}
+	if (!index_block(lv,index)) {
+		dprintk("no data, pos %u\n", idx);
+		if (get_free_block(lv, lblock, index))
+			return NULL;
+		return index;
+	}
+	MD_BUG();
+	return index;
+}
+
+static lv_lptr_t * find_index (lv_t *lv, unsigned int lblock)
+{
+	lv_lptr_t * index = index_child(&lv->root_index);
+	int idx, l;
+
+	for (l = INDEX_DEPTH-1; l >= 0; l--) {
+		idx = (lblock >> (INDEX_BITS*l)) & INDEX_MASK;
+		index += idx;
+		if (!l)
+			break;
+		if (index_free(index))
+			return NULL;
+		if (!index_present(index))
+			read_index_group(lv, index);
+		if (!index_present(index)) {
+			MD_BUG();
+			return NULL;
+		}
+		index = index_child(index);
+	}
+	if (!index_block(lv,index))
+		return NULL;
+	return index;
+}
+
+static int read_root_index(lv_t *lv)
+{
+	int err;
+	lv_lptr_t *index = &lv->root_index;
+
+	if (!index_block(lv, index)) {
+		printk("LV has no root index yet, creating.\n");
+
+		err = alloc_index_group (lv, 0, index);
+		if (err) {
+			printk("could not create index group, err:%d\n", err);
+			return err;
+		}
+		lv->vg->vg_sb->lv_array[lv->log_id].lv_root_idx =
+					lv->root_index.data;
+	} else {
+		printk("LV already has a root index.\n");
+		printk("... at <%s:%d>.\n",
+			partition_name(index_dev(lv, index)),
+			index_block(lv, index));
+
+		read_index_group(lv, index);
+	}
+	return 0;
+}
+
+static int init_pv(pv_t *pv)
+{
+	struct buffer_head *bh;
+	pv_sb_t *pv_sb;
+
+        bh = bread (pv->dev, 0, HSM_BLOCKSIZE);
+	if (!bh) {
+		MD_BUG();
+		return -1;
+	}
+
+	pv_sb = (pv_sb_t *) bh->b_data;
+	pv->pv_sb = pv_sb;
+
+	if (pv_sb->pv_magic != HSM_PV_SB_MAGIC) {
+		printk("%s is not a PV, has magic %x instead of %x!\n",
+			partition_name(pv->dev), pv_sb->pv_magic,
+			HSM_PV_SB_MAGIC);
+		return -1;
+	}
+	printk("%s detected as a valid PV (#%d).\n", partition_name(pv->dev),
+							pv->phys_nr);
+	printk("... created under HSM version %d.%d.%d, at %x.\n",
+	    pv_sb->pv_major, pv_sb->pv_minor, pv_sb->pv_patch, pv_sb->pv_ctime);
+	printk("... total # of blocks: %d (%d left unallocated).\n",
+			 pv_sb->pv_total_size, pv_sb->pv_blocks_left);
+
+	printk("... block size: %d bytes.\n", pv_sb->pv_block_size);
+	printk("... block descriptor size: %d bytes.\n", pv_sb->pv_pptr_size);
+	printk("... block group size: %d blocks.\n", pv_sb->pv_bg_size);
+	printk("... # of block groups: %d.\n", pv_sb->pv_block_groups);
+
+	if (pv_sb->pv_block_groups*sizeof(pv_bg_desc_t) > PAGE_SIZE) {
+		MD_BUG();
+		return 1;
+	}
+	pv->bg_array = (pv_bg_desc_t *)__get_free_page(GFP_KERNEL);
+	if (!pv->bg_array) {
+		MD_BUG();
+		return 1;
+	}
+	memset(pv->bg_array, 0, PAGE_SIZE);
+
+	return 0;
+}
+
+static int free_pv(pv_t *pv)
+{
+	struct buffer_head *bh;
+
+	dprintk("freeing PV %d ...\n", pv->phys_nr);
+
+	if (pv->bg_array) {
+		int i;
+
+		dprintk(".... freeing BGs ...\n");
+		for (i = 0; i < pv->pv_sb->pv_block_groups; i++) {
+			unsigned int bg_pos = i * pv->pv_sb->pv_bg_size + 2;
+			pv_bg_desc_t *desc = pv->bg_array + i;
+
+			if (desc->bg) {
+				dprintk(".... freeing BG %d ...\n", i);
+	        		bh = getblk (pv->dev, bg_pos, HSM_BLOCKSIZE);
+				mark_buffer_dirty(bh, 1);
+				brelse(bh);
+				brelse(bh);
+			}
+		}
+		free_page((unsigned long)pv->bg_array);
+	} else
+		MD_BUG();
+
+        bh = getblk (pv->dev, 0, HSM_BLOCKSIZE);
+	if (!bh) {
+		MD_BUG();
+		return -1;
+	}
+	mark_buffer_dirty(bh, 1);
+	brelse(bh);
+	brelse(bh);
+
+	return 0;
+}
+
+struct semaphore hsm_sem = MUTEX;
+
+#define HSM_SECTORS (HSM_BLOCKSIZE/512)
+
+static int hsm_map (mddev_t *mddev, kdev_t dev, kdev_t *rdev,
+			unsigned long *rsector, unsigned long bsectors)
+{
+	lv_t *lv = kdev_to_lv(dev);
+	lv_lptr_t *index;
+	unsigned int lblock = *rsector / HSM_SECTORS;
+	unsigned int offset = *rsector % HSM_SECTORS;
+	int err = -EIO;
+
+	if (!lv) {
+		printk("HSM: md%d not a Logical Volume!\n", mdidx(mddev));
+		goto out;
+	}
+	if (offset + bsectors > HSM_SECTORS) {
+		MD_BUG();
+		goto out;
+	}
+	down(&hsm_sem);
+	index = find_index(lv, lblock);
+	if (!index) {
+		printk("no block %u yet ... allocating\n", lblock);
+		index = alloc_fixed_index(lv, lblock);
+	}
+
+	err = 0;
+
+	printk(" %u <%s : %ld(%ld)> -> ", lblock,
+		partition_name(*rdev), *rsector, bsectors);
+
+	*rdev = index_dev(lv, index);
+	*rsector = index_block(lv, index) * HSM_SECTORS + offset;
+
+	printk(" <%s : %ld> %u\n",
+		partition_name(*rdev), *rsector, index_block(lv, index));
+
+	up(&hsm_sem);
+out:
+	return err;
+}
+
+static void free_index (lv_t *lv, lv_lptr_t * index)
+{
+	struct buffer_head *bh;
+
+	printk("tryin to get cached block for index group <%s:%d>\n",
+		partition_name(index_dev(lv, index)), index_block(lv, index));
+
+	bh = getblk(index_dev(lv, index), index_block(lv, index),HSM_BLOCKSIZE);
+
+	printk("....FREEING ");
+	print_index_list(lv, index);
+
+	if (bh) {
+		if (!buffer_uptodate(bh))
+			MD_BUG();
+		if ((lv_lptr_t *)bh->b_data != index_child(index)) {
+			printk("huh? b_data is %p, index content is %p.\n",
+				bh->b_data, index_child(index));
+		} else 
+			printk("good, b_data == index content == %p.\n",
+				index_child(index));
+		printk("b_count == %d, writing.\n", bh->b_count);
+		mark_buffer_dirty(bh, 1);
+		brelse(bh);
+		brelse(bh);
+		printk("done.\n");
+	} else {
+		printk("FAILED!\n");
+	}
+	print_index_list(lv, index);
+	index_child(index) = NULL;
+}
+
+static void free_index_group (lv_t *lv, int level, lv_lptr_t * index_0)
+{
+	char dots [3*8];
+	lv_lptr_t * index;
+	int i, nr_dots;
+
+	nr_dots = (INDEX_DEPTH-level)*3;
+ 	memcpy(dots,"...............",nr_dots);
+	dots[nr_dots] = 0;
+
+	dprintk("%s level %d index group block:\n", dots, level);
+
+
+	index = index_0;
+	for (i = 0; i < HSM_LPTRS_PER_BLOCK; i++) {
+		if (index->data.phys_block) {
+			dprintk("%s block <%u,%u,%x>\n", dots,
+				index->data.phys_nr,
+				index->data.phys_block,
+				index->cpu_addr);
+			if (level && index_present(index)) {
+				dprintk("%s==> deeper one level\n", dots);
+				free_index_group(lv, level-1,
+						index_child(index));
+				dprintk("%s freeing index group block %p ...",
+						dots, index_child(index));
+				free_index(lv, index);
+			}
+		}
+		index++;
+	}
+	dprintk("%s DONE: level %d index group block.\n", dots, level);
+}
+
+static void free_lv_indextree (lv_t *lv)
+{
+	dprintk("freeing LV %d ...\n", lv->log_id);
+	dprintk("..root index: %p\n", index_child(&lv->root_index));
+	dprintk("..INDEX TREE:\n");
+	free_index_group(lv, INDEX_DEPTH-1, index_child(&lv->root_index));
+	dprintk("..freeing root index %p ...", index_child(&lv->root_index));
+	dprintk("root block <%u,%u,%x>\n", lv->root_index.data.phys_nr,
+		lv->root_index.data.phys_block, lv->root_index.cpu_addr);
+	free_index(lv, &lv->root_index);
+	dprintk("..INDEX TREE done.\n");
+	fsync_dev(lv->vg->pv_array[0].dev); /* fix me */
+	lv->vg->vg_sb->lv_array[lv->log_id].lv_free_indices = lv->free_indices;
+}
+
+static void print_index_group (lv_t *lv, int level, lv_lptr_t * index_0)
+{
+	char dots [3*5];
+	lv_lptr_t * index;
+	int i, nr_dots;
+
+	nr_dots = (INDEX_DEPTH-level)*3;
+ 	memcpy(dots,"...............",nr_dots);
+	dots[nr_dots] = 0;
+
+	dprintk("%s level %d index group block:\n", dots, level);
+
+
+	for (i = 0; i < HSM_LPTRS_PER_BLOCK; i++) {
+		index = index_0 + i;
+		if (index->data.phys_block) {
+			dprintk("%s block <%u,%u,%x>\n", dots,
+				index->data.phys_nr,
+				index->data.phys_block,
+				index->cpu_addr);
+			if (level && index_present(index)) {
+				dprintk("%s==> deeper one level\n", dots);
+				print_index_group(lv, level-1,
+							index_child(index));
+			}
+		}
+	}
+	dprintk("%s DONE: level %d index group block.\n", dots, level);
+}
+
+static void print_lv (lv_t *lv)
+{
+	dprintk("printing LV %d ...\n", lv->log_id);
+	dprintk("..root index: %p\n", index_child(&lv->root_index));
+	dprintk("..INDEX TREE:\n");
+	print_index_group(lv, INDEX_DEPTH-1, index_child(&lv->root_index));
+	dprintk("..INDEX TREE done.\n");
+}
+
+static int map_lv (lv_t *lv)
+{
+	kdev_t dev = lv->dev;
+	unsigned int nr = MINOR(dev);
+	mddev_t *mddev = lv->vg->mddev;
+
+	if (MAJOR(dev) != MD_MAJOR) {
+		MD_BUG();
+		return -1;
+	}
+	if (kdev_to_mddev(dev)) {
+		MD_BUG();
+		return -1;
+	}
+	md_hd_struct[nr].start_sect = 0;
+	md_hd_struct[nr].nr_sects = md_size[mdidx(mddev)] << 1;
+	md_size[nr] = md_size[mdidx(mddev)];
+	add_mddev_mapping(mddev, dev, lv);
+
+	return 0;
+}
+
+static int unmap_lv (lv_t *lv)
+{
+	kdev_t dev = lv->dev;
+	unsigned int nr = MINOR(dev);
+
+	if (MAJOR(dev) != MD_MAJOR) {
+		MD_BUG();
+		return -1;
+	}
+	md_hd_struct[nr].start_sect = 0;
+	md_hd_struct[nr].nr_sects = 0;
+	md_size[nr] = 0;
+	del_mddev_mapping(lv->vg->mddev, dev);
+
+	return 0;
+}
+
+static int init_vg (vg_t *vg)
+{
+	int i;
+	lv_t *lv;
+	kdev_t dev;
+	vg_sb_t *vg_sb;
+	struct buffer_head *bh;
+	lv_descriptor_t *lv_desc;
+
+	/*
+	 * fix me: read all PVs and compare the SB
+	 */
+        dev = vg->pv_array[0].dev;
+        bh = bread (dev, 1, HSM_BLOCKSIZE);
+	if (!bh) {
+		MD_BUG();
+		return -1;
+	}
+
+	vg_sb = (vg_sb_t *) bh->b_data;
+	vg->vg_sb = vg_sb;
+
+	if (vg_sb->vg_magic != HSM_VG_SB_MAGIC) {
+		printk("%s is not a valid VG, has magic %x instead of %x!\n",
+			partition_name(dev), vg_sb->vg_magic,
+			HSM_VG_SB_MAGIC);
+		return -1;
+	}
+
+	vg->nr_lv = 0;
+	for (i = 0; i < HSM_MAX_LVS_PER_VG; i++) {
+		unsigned int id;
+		lv_desc = vg->vg_sb->lv_array + i;
+
+		id = lv_desc->lv_id;
+		if (!id) {
+			printk("... LV desc %d empty\n", i);
+			continue;
+		}
+		if (id >= HSM_MAX_LVS_PER_VG) {
+			MD_BUG();
+			continue;
+		}
+
+		lv = vg->lv_array + id;
+		if (lv->vg) {
+			MD_BUG();
+			continue;
+		}
+		lv->log_id = id;
+		lv->vg = vg;
+		lv->max_indices = lv_desc->lv_max_indices;
+		lv->free_indices = lv_desc->lv_free_indices;
+		lv->root_index.data = lv_desc->lv_root_idx;
+		lv->dev = MKDEV(MD_MAJOR, lv_desc->md_id);
+
+		vg->nr_lv++;
+
+		map_lv(lv);
+		if (read_root_index(lv)) {
+			vg->nr_lv--;
+			unmap_lv(lv);
+			memset(lv, 0, sizeof(*lv));
+		}
+	}
+	if (vg->nr_lv != vg_sb->nr_lvs)
+		MD_BUG();
+
+	return 0;
+}
+
+static int hsm_run (mddev_t *mddev)
+{
+	int i;
+	vg_t *vg;
+	mdk_rdev_t *rdev;
+
+	MOD_INC_USE_COUNT;
+
+	vg = kmalloc (sizeof (*vg), GFP_KERNEL);
+	if (!vg)
+		goto out;
+	memset(vg, 0, sizeof(*vg));
+	mddev->private = vg;
+	vg->mddev = mddev;
+
+	if (md_check_ordering(mddev)) {
+		printk("hsm: disks are not ordered, aborting!\n");
+		goto out;
+	}
+
+	set_blocksize (mddev_to_kdev(mddev), HSM_BLOCKSIZE);
+
+	vg->nr_pv = mddev->nb_dev;
+	ITERATE_RDEV_ORDERED(mddev,rdev,i) {
+		pv_t *pv = vg->pv_array + i;
+
+		pv->dev = rdev->dev;
+		fsync_dev (pv->dev);
+		set_blocksize (pv->dev, HSM_BLOCKSIZE);
+		pv->phys_nr = i;
+		if (init_pv(pv))
+			goto out;
+	}
+
+	init_vg(vg);
+
+	return 0;
+
+out:
+	if (vg) {
+		kfree(vg);
+		mddev->private = NULL;
+	}
+	MOD_DEC_USE_COUNT;
+
+	return 1;
+}
+
+static int hsm_stop (mddev_t *mddev)
+{
+	lv_t *lv;
+	vg_t *vg;
+	int i;
+
+	vg = mddev_to_vg(mddev);
+
+	for (i = 0; i < HSM_MAX_LVS_PER_VG; i++) {
+		lv = vg->lv_array + i;
+		if (!lv->log_id)
+			continue;
+		print_lv(lv);
+		free_lv_indextree(lv);
+		unmap_lv(lv);
+	}
+	for (i = 0; i < vg->nr_pv; i++)
+		free_pv(vg->pv_array + i);
+
+	kfree(vg);
+
+	MOD_DEC_USE_COUNT;
+
+	return 0;
+}
+
+
+static int hsm_status (char *page, mddev_t *mddev)
+{
+	int sz = 0, i;
+	lv_t *lv;
+	vg_t *vg;
+
+	vg = mddev_to_vg(mddev);
+
+	for (i = 0; i < HSM_MAX_LVS_PER_VG; i++) {
+		lv = vg->lv_array + i;
+		if (!lv->log_id)
+			continue;
+		sz += sprintf(page+sz, "<LV%d %d/%d blocks used> ", lv->log_id,
+			lv->max_indices - lv->free_indices, lv->max_indices);
+	}
+	return sz;
+}
+
+
+static mdk_personality_t hsm_personality=
+{
+	"hsm",
+	hsm_map,
+	NULL,
+	NULL,
+	hsm_run,
+	hsm_stop,
+	hsm_status,
+	NULL,
+	0,
+	NULL,
+	NULL,
+	NULL,
+	NULL
+};
+
+#ifndef MODULE
+
+md__initfunc(void hsm_init (void))
+{
+	register_md_personality (HSM, &hsm_personality);
+}
+
+#else
+
+int init_module (void)
+{
+	return (register_md_personality (HSM, &hsm_personality));
+}
+
+void cleanup_module (void)
+{
+	unregister_md_personality (HSM);
+}
+
+#endif
+
+/*
+ * This Linus-trick catches bugs via the linker.
+ */
+
+extern void __BUG__in__hsm_dot_c_1(void);
+extern void __BUG__in__hsm_dot_c_2(void);
+extern void __BUG__in__hsm_dot_c_3(void);
+extern void __BUG__in__hsm_dot_c_4(void);
+extern void __BUG__in__hsm_dot_c_5(void);
+extern void __BUG__in__hsm_dot_c_6(void);
+extern void __BUG__in__hsm_dot_c_7(void);
+ 
+void bugcatcher (void)
+{
+        if (sizeof(pv_block_group_t) != HSM_BLOCKSIZE)
+                __BUG__in__hsm_dot_c_1();
+        if (sizeof(lv_index_block_t) != HSM_BLOCKSIZE)
+                __BUG__in__hsm_dot_c_2();
+
+        if (sizeof(pv_sb_t) != HSM_BLOCKSIZE)
+                __BUG__in__hsm_dot_c_4();
+        if (sizeof(lv_sb_t) != HSM_BLOCKSIZE)
+                __BUG__in__hsm_dot_c_3();
+	if (sizeof(vg_sb_t) != HSM_BLOCKSIZE)
+                __BUG__in__hsm_dot_c_6();
+
+	if (sizeof(lv_lptr_t) != 16)
+                __BUG__in__hsm_dot_c_5();
+	if (sizeof(pv_pptr_t) != 16)
+                __BUG__in__hsm_dot_c_6();
+}
+
diff -urN linux/drivers/block/linear.c /tmp/linux/drivers/block/linear.c
--- linux/drivers/block/linear.c	Sat Nov  8 12:39:12 1997
+++ /tmp/linux/drivers/block/linear.c	Fri Feb  2 19:06:00 2001
@@ -1,4 +1,3 @@
-
 /*
    linear.c : Multiple Devices driver for Linux
               Copyright (C) 1994-96 Marc ZYNGIER
@@ -19,186 +18,207 @@
 
 #include <linux/module.h>
 
-#include <linux/md.h>
+#include <linux/raid/md.h>
 #include <linux/malloc.h>
-#include <linux/init.h>
 
-#include "linear.h"
+#include <linux/raid/linear.h>
 
 #define MAJOR_NR MD_MAJOR
 #define MD_DRIVER
 #define MD_PERSONALITY
 
-static int linear_run (int minor, struct md_dev *mddev)
+static int linear_run (mddev_t *mddev)
 {
-  int cur=0, i, size, dev0_size, nb_zone;
-  struct linear_data *data;
-
-  MOD_INC_USE_COUNT;
-
-  mddev->private=kmalloc (sizeof (struct linear_data), GFP_KERNEL);
-  data=(struct linear_data *) mddev->private;
-
-  /*
-     Find out the smallest device. This was previously done
-     at registry time, but since it violates modularity,
-     I moved it here... Any comment ? ;-)
-   */
-
-  data->smallest=mddev->devices;
-  for (i=1; i<mddev->nb_dev; i++)
-    if (data->smallest->size > mddev->devices[i].size)
-      data->smallest=mddev->devices+i;
-  
-  nb_zone=data->nr_zones=
-    md_size[minor]/data->smallest->size +
-    (md_size[minor]%data->smallest->size ? 1 : 0);
-  
-  data->hash_table=kmalloc (sizeof (struct linear_hash)*nb_zone, GFP_KERNEL);
-
-  size=mddev->devices[cur].size;
+	linear_conf_t *conf;
+	struct linear_hash *table;
+	mdk_rdev_t *rdev;
+	int size, i, j, nb_zone;
+	unsigned int curr_offset;
+
+	MOD_INC_USE_COUNT;
+
+	conf = kmalloc (sizeof (*conf), GFP_KERNEL);
+	if (!conf)
+		goto out;
+	mddev->private = conf;
+
+	if (md_check_ordering(mddev)) {
+		printk("linear: disks are not ordered, aborting!\n");
+		goto out;
+	}
+	/*
+	 * Find the smallest device.
+	 */
+
+	conf->smallest = NULL;
+	curr_offset = 0;
+	ITERATE_RDEV_ORDERED(mddev,rdev,j) {
+		dev_info_t *disk = conf->disks + j;
+
+		disk->dev = rdev->dev;
+		disk->size = rdev->size;
+		disk->offset = curr_offset;
+
+		curr_offset += disk->size;
+
+		if (!conf->smallest || (disk->size < conf->smallest->size))
+			conf->smallest = disk;
+	}
+
+	nb_zone = conf->nr_zones =
+		md_size[mdidx(mddev)] / conf->smallest->size +
+		((md_size[mdidx(mddev)] % conf->smallest->size) ? 1 : 0);
+  
+	conf->hash_table = kmalloc (sizeof (struct linear_hash) * nb_zone,
+					GFP_KERNEL);
+	if (!conf->hash_table)
+		goto out;
+
+	/*
+	 * Here we generate the linear hash table
+	 */
+	table = conf->hash_table;
+	i = 0;
+	size = 0;
+	for (j = 0; j < mddev->nb_dev; j++) {
+		dev_info_t *disk = conf->disks + j;
+
+		if (size < 0) {
+			table->dev1 = disk;
+			table++;
+		}
+		size += disk->size;
+
+		while (size) {
+			table->dev0 = disk;
+			size -= conf->smallest->size;
+			if (size < 0)
+				break;
+			table->dev1 = NULL;
+			table++;
+		}
+	}
+	table->dev1 = NULL;
+
+	return 0;
+
+out:
+	if (conf)
+		kfree(conf);
+	MOD_DEC_USE_COUNT;
+	return 1;
+}
+
+static int linear_stop (mddev_t *mddev)
+{
+	linear_conf_t *conf = mddev_to_conf(mddev);
+  
+	kfree(conf->hash_table);
+	kfree(conf);
 
-  i=0;
-  while (cur<mddev->nb_dev)
-  {
-    data->hash_table[i].dev0=mddev->devices+cur;
+	MOD_DEC_USE_COUNT;
 
-    if (size>=data->smallest->size) /* If we completely fill the slot */
-    {
-      data->hash_table[i++].dev1=NULL;
-      size-=data->smallest->size;
-
-      if (!size)
-      {
-	if (++cur==mddev->nb_dev) continue;
-	size=mddev->devices[cur].size;
-      }
-
-      continue;
-    }
-
-    if (++cur==mddev->nb_dev) /* Last dev, set dev1 as NULL */
-    {
-      data->hash_table[i].dev1=NULL;
-      continue;
-    }
-
-    dev0_size=size;		/* Here, we use a 2nd dev to fill the slot */
-    size=mddev->devices[cur].size;
-    data->hash_table[i++].dev1=mddev->devices+cur;
-    size-=(data->smallest->size - dev0_size);
-  }
-
-  return 0;
-}
-
-static int linear_stop (int minor, struct md_dev *mddev)
-{
-  struct linear_data *data=(struct linear_data *) mddev->private;
-  
-  kfree (data->hash_table);
-  kfree (data);
-
-  MOD_DEC_USE_COUNT;
-
-  return 0;
+	return 0;
 }
 
 
-static int linear_map (struct md_dev *mddev, kdev_t *rdev,
+static int linear_map (mddev_t *mddev, kdev_t dev, kdev_t *rdev,
 		       unsigned long *rsector, unsigned long size)
 {
-  struct linear_data *data=(struct linear_data *) mddev->private;
-  struct linear_hash *hash;
-  struct real_dev *tmp_dev;
-  long block;
-
-  block=*rsector >> 1;
-  hash=data->hash_table+(block/data->smallest->size);
-  
-  if (block >= (hash->dev0->size + hash->dev0->offset))
-  {
-    if (!hash->dev1)
-    {
-      printk ("linear_map : hash->dev1==NULL for block %ld\n", block);
-      return (-1);
-    }
-    
-    tmp_dev=hash->dev1;
-  }
-  else
-    tmp_dev=hash->dev0;
+	linear_conf_t *conf = mddev_to_conf(mddev);
+	struct linear_hash *hash;
+	dev_info_t *tmp_dev;
+	long block;
+
+	block = *rsector >> 1;
+	hash = conf->hash_table + (block / conf->smallest->size);
+  
+	if (block >= (hash->dev0->size + hash->dev0->offset))
+	{
+		if (!hash->dev1)
+		{
+			printk ("linear_map : hash->dev1==NULL for block %ld\n",
+						block);
+			return -1;
+		}
+		tmp_dev = hash->dev1;
+	} else
+		tmp_dev = hash->dev0;
     
-  if (block >= (tmp_dev->size + tmp_dev->offset) || block < tmp_dev->offset)
-    printk ("Block %ld out of bounds on dev %s size %d offset %d\n",
-	    block, kdevname(tmp_dev->dev), tmp_dev->size, tmp_dev->offset);
+	if (block >= (tmp_dev->size + tmp_dev->offset)
+				|| block < tmp_dev->offset)
+	printk ("Block %ld out of bounds on dev %s size %d offset %d\n",
+		block, kdevname(tmp_dev->dev), tmp_dev->size, tmp_dev->offset);
   
-  *rdev=tmp_dev->dev;
-  *rsector=(block-(tmp_dev->offset)) << 1;
+	*rdev = tmp_dev->dev;
+	*rsector = (block - tmp_dev->offset) << 1;
 
-  return (0);
+	return 0;
 }
 
-static int linear_status (char *page, int minor, struct md_dev *mddev)
+static int linear_status (char *page, mddev_t *mddev)
 {
-  int sz=0;
+	int sz=0;
 
 #undef MD_DEBUG
 #ifdef MD_DEBUG
-  int j;
-  struct linear_data *data=(struct linear_data *) mddev->private;
+	int j;
+	linear_conf_t *conf = mddev_to_conf(mddev);
   
-  sz+=sprintf (page+sz, "      ");
-  for (j=0; j<data->nr_zones; j++)
-  {
-    sz+=sprintf (page+sz, "[%s",
-		 partition_name (data->hash_table[j].dev0->dev));
-
-    if (data->hash_table[j].dev1)
-      sz+=sprintf (page+sz, "/%s] ",
-		   partition_name(data->hash_table[j].dev1->dev));
-    else
-      sz+=sprintf (page+sz, "] ");
-  }
-
-  sz+=sprintf (page+sz, "\n");
+	sz += sprintf(page+sz, "      ");
+	for (j = 0; j < conf->nr_zones; j++)
+	{
+		sz += sprintf(page+sz, "[%s",
+			partition_name(conf->hash_table[j].dev0->dev));
+
+		if (conf->hash_table[j].dev1)
+			sz += sprintf(page+sz, "/%s] ",
+			  partition_name(conf->hash_table[j].dev1->dev));
+		else
+			sz += sprintf(page+sz, "] ");
+	}
+	sz += sprintf(page+sz, "\n");
 #endif
-  sz+=sprintf (page+sz, " %dk rounding", 1<<FACTOR_SHIFT(FACTOR(mddev)));
-  return sz;
+	sz += sprintf(page+sz, " %dk rounding", mddev->param.chunk_size/1024);
+	return sz;
 }
 
 
-static struct md_personality linear_personality=
+static mdk_personality_t linear_personality=
 {
-  "linear",
-  linear_map,
-  NULL,
-  NULL,
-  linear_run,
-  linear_stop,
-  linear_status,
-  NULL,				/* no ioctls */
-  0
+	"linear",
+	linear_map,
+	NULL,
+	NULL,
+	linear_run,
+	linear_stop,
+	linear_status,
+	NULL,
+	0,
+	NULL,
+	NULL,
+	NULL,
+	NULL
 };
 
-
 #ifndef MODULE
 
-__initfunc(void linear_init (void))
+md__initfunc(void linear_init (void))
 {
-  register_md_personality (LINEAR, &linear_personality);
+	register_md_personality (LINEAR, &linear_personality);
 }
 
 #else
 
 int init_module (void)
 {
-  return (register_md_personality (LINEAR, &linear_personality));
+	return (register_md_personality (LINEAR, &linear_personality));
 }
 
 void cleanup_module (void)
 {
-  unregister_md_personality (LINEAR);
+	unregister_md_personality (LINEAR);
 }
 
 #endif
+
diff -urN linux/drivers/block/linear.h /tmp/linux/drivers/block/linear.h
--- linux/drivers/block/linear.h	Fri Nov 22 07:07:23 1996
+++ /tmp/linux/drivers/block/linear.h	Wed Dec 31 17:00:00 1969
@@ -1,16 +0,0 @@
-#ifndef _LINEAR_H
-#define _LINEAR_H
-
-struct linear_hash
-{
-  struct real_dev *dev0, *dev1;
-};
-
-struct linear_data
-{
-  struct linear_hash *hash_table; /* Dynamically allocated */
-  struct real_dev *smallest;
-  int nr_zones;
-};
-
-#endif
diff -urN linux/drivers/block/ll_rw_blk.c /tmp/linux/drivers/block/ll_rw_blk.c
--- linux/drivers/block/ll_rw_blk.c	Sun Dec 10 17:49:41 2000
+++ /tmp/linux/drivers/block/ll_rw_blk.c	Fri Feb  2 19:06:00 2001
@@ -23,6 +23,7 @@
 #include <asm/io.h>
 #include <asm/uaccess.h>
 #include <linux/blk.h>
+#include <linux/raid/md.h>
 
 #include <linux/module.h>
 
@@ -53,6 +54,11 @@
 spinlock_t io_request_lock = SPIN_LOCK_UNLOCKED;
 
 /*
+ * per-major idle-IO detection
+ */
+unsigned long io_events[MAX_BLKDEV] = {0, };
+
+/*
  * used to wait on when there are no free requests
  */
 struct wait_queue * wait_for_request;
@@ -583,6 +589,8 @@
 		return;
 	/* Maybe the above fixes it, and maybe it doesn't boot. Life is interesting */
 	lock_buffer(bh);
+	if (!buffer_lowprio(bh))
+		io_events[major]++;
 
 	if (blk_size[major]) {
 		unsigned long maxsector = (blk_size[major][MINOR(bh->b_rdev)] << 1) + 1;
@@ -832,7 +840,7 @@
 		bh[i]->b_rsector=bh[i]->b_blocknr*(bh[i]->b_size >> 9);
 #ifdef CONFIG_BLK_DEV_MD
 		if (major==MD_MAJOR &&
-		    md_map (MINOR(bh[i]->b_dev), &bh[i]->b_rdev,
+			md_map (bh[i]->b_dev, &bh[i]->b_rdev,
 			    &bh[i]->b_rsector, bh[i]->b_size >> 9)) {
 		        printk (KERN_ERR
 				"Bad md_map in ll_rw_block\n");
@@ -852,7 +860,7 @@
 			set_bit(BH_Req, &bh[i]->b_state);
 #ifdef CONFIG_BLK_DEV_MD
 			if (MAJOR(bh[i]->b_dev) == MD_MAJOR) {
-				md_make_request(MINOR (bh[i]->b_dev), rw, bh[i]);
+				md_make_request(bh[i], rw);
 				continue;
 			}
 #endif
diff -urN linux/drivers/block/md.c /tmp/linux/drivers/block/md.c
--- linux/drivers/block/md.c	Mon Sep  4 11:39:16 2000
+++ /tmp/linux/drivers/block/md.c	Fri Feb  2 19:06:00 2001
@@ -1,21 +1,17 @@
-
 /*
    md.c : Multiple Devices driver for Linux
-          Copyright (C) 1994-96 Marc ZYNGIER
-	  <zyngier@ufr-info-p7.ibp.fr> or
-	  <maz@gloups.fdn.fr>
+          Copyright (C) 1998, 1999 Ingo Molnar
 
-   A lot of inspiration came from hd.c ...
+     completely rewritten, based on the MD driver code from Marc Zyngier
 
-   kerneld support by Boris Tobotras <boris@xtalk.msk.su>
-   boot support for linear and striped mode by Harald Hoyer <HarryH@Royal.Net>
+   Changes:
 
-   RAID-1/RAID-5 extensions by:
-        Ingo Molnar, Miguel de Icaza, Gadi Oxman
+   - RAID-1/RAID-5 extensions by Miguel de Icaza, Gadi Oxman, Ingo Molnar
+   - boot support for linear and striped mode by Harald Hoyer <HarryH@Royal.Net>
+   - kerneld support by Boris Tobotras <boris@xtalk.msk.su>
+   - kmod support by: Cyrus Durgin
+   - RAID0 bugfixes: Mark Anthony Lisher <markal@iname.com>
 
-   Changes for kmod by:
-   	Cyrus Durgin
-   
    This program is free software; you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation; either version 2, or (at your option)
@@ -26,807 +22,3007 @@
    Software Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.  
 */
 
-/*
- * Current RAID-1,4,5 parallel reconstruction speed limit is 1024 KB/sec, so
- * the extra system load does not show up that much. Increase it if your
- * system can take more.
- */
-#define SPEED_LIMIT 1024
+#include <linux/raid/md.h>
+#include <linux/raid/xor.h>
 
-#include <linux/config.h>
-#include <linux/module.h>
-#include <linux/version.h>
-#include <linux/malloc.h>
-#include <linux/mm.h>
-#include <linux/md.h>
-#include <linux/hdreg.h>
-#include <linux/stat.h>
-#include <linux/fs.h>
-#include <linux/proc_fs.h>
-#include <linux/blkdev.h>
-#include <linux/genhd.h>
-#include <linux/smp_lock.h>
 #ifdef CONFIG_KMOD
 #include <linux/kmod.h>
 #endif
-#include <linux/errno.h>
-#include <linux/init.h>
 
 #define __KERNEL_SYSCALLS__
 #include <linux/unistd.h>
 
+#include <asm/unaligned.h>
+
+extern asmlinkage int sys_sched_yield(void);
+extern asmlinkage int sys_setsid(void);
+
+extern unsigned long io_events[MAX_BLKDEV];
+
 #define MAJOR_NR MD_MAJOR
 #define MD_DRIVER
 
 #include <linux/blk.h>
-#include <asm/uaccess.h>
-#include <asm/bitops.h>
-#include <asm/atomic.h>
 
 #ifdef CONFIG_MD_BOOT
-extern kdev_t name_to_kdev_t(char *line) __init;
+extern kdev_t name_to_kdev_t(char *line) md__init;
 #endif
 
-static struct hd_struct md_hd_struct[MAX_MD_DEV];
-static int md_blocksizes[MAX_MD_DEV];
-int md_maxreadahead[MAX_MD_DEV];
-#if SUPPORT_RECONSTRUCTION
-static struct md_thread *md_sync_thread = NULL;
-#endif /* SUPPORT_RECONSTRUCTION */
+static mdk_personality_t *pers[MAX_PERSONALITY] = {NULL, };
+
+/*
+ * these have to be allocated separately because external
+ * subsystems want to have a pre-defined structure
+ */
+struct hd_struct md_hd_struct[MAX_MD_DEVS];
+static int md_blocksizes[MAX_MD_DEVS];
+static int md_maxreadahead[MAX_MD_DEVS];
+static mdk_thread_t *md_recovery_thread = NULL;
 
-int md_size[MAX_MD_DEV]={0, };
+int md_size[MAX_MD_DEVS] = {0, };
 
 static void md_geninit (struct gendisk *);
 
 static struct gendisk md_gendisk=
 {
-  MD_MAJOR,
-  "md",
-  0,
-  1,
-  MAX_MD_DEV,
-  md_geninit,
-  md_hd_struct,
-  md_size,
-  MAX_MD_DEV,
-  NULL,
-  NULL
+	MD_MAJOR,
+	"md",
+	0,
+	1,
+	MAX_MD_DEVS,
+	md_geninit,
+	md_hd_struct,
+	md_size,
+	MAX_MD_DEVS,
+	NULL,
+	NULL
 };
 
-static struct md_personality *pers[MAX_PERSONALITY]={NULL, };
-struct md_dev md_dev[MAX_MD_DEV];
-
-int md_thread(void * arg);
+/*
+ * Current RAID-1,4,5 parallel reconstruction 'guaranteed speed limit'
+ * is 100 KB/sec, so the extra system load does not show up that much.
+ * Increase it if you want to have more _guaranteed_ speed. Note that
+ * the RAID driver will use the maximum available bandwith if the IO
+ * subsystem is idle.
+ *
+ * you can change it via /proc/sys/dev/speed-limit
+ */
 
-static struct gendisk *find_gendisk (kdev_t dev)
-{
-  struct gendisk *tmp=gendisk_head;
+static int sysctl_speed_limit = 100;
 
-  while (tmp != NULL)
-  {
-    if (tmp->major==MAJOR(dev))
-      return (tmp);
-    
-    tmp=tmp->next;
-  }
+static struct ctl_table_header *md_table_header;
 
-  return (NULL);
-}
+static ctl_table md_table[] = {
+	{DEV_MD_SPEED_LIMIT, "speed-limit",
+	 &sysctl_speed_limit, sizeof(int), 0644, NULL, &proc_dointvec},
+	{0}
+};
 
-char *partition_name (kdev_t dev)
-{
-  static char name[40];		/* This should be long
-				   enough for a device name ! */
-  struct gendisk *hd = find_gendisk (dev);
+static ctl_table md_dir_table[] = {
+        {DEV_MD, "md", NULL, 0, 0555, md_table},
+        {0}
+};
 
-  if (!hd)
-  {
-    sprintf (name, "[dev %s]", kdevname(dev));
-    return (name);
-  }
+static ctl_table md_root_table[] = {
+        {CTL_DEV, "dev", NULL, 0, 0555, md_dir_table},
+        {0}
+};
 
-  return disk_name (hd, MINOR(dev), name);  /* routine in genhd.c */
+static void md_register_sysctl(void)
+{
+        md_table_header = register_sysctl_table(md_root_table, 1);
 }
 
-static int legacy_raid_sb (int minor, int pnum)
+void md_unregister_sysctl(void)
 {
-	int i, factor;
+        unregister_sysctl_table(md_table_header);
+}
+
+/*
+ * The mapping between kdev and mddev is not necessary a simple
+ * one! Eg. HSM uses several sub-devices to implement Logical
+ * Volumes. All these sub-devices map to the same mddev.
+ */
+dev_mapping_t mddev_map [MAX_MD_DEVS] = { {NULL, 0}, };
 
-	factor = 1 << FACTOR_SHIFT(FACTOR((md_dev+minor)));
+void add_mddev_mapping (mddev_t * mddev, kdev_t dev, void *data)
+{
+	unsigned int minor = MINOR(dev);
 
-	/*****
-	 * do size and offset calculations.
-	 */
-	for (i=0; i<md_dev[minor].nb_dev; i++) {
-		md_dev[minor].devices[i].size &= ~(factor - 1);
-		md_size[minor] += md_dev[minor].devices[i].size;
-		md_dev[minor].devices[i].offset=i ? (md_dev[minor].devices[i-1].offset + 
-							md_dev[minor].devices[i-1].size) : 0;
+	if (MAJOR(dev) != MD_MAJOR) {
+		MD_BUG();
+		return;
 	}
-	if (pnum == RAID0 >> PERSONALITY_SHIFT)
-		md_maxreadahead[minor] = MD_DEFAULT_DISK_READAHEAD * md_dev[minor].nb_dev;
-	return 0;
+	if (mddev_map[minor].mddev != NULL) {
+		MD_BUG();
+		return;
+	}
+	mddev_map[minor].mddev = mddev;
+	mddev_map[minor].data = data;
 }
 
-static void free_sb (struct md_dev *mddev)
+void del_mddev_mapping (mddev_t * mddev, kdev_t dev)
 {
-	int i;
-	struct real_dev *realdev;
+	unsigned int minor = MINOR(dev);
 
-	if (mddev->sb) {
-		free_page((unsigned long) mddev->sb);
-		mddev->sb = NULL;
+	if (MAJOR(dev) != MD_MAJOR) {
+		MD_BUG();
+		return;
 	}
-	for (i = 0; i <mddev->nb_dev; i++) {
-		realdev = mddev->devices + i;
-		if (realdev->sb) {
-			free_page((unsigned long) realdev->sb);
-			realdev->sb = NULL;
-		}
+	if (mddev_map[minor].mddev != mddev) {
+		MD_BUG();
+		return;
 	}
+	mddev_map[minor].mddev = NULL;
+	mddev_map[minor].data = NULL;
 }
 
 /*
- * Check one RAID superblock for generic plausibility
+ * Enables to iterate over all existing md arrays
  */
+static MD_LIST_HEAD(all_mddevs);
 
-#define BAD_MAGIC KERN_ERR \
-"md: %s: invalid raid superblock magic (%x) on block %u\n"
+static mddev_t * alloc_mddev (kdev_t dev)
+{
+	mddev_t * mddev;
 
-#define OUT_OF_MEM KERN_ALERT \
-"md: out of memory.\n"
+	if (MAJOR(dev) != MD_MAJOR) {
+		MD_BUG();
+		return 0;
+	}
+	mddev = (mddev_t *) kmalloc(sizeof(*mddev), GFP_KERNEL);
+	if (!mddev)
+		return NULL;
+		
+	memset(mddev, 0, sizeof(*mddev));
 
-#define NO_DEVICE KERN_ERR \
-"md: disabled device %s\n"
+	mddev->__minor = MINOR(dev);
+	mddev->reconfig_sem = MUTEX;
+	mddev->recovery_sem = MUTEX;
+	mddev->resync_sem = MUTEX;
+	MD_INIT_LIST_HEAD(&mddev->disks);
+	/*
+	 * The 'base' mddev is the one with data NULL.
+	 * personalities can create additional mddevs 
+	 * if necessary.
+	 */
+	add_mddev_mapping(mddev, dev, 0);
+	md_list_add(&mddev->all_mddevs, &all_mddevs);
 
-#define SUCCESS 0
-#define FAILURE -1
+	return mddev;
+}
 
-static int analyze_one_sb (struct real_dev * rdev)
+static void free_mddev (mddev_t *mddev)
 {
-	int ret = FAILURE;
-	struct buffer_head *bh;
-	kdev_t dev = rdev->dev;
-	md_superblock_t *sb;
+	if (!mddev) {
+		MD_BUG();
+		return;
+	}
 
 	/*
-	 * Read the superblock, it's at the end of the disk
+	 * Make sure nobody else is using this mddev
+	 * (careful, we rely on the global kernel lock here)
 	 */
-	rdev->sb_offset = MD_NEW_SIZE_BLOCKS (blk_size[MAJOR(dev)][MINOR(dev)]);
-	set_blocksize (dev, MD_SB_BYTES);
-	bh = bread (dev, rdev->sb_offset / MD_SB_BLOCKS, MD_SB_BYTES);
-
-	if (bh) {
-		sb = (md_superblock_t *) bh->b_data;
-		if (sb->md_magic != MD_SB_MAGIC) {
-			printk (BAD_MAGIC, kdevname(dev),
-					 sb->md_magic, rdev->sb_offset);
-			goto abort;
-		}
-		rdev->sb = (md_superblock_t *) __get_free_page(GFP_KERNEL);
-		if (!rdev->sb) {
-			printk (OUT_OF_MEM);
-			goto abort;
-		}
-		memcpy (rdev->sb, bh->b_data, MD_SB_BYTES);
+	while (md_atomic_read(&mddev->resync_sem.count) != 1)
+		schedule();
+	while (md_atomic_read(&mddev->recovery_sem.count) != 1)
+		schedule();
 
-		rdev->size = sb->size;
-	} else
-		printk (NO_DEVICE,kdevname(rdev->dev));
-	ret = SUCCESS;
-abort:
-	if (bh)
-		brelse (bh);
-	return ret;
+	del_mddev_mapping(mddev, MKDEV(MD_MAJOR, mdidx(mddev)));
+	md_list_del(&mddev->all_mddevs);
+	MD_INIT_LIST_HEAD(&mddev->all_mddevs);
+	kfree(mddev);
 }
 
-#undef SUCCESS
-#undef FAILURE
-
-#undef BAD_MAGIC
-#undef OUT_OF_MEM
-#undef NO_DEVICE
 
-/*
- * Check a full RAID array for plausibility
- */
+struct gendisk * find_gendisk (kdev_t dev)
+{
+	struct gendisk *tmp = gendisk_head;
 
-#define INCONSISTENT KERN_ERR \
-"md: superblock inconsistency -- run ckraid\n"
+	while (tmp != NULL) {
+		if (tmp->major == MAJOR(dev))
+			return (tmp);
+		tmp = tmp->next;
+	}
+	return (NULL);
+}
 
-#define OUT_OF_DATE KERN_ERR \
-"md: superblock update time inconsistenty -- using the most recent one\n"
+mdk_rdev_t * find_rdev_nr(mddev_t *mddev, int nr)
+{
+	mdk_rdev_t * rdev;
+	struct md_list_head *tmp;
 
-#define OLD_VERSION KERN_ALERT \
-"md: %s: unsupported raid array version %d.%d.%d\n"
+	ITERATE_RDEV(mddev,rdev,tmp) {
+		if (rdev->desc_nr == nr)
+			return rdev;
+	}
+	return NULL;
+}
 
-#define NOT_CLEAN KERN_ERR \
-"md: %s: raid array is not clean -- run ckraid\n"
+mdk_rdev_t * find_rdev(mddev_t * mddev, kdev_t dev)
+{
+	struct md_list_head *tmp;
+	mdk_rdev_t *rdev;
 
-#define NOT_CLEAN_IGNORE KERN_ERR \
-"md: %s: raid array is not clean -- reconstructing parity\n"
+	ITERATE_RDEV(mddev,rdev,tmp) {
+		if (rdev->dev == dev)
+			return rdev;
+	}
+	return NULL;
+}
 
-#define UNKNOWN_LEVEL KERN_ERR \
-"md: %s: unsupported raid level %d\n"
+static MD_LIST_HEAD(device_names);
 
-static int analyze_sbs (int minor, int pnum)
+char * partition_name (kdev_t dev)
 {
-	struct md_dev *mddev = md_dev + minor;
-	int i, N = mddev->nb_dev, out_of_date = 0;
-	struct real_dev * disks = mddev->devices;
-	md_superblock_t *sb, *freshest = NULL;
+	struct gendisk *hd;
+	static char nomem [] = "<nomem>";
+	dev_name_t *dname;
+	struct md_list_head *tmp = device_names.next;
 
-	/*
-	 * RAID-0 and linear don't use a RAID superblock
-	 */
-	if (pnum == RAID0 >> PERSONALITY_SHIFT ||
-		pnum == LINEAR >> PERSONALITY_SHIFT)
-			return legacy_raid_sb (minor, pnum);
+	while (tmp != &device_names) {
+		dname = md_list_entry(tmp, dev_name_t, list);
+		if (dname->dev == dev)
+			return dname->name;
+		tmp = tmp->next;
+	}
+
+	dname = (dev_name_t *) kmalloc(sizeof(*dname), GFP_KERNEL);
 
+	if (!dname)
+		return nomem;
 	/*
-	 * Verify the RAID superblock on each real device
+	 * ok, add this new device name to the list
 	 */
-	for (i = 0; i < N; i++)
-		if (analyze_one_sb(disks+i))
-			goto abort;
+	hd = find_gendisk (dev);
+
+	if (!hd)
+		sprintf (dname->name, "[dev %s]", kdevname(dev));
+	else
+		disk_name (hd, MINOR(dev), dname->name);
+
+	dname->dev = dev;
+	md_list_add(&dname->list, &device_names);
+
+	return dname->name;
+}
+
+static unsigned int calc_dev_sboffset (kdev_t dev, mddev_t *mddev,
+						int persistent)
+{
+	unsigned int size = 0;
+
+	if (blk_size[MAJOR(dev)])
+		size = blk_size[MAJOR(dev)][MINOR(dev)];
+	if (persistent)
+		size = MD_NEW_SIZE_BLOCKS(size);
+	return size;
+}
+
+static unsigned int calc_dev_size (kdev_t dev, mddev_t *mddev, int persistent)
+{
+	unsigned int size;
+
+	size = calc_dev_sboffset(dev, mddev, persistent);
+	if (!mddev->sb) {
+		MD_BUG();
+		return size;
+	}
+	if (mddev->sb->chunk_size)
+		size &= ~(mddev->sb->chunk_size/1024 - 1);
+	return size;
+}
+
+/*
+ * We check wether all devices are numbered from 0 to nb_dev-1. The
+ * order is guaranteed even after device name changes.
+ *
+ * Some personalities (raid0, linear) use this. Personalities that
+ * provide data have to be able to deal with loss of individual
+ * disks, so they do their checking themselves.
+ */
+int md_check_ordering (mddev_t *mddev)
+{
+	int i, c;
+	mdk_rdev_t *rdev;
+	struct md_list_head *tmp;
 
 	/*
-	 * The superblock constant part has to be the same
-	 * for all disks in the array.
+	 * First, all devices must be fully functional
 	 */
-	sb = NULL;
-	for (i = 0; i < N; i++) {
-		if (!disks[i].sb)
-			continue;
-		if (!sb) {
-			sb = disks[i].sb;
-			continue;
-		}
-		if (memcmp(sb,
-			   disks[i].sb, MD_SB_GENERIC_CONSTANT_WORDS * 4)) {
-			printk (INCONSISTENT);
+	ITERATE_RDEV(mddev,rdev,tmp) {
+		if (rdev->faulty) {
+			printk("md: md%d's device %s faulty, aborting.\n",
+				mdidx(mddev), partition_name(rdev->dev));
 			goto abort;
 		}
 	}
 
-	/*
-	 * OK, we have all disks and the array is ready to run. Let's
-	 * find the freshest superblock, that one will be the superblock
-	 * that represents the whole array.
-	 */
-	if ((sb = mddev->sb = (md_superblock_t *) __get_free_page (GFP_KERNEL)) == NULL)
+	c = 0;
+	ITERATE_RDEV(mddev,rdev,tmp) {
+		c++;
+	}
+	if (c != mddev->nb_dev) {
+		MD_BUG();
 		goto abort;
-	freshest = NULL;
-	for (i = 0; i < N; i++) {
-		if (!disks[i].sb)
-			continue;
-		if (!freshest) {
-			freshest = disks[i].sb;
-			continue;
-		}
-		/*
-		 * Find the newest superblock version
-		 */
-		if (disks[i].sb->utime != freshest->utime) {
-			out_of_date = 1;
-			if (disks[i].sb->utime > freshest->utime)
-				freshest = disks[i].sb;
-		}
 	}
-	if (out_of_date)
-		printk(OUT_OF_DATE);
-	memcpy (sb, freshest, sizeof(*freshest));
-
-	/*
-	 * Check if we can support this RAID array
-	 */
-	if (sb->major_version != MD_MAJOR_VERSION ||
-			sb->minor_version > MD_MINOR_VERSION) {
-
-		printk (OLD_VERSION, kdevname(MKDEV(MD_MAJOR, minor)),
-				sb->major_version, sb->minor_version,
-				sb->patch_version);
+	if (mddev->nb_dev != mddev->sb->raid_disks) {
+		printk("md: md%d, array needs %d disks, has %d, aborting.\n",
+			mdidx(mddev), mddev->sb->raid_disks, mddev->nb_dev);
 		goto abort;
 	}
-
 	/*
-	 * We need to add this as a superblock option.
+	 * Now the numbering check
 	 */
-#if SUPPORT_RECONSTRUCTION
-	if (sb->state != (1 << MD_SB_CLEAN)) {
-		if (sb->level == 1) {
-			printk (NOT_CLEAN, kdevname(MKDEV(MD_MAJOR, minor)));
+	for (i = 0; i < mddev->nb_dev; i++) {
+		c = 0;
+		ITERATE_RDEV(mddev,rdev,tmp) {
+			if (rdev->desc_nr == i)
+				c++;
+		}
+		if (c == 0) {
+			printk("md: md%d, missing disk #%d, aborting.\n",
+				mdidx(mddev), i);
 			goto abort;
-		} else
-			printk (NOT_CLEAN_IGNORE, kdevname(MKDEV(MD_MAJOR, minor)));
-	}
-#else
-	if (sb->state != (1 << MD_SB_CLEAN)) {
-		printk (NOT_CLEAN, kdevname(MKDEV(MD_MAJOR, minor)));
-		goto abort;
-	}
-#endif /* SUPPORT_RECONSTRUCTION */
-
-	switch (sb->level) {
-		case 1:
-			md_size[minor] = sb->size;
-			md_maxreadahead[minor] = MD_DEFAULT_DISK_READAHEAD;
-			break;
-		case 4:
-		case 5:
-			md_size[minor] = sb->size * (sb->raid_disks - 1);
-			md_maxreadahead[minor] = MD_DEFAULT_DISK_READAHEAD * (sb->raid_disks - 1);
-			break;
-		default:
-			printk (UNKNOWN_LEVEL, kdevname(MKDEV(MD_MAJOR, minor)),
-					sb->level);
+		}
+		if (c > 1) {
+			printk("md: md%d, too many disks #%d, aborting.\n",
+				mdidx(mddev), i);
 			goto abort;
+		}
 	}
 	return 0;
 abort:
-	free_sb(mddev);
 	return 1;
 }
 
-#undef INCONSISTENT
-#undef OUT_OF_DATE
-#undef OLD_VERSION
-#undef NOT_CLEAN
-#undef OLD_LEVEL
-
-int md_update_sb(int minor)
+static unsigned int zoned_raid_size (mddev_t *mddev)
 {
-	struct md_dev *mddev = md_dev + minor;
-	struct buffer_head *bh;
-	md_superblock_t *sb = mddev->sb;
-	struct real_dev *realdev;
-	kdev_t dev;
-	int i;
-	u32 sb_offset;
+	unsigned int mask;
+	mdk_rdev_t * rdev;
+	struct md_list_head *tmp;
 
-	sb->utime = CURRENT_TIME;
-	for (i = 0; i < mddev->nb_dev; i++) {
-		realdev = mddev->devices + i;
-		if (!realdev->sb)
-			continue;
-		dev = realdev->dev;
-		sb_offset = realdev->sb_offset;
-		set_blocksize(dev, MD_SB_BYTES);
-		printk("md: updating raid superblock on device %s, sb_offset == %u\n", kdevname(dev), sb_offset);
-		bh = getblk(dev, sb_offset / MD_SB_BLOCKS, MD_SB_BYTES);
-		if (bh) {
-			sb = (md_superblock_t *) bh->b_data;
-			memcpy(sb, mddev->sb, MD_SB_BYTES);
-			memcpy(&sb->descriptor, sb->disks + realdev->sb->descriptor.number, MD_SB_DESCRIPTOR_WORDS * 4);
-			mark_buffer_uptodate(bh, 1);
-			mark_buffer_dirty(bh, 1);
-			ll_rw_block(WRITE, 1, &bh);
-			wait_on_buffer(bh);
-			bforget(bh);
-			fsync_dev(dev);
-			invalidate_buffers(dev);
-		} else
-			printk(KERN_ERR "md: getblk failed for device %s\n", kdevname(dev));
+	if (!mddev->sb) {
+		MD_BUG();
+		return -EINVAL;
+	}
+	/*
+	 * do size and offset calculations.
+	 */
+	mask = ~(mddev->sb->chunk_size/1024 - 1);
+printk("mask %08x\n", mask);
+
+	ITERATE_RDEV(mddev,rdev,tmp) {
+printk(" rdev->size: %d\n", rdev->size);
+		rdev->size &= mask;
+printk(" masked rdev->size: %d\n", rdev->size);
+		md_size[mdidx(mddev)] += rdev->size;
+printk("  new md_size: %d\n", md_size[mdidx(mddev)]);
 	}
 	return 0;
 }
 
-static int do_md_run (int minor, int repart)
+static void remove_descriptor (mdp_disk_t *disk, mdp_super_t *sb)
 {
-  int pnum, i, min, factor, err;
+	if (disk_active(disk)) {
+		sb->working_disks--;
+	} else {
+		if (disk_spare(disk)) {
+			sb->spare_disks--;
+			sb->working_disks--;
+		} else	{
+			sb->failed_disks--;
+		}
+	}
+	sb->nr_disks--;
+	disk->major = 0;
+	disk->minor = 0;
+	mark_disk_removed(disk);
+}
 
-  if (!md_dev[minor].nb_dev)
-    return -EINVAL;
-  
-  if (md_dev[minor].pers)
-    return -EBUSY;
+#define BAD_MAGIC KERN_ERR \
+"md: invalid raid superblock magic on %s\n"
 
-  md_dev[minor].repartition=repart;
-  
-  if ((pnum=PERSONALITY(&md_dev[minor]) >> (PERSONALITY_SHIFT))
-      >= MAX_PERSONALITY)
-    return -EINVAL;
-
-  /* Only RAID-1 and RAID-5 can have MD devices as underlying devices */
-  if (pnum != (RAID1 >> PERSONALITY_SHIFT) && pnum != (RAID5 >> PERSONALITY_SHIFT)){
-	  for (i = 0; i < md_dev [minor].nb_dev; i++)
-		  if (MAJOR (md_dev [minor].devices [i].dev) == MD_MAJOR)
-			  return -EINVAL;
-  }
-  if (!pers[pnum])
-  {
-#ifdef CONFIG_KMOD
-    char module_name[80];
-    sprintf (module_name, "md-personality-%d", pnum);
-    request_module (module_name);
-    if (!pers[pnum])
-#endif
-      return -EINVAL;
-  }
-  
-  factor = min = 1 << FACTOR_SHIFT(FACTOR((md_dev+minor)));
-  
-  for (i=0; i<md_dev[minor].nb_dev; i++)
-    if (md_dev[minor].devices[i].size<min)
-    {
-      printk ("Dev %s smaller than %dk, cannot shrink\n",
-	      partition_name (md_dev[minor].devices[i].dev), min);
-      return -EINVAL;
-    }
-
-  for (i=0; i<md_dev[minor].nb_dev; i++) {
-    fsync_dev(md_dev[minor].devices[i].dev);
-    invalidate_buffers(md_dev[minor].devices[i].dev);
-  }
-  
-  /* Resize devices according to the factor. It is used to align
-     partitions size on a given chunk size. */
-  md_size[minor]=0;
-
-  /*
-   * Analyze the raid superblock
-   */ 
-  if (analyze_sbs(minor, pnum))
-    return -EINVAL;
+#define BAD_MINOR KERN_ERR \
+"md: %s: invalid raid minor (%x)\n"
 
-  md_dev[minor].pers=pers[pnum];
-  
-  if ((err=md_dev[minor].pers->run (minor, md_dev+minor)))
-  {
-    md_dev[minor].pers=NULL;
-    free_sb(md_dev + minor);
-    return (err);
-  }
-
-  if (pnum != RAID0 >> PERSONALITY_SHIFT && pnum != LINEAR >> PERSONALITY_SHIFT)
-  {
-    md_dev[minor].sb->state &= ~(1 << MD_SB_CLEAN);
-    md_update_sb(minor);
-  }
-
-  /* FIXME : We assume here we have blocks
-     that are twice as large as sectors.
-     THIS MAY NOT BE TRUE !!! */
-  md_hd_struct[minor].start_sect=0;
-  md_hd_struct[minor].nr_sects=md_size[minor]<<1;
-  
-  read_ahead[MD_MAJOR] = 128;
-  return (0);
-}
+#define OUT_OF_MEM KERN_ALERT \
+"md: out of memory.\n"
+
+#define NO_SB KERN_ERR \
+"md: disabled device %s, could not read superblock.\n"
 
-static int do_md_stop (int minor, struct inode *inode)
+#define BAD_CSUM KERN_WARNING \
+"md: invalid superblock checksum on %s\n"
+
+static int alloc_array_sb (mddev_t * mddev)
 {
-	int i;
-  
-	if (inode->i_count>1 || md_dev[minor].busy>1) {
-		/*
-		 * ioctl : one open channel
-		 */
-		printk ("STOP_MD md%x failed : i_count=%d, busy=%d\n",
-				minor, inode->i_count, md_dev[minor].busy);
-		return -EBUSY;
-	}
-  
-	if (md_dev[minor].pers) {
-		/*
-		 * It is safe to call stop here, it only frees private
-		 * data. Also, it tells us if a device is unstoppable
-		 * (eg. resyncing is in progress)
-		 */
-		if (md_dev[minor].pers->stop (minor, md_dev+minor))
-			return -EBUSY;
-		/*
-		 *  The device won't exist anymore -> flush it now
-		 */
-		fsync_dev (inode->i_rdev);
-		invalidate_buffers (inode->i_rdev);
-		if (md_dev[minor].sb) {
-			md_dev[minor].sb->state |= 1 << MD_SB_CLEAN;
-			md_update_sb(minor);
-		}
+	if (mddev->sb) {
+		MD_BUG();
+		return 0;
 	}
-  
-	/* Remove locks. */
-	if (md_dev[minor].sb)
-	free_sb(md_dev + minor);
-	for (i=0; i<md_dev[minor].nb_dev; i++)
-		clear_inode (md_dev[minor].devices[i].inode);
-
-	md_dev[minor].nb_dev=md_size[minor]=0;
-	md_hd_struct[minor].nr_sects=0;
-	md_dev[minor].pers=NULL;
-  
-	read_ahead[MD_MAJOR] = 128;
-  
-	return (0);
+
+	mddev->sb = (mdp_super_t *) __get_free_page (GFP_KERNEL);
+	if (!mddev->sb)
+		return -ENOMEM;
+	md_clear_page((unsigned long)mddev->sb);
+	return 0;
 }
 
-static int do_md_add (int minor, kdev_t dev)
+static int alloc_disk_sb (mdk_rdev_t * rdev)
 {
-	int i;
-	int hot_add=0;
-	struct real_dev *realdev;
+	if (rdev->sb)
+		MD_BUG();
 
-	if (md_dev[minor].nb_dev==MAX_REAL)
+	rdev->sb = (mdp_super_t *) __get_free_page(GFP_KERNEL);
+	if (!rdev->sb) {
+		printk (OUT_OF_MEM);
 		return -EINVAL;
+	}
+	md_clear_page((unsigned long)rdev->sb);
 
-	if (!fs_may_mount (dev))
-		return -EBUSY;
+	return 0;
+}
 
-	if (blk_size[MAJOR(dev)] == NULL || blk_size[MAJOR(dev)][MINOR(dev)] == 0) {
-		printk("md_add(): zero device size, huh, bailing out.\n");
-		return -EINVAL;
+static void free_disk_sb (mdk_rdev_t * rdev)
+{
+	if (rdev->sb) {
+		free_page((unsigned long) rdev->sb);
+		rdev->sb = NULL;
+		rdev->sb_offset = 0;
+		rdev->size = 0;
+	} else {
+		if (!rdev->faulty)
+			MD_BUG();
 	}
+}
 
-	if (md_dev[minor].pers) {
-		/*
-		 * The array is already running, hot-add the drive, or
-		 * bail out:
-		 */
-		if (!md_dev[minor].pers->hot_add_disk)
-			return -EBUSY;
-		else
-			hot_add=1;
+static void mark_rdev_faulty (mdk_rdev_t * rdev)
+{
+	unsigned long flags;
+
+	if (!rdev) {
+		MD_BUG();
+		return;
 	}
+	save_flags(flags);
+	cli();
+	free_disk_sb(rdev);
+	rdev->faulty = 1;
+	restore_flags(flags);
+}
+
+static int read_disk_sb (mdk_rdev_t * rdev)
+{
+	int ret = -EINVAL;
+	struct buffer_head *bh = NULL;
+	kdev_t dev = rdev->dev;
+	mdp_super_t *sb;
+	u32 sb_offset;
 
+	if (!rdev->sb) {
+		MD_BUG();
+		goto abort;
+	}	
+	
 	/*
-	 * Careful. We cannot increase nb_dev for a running array.
+	 * Calculate the position of the superblock,
+	 * it's at the end of the disk
 	 */
-	i=md_dev[minor].nb_dev;
-	realdev = &md_dev[minor].devices[i];
-	realdev->dev=dev;
-  
-	/* Lock the device by inserting a dummy inode. This doesn't
-	   smell very good, but I need to be consistent with the
-	   mount stuff, specially with fs_may_mount. If someone have
-	   a better idea, please help ! */
-  
-	realdev->inode=get_empty_inode ();
-	realdev->inode->i_dev=dev; 	/* don't care about other fields */
-	insert_inode_hash (realdev->inode);
-  
-	/* Sizes are now rounded at run time */
-  
-/*  md_dev[minor].devices[i].size=gen_real->sizes[MINOR(dev)]; HACKHACK*/
-
-	realdev->size=blk_size[MAJOR(dev)][MINOR(dev)];
+	sb_offset = calc_dev_sboffset(rdev->dev, rdev->mddev, 1);
+	rdev->sb_offset = sb_offset;
+	printk("(read) %s's sb offset: %d", partition_name(dev),
+							 sb_offset);
+	fsync_dev(dev);
+	set_blocksize (dev, MD_SB_BYTES);
+	bh = bread (dev, sb_offset / MD_SB_BLOCKS, MD_SB_BYTES);
 
-	if (hot_add) {
+	if (bh) {
+		sb = (mdp_super_t *) bh->b_data;
+		memcpy (rdev->sb, sb, MD_SB_BYTES);
+	} else {
+		printk (NO_SB,partition_name(rdev->dev));
+		goto abort;
+	}
+	printk(" [events: %08lx]\n", (unsigned long)get_unaligned(&rdev->sb->events));
+	ret = 0;
+abort:
+	if (bh)
+		brelse (bh);
+	return ret;
+}
+
+static unsigned int calc_sb_csum (mdp_super_t * sb)
+{
+	unsigned int disk_csum, csum;
+
+	disk_csum = sb->sb_csum;
+	sb->sb_csum = 0;
+	csum = csum_partial((void *)sb, MD_SB_BYTES, 0);
+	sb->sb_csum = disk_csum;
+	return csum;
+}
+
+/*
+ * Check one RAID superblock for generic plausibility
+ */
+
+static int check_disk_sb (mdk_rdev_t * rdev)
+{
+	mdp_super_t *sb;
+	int ret = -EINVAL;
+
+	sb = rdev->sb;
+	if (!sb) {
+		MD_BUG();
+		goto abort;
+	}
+
+	if (sb->md_magic != MD_SB_MAGIC) {
+		printk (BAD_MAGIC, partition_name(rdev->dev));
+		goto abort;
+	}
+
+	if (sb->md_minor >= MAX_MD_DEVS) {
+		printk (BAD_MINOR, partition_name(rdev->dev),
+							sb->md_minor);
+		goto abort;
+	}
+
+	if (calc_sb_csum(sb) != sb->sb_csum)
+		printk(BAD_CSUM, partition_name(rdev->dev));
+	ret = 0;
+abort:
+	return ret;
+}
+
+static kdev_t dev_unit(kdev_t dev)
+{
+	unsigned int mask;
+	struct gendisk *hd = find_gendisk(dev);
+
+	if (!hd)
+		return 0;
+	mask = ~((1 << hd->minor_shift) - 1);
+
+	return MKDEV(MAJOR(dev), MINOR(dev) & mask);
+}
+
+static mdk_rdev_t * match_dev_unit(mddev_t *mddev, kdev_t dev)
+{
+	struct md_list_head *tmp;
+	mdk_rdev_t *rdev;
+
+	ITERATE_RDEV(mddev,rdev,tmp)
+		if (dev_unit(rdev->dev) == dev_unit(dev))
+			return rdev;
+
+	return NULL;
+}
+
+static int match_mddev_units(mddev_t *mddev1, mddev_t *mddev2)
+{
+	struct md_list_head *tmp;
+	mdk_rdev_t *rdev;
+
+	ITERATE_RDEV(mddev1,rdev,tmp)
+		if (match_dev_unit(mddev2, rdev->dev))
+			return 1;
+
+	return 0;
+}
+
+static MD_LIST_HEAD(all_raid_disks);
+static MD_LIST_HEAD(pending_raid_disks);
+
+static void bind_rdev_to_array (mdk_rdev_t * rdev, mddev_t * mddev)
+{
+	mdk_rdev_t *same_pdev;
+
+	if (rdev->mddev) {
+		MD_BUG();
+		return;
+	}
+	same_pdev = match_dev_unit(mddev, rdev->dev);
+	if (same_pdev)
+		printk( KERN_WARNING
+"md%d: WARNING: %s appears to be on the same physical disk as %s. True\n"
+"     protection against single-disk failure might be compromised.\n",
+ 			mdidx(mddev), partition_name(rdev->dev),
+				partition_name(same_pdev->dev));
+		
+	md_list_add(&rdev->same_set, &mddev->disks);
+	rdev->mddev = mddev;
+	mddev->nb_dev++;
+	printk("bind<%s,%d>\n", partition_name(rdev->dev), mddev->nb_dev);
+}
+
+static void unbind_rdev_from_array (mdk_rdev_t * rdev)
+{
+	if (!rdev->mddev) {
+		MD_BUG();
+		return;
+	}
+	md_list_del(&rdev->same_set);
+	MD_INIT_LIST_HEAD(&rdev->same_set);
+	rdev->mddev->nb_dev--;
+	printk("unbind<%s,%d>\n", partition_name(rdev->dev),
+						 rdev->mddev->nb_dev);
+	rdev->mddev = NULL;
+}
+
+/*
+ * prevent the device from being mounted, repartitioned or
+ * otherwise reused by a RAID array (or any other kernel
+ * subsystem), by opening the device. [simply getting an
+ * inode is not enough, the SCSI module usage code needs
+ * an explicit open() on the device]
+ */
+static int lock_rdev (mdk_rdev_t *rdev)
+{
+	int err = 0;
+
+	/*
+	 * First insert a dummy inode.
+	 */
+	if (rdev->inode)
+		MD_BUG();
+	rdev->inode = get_empty_inode();
+ 	/*
+	 * we dont care about any other fields
+	 */
+	rdev->inode->i_dev = rdev->inode->i_rdev = rdev->dev;
+	insert_inode_hash(rdev->inode);
+
+	memset(&rdev->filp, 0, sizeof(rdev->filp));
+	rdev->filp.f_mode = 3; /* read write */
+	err = blkdev_open(rdev->inode, &rdev->filp);
+	if (err) {
+		printk("blkdev_open() failed: %d\n", err);
+		clear_inode(rdev->inode);
+		rdev->inode = NULL;
+	}
+	return err;
+}
+
+static void unlock_rdev (mdk_rdev_t *rdev)
+{
+	blkdev_release(rdev->inode);
+	if (!rdev->inode)
+		MD_BUG();
+	clear_inode(rdev->inode);
+	rdev->inode = NULL;
+}
+
+static void export_rdev (mdk_rdev_t * rdev)
+{
+	printk("export_rdev(%s)\n",partition_name(rdev->dev));
+	if (rdev->mddev)
+		MD_BUG();
+	unlock_rdev(rdev);
+	free_disk_sb(rdev);
+	md_list_del(&rdev->all);
+	MD_INIT_LIST_HEAD(&rdev->all);
+	if (rdev->pending.next != &rdev->pending) {
+		printk("(%s was pending)\n",partition_name(rdev->dev));
+		md_list_del(&rdev->pending);
+		MD_INIT_LIST_HEAD(&rdev->pending);
+	}
+	rdev->dev = 0;
+	rdev->faulty = 0;
+	kfree(rdev);
+}
+
+static void kick_rdev_from_array (mdk_rdev_t * rdev)
+{
+	unbind_rdev_from_array(rdev);
+	export_rdev(rdev);
+}
+
+static void export_array (mddev_t *mddev)
+{
+	struct md_list_head *tmp;
+	mdk_rdev_t *rdev;
+	mdp_super_t *sb = mddev->sb;
+
+	if (mddev->sb) {
+		mddev->sb = NULL;
+		free_page((unsigned long) sb);
+	}
+
+	ITERATE_RDEV(mddev,rdev,tmp) {
+		if (!rdev->mddev) {
+			MD_BUG();
+			continue;
+		}
+		kick_rdev_from_array(rdev);
+	}
+	if (mddev->nb_dev)
+		MD_BUG();
+}
+
+#undef BAD_CSUM
+#undef BAD_MAGIC
+#undef OUT_OF_MEM
+#undef NO_SB
+
+static void print_desc(mdp_disk_t *desc)
+{
+	printk(" DISK<N:%d,%s(%d,%d),R:%d,S:%d>\n", desc->number,
+		partition_name(MKDEV(desc->major,desc->minor)),
+		desc->major,desc->minor,desc->raid_disk,desc->state);
+}
+
+static void print_sb(mdp_super_t *sb)
+{
+	int i;
+
+	printk("  SB: (V:%d.%d.%d) ID:<%08x.%08x.%08x.%08x> CT:%08x\n",
+		sb->major_version, sb->minor_version, sb->patch_version,
+		sb->set_uuid0, sb->set_uuid1, sb->set_uuid2, sb->set_uuid3,
+		sb->ctime);
+	printk("     L%d S%08d ND:%d RD:%d md%d LO:%d CS:%d\n", sb->level,
+		sb->size, sb->nr_disks, sb->raid_disks, sb->md_minor,
+		sb->layout, sb->chunk_size);
+	printk("     UT:%08x ST:%d AD:%d WD:%d FD:%d SD:%d CSUM:%08x E:%08lx\n",
+		sb->utime, sb->state, sb->active_disks, sb->working_disks,
+		sb->failed_disks, sb->spare_disks,
+		sb->sb_csum, (unsigned long)get_unaligned(&sb->events));
+
+	for (i = 0; i < MD_SB_DISKS; i++) {
+		mdp_disk_t *desc;
+
+		desc = sb->disks + i;
+		printk("     D %2d: ", i);
+		print_desc(desc);
+	}
+	printk("     THIS: ");
+	print_desc(&sb->this_disk);
+
+}
+
+static void print_rdev(mdk_rdev_t *rdev)
+{
+	printk(" rdev %s: O:%s, SZ:%08d F:%d DN:%d ",
+		partition_name(rdev->dev), partition_name(rdev->old_dev),
+		rdev->size, rdev->faulty, rdev->desc_nr);
+	if (rdev->sb) {
+		printk("rdev superblock:\n");
+		print_sb(rdev->sb);
+	} else
+		printk("no rdev superblock!\n");
+}
+
+void md_print_devices (void)
+{
+	struct md_list_head *tmp, *tmp2;
+	mdk_rdev_t *rdev;
+	mddev_t *mddev;
+
+	printk("\n");
+	printk("       **********************************\n");
+	printk("       * <COMPLETE RAID STATE PRINTOUT> *\n");
+	printk("       **********************************\n");
+	ITERATE_MDDEV(mddev,tmp) {
+		printk("md%d: ", mdidx(mddev));
+
+		ITERATE_RDEV(mddev,rdev,tmp2)
+			printk("<%s>", partition_name(rdev->dev));
+
+		if (mddev->sb) {
+			printk(" array superblock:\n");
+			print_sb(mddev->sb);
+		} else
+			printk(" no array superblock.\n");
+
+		ITERATE_RDEV(mddev,rdev,tmp2)
+			print_rdev(rdev);
+	}
+	printk("       **********************************\n");
+	printk("\n");
+}
+
+static int sb_equal ( mdp_super_t *sb1, mdp_super_t *sb2)
+{
+	int ret;
+	mdp_super_t *tmp1, *tmp2;
+
+	tmp1 = kmalloc(sizeof(*tmp1),GFP_KERNEL);
+	tmp2 = kmalloc(sizeof(*tmp2),GFP_KERNEL);
+
+	if (!tmp1 || !tmp2) {
+		ret = 0;
+		goto abort;
+	}
+
+	*tmp1 = *sb1;
+	*tmp2 = *sb2;
+
+	/*
+	 * nr_disks is not constant
+	 */
+	tmp1->nr_disks = 0;
+	tmp2->nr_disks = 0;
+
+	if (memcmp(tmp1, tmp2, MD_SB_GENERIC_CONSTANT_WORDS * 4))
+		ret = 0;
+	else
+		ret = 1;
+
+abort:
+	if (tmp1)
+		kfree(tmp1);
+	if (tmp2)
+		kfree(tmp2);
+
+	return ret;
+}
+
+static int uuid_equal(mdk_rdev_t *rdev1, mdk_rdev_t *rdev2)
+{
+	if (	(rdev1->sb->set_uuid0 == rdev2->sb->set_uuid0) &&
+		(rdev1->sb->set_uuid1 == rdev2->sb->set_uuid1) &&
+		(rdev1->sb->set_uuid2 == rdev2->sb->set_uuid2) &&
+		(rdev1->sb->set_uuid3 == rdev2->sb->set_uuid3))
+
+		return 1;
+
+	return 0;
+}
+
+static mdk_rdev_t * find_rdev_all (kdev_t dev)
+{
+	struct md_list_head *tmp;
+	mdk_rdev_t *rdev;
+
+	tmp = all_raid_disks.next;
+	while (tmp != &all_raid_disks) {
+		rdev = md_list_entry(tmp, mdk_rdev_t, all);
+		if (rdev->dev == dev)
+			return rdev;
+		tmp = tmp->next;
+	}
+	return NULL;
+}
+
+#define GETBLK_FAILED KERN_ERR \
+"md: getblk failed for device %s\n"
+
+static int write_disk_sb(mdk_rdev_t * rdev)
+{
+	struct buffer_head *bh;
+	kdev_t dev;
+	u32 sb_offset, size;
+	mdp_super_t *sb;
+
+	if (!rdev->sb) {
+		MD_BUG();
+		return -1;
+	}
+	if (rdev->faulty) {
+		MD_BUG();
+		return -1;
+	}
+	if (rdev->sb->md_magic != MD_SB_MAGIC) {
+		MD_BUG();
+		return -1;
+	}
+
+	dev = rdev->dev;
+	sb_offset = calc_dev_sboffset(dev, rdev->mddev, 1);
+	if (rdev->sb_offset != sb_offset) {
+		printk("%s's sb offset has changed from %d to %d, skipping\n", partition_name(dev), rdev->sb_offset, sb_offset);
+		goto skip;
+	}
+	/*
+	 * If the disk went offline meanwhile and it's just a spare, then
+	 * it's size has changed to zero silently, and the MD code does
+	 * not yet know that it's faulty.
+	 */
+	size = calc_dev_size(dev, rdev->mddev, 1);
+	if (size != rdev->size) {
+		printk("%s's size has changed from %d to %d since import, skipping\n", partition_name(dev), rdev->size, size);
+		goto skip;
+	}
+
+	printk("(write) %s's sb offset: %d\n", partition_name(dev), sb_offset);
+	fsync_dev(dev);
+	set_blocksize(dev, MD_SB_BYTES);
+	bh = getblk(dev, sb_offset / MD_SB_BLOCKS, MD_SB_BYTES);
+	if (!bh) {
+		printk(GETBLK_FAILED, partition_name(dev));
+		return 1;
+	}
+	memset(bh->b_data,0,bh->b_size);
+	sb = (mdp_super_t *) bh->b_data;
+	memcpy(sb, rdev->sb, MD_SB_BYTES);
+
+	mark_buffer_uptodate(bh, 1);
+	mark_buffer_dirty(bh, 1);
+	ll_rw_block(WRITE, 1, &bh);
+	wait_on_buffer(bh);
+	brelse(bh);
+	fsync_dev(dev);
+skip:
+	return 0;
+}
+#undef GETBLK_FAILED KERN_ERR
+
+static void set_this_disk(mddev_t *mddev, mdk_rdev_t *rdev)
+{
+	int i, ok = 0;
+	mdp_disk_t *desc;
+
+	for (i = 0; i < MD_SB_DISKS; i++) {
+		desc = mddev->sb->disks + i;
+#if 0
+		if (disk_faulty(desc)) {
+			if (MKDEV(desc->major,desc->minor) == rdev->dev)
+				ok = 1;
+			continue;
+		}
+#endif
+		if (MKDEV(desc->major,desc->minor) == rdev->dev) {
+			rdev->sb->this_disk = *desc;
+			rdev->desc_nr = desc->number;
+			ok = 1;
+			break;
+		}
+	}
+
+	if (!ok) {
+		MD_BUG();
+	}
+}
+
+static int sync_sbs(mddev_t * mddev)
+{
+	mdk_rdev_t *rdev;
+	mdp_super_t *sb;
+        struct md_list_head *tmp;
+
+        ITERATE_RDEV(mddev,rdev,tmp) {
+		if (rdev->faulty)
+			continue;
+		sb = rdev->sb;
+		*sb = *mddev->sb;
+		set_this_disk(mddev, rdev);
+		sb->sb_csum = calc_sb_csum(sb);
+	}
+	return 0;
+}
+
+int md_update_sb(mddev_t * mddev)
+{
+	int first, err, count = 100;
+        struct md_list_head *tmp;
+	mdk_rdev_t *rdev;
+	__u64 ev;
+
+repeat:
+	mddev->sb->utime = CURRENT_TIME;
+	ev = get_unaligned(&mddev->sb->events);
+	++ev;
+	put_unaligned(ev,&mddev->sb->events);
+	if (ev == (__u64)0) {
+		/*
+		 * oops, this 64-bit counter should never wrap.
+		 * Either we are in around ~1 trillion A.C., assuming
+		 * 1 reboot per second, or we have a bug:
+		 */
+		MD_BUG();
+		--ev;
+		put_unaligned(ev,&mddev->sb->events);
+	}
+	sync_sbs(mddev);
+
+	/*
+	 * do not write anything to disk if using
+	 * nonpersistent superblocks
+	 */
+	if (mddev->sb->not_persistent)
+		return 0;
+
+	printk(KERN_INFO "md: updating md%d RAID superblock on device\n",
+					mdidx(mddev));
+
+	first = 1;
+	err = 0;
+        ITERATE_RDEV(mddev,rdev,tmp) {
+		if (!first) {
+			first = 0;
+			printk(", ");
+		}
+		if (rdev->faulty)
+			printk("(skipping faulty ");
+		printk("%s ", partition_name(rdev->dev));
+		if (!rdev->faulty) {
+			printk("[events: %08lx]",
+			       (unsigned long)get_unaligned(&rdev->sb->events));
+			err += write_disk_sb(rdev);
+		} else
+			printk(")\n");
+	}
+	printk(".\n");
+	if (err) {
+		printk("errors occured during superblock update, repeating\n");
+		if (--count)
+			goto repeat;
+		printk("excessive errors occured during superblock update, exiting\n");
+	}
+	return 0;
+}
+
+/*
+ * Import a device. If 'on_disk', then sanity check the superblock
+ *
+ * mark the device faulty if:
+ *
+ *   - the device is nonexistent (zero size)
+ *   - the device has no valid superblock
+ *
+ * a faulty rdev _never_ has rdev->sb set.
+ */
+static int md_import_device (kdev_t newdev, int on_disk)
+{
+	int err;
+	mdk_rdev_t *rdev;
+	unsigned int size;
+
+	if (find_rdev_all(newdev))
+		return -EEXIST;
+
+	rdev = (mdk_rdev_t *) kmalloc(sizeof(*rdev), GFP_KERNEL);
+	if (!rdev) {
+		printk("could not alloc mem for %s!\n", partition_name(newdev));
+		return -ENOMEM;
+	}
+	memset(rdev, 0, sizeof(*rdev));
+
+	if (!fs_may_mount(newdev)) {
+		printk("md: can not import %s, has active inodes!\n",
+			partition_name(newdev));
+		err = -EBUSY;
+		goto abort_free;
+	}
+
+	if ((err = alloc_disk_sb(rdev)))
+		goto abort_free;
+
+	rdev->dev = newdev;
+	if (lock_rdev(rdev)) {
+		printk("md: could not lock %s, zero-size? Marking faulty.\n",
+			partition_name(newdev));
+		err = -EINVAL;
+		goto abort_free;
+	}
+	rdev->desc_nr = -1;
+	rdev->faulty = 0;
+
+	size = 0;
+	if (blk_size[MAJOR(newdev)])
+		size = blk_size[MAJOR(newdev)][MINOR(newdev)];
+	if (!size) {
+		printk("md: %s has zero size, marking faulty!\n",
+				partition_name(newdev));
+		err = -EINVAL;
+		goto abort_free;
+	}
+
+	if (on_disk) {
+		if ((err = read_disk_sb(rdev))) {
+			printk("md: could not read %s's sb, not importing!\n",
+					partition_name(newdev));
+			goto abort_free;
+		}
+		if ((err = check_disk_sb(rdev))) {
+			printk("md: %s has invalid sb, not importing!\n",
+					partition_name(newdev));
+			goto abort_free;
+		}
+
+		rdev->old_dev = MKDEV(rdev->sb->this_disk.major,
+					rdev->sb->this_disk.minor);
+		rdev->desc_nr = rdev->sb->this_disk.number;
+	}
+	md_list_add(&rdev->all, &all_raid_disks);
+	MD_INIT_LIST_HEAD(&rdev->pending);
+
+	if (rdev->faulty && rdev->sb)
+		free_disk_sb(rdev);
+	return 0;
+
+abort_free:
+	if (rdev->sb) {
+		if (rdev->inode)
+			unlock_rdev(rdev);
+		free_disk_sb(rdev);
+	}
+	kfree(rdev);
+	return err;
+}
+
+/*
+ * Check a full RAID array for plausibility
+ */
+
+#define INCONSISTENT KERN_ERR \
+"md: fatal superblock inconsistency in %s -- removing from array\n"
+
+#define OUT_OF_DATE KERN_ERR \
+"md: superblock update time inconsistency -- using the most recent one\n"
+
+#define OLD_VERSION KERN_ALERT \
+"md: md%d: unsupported raid array version %d.%d.%d\n"
+
+#define NOT_CLEAN_IGNORE KERN_ERR \
+"md: md%d: raid array is not clean -- starting background reconstruction\n"
+
+#define UNKNOWN_LEVEL KERN_ERR \
+"md: md%d: unsupported raid level %d\n"
+
+static int analyze_sbs (mddev_t * mddev)
+{
+	int out_of_date = 0, i;
+	struct md_list_head *tmp, *tmp2;
+	mdk_rdev_t *rdev, *rdev2, *freshest;
+	mdp_super_t *sb;
+
+	/*
+	 * Verify the RAID superblock on each real device
+	 */
+	ITERATE_RDEV(mddev,rdev,tmp) {
+		if (rdev->faulty) {
+			MD_BUG();
+			goto abort;
+		}
+		if (!rdev->sb) {
+			MD_BUG();
+			goto abort;
+		}
+		if (check_disk_sb(rdev))
+			goto abort;
+	}
+
+	/*
+	 * The superblock constant part has to be the same
+	 * for all disks in the array.
+	 */
+	sb = NULL;
+
+	ITERATE_RDEV(mddev,rdev,tmp) {
+		if (!sb) {
+			sb = rdev->sb;
+			continue;
+		}
+		if (!sb_equal(sb, rdev->sb)) {
+			printk (INCONSISTENT, partition_name(rdev->dev));
+			kick_rdev_from_array(rdev);
+			continue;
+		}
+	}
+
+	/*
+	 * OK, we have all disks and the array is ready to run. Let's
+	 * find the freshest superblock, that one will be the superblock
+	 * that represents the whole array.
+	 */
+	if (!mddev->sb)
+		if (alloc_array_sb(mddev))
+			goto abort;
+	sb = mddev->sb;
+	freshest = NULL;
+
+	ITERATE_RDEV(mddev,rdev,tmp) {
+		__u64 ev1, ev2;
+		/*
+		 * if the checksum is invalid, use the superblock
+		 * only as a last resort. (decrease it's age by
+		 * one event)
+		 */
+		if (calc_sb_csum(rdev->sb) != rdev->sb->sb_csum) {
+			__u64 ev = get_unaligned(&rdev->sb->events);
+			if (ev != (__u64)0) {
+				--ev;
+				put_unaligned(ev,&rdev->sb->events);
+			}
+		}
+
+		printk("%s's event counter: %08lx\n", partition_name(rdev->dev),
+		       (unsigned long)get_unaligned(&rdev->sb->events));
+		if (!freshest) {
+			freshest = rdev;
+			continue;
+		}
+		/*
+		 * Find the newest superblock version
+		 */
+		ev1 = get_unaligned(&rdev->sb->events);
+		ev2 = get_unaligned(&freshest->sb->events);
+		if (ev1 != ev2) {
+			out_of_date = 1;
+			if (ev1 > ev2)
+				freshest = rdev;
+		}
+	}
+	if (out_of_date) {
+		printk(OUT_OF_DATE);
+		printk("freshest: %s\n", partition_name(freshest->dev));
+	}
+	memcpy (sb, freshest->sb, sizeof(*sb));
+
+	/*
+	 * at this point we have picked the 'best' superblock
+	 * from all available superblocks.
+	 * now we validate this superblock and kick out possibly
+	 * failed disks.
+	 */
+	ITERATE_RDEV(mddev,rdev,tmp) {
+		/*
+		 * Kick all non-fresh devices faulty
+		 */
+		__u64 ev1, ev2;
+		ev1 = get_unaligned(&rdev->sb->events);
+		ev2 = get_unaligned(&sb->events);
+		++ev1;
+		if (ev1 < ev2) {
+			printk("md: kicking non-fresh %s from array!\n",
+						partition_name(rdev->dev));
+			kick_rdev_from_array(rdev);
+			continue;
+		}
+	}
+
+	/*
+	 * Fix up changed device names ... but only if this disk has a
+	 * recent update time. Use faulty checksum ones too.
+	 */
+	ITERATE_RDEV(mddev,rdev,tmp) {
+		__u64 ev1, ev2, ev3;
+		if (rdev->faulty) { /* REMOVEME */
+			MD_BUG();
+			goto abort;
+		}
+		ev1 = get_unaligned(&rdev->sb->events);
+		ev2 = get_unaligned(&sb->events);
+		ev3 = ev2;
+		--ev3;
+		if ((rdev->dev != rdev->old_dev) &&
+		    ((ev1 == ev2) || (ev1 == ev3))) {
+			mdp_disk_t *desc;
+
+			printk("md: device name has changed from %s to %s since last import!\n", partition_name(rdev->old_dev), partition_name(rdev->dev));
+			if (rdev->desc_nr == -1) {
+				MD_BUG();
+				goto abort;
+			}
+			desc = &sb->disks[rdev->desc_nr];
+			if (rdev->old_dev != MKDEV(desc->major, desc->minor)) {
+				MD_BUG();
+				goto abort;
+			}
+			desc->major = MAJOR(rdev->dev);
+			desc->minor = MINOR(rdev->dev);
+			desc = &rdev->sb->this_disk;
+			desc->major = MAJOR(rdev->dev);
+			desc->minor = MINOR(rdev->dev);
+		}
+	}
+
+	/*
+	 * Remove unavailable and faulty devices ...
+	 *
+	 * note that if an array becomes completely unrunnable due to
+	 * missing devices, we do not write the superblock back, so the
+	 * administrator has a chance to fix things up. The removal thus
+	 * only happens if it's nonfatal to the contents of the array.
+	 */
+	for (i = 0; i < MD_SB_DISKS; i++) {
+		int found;
+		mdp_disk_t *desc;
+		kdev_t dev;
+
+		desc = sb->disks + i;
+		dev = MKDEV(desc->major, desc->minor);
+
+		/*
+		 * We kick faulty devices/descriptors immediately.
+		 */
+		if (disk_faulty(desc)) {
+			found = 0;
+			ITERATE_RDEV(mddev,rdev,tmp) {
+				if (rdev->desc_nr != desc->number)
+					continue;
+				printk("md%d: kicking faulty %s!\n",
+					mdidx(mddev),partition_name(rdev->dev));
+				kick_rdev_from_array(rdev);
+				found = 1;
+				break;
+			}
+			if (!found) {
+				if (dev == MKDEV(0,0))
+					continue;
+				printk("md%d: removing former faulty %s!\n",
+					mdidx(mddev), partition_name(dev));
+			}
+			remove_descriptor(desc, sb);
+			continue;
+		}
+
+		if (dev == MKDEV(0,0))
+			continue;
+		/*
+		 * Is this device present in the rdev ring?
+		 */
+		found = 0;
+		ITERATE_RDEV(mddev,rdev,tmp) {
+			if (rdev->desc_nr == desc->number) {
+				found = 1;
+				break;
+			}
+		}
+		if (found) 
+			continue;
+
+		printk("md%d: former device %s is unavailable, removing from array!\n", mdidx(mddev), partition_name(dev));
+		remove_descriptor(desc, sb);
+	}
+
+	/*
+	 * Double check wether all devices mentioned in the
+	 * superblock are in the rdev ring.
+	 */
+	for (i = 0; i < MD_SB_DISKS; i++) {
+		mdp_disk_t *desc;
+		kdev_t dev;
+
+		desc = sb->disks + i;
+		dev = MKDEV(desc->major, desc->minor);
+
+		if (dev == MKDEV(0,0))
+			continue;
+
+		if (disk_faulty(desc)) {
+			MD_BUG();
+			goto abort;
+		}
+
+		rdev = find_rdev(mddev, dev);
+		if (!rdev) {
+			MD_BUG();
+			goto abort;
+		}
+	}
+
+	/*
+	 * Do a final reality check.
+	 */
+	ITERATE_RDEV(mddev,rdev,tmp) {
+		if (rdev->desc_nr == -1) {
+			MD_BUG();
+			goto abort;
+		}
+		/*
+		 * is the desc_nr unique?
+		 */
+		ITERATE_RDEV(mddev,rdev2,tmp2) {
+			if ((rdev2 != rdev) &&
+					(rdev2->desc_nr == rdev->desc_nr)) {
+				MD_BUG();
+				goto abort;
+			}
+		}
+		/*
+		 * is the device unique?
+		 */
+		ITERATE_RDEV(mddev,rdev2,tmp2) {
+			if ((rdev2 != rdev) &&
+					(rdev2->dev == rdev->dev)) {
+				MD_BUG();
+				goto abort;
+			}
+		}
+	}
+
+	/*
+	 * Check if we can support this RAID array
+	 */
+	if (sb->major_version != MD_MAJOR_VERSION ||
+			sb->minor_version > MD_MINOR_VERSION) {
+
+		printk (OLD_VERSION, mdidx(mddev), sb->major_version,
+				sb->minor_version, sb->patch_version);
+		goto abort;
+	}
+
+	if ((sb->state != (1 << MD_SB_CLEAN)) && ((sb->level == 1) ||
+			(sb->level == 4) || (sb->level == 5)))
+		printk (NOT_CLEAN_IGNORE, mdidx(mddev));
+
+	return 0;
+abort:
+	return 1;
+}
+
+#undef INCONSISTENT
+#undef OUT_OF_DATE
+#undef OLD_VERSION
+#undef OLD_LEVEL
+
+static int device_size_calculation (mddev_t * mddev)
+{
+	int data_disks = 0, persistent;
+	unsigned int readahead;
+	mdp_super_t *sb = mddev->sb;
+	struct md_list_head *tmp;
+	mdk_rdev_t *rdev;
+
+	/*
+	 * Do device size calculation. Bail out if too small.
+	 * (we have to do this after having validated chunk_size,
+	 * because device size has to be modulo chunk_size)
+	 */ 
+	persistent = !mddev->sb->not_persistent;
+	ITERATE_RDEV(mddev,rdev,tmp) {
+		if (rdev->faulty)
+			continue;
+		if (rdev->size) {
+			MD_BUG();
+			continue;
+		}
+		rdev->size = calc_dev_size(rdev->dev, mddev, persistent);
+		if (rdev->size < sb->chunk_size / 1024) {
+			printk (KERN_WARNING
+				"Dev %s smaller than chunk_size: %dk < %dk\n",
+				partition_name(rdev->dev),
+				rdev->size, sb->chunk_size / 1024);
+			return -EINVAL;
+		}
+	}
+
+	switch (sb->level) {
+		case -3:
+			data_disks = 1;
+			break;
+		case -2:
+			data_disks = 1;
+			break;
+		case -1:
+			zoned_raid_size(mddev);
+			data_disks = 1;
+			break;
+		case 0:
+			zoned_raid_size(mddev);
+			data_disks = sb->raid_disks;
+			break;
+		case 1:
+			data_disks = 1;
+			break;
+		case 4:
+		case 5:
+			data_disks = sb->raid_disks-1;
+			break;
+		default:
+			printk (UNKNOWN_LEVEL, mdidx(mddev), sb->level);
+			goto abort;
+	}
+	if (!md_size[mdidx(mddev)])
+		md_size[mdidx(mddev)] = sb->size * data_disks;
+
+	readahead = MD_READAHEAD;
+	if ((sb->level == 0) || (sb->level == 4) || (sb->level == 5))
+		readahead = mddev->sb->chunk_size * 4 * data_disks;
+		if (readahead < data_disks * MAX_SECTORS*512*2) 
+			readahead = data_disks * MAX_SECTORS*512*2;
+	else {
+		if (sb->level == -3)
+			readahead = 0;
+	}
+	md_maxreadahead[mdidx(mddev)] = readahead;
+
+	printk(KERN_INFO "md%d: max total readahead window set to %dk\n",
+		mdidx(mddev), readahead/1024);
+
+	printk(KERN_INFO
+		"md%d: %d data-disks, max readahead per data-disk: %dk\n",
+			mdidx(mddev), data_disks, readahead/data_disks/1024);
+	return 0;
+abort:
+	return 1;
+}
+
+
+#define TOO_BIG_CHUNKSIZE KERN_ERR \
+"too big chunk_size: %d > %d\n"
+
+#define TOO_SMALL_CHUNKSIZE KERN_ERR \
+"too small chunk_size: %d < %ld\n"
+
+#define BAD_CHUNKSIZE KERN_ERR \
+"no chunksize specified, see 'man raidtab'\n"
+
+static int do_md_run (mddev_t * mddev)
+{
+	int pnum, err;
+	int chunk_size;
+	struct md_list_head *tmp;
+	mdk_rdev_t *rdev;
+
+
+	if (!mddev->nb_dev) {
+		MD_BUG();
+		return -EINVAL;
+	}
+  
+	if (mddev->pers)
+		return -EBUSY;
+
+	/*
+	 * Resize disks to align partitions size on a given
+	 * chunk size.
+	 */
+	md_size[mdidx(mddev)] = 0;
+
+	/*
+	 * Analyze all RAID superblock(s)
+	 */ 
+	if (analyze_sbs(mddev)) {
+		MD_BUG();
+		return -EINVAL;
+	}
+
+	chunk_size = mddev->sb->chunk_size;
+	pnum = level_to_pers(mddev->sb->level);
+
+	mddev->param.chunk_size = chunk_size;
+	mddev->param.personality = pnum;
+
+	if (chunk_size > MAX_CHUNK_SIZE) {
+		printk(TOO_BIG_CHUNKSIZE, chunk_size, MAX_CHUNK_SIZE);
+		return -EINVAL;
+	}
+	/*
+	 * chunk-size has to be a power of 2 and multiples of PAGE_SIZE
+	 */
+	if ( (1 << ffz(~chunk_size)) != chunk_size) {
+		MD_BUG();
+		return -EINVAL;
+	}
+	if (chunk_size < PAGE_SIZE) {
+		printk(TOO_SMALL_CHUNKSIZE, chunk_size, PAGE_SIZE);
+		return -EINVAL;
+	}
+
+	if (pnum >= MAX_PERSONALITY) {
+		MD_BUG();
+		return -EINVAL;
+	}
+
+	if ((pnum != RAID1) && (pnum != LINEAR) && !chunk_size) {
+		/*
+		 * 'default chunksize' in the old md code used to
+		 * be PAGE_SIZE, baaad.
+		 * we abort here to be on the safe side. We dont
+		 * want to continue the bad practice.
+		 */
+		printk(BAD_CHUNKSIZE);
+		return -EINVAL;
+	}
+
+	if (!pers[pnum])
+	{
+#ifdef CONFIG_KMOD
+		char module_name[80];
+		sprintf (module_name, "md-personality-%d", pnum);
+		request_module (module_name);
+		if (!pers[pnum])
+#endif
+			return -EINVAL;
+	}
+  
+	if (device_size_calculation(mddev))
+		return -EINVAL;
+
+	/*
+	 * Drop all container device buffers, from now on
+	 * the only valid external interface is through the md
+	 * device.
+	 */
+	ITERATE_RDEV(mddev,rdev,tmp) {
+		if (rdev->faulty)
+			continue;
+		fsync_dev(rdev->dev);
+		invalidate_buffers(rdev->dev);
+	}
+  
+	mddev->pers = pers[pnum];
+  
+	err = mddev->pers->run(mddev);
+	if (err) {
+		printk("pers->run() failed ...\n");
+		mddev->pers = NULL;
+		return -EINVAL;
+	}
+
+	mddev->sb->state &= ~(1 << MD_SB_CLEAN);
+	md_update_sb(mddev);
+
+	/*
+	 * md_size has units of 1K blocks, which are
+	 * twice as large as sectors.
+	 */
+	md_hd_struct[mdidx(mddev)].start_sect = 0;
+	md_hd_struct[mdidx(mddev)].nr_sects = md_size[mdidx(mddev)] << 1;
+  
+	read_ahead[MD_MAJOR] = 1024;
+	return (0);
+}
+
+#undef TOO_BIG_CHUNKSIZE
+#undef BAD_CHUNKSIZE
+
+#define OUT(x) do { err = (x); goto out; } while (0)
+
+static int restart_array (mddev_t *mddev)
+{
+	int err = 0;
+ 
+	/*
+	 * Complain if it has no devices
+	 */
+	if (!mddev->nb_dev)
+		OUT(-ENXIO);
+
+	if (mddev->pers) {
+		if (!mddev->ro)
+			OUT(-EBUSY);
+
+		mddev->ro = 0;
+		set_device_ro(mddev_to_kdev(mddev), 0);
+
+		printk (KERN_INFO
+			"md%d switched to read-write mode.\n", mdidx(mddev));
+		/*
+		 * Kick recovery or resync if necessary
+		 */
+		md_recover_arrays();
+		if (mddev->pers->restart_resync)
+			mddev->pers->restart_resync(mddev);
+	} else
+		err = -EINVAL;
+ 
+out:
+	return err;
+}
+
+#define STILL_MOUNTED KERN_WARNING \
+"md: md%d still mounted.\n"
+
+static int do_md_stop (mddev_t * mddev, int ro)
+{
+	int err = 0, resync_interrupted = 0;
+	kdev_t dev = mddev_to_kdev(mddev);
+ 
+	if (!ro && !fs_may_mount (dev)) {
+		printk (STILL_MOUNTED, mdidx(mddev));
+		OUT(-EBUSY);
+	}
+  
+	/*
+	 * complain if it's already stopped
+	 */
+	if (!mddev->nb_dev)
+		OUT(-ENXIO);
+
+	if (mddev->pers) {
+		/*
+		 * It is safe to call stop here, it only frees private
+		 * data. Also, it tells us if a device is unstoppable
+		 * (eg. resyncing is in progress)
+		 */
+		if (mddev->pers->stop_resync)
+			if (mddev->pers->stop_resync(mddev))
+				resync_interrupted = 1;
+
+		if (mddev->recovery_running)
+			md_interrupt_thread(md_recovery_thread);
+
+		/*
+		 * This synchronizes with signal delivery to the
+		 * resync or reconstruction thread. It also nicely
+		 * hangs the process if some reconstruction has not
+		 * finished.
+		 */
+		down(&mddev->recovery_sem);
+		up(&mddev->recovery_sem);
+
+		/*
+		 *  sync and invalidate buffers because we cannot kill the
+		 *  main thread with valid IO transfers still around.
+		 *  the kernel lock protects us from new requests being
+		 *  added after invalidate_buffers().
+		 */
+		fsync_dev (mddev_to_kdev(mddev));
+		fsync_dev (dev);
+		invalidate_buffers (dev);
+
+		if (ro) {
+			if (mddev->ro)
+				OUT(-ENXIO);
+			mddev->ro = 1;
+		} else {
+			if (mddev->ro)
+				set_device_ro(dev, 0);
+			if (mddev->pers->stop(mddev)) {
+				if (mddev->ro)
+					set_device_ro(dev, 1);
+				OUT(-EBUSY);
+			}
+			if (mddev->ro)
+				mddev->ro = 0;
+		}
+		if (mddev->sb) {
+			/*
+			 * mark it clean only if there was no resync
+			 * interrupted.
+			 */
+			if (!mddev->recovery_running && !resync_interrupted) {
+				printk("marking sb clean...\n");
+				mddev->sb->state |= 1 << MD_SB_CLEAN;
+			}
+			md_update_sb(mddev);
+		}
+		if (ro)
+			set_device_ro(dev, 1);
+	}
+ 
+	/*
+	 * Free resources if final stop
+	 */
+	if (!ro) {
+		export_array(mddev);
+		md_size[mdidx(mddev)] = 0;
+		md_hd_struct[mdidx(mddev)].nr_sects = 0;
+		free_mddev(mddev);
+
+		printk (KERN_INFO "md%d stopped.\n", mdidx(mddev));
+	} else
+		printk (KERN_INFO
+			"md%d switched to read-only mode.\n", mdidx(mddev));
+out:
+	return err;
+}
+
+#undef OUT
+
+/*
+ * We have to safely support old arrays too.
+ */
+int detect_old_array (mdp_super_t *sb)
+{
+	if (sb->major_version > 0)
+		return 0;
+	if (sb->minor_version >= 90)
+		return 0;
+
+	return -EINVAL;
+}
+
+
+static void autorun_array (mddev_t *mddev)
+{
+	mdk_rdev_t *rdev;
+        struct md_list_head *tmp;
+	int err;
+
+	if (mddev->disks.prev == &mddev->disks) {
+		MD_BUG();
+		return;
+	}
+
+	printk("running: ");
+
+        ITERATE_RDEV(mddev,rdev,tmp) {
+		printk("<%s>", partition_name(rdev->dev));
+	}
+	printk("\nnow!\n");
+
+	err = do_md_run (mddev);
+	if (err) {
+		printk("do_md_run() returned %d\n", err);
+		/*
+		 * prevent the writeback of an unrunnable array
+		 */
+		mddev->sb_dirty = 0;
+		do_md_stop (mddev, 0);
+	}
+}
+
+/*
+ * lets try to run arrays based on all disks that have arrived
+ * until now. (those are in the ->pending list)
+ *
+ * the method: pick the first pending disk, collect all disks with
+ * the same UUID, remove all from the pending list and put them into
+ * the 'same_array' list. Then order this list based on superblock
+ * update time (freshest comes first), kick out 'old' disks and
+ * compare superblocks. If everything's fine then run it.
+ */
+static void autorun_devices (void)
+{
+	struct md_list_head candidates;
+	struct md_list_head *tmp;
+	mdk_rdev_t *rdev0, *rdev;
+	mddev_t *mddev;
+	kdev_t md_kdev;
+
+
+	printk("autorun ...\n");
+	while (pending_raid_disks.next != &pending_raid_disks) {
+		rdev0 = md_list_entry(pending_raid_disks.next,
+					 mdk_rdev_t, pending);
+
+		printk("considering %s ...\n", partition_name(rdev0->dev));
+		MD_INIT_LIST_HEAD(&candidates);
+		ITERATE_RDEV_PENDING(rdev,tmp) {
+			if (uuid_equal(rdev0, rdev)) {
+				if (!sb_equal(rdev0->sb, rdev->sb)) {
+					printk("%s has same UUID as %s, but superblocks differ ...\n", partition_name(rdev->dev), partition_name(rdev0->dev));
+					continue;
+				}
+				printk("  adding %s ...\n", partition_name(rdev->dev));
+				md_list_del(&rdev->pending);
+				md_list_add(&rdev->pending, &candidates);
+			}
+		}
 		/*
-		 * Check the superblock for consistency.
-		 * The personality itself has to check whether it's getting
-		 * added with the proper flags.  The personality has to be
-                 * checked too. ;)
+		 * now we have a set of devices, with all of them having
+		 * mostly sane superblocks. It's time to allocate the
+		 * mddev.
 		 */
-		if (analyze_one_sb (realdev))
+		md_kdev = MKDEV(MD_MAJOR, rdev0->sb->md_minor);
+		mddev = kdev_to_mddev(md_kdev);
+		if (mddev) {
+			printk("md%d already running, cannot run %s\n",
+				 mdidx(mddev), partition_name(rdev0->dev));
+			ITERATE_RDEV_GENERIC(candidates,pending,rdev,tmp)
+				export_rdev(rdev);
+			continue;
+		}
+		mddev = alloc_mddev(md_kdev);
+		printk("created md%d\n", mdidx(mddev));
+		ITERATE_RDEV_GENERIC(candidates,pending,rdev,tmp) {
+			bind_rdev_to_array(rdev, mddev);
+			md_list_del(&rdev->pending);
+			MD_INIT_LIST_HEAD(&rdev->pending);
+		}
+		autorun_array(mddev);
+	}
+	printk("... autorun DONE.\n");
+}
+
+/*
+ * import RAID devices based on one partition
+ * if possible, the array gets run as well.
+ */
+
+#define BAD_VERSION KERN_ERR \
+"md: %s has RAID superblock version 0.%d, autodetect needs v0.90 or higher\n"
+
+#define OUT_OF_MEM KERN_ALERT \
+"md: out of memory.\n"
+
+#define NO_DEVICE KERN_ERR \
+"md: disabled device %s\n"
+
+#define AUTOADD_FAILED KERN_ERR \
+"md: auto-adding devices to md%d FAILED (error %d).\n"
+
+#define AUTOADD_FAILED_USED KERN_ERR \
+"md: cannot auto-add device %s to md%d, already used.\n"
+
+#define AUTORUN_FAILED KERN_ERR \
+"md: auto-running md%d FAILED (error %d).\n"
+
+#define MDDEV_BUSY KERN_ERR \
+"md: cannot auto-add to md%d, already running.\n"
+
+#define AUTOADDING KERN_INFO \
+"md: auto-adding devices to md%d, based on %s's superblock.\n"
+
+#define AUTORUNNING KERN_INFO \
+"md: auto-running md%d.\n"
+
+static int autostart_array (kdev_t startdev)
+{
+	int err = -EINVAL, i;
+	mdp_super_t *sb = NULL;
+	mdk_rdev_t *start_rdev = NULL, *rdev;
+
+	if (md_import_device(startdev, 1)) {
+		printk("could not import %s!\n", partition_name(startdev));
+		goto abort;
+	}
+
+	start_rdev = find_rdev_all(startdev);
+	if (!start_rdev) {
+		MD_BUG();
+		goto abort;
+	}
+	if (start_rdev->faulty) {
+		printk("can not autostart based on faulty %s!\n",
+						partition_name(startdev));
+		goto abort;
+	}
+	md_list_add(&start_rdev->pending, &pending_raid_disks);
+
+	sb = start_rdev->sb;
+
+	err = detect_old_array(sb);
+	if (err) {
+		printk("array version is too old to be autostarted, use raidtools 0.90 mkraid --upgrade\nto upgrade the array without data loss!\n");
+		goto abort;
+	}
+
+	for (i = 0; i < MD_SB_DISKS; i++) {
+		mdp_disk_t *desc;
+		kdev_t dev;
+
+		desc = sb->disks + i;
+		dev = MKDEV(desc->major, desc->minor);
+
+		if (dev == MKDEV(0,0))
+			continue;
+		if (dev == startdev)
+			continue;
+		if (md_import_device(dev, 1)) {
+			printk("could not import %s, trying to run array nevertheless.\n", partition_name(dev));
+			continue;
+		}
+		rdev = find_rdev_all(dev);
+		if (!rdev) {
+			MD_BUG();
+			goto abort;
+		}
+		md_list_add(&rdev->pending, &pending_raid_disks);
+	}
+
+	/*
+	 * possibly return codes
+	 */
+	autorun_devices();
+	return 0;
+
+abort:
+	if (start_rdev)
+		export_rdev(start_rdev);
+	return err;
+}
+
+#undef BAD_VERSION
+#undef OUT_OF_MEM
+#undef NO_DEVICE
+#undef AUTOADD_FAILED_USED
+#undef AUTOADD_FAILED
+#undef AUTORUN_FAILED
+#undef AUTOADDING
+#undef AUTORUNNING
+
+struct {
+	int set;
+	int noautodetect;
+
+} raid_setup_args md__initdata = { 0, 0 };
+
+/*
+ * Searches all registered partitions for autorun RAID arrays
+ * at boot time.
+ */
+md__initfunc(void autodetect_raid(void))
+{
+#ifdef CONFIG_AUTODETECT_RAID
+	struct gendisk *disk;
+	mdk_rdev_t *rdev;
+	int i;
+
+	if (raid_setup_args.noautodetect) {
+		printk(KERN_INFO "skipping autodetection of RAID arrays\n");
+		return;
+	}
+	printk(KERN_INFO "autodetecting RAID arrays\n");
+
+	for (disk = gendisk_head ; disk ; disk = disk->next) {
+		for (i = 0; i < disk->max_p*disk->max_nr; i++) {
+			kdev_t dev = MKDEV(disk->major,i);
+
+			if (disk->part[i].type == LINUX_OLD_RAID_PARTITION) {
+				printk(KERN_ALERT
+"md: %s's partition type has to be changed from type 0x86 to type 0xfd\n"
+"    to maintain interoperability with other OSs! Autodetection support for\n"
+"    type 0x86 will be deleted after some migration timeout. Sorry.\n",
+					partition_name(dev));
+				disk->part[i].type = LINUX_RAID_PARTITION;
+			}
+			if (disk->part[i].type != LINUX_RAID_PARTITION)
+				continue;
+
+			if (md_import_device(dev,1)) {
+				printk(KERN_ALERT "could not import %s!\n",
+							partition_name(dev));
+				continue;
+			}
+			/*
+			 * Sanity checks:
+			 */
+			rdev = find_rdev_all(dev);
+			if (!rdev) {
+				MD_BUG();
+				continue;
+			}
+			if (rdev->faulty) {
+				MD_BUG();
+				continue;
+			}
+			md_list_add(&rdev->pending, &pending_raid_disks);
+		}
+	}
+
+	autorun_devices();
+#endif
+}
+
+static int get_version (void * arg)
+{
+	mdu_version_t ver;
+
+	ver.major = MD_MAJOR_VERSION;
+	ver.minor = MD_MINOR_VERSION;
+	ver.patchlevel = MD_PATCHLEVEL_VERSION;
+
+	if (md_copy_to_user(arg, &ver, sizeof(ver)))
+		return -EFAULT;
+
+	return 0;
+}
+
+#define SET_FROM_SB(x) info.x = mddev->sb->x
+static int get_array_info (mddev_t * mddev, void * arg)
+{
+	mdu_array_info_t info;
+
+	if (!mddev->sb)
+		return -EINVAL;
+
+	SET_FROM_SB(major_version);
+	SET_FROM_SB(minor_version);
+	SET_FROM_SB(patch_version);
+	SET_FROM_SB(ctime);
+	SET_FROM_SB(level);
+	SET_FROM_SB(size);
+	SET_FROM_SB(nr_disks);
+	SET_FROM_SB(raid_disks);
+	SET_FROM_SB(md_minor);
+	SET_FROM_SB(not_persistent);
+
+	SET_FROM_SB(utime);
+	SET_FROM_SB(state);
+	SET_FROM_SB(active_disks);
+	SET_FROM_SB(working_disks);
+	SET_FROM_SB(failed_disks);
+	SET_FROM_SB(spare_disks);
+
+	SET_FROM_SB(layout);
+	SET_FROM_SB(chunk_size);
+
+	if (md_copy_to_user(arg, &info, sizeof(info)))
+		return -EFAULT;
+
+	return 0;
+}
+#undef SET_FROM_SB
+
+#define SET_FROM_SB(x) info.x = mddev->sb->disks[nr].x
+static int get_disk_info (mddev_t * mddev, void * arg)
+{
+	mdu_disk_info_t info;
+	unsigned int nr;
+
+	if (!mddev->sb)
+		return -EINVAL;
+
+	if (md_copy_from_user(&info, arg, sizeof(info)))
+		return -EFAULT;
+
+	nr = info.number;
+	if (nr >= mddev->sb->nr_disks)
+		return -EINVAL;
+
+	SET_FROM_SB(major);
+	SET_FROM_SB(minor);
+	SET_FROM_SB(raid_disk);
+	SET_FROM_SB(state);
+
+	if (md_copy_to_user(arg, &info, sizeof(info)))
+		return -EFAULT;
+
+	return 0;
+}
+#undef SET_FROM_SB
+
+#define SET_SB(x) mddev->sb->disks[nr].x = info.x
+
+static int add_new_disk (mddev_t * mddev, void * arg)
+{
+	int err, size, persistent;
+	mdu_disk_info_t info;
+	mdk_rdev_t *rdev;
+	unsigned int nr;
+	kdev_t dev;
+
+	if (!mddev->sb)
+		return -EINVAL;
+
+	if (md_copy_from_user(&info, arg, sizeof(info)))
+		return -EFAULT;
+
+	nr = info.number;
+	if (nr >= mddev->sb->nr_disks)
+		return -EINVAL;
+
+	dev = MKDEV(info.major,info.minor);
+
+	if (find_rdev_all(dev)) {
+		printk("device %s already used in a RAID array!\n", 
+				partition_name(dev));
+		return -EBUSY;
+	}
+
+	SET_SB(number);
+	SET_SB(major);
+	SET_SB(minor);
+	SET_SB(raid_disk);
+	SET_SB(state);
+ 
+	if ((info.state & (1<<MD_DISK_FAULTY))==0) {
+		err = md_import_device (dev, 0);
+		if (err) {
+			printk("md: error, md_import_device() returned %d\n", err);
+			return -EINVAL;
+		}
+		rdev = find_rdev_all(dev);
+		if (!rdev) {
+			MD_BUG();
 			return -EINVAL;
+		}
+ 
+		rdev->old_dev = dev;
+		rdev->desc_nr = info.number;
+ 
+		bind_rdev_to_array(rdev, mddev);
+ 
+		persistent = !mddev->sb->not_persistent;
+		if (!persistent)
+			printk("nonpersistent superblock ...\n");
+		if (!mddev->sb->chunk_size)
+			printk("no chunksize?\n");
+ 
+		size = calc_dev_size(dev, mddev, persistent);
+		rdev->sb_offset = calc_dev_sboffset(dev, mddev, persistent);
+ 
+		if (!mddev->sb->size || (mddev->sb->size > size))
+			mddev->sb->size = size;
+	}
+ 
+	/*
+	 * sync all other superblocks with the main superblock
+	 */
+	sync_sbs(mddev);
+
+	return 0;
+}
+#undef SET_SB
+
+static int hot_remove_disk (mddev_t * mddev, kdev_t dev)
+{
+	int err;
+	mdk_rdev_t *rdev;
+	mdp_disk_t *disk;
+
+	if (!mddev->pers)
+		return -ENODEV;
+
+	printk("trying to remove %s from md%d ... \n",
+		partition_name(dev), mdidx(mddev));
+
+	if (!mddev->pers->diskop) {
+		printk("md%d: personality does not support diskops!\n",
+								 mdidx(mddev));
+		return -EINVAL;
+	}
+
+	rdev = find_rdev(mddev, dev);
+	if (!rdev)
+		return -ENXIO;
+
+	if (rdev->desc_nr == -1) {
+		MD_BUG();
+		return -EINVAL;
+	}
+	disk = &mddev->sb->disks[rdev->desc_nr];
+	if (disk_active(disk))
+		goto busy;
+	if (disk_removed(disk)) {
+		MD_BUG();
+		return -EINVAL;
+	}
+	
+	err = mddev->pers->diskop(mddev, &disk, DISKOP_HOT_REMOVE_DISK);
+	if (err == -EBUSY)
+		goto busy;
+	if (err) {
+		MD_BUG();
+		return -EINVAL;
+	}
+
+	remove_descriptor(disk, mddev->sb);
+	kick_rdev_from_array(rdev);
+	mddev->sb_dirty = 1;
+	md_update_sb(mddev);
+
+	return 0;
+busy:
+	printk("cannot remove active disk %s from md%d ... \n",
+		partition_name(dev), mdidx(mddev));
+	return -EBUSY;
+}
+
+static int hot_add_disk (mddev_t * mddev, kdev_t dev)
+{
+	int i, err, persistent;
+	unsigned int size;
+	mdk_rdev_t *rdev;
+	mdp_disk_t *disk;
+
+	if (!mddev->pers)
+		return -ENODEV;
+
+	printk("trying to hot-add %s to md%d ... \n",
+		partition_name(dev), mdidx(mddev));
+
+	if (!mddev->pers->diskop) {
+		printk("md%d: personality does not support diskops!\n",
+								 mdidx(mddev));
+		return -EINVAL;
+	}
+
+	persistent = !mddev->sb->not_persistent;
+	size = calc_dev_size(dev, mddev, persistent);
+
+	if (size < mddev->sb->size) {
+		printk("md%d: disk size %d blocks < array size %d\n",
+				mdidx(mddev), size, mddev->sb->size);
+		return -ENOSPC;
+	}
+
+	rdev = find_rdev(mddev, dev);
+	if (rdev)
+		return -EBUSY;
+
+	err = md_import_device (dev, 0);
+	if (err) {
+		printk("md: error, md_import_device() returned %d\n", err);
+		return -EINVAL;
+	}
+	rdev = find_rdev_all(dev);
+	if (!rdev) {
+		MD_BUG();
+		return -EINVAL;
+	}
+	if (rdev->faulty) {
+		printk("md: can not hot-add faulty %s disk to md%d!\n",
+				partition_name(dev), mdidx(mddev));
+		err = -EINVAL;
+		goto abort_export;
+	}
+	bind_rdev_to_array(rdev, mddev);
+
+	/*
+	 * The rest should better be atomic, we can have disk failures
+	 * noticed in interrupt contexts ...
+	 */
+	cli();
+	rdev->old_dev = dev;
+	rdev->size = size;
+	rdev->sb_offset = calc_dev_sboffset(dev, mddev, persistent);
+
+	disk = mddev->sb->disks + mddev->sb->raid_disks;
+	for (i = mddev->sb->raid_disks; i < MD_SB_DISKS; i++) {
+		disk = mddev->sb->disks + i;
+
+		if (!disk->major && !disk->minor)
+			break;
+		if (disk_removed(disk))
+			break;
+	}
+	if (i == MD_SB_DISKS) {
+		sti();
+		printk("md%d: can not hot-add to full array!\n", mdidx(mddev));
+		err = -EBUSY;
+		goto abort_unbind_export;
+	}
+
+	if (disk_removed(disk)) {
 		/*
-		 * hot_add has to bump up nb_dev itself
+		 * reuse slot
 		 */
-		if (md_dev[minor].pers->hot_add_disk (&md_dev[minor], dev)) {
-			/*
-			 * FIXME: here we should free up the inode and stuff
-			 */
-			printk ("FIXME\n");
-			return -EINVAL;
+		if (disk->number != i) {
+			sti();
+			MD_BUG();
+			err = -EINVAL;
+			goto abort_unbind_export;
 		}
-	} else
-		md_dev[minor].nb_dev++;
+	} else {
+		disk->number = i;
+	}
 
-	printk ("REGISTER_DEV %s to md%x done\n", partition_name(dev), minor);
-	return (0);
+	disk->raid_disk = disk->number;
+	disk->major = MAJOR(dev);
+	disk->minor = MINOR(dev);
+
+	if (mddev->pers->diskop(mddev, &disk, DISKOP_HOT_ADD_DISK)) {
+		sti();
+		MD_BUG();
+		err = -EINVAL;
+		goto abort_unbind_export;
+	}
+
+	mark_disk_spare(disk);
+	mddev->sb->nr_disks++;
+	mddev->sb->spare_disks++;
+	mddev->sb->working_disks++;
+
+	mddev->sb_dirty = 1;
+
+	sti();
+	md_update_sb(mddev);
+
+	/*
+	 * Kick recovery, maybe this spare has to be added to the
+	 * array immediately.
+	 */
+	md_recover_arrays();
+
+	return 0;
+
+abort_unbind_export:
+	unbind_rdev_from_array(rdev);
+
+abort_export:
+	export_rdev(rdev);
+	return err;
+}
+
+#define SET_SB(x) mddev->sb->x = info.x
+static int set_array_info (mddev_t * mddev, void * arg)
+{
+	mdu_array_info_t info;
+
+	if (mddev->sb) {
+		printk("array md%d already has a superblock!\n", 
+				mdidx(mddev));
+		return -EBUSY;
+	}
+
+	if (md_copy_from_user(&info, arg, sizeof(info)))
+		return -EFAULT;
+
+	if (alloc_array_sb(mddev))
+		return -ENOMEM;
+
+	mddev->sb->major_version = MD_MAJOR_VERSION;
+	mddev->sb->minor_version = MD_MINOR_VERSION;
+	mddev->sb->patch_version = MD_PATCHLEVEL_VERSION;
+	mddev->sb->ctime = CURRENT_TIME;
+
+	SET_SB(level);
+	SET_SB(size);
+	SET_SB(nr_disks);
+	SET_SB(raid_disks);
+	SET_SB(md_minor);
+	SET_SB(not_persistent);
+
+	SET_SB(state);
+	SET_SB(active_disks);
+	SET_SB(working_disks);
+	SET_SB(failed_disks);
+	SET_SB(spare_disks);
+
+	SET_SB(layout);
+	SET_SB(chunk_size);
+
+	mddev->sb->md_magic = MD_SB_MAGIC;
+
+	/*
+	 * Generate a 128 bit UUID
+	 */
+	get_random_bytes(&mddev->sb->set_uuid0, 4);
+	get_random_bytes(&mddev->sb->set_uuid1, 4);
+	get_random_bytes(&mddev->sb->set_uuid2, 4);
+	get_random_bytes(&mddev->sb->set_uuid3, 4);
+
+	return 0;
+}
+#undef SET_SB
+
+static int set_disk_info (mddev_t * mddev, void * arg)
+{
+	printk("not yet");
+	return -EINVAL;
+}
+
+static int clear_array (mddev_t * mddev)
+{
+	printk("not yet");
+	return -EINVAL;
+}
+
+static int write_raid_info (mddev_t * mddev)
+{
+	printk("not yet");
+	return -EINVAL;
+}
+
+static int protect_array (mddev_t * mddev)
+{
+	printk("not yet");
+	return -EINVAL;
+}
+
+static int unprotect_array (mddev_t * mddev)
+{
+	printk("not yet");
+	return -EINVAL;
+}
+
+static int set_disk_faulty (mddev_t *mddev, kdev_t dev)
+{
+	int ret;
+
+	fsync_dev(mddev_to_kdev(mddev));
+	ret = md_error(mddev_to_kdev(mddev), dev);
+	return ret;
 }
 
 static int md_ioctl (struct inode *inode, struct file *file,
                      unsigned int cmd, unsigned long arg)
 {
-  int minor, err;
-  struct hd_geometry *loc = (struct hd_geometry *) arg;
+	unsigned int minor;
+	int err = 0;
+	struct hd_geometry *loc = (struct hd_geometry *) arg;
+	mddev_t *mddev = NULL;
+	kdev_t dev;
 
-  if (!capable(CAP_SYS_ADMIN))
-    return -EACCES;
+	if (!md_capable_admin())
+		return -EACCES;
 
-  if (((minor=MINOR(inode->i_rdev)) & 0x80) &&
-      (minor & 0x7f) < MAX_PERSONALITY &&
-      pers[minor & 0x7f] &&
-      pers[minor & 0x7f]->ioctl)
-    return (pers[minor & 0x7f]->ioctl (inode, file, cmd, arg));
-  
-  if (minor >= MAX_MD_DEV)
-    return -EINVAL;
+	dev = inode->i_rdev;
+	minor = MINOR(dev);
+	if (minor >= MAX_MD_DEVS)
+		return -EINVAL;
 
-  switch (cmd)
-  {
-    case REGISTER_DEV:
-      return do_md_add (minor, to_kdev_t ((dev_t) arg));
+	/*
+	 * Commands dealing with the RAID driver but not any
+	 * particular array:
+	 */
+	switch (cmd)
+	{
+		case RAID_VERSION:
+			err = get_version((void *)arg);
+			goto done;
+
+		case PRINT_RAID_DEBUG:
+			err = 0;
+			md_print_devices();
+			goto done_unlock;
+      
+		case BLKGETSIZE:   /* Return device size */
+			if (!arg) {
+				err = -EINVAL;
+				goto abort;
+			}
+			err = md_put_user(md_hd_struct[minor].nr_sects,
+						(long *) arg);
+			goto done;
 
-    case START_MD:
-      return do_md_run (minor, (int) arg);
+		case BLKFLSBUF:
+			fsync_dev(dev);
+			invalidate_buffers(dev);
+			goto done;
 
-    case STOP_MD:
-      return do_md_stop (minor, inode);
-      
-    case BLKGETSIZE:   /* Return device size */
-    if  (!arg)  return -EINVAL;
-    err = put_user (md_hd_struct[MINOR(inode->i_rdev)].nr_sects, (long *) arg);
-    if (err)
-      return err;
-    break;
-
-    case BLKFLSBUF:
-    fsync_dev (inode->i_rdev);
-    invalidate_buffers (inode->i_rdev);
-    break;
-
-    case BLKRASET:
-    if (arg > 0xff)
-      return -EINVAL;
-    read_ahead[MAJOR(inode->i_rdev)] = arg;
-    return 0;
-    
-    case BLKRAGET:
-    if  (!arg)  return -EINVAL;
-    err = put_user (read_ahead[MAJOR(inode->i_rdev)], (long *) arg);
-    if (err)
-      return err;
-    break;
-
-    /* We have a problem here : there is no easy way to give a CHS
-       virtual geometry. We currently pretend that we have a 2 heads
-       4 sectors (with a BIG number of cylinders...). This drives dosfs
-       just mad... ;-) */
-    
-    case HDIO_GETGEO:
-    if (!loc)  return -EINVAL;
-    err = put_user (2, (char *) &loc->heads);
-    if (err)
-      return err;
-    err = put_user (4, (char *) &loc->sectors);
-    if (err)
-      return err;
-    err = put_user (md_hd_struct[minor].nr_sects/8, (short *) &loc->cylinders);
-    if (err)
-      return err;
-    err = put_user (md_hd_struct[MINOR(inode->i_rdev)].start_sect,
-		(long *) &loc->start);
-    if (err)
-      return err;
-    break;
-    
-    RO_IOCTLS(inode->i_rdev,arg);
+		case BLKRASET:
+			if (arg > 0xff) {
+				err = -EINVAL;
+				goto abort;
+			}
+			read_ahead[MAJOR(dev)] = arg;
+			goto done;
     
-    default:
-    return -EINVAL;
-  }
+		case BLKRAGET:
+			if (!arg) {
+				err = -EINVAL;
+				goto abort;
+			}
+			err = md_put_user (read_ahead[
+				MAJOR(dev)], (long *) arg);
+			goto done;
+		default:
+	}
+
+	/*
+	 * Commands creating/starting a new array:
+	 */
+
+	mddev = kdev_to_mddev(dev);
+
+	switch (cmd)
+	{
+		case SET_ARRAY_INFO:
+		case START_ARRAY:
+			if (mddev) {
+				printk("array md%d already exists!\n",
+								mdidx(mddev));
+				err = -EEXIST;
+				goto abort;
+			}
+		default:
+	}
+
+	switch (cmd)
+	{
+		case SET_ARRAY_INFO:
+			mddev = alloc_mddev(dev);
+			if (!mddev) {
+				err = -ENOMEM;
+				goto abort;
+			}
+			/*
+			 * alloc_mddev() should possibly self-lock.
+			 */
+			err = lock_mddev(mddev);
+			if (err) {
+				printk("ioctl, reason %d, cmd %d\n", err, cmd);
+				goto abort;
+			}
+			err = set_array_info(mddev, (void *)arg);
+			if (err) {
+				printk("couldnt set array info. %d\n", err);
+				goto abort;
+			}
+			goto done_unlock;
+
+		case START_ARRAY:
+			/*
+			 * possibly make it lock the array ...
+			 */
+			err = autostart_array((kdev_t)arg);
+			if (err) {
+				printk("autostart %s failed!\n",
+					partition_name((kdev_t)arg));
+				goto abort;
+			}
+			goto done;
+      
+		default:
+	}
+      
+	/*
+	 * Commands querying/configuring an existing array:
+	 */
+
+	if (!mddev) {
+		err = -ENODEV;
+		goto abort;
+	}
+	err = lock_mddev(mddev);
+	if (err) {
+		printk("ioctl lock interrupted, reason %d, cmd %d\n",err, cmd);
+		goto abort;
+	}
+
+	/*
+	 * Commands even a read-only array can execute:
+	 */
+	switch (cmd)
+	{
+		case GET_ARRAY_INFO:
+			err = get_array_info(mddev, (void *)arg);
+			goto done_unlock;
+
+		case GET_DISK_INFO:
+			err = get_disk_info(mddev, (void *)arg);
+			goto done_unlock;
+      
+		case RESTART_ARRAY_RW:
+			err = restart_array(mddev);
+			goto done_unlock;
+
+		case STOP_ARRAY:
+			err = do_md_stop (mddev, 0);
+			goto done_unlock;
+      
+		case STOP_ARRAY_RO:
+			err = do_md_stop (mddev, 1);
+			goto done_unlock;
+      
+	/*
+	 * We have a problem here : there is no easy way to give a CHS
+	 * virtual geometry. We currently pretend that we have a 2 heads
+	 * 4 sectors (with a BIG number of cylinders...). This drives
+	 * dosfs just mad... ;-)
+	 */
+		case HDIO_GETGEO:
+			if (!loc) {
+				err = -EINVAL;
+				goto abort_unlock;
+			}
+			err = md_put_user (2, (char *) &loc->heads);
+			if (err)
+				goto abort_unlock;
+			err = md_put_user (4, (char *) &loc->sectors);
+			if (err)
+				goto abort_unlock;
+			err = md_put_user (md_hd_struct[mdidx(mddev)].nr_sects/8,
+						(short *) &loc->cylinders);
+			if (err)
+				goto abort_unlock;
+			err = md_put_user (md_hd_struct[minor].start_sect,
+						(long *) &loc->start);
+			goto done_unlock;
+	}
 
-  return (0);
+	/*
+	 * The remaining ioctls are changing the state of the
+	 * superblock, so we do not allow read-only arrays
+	 * here:
+	 */
+	if (mddev->ro) {
+		err = -EROFS;
+		goto abort_unlock;
+	}
+
+	switch (cmd)
+	{
+		case CLEAR_ARRAY:
+			err = clear_array(mddev);
+			goto done_unlock;
+      
+		case ADD_NEW_DISK:
+			err = add_new_disk(mddev, (void *)arg);
+			goto done_unlock;
+      
+		case HOT_REMOVE_DISK:
+			err = hot_remove_disk(mddev, (kdev_t)arg);
+			goto done_unlock;
+      
+		case HOT_ADD_DISK:
+			err = hot_add_disk(mddev, (kdev_t)arg);
+			goto done_unlock;
+      
+		case SET_DISK_INFO:
+			err = set_disk_info(mddev, (void *)arg);
+			goto done_unlock;
+      
+		case WRITE_RAID_INFO:
+			err = write_raid_info(mddev);
+			goto done_unlock;
+      
+		case UNPROTECT_ARRAY:
+			err = unprotect_array(mddev);
+			goto done_unlock;
+      
+		case PROTECT_ARRAY:
+			err = protect_array(mddev);
+			goto done_unlock;
+
+		case SET_DISK_FAULTY:
+			err = set_disk_faulty(mddev, (kdev_t)arg);
+			goto done_unlock;
+      
+		case RUN_ARRAY:
+		{
+			mdu_param_t param;
+
+			err = md_copy_from_user(&param, (mdu_param_t *)arg,
+							 sizeof(param));
+			if (err)
+				goto abort_unlock;
+
+			err = do_md_run (mddev);
+			/*
+			 * we have to clean up the mess if
+			 * the array cannot be run for some
+			 * reason ...
+			 */
+			if (err) {
+				mddev->sb_dirty = 0;
+				do_md_stop (mddev, 0);
+			}
+			goto done_unlock;
+		}
+      
+		default:
+			printk(KERN_WARNING "%s(pid %d) used obsolete MD ioctl, upgrade your software to use new ictls.\n", current->comm, current->pid);
+			err = -EINVAL;
+			goto abort_unlock;
+	}
+
+done_unlock:
+abort_unlock:
+	if (mddev)
+		unlock_mddev(mddev);
+	else
+		printk("huh11?\n");
+
+	return err;
+done:
+	if (err)
+		printk("huh12?\n");
+abort:
+	return err;
 }
 
+
+#if LINUX_VERSION_CODE < LinuxVersionCode(2,1,0)
+
 static int md_open (struct inode *inode, struct file *file)
 {
-  int minor=MINOR(inode->i_rdev);
+	/*
+	 * Always succeed
+	 */
+	return (0);
+}
+
+static void md_release (struct inode *inode, struct file *file)
+{
+	sync_dev(inode->i_rdev);
+}
+
+
+static int md_read (struct inode *inode, struct file *file,
+						char *buf, int count)
+{
+	mddev_t *mddev = kdev_to_mddev(MD_FILE_TO_INODE(file)->i_rdev);
 
-  md_dev[minor].busy++;
-  return (0);			/* Always succeed */
+	if (!mddev || !mddev->pers)
+		return -ENXIO;
+
+	return block_read (inode, file, buf, count);
 }
 
+static int md_write (struct inode *inode, struct file *file,
+						const char *buf, int count)
+{
+	mddev_t *mddev = kdev_to_mddev(MD_FILE_TO_INODE(file)->i_rdev);
+
+	if (!mddev || !mddev->pers)
+		return -ENXIO;
 
-static int md_release (struct inode *inode, struct file *file)
+	return block_write (inode, file, buf, count);
+}
+
+static struct file_operations md_fops=
 {
-  int minor=MINOR(inode->i_rdev);
+	NULL,
+	md_read,
+	md_write,
+	NULL,
+	NULL,
+	md_ioctl,
+	NULL,
+	md_open,
+	md_release,
+	block_fsync
+};
+
+#else
 
-  sync_dev (inode->i_rdev);
-  md_dev[minor].busy--;
-  return 0;
+static int md_open (struct inode *inode, struct file *file)
+{
+	/*
+	 * Always succeed
+	 */
+	return (0);
 }
 
+static int md_release (struct inode *inode, struct file *file)
+{
+	sync_dev(inode->i_rdev);
+	return 0;
+}
 
 static ssize_t md_read (struct file *file, char *buf, size_t count,
 			loff_t *ppos)
 {
-  int minor=MINOR(file->f_dentry->d_inode->i_rdev);
+	mddev_t *mddev = kdev_to_mddev(MD_FILE_TO_INODE(file)->i_rdev);
 
-  if (!md_dev[minor].pers)	/* Check if device is being run */
-    return -ENXIO;
+	if (!mddev || !mddev->pers)
+		return -ENXIO;
 
-  return block_read(file, buf, count, ppos);
+	return block_read(file, buf, count, ppos);
 }
 
 static ssize_t md_write (struct file *file, const char *buf,
 			 size_t count, loff_t *ppos)
 {
-  int minor=MINOR(file->f_dentry->d_inode->i_rdev);
+	mddev_t *mddev = kdev_to_mddev(MD_FILE_TO_INODE(file)->i_rdev);
 
-  if (!md_dev[minor].pers)	/* Check if device is being run */
-    return -ENXIO;
+	if (!mddev || !mddev->pers)
+		return -ENXIO;
 
-  return block_write(file, buf, count, ppos);
+	return block_write(file, buf, count, ppos);
 }
 
 static struct file_operations md_fops=
 {
-  NULL,
-  md_read,
-  md_write,
-  NULL,
-  NULL,
-  md_ioctl,
-  NULL,
-  md_open,
-  NULL,
-  md_release,
-  block_fsync
+	NULL,
+	md_read,
+	md_write,
+	NULL,
+	NULL,
+	md_ioctl,
+	NULL,
+	md_open,
+	NULL,
+	md_release,
+	block_fsync
 };
 
-int md_map (int minor, kdev_t *rdev, unsigned long *rsector, unsigned long size)
+#endif
+
+int md_map (kdev_t dev, kdev_t *rdev,
+			 unsigned long *rsector, unsigned long size)
 {
-  if ((unsigned int) minor >= MAX_MD_DEV)
-  {
-    printk ("Bad md device %d\n", minor);
-    return (-1);
-  }
-  
-  if (!md_dev[minor].pers)
-  {
-    printk ("Oops ! md%d not running, giving up !\n", minor);
-    return (-1);
-  }
+	int err;
+	mddev_t *mddev = kdev_to_mddev(dev);
 
-  return (md_dev[minor].pers->map(md_dev+minor, rdev, rsector, size));
+	if (!mddev || !mddev->pers) {
+		err = -ENXIO;
+		goto out;
+	}
+
+	err = mddev->pers->map(mddev, dev, rdev, rsector, size);
+out:
+	return err;
 }
   
-int md_make_request (int minor, int rw, struct buffer_head * bh)
+int md_make_request (struct buffer_head * bh, int rw)
 {
-	if (md_dev [minor].pers->make_request) {
-		if (buffer_locked(bh))
-			return 0;
+	int err;
+	mddev_t *mddev = kdev_to_mddev(bh->b_dev);
+
+	if (!mddev || !mddev->pers) {
+		err = -ENXIO;
+		goto out;
+	}
+
+	if (mddev->pers->make_request) {
+		if (buffer_locked(bh)) {
+			err = 0;
+			goto out;
+		}
 		set_bit(BH_Lock, &bh->b_state);
 		if (rw == WRITE || rw == WRITEA) {
 			if (!buffer_dirty(bh)) {
-				bh->b_end_io(bh, test_bit(BH_Uptodate, &bh->b_state));
-				return 0;
+				bh->b_end_io(bh, buffer_uptodate(bh));
+				err = 0;
+				goto out;
 			}
 		}
 		if (rw == READ || rw == READA) {
 			if (buffer_uptodate(bh)) {
-				bh->b_end_io(bh, test_bit(BH_Uptodate, &bh->b_state));
-				return 0;
+				bh->b_end_io(bh, buffer_uptodate(bh));
+				err = 0;
+				goto out;
 			}
 		}
-		return (md_dev[minor].pers->make_request(md_dev+minor, rw, bh));
+		err = mddev->pers->make_request(mddev, rw, bh);
 	} else {
 		make_request (MAJOR(bh->b_rdev), rw, bh);
-		return 0;
+		err = 0;
 	}
+out:
+	return err;
 }
 
 static void do_md_request (void)
 {
-  printk ("Got md request, not good...");
-  return;
+	printk(KERN_ALERT "Got md request, not good...");
+	return;
+}
+
+int md_thread(void * arg)
+{
+	mdk_thread_t *thread = arg;
+
+	md_lock_kernel();
+	exit_mm(current);
+	exit_files(current);
+	exit_fs(current);
+
+	/*
+	 * Detach thread
+	 */
+	sys_setsid();
+	sprintf(current->comm, thread->name);
+	md_init_signals();
+	md_flush_signals();
+	thread->tsk = current;
+
+	/*
+	 * md_thread is a 'system-thread', it's priority should be very
+	 * high. We avoid resource deadlocks individually in each
+	 * raid personality. (RAID5 does preallocation) We also use RR and
+	 * the very same RT priority as kswapd, thus we will never get
+	 * into a priority inversion deadlock.
+	 *
+	 * we definitely have to have equal or higher priority than
+	 * bdflush, otherwise bdflush will deadlock if there are too
+	 * many dirty RAID5 blocks.
+	 */
+	current->policy = SCHED_OTHER;
+	current->priority = 40;
+
+	up(thread->sem);
+
+	for (;;) {
+		cli();
+		if (!test_bit(THREAD_WAKEUP, &thread->flags)) {
+			if (!thread->run)
+				break;
+			interruptible_sleep_on(&thread->wqueue);
+		}
+		sti();
+		clear_bit(THREAD_WAKEUP, &thread->flags);
+		if (thread->run) {
+			thread->run(thread->data);
+			run_task_queue(&tq_disk);
+		}
+		if (md_signal_pending(current)) {
+			printk("%8s(%d) flushing signals.\n", current->comm,
+				current->pid);
+			md_flush_signals();
+		}
+	}
+	sti();
+	up(thread->sem);
+	return 0;
 }
 
-void md_wakeup_thread(struct md_thread *thread)
+void md_wakeup_thread(mdk_thread_t *thread)
 {
 	set_bit(THREAD_WAKEUP, &thread->flags);
 	wake_up(&thread->wqueue);
 }
 
-struct md_thread *md_register_thread (void (*run) (void *), void *data)
+mdk_thread_t *md_register_thread (void (*run) (void *),
+						void *data, const char *name)
 {
-	struct md_thread *thread = (struct md_thread *)
-		kmalloc(sizeof(struct md_thread), GFP_KERNEL);
+	mdk_thread_t *thread;
 	int ret;
 	struct semaphore sem = MUTEX_LOCKED;
 	
-	if (!thread) return NULL;
+	thread = (mdk_thread_t *) kmalloc
+				(sizeof(mdk_thread_t), GFP_KERNEL);
+	if (!thread)
+		return NULL;
 	
-	memset(thread, 0, sizeof(struct md_thread));
+	memset(thread, 0, sizeof(mdk_thread_t));
 	init_waitqueue(&thread->wqueue);
 	
 	thread->sem = &sem;
 	thread->run = run;
 	thread->data = data;
+	thread->name = name;
 	ret = kernel_thread(md_thread, thread, 0);
 	if (ret < 0) {
 		kfree(thread);
@@ -836,270 +3032,407 @@
 	return thread;
 }
 
-void md_unregister_thread (struct md_thread *thread)
+void md_interrupt_thread (mdk_thread_t *thread)
+{
+	if (!thread->tsk) {
+		MD_BUG();
+		return;
+	}
+	printk("interrupting MD-thread pid %d\n", thread->tsk->pid);
+	send_sig(SIGKILL, thread->tsk, 1);
+}
+
+void md_unregister_thread (mdk_thread_t *thread)
 {
 	struct semaphore sem = MUTEX_LOCKED;
 	
 	thread->sem = &sem;
 	thread->run = NULL;
-	if (thread->tsk)
-		printk("Killing md_thread %d %p %s\n",
-		       thread->tsk->pid, thread->tsk, thread->tsk->comm);
-	else
-		printk("Aiee. md_thread has 0 tsk\n");
-	send_sig(SIGKILL, thread->tsk, 1);
-	printk("downing on %p\n", &sem);
+	thread->name = NULL;
+	if (!thread->tsk) {
+		MD_BUG();
+		return;
+	}
+	md_interrupt_thread(thread);
 	down(&sem);
 }
 
-#define SHUTDOWN_SIGS   (sigmask(SIGKILL)|sigmask(SIGINT)|sigmask(SIGTERM))
-
-int md_thread(void * arg)
+void md_recover_arrays (void)
 {
-	struct md_thread *thread = arg;
-
-	lock_kernel();
-	exit_mm(current);
-	exit_files(current);
-	exit_fs(current);
-	
-	current->session = 1;
-	current->pgrp = 1;
-	sprintf(current->comm, "md_thread");
-	siginitsetinv(&current->blocked, SHUTDOWN_SIGS);
-	thread->tsk = current;
-	up(thread->sem);
-
-	for (;;) {
-		cli();
-		if (!test_bit(THREAD_WAKEUP, &thread->flags)) {
-			do {
-			        spin_lock(&current->sigmask_lock);
-				flush_signals(current);
-	  			spin_unlock(&current->sigmask_lock);
-				interruptible_sleep_on(&thread->wqueue);
-				cli();
-				if (test_bit(THREAD_WAKEUP, &thread->flags))
-					break;
-				if (!thread->run) {
-					sti();
-					up(thread->sem);
-					return 0;
-				}
-			} while (signal_pending(current));
-		}
-		sti();
-		clear_bit(THREAD_WAKEUP, &thread->flags);
-		if (thread->run) {
-			thread->run(thread->data);
-			run_task_queue(&tq_disk);
-		}
+	if (!md_recovery_thread) {
+		MD_BUG();
+		return;
 	}
+	md_wakeup_thread(md_recovery_thread);
 }
 
-EXPORT_SYMBOL(md_size);
-EXPORT_SYMBOL(md_maxreadahead);
-EXPORT_SYMBOL(register_md_personality);
-EXPORT_SYMBOL(unregister_md_personality);
-EXPORT_SYMBOL(partition_name);
-EXPORT_SYMBOL(md_dev);
-EXPORT_SYMBOL(md_error);
-EXPORT_SYMBOL(md_register_thread);
-EXPORT_SYMBOL(md_unregister_thread);
-EXPORT_SYMBOL(md_update_sb);
-EXPORT_SYMBOL(md_map);
-EXPORT_SYMBOL(md_wakeup_thread);
-EXPORT_SYMBOL(md_do_sync);
 
-#ifdef CONFIG_PROC_FS
-static struct proc_dir_entry proc_md = {
-	PROC_MD, 6, "mdstat",
-	S_IFREG | S_IRUGO, 1, 0, 0,
-	0, &proc_array_inode_operations,
-};
+int md_error (kdev_t dev, kdev_t rdev)
+{
+	mddev_t *mddev = kdev_to_mddev(dev);
+	mdk_rdev_t * rrdev;
+	int rc;
+
+	if (!mddev) {
+		MD_BUG();
+		return 0;
+	}
+	rrdev = find_rdev(mddev, rdev);
+	mark_rdev_faulty(rrdev);
+	/*
+	 * if recovery was running, stop it now.
+	 */
+	if (mddev->pers->stop_resync)
+		mddev->pers->stop_resync(mddev);
+	if (mddev->recovery_running)
+		md_interrupt_thread(md_recovery_thread);
+	if (mddev->pers->error_handler) {
+		rc = mddev->pers->error_handler(mddev, rdev);
+		md_recover_arrays();
+		return rc;
+	}
+#if 0
+	/*
+	 * Drop all buffers in the failed array.
+	 * _not_. This is called from IRQ handlers ...
+	 */
+	invalidate_buffers(rdev);
 #endif
+	return 0;
+}
 
-static void md_geninit (struct gendisk *gdisk)
+static int status_unused (char * page)
 {
-  int i;
-  
-  for(i=0;i<MAX_MD_DEV;i++)
-  {
-    md_blocksizes[i] = 1024;
-    md_maxreadahead[i] = MD_DEFAULT_DISK_READAHEAD;
-    md_gendisk.part[i].start_sect=-1; /* avoid partition check */
-    md_gendisk.part[i].nr_sects=0;
-    md_dev[i].pers=NULL;
-  }
+	int sz = 0, i = 0;
+	mdk_rdev_t *rdev;
+	struct md_list_head *tmp;
 
-  blksize_size[MD_MAJOR] = md_blocksizes;
-  max_readahead[MD_MAJOR] = md_maxreadahead;
+	sz += sprintf(page + sz, "unused devices: ");
 
-#ifdef CONFIG_PROC_FS
-  proc_register(&proc_root, &proc_md);
-#endif
+	ITERATE_RDEV_ALL(rdev,tmp) {
+		if (!rdev->same_set.next && !rdev->same_set.prev) {
+			/*
+			 * The device is not yet used by any array.
+			 */
+			i++;
+			sz += sprintf(page + sz, "%s ",
+				partition_name(rdev->dev));
+		}
+	}
+	if (!i)
+		sz += sprintf(page + sz, "<none>");
+
+	sz += sprintf(page + sz, "\n");
+	return sz;
 }
 
-int md_error (kdev_t mddev, kdev_t rdev)
+
+static int status_resync (char * page, mddev_t * mddev)
 {
-    unsigned int minor = MINOR (mddev);
-    int rc;
+	int sz = 0;
+	unsigned int blocksize, max_blocks, resync, res, dt, tt, et;
 
-    if (MAJOR(mddev) != MD_MAJOR || minor > MAX_MD_DEV)
-	panic ("md_error gets unknown device\n");
-    if (!md_dev [minor].pers)
-	panic ("md_error gets an error for an unknown device\n");
-    if (md_dev [minor].pers->error_handler) {
-	rc = md_dev [minor].pers->error_handler (md_dev+minor, rdev);
-#if SUPPORT_RECONSTRUCTION
-	md_wakeup_thread(md_sync_thread);
-#endif /* SUPPORT_RECONSTRUCTION */
-	return rc;
-    }
-    return 0;
+	resync = mddev->curr_resync;
+	blocksize = blksize_size[MD_MAJOR][mdidx(mddev)];
+	max_blocks = blk_size[MD_MAJOR][mdidx(mddev)] / (blocksize >> 10);
+
+	/*
+	 * Should not happen.
+	 */		
+	if (!max_blocks) {
+		MD_BUG();
+		return 0;
+	}
+	res = resync*100/max_blocks;
+	if (!mddev->recovery_running)
+		/*
+		 * true resync
+		 */
+		sz += sprintf(page + sz, " resync=%u%%", res);
+	else
+		/*
+		 * recovery ...
+		 */
+		sz += sprintf(page + sz, " recovery=%u%%", res);
+
+	/*
+	 * We do not want to overflow, so the order of operands and
+	 * the * 100 / 100 trick are important. We do a +1 to be
+	 * safe against division by zero. We only estimate anyway.
+	 *
+	 * dt: time until now
+	 * tt: total time
+	 * et: estimated finish time
+	 */
+	dt = ((jiffies - mddev->resync_start) / HZ);
+	tt = (dt * (max_blocks / (resync/100+1)))/100;
+	if (tt > dt)
+		et = tt - dt;
+	else
+		/*
+		 * ignore rounding effects near finish time
+		 */
+		et = 0;
+	
+	sz += sprintf(page + sz, " finish=%u.%umin", et / 60, (et % 60)/6);
+
+	return sz;
 }
 
 int get_md_status (char *page)
 {
-  int sz=0, i, j, size;
-
-  sz+=sprintf( page+sz, "Personalities : ");
-  for (i=0; i<MAX_PERSONALITY; i++)
-    if (pers[i])
-      sz+=sprintf (page+sz, "[%d %s] ", i, pers[i]->name);
-
-  page[sz-1]='\n';
-
-  sz+=sprintf (page+sz, "read_ahead ");
-  if (read_ahead[MD_MAJOR]==INT_MAX)
-    sz+=sprintf (page+sz, "not set\n");
-  else
-    sz+=sprintf (page+sz, "%d sectors\n", read_ahead[MD_MAJOR]);
+	int sz = 0, j, size;
+	struct md_list_head *tmp, *tmp2;
+	mdk_rdev_t *rdev;
+	mddev_t *mddev;
+
+	sz += sprintf(page + sz, "Personalities : ");
+	for (j = 0; j < MAX_PERSONALITY; j++)
+	if (pers[j])
+		sz += sprintf(page+sz, "[%s] ", pers[j]->name);
+
+	sz += sprintf(page+sz, "\n");
+
+
+	sz += sprintf(page+sz, "read_ahead ");
+	if (read_ahead[MD_MAJOR] == INT_MAX)
+		sz += sprintf(page+sz, "not set\n");
+	else
+		sz += sprintf(page+sz, "%d sectors\n", read_ahead[MD_MAJOR]);
   
-  for (i=0; i<MAX_MD_DEV; i++)
-  {
-    sz+=sprintf (page+sz, "md%d : %sactive", i, md_dev[i].pers ? "" : "in");
-
-    if (md_dev[i].pers)
-      sz+=sprintf (page+sz, " %s", md_dev[i].pers->name);
+	ITERATE_MDDEV(mddev,tmp) {
+		sz += sprintf(page + sz, "md%d : %sactive", mdidx(mddev),
+						mddev->pers ? "" : "in");
+		if (mddev->pers) {
+			if (mddev->ro)	
+				sz += sprintf(page + sz, " (read-only)");
+			sz += sprintf(page + sz, " %s", mddev->pers->name);
+		}
 
-    size=0;
-    for (j=0; j<md_dev[i].nb_dev; j++)
-    {
-      sz+=sprintf (page+sz, " %s",
-		   partition_name(md_dev[i].devices[j].dev));
-      size+=md_dev[i].devices[j].size;
-    }
+		size = 0;
+		ITERATE_RDEV(mddev,rdev,tmp2) {
+			sz += sprintf(page + sz, " %s[%d]",
+				partition_name(rdev->dev), rdev->desc_nr);
+			if (rdev->faulty) {
+				sz += sprintf(page + sz, "(F)");
+				continue;
+			}
+			size += rdev->size;
+		}
 
-    if (md_dev[i].nb_dev) {
-      if (md_dev[i].pers)
-        sz+=sprintf (page+sz, " %d blocks", md_size[i]);
-      else
-        sz+=sprintf (page+sz, " %d blocks", size);
-    }
+		if (mddev->nb_dev) {
+			if (mddev->pers)
+				sz += sprintf(page + sz, " %d blocks",
+						 md_size[mdidx(mddev)]);
+			else
+				sz += sprintf(page + sz, " %d blocks", size);
+		}
 
-    if (!md_dev[i].pers)
-    {
-      sz+=sprintf (page+sz, "\n");
-      continue;
-    }
+		if (!mddev->pers) {
+			sz += sprintf(page+sz, "\n");
+			continue;
+		}
 
-    if (md_dev[i].pers->max_invalid_dev)
-      sz+=sprintf (page+sz, " maxfault=%ld", MAX_FAULT(md_dev+i));
+		sz += mddev->pers->status (page+sz, mddev);
 
-    sz+=md_dev[i].pers->status (page+sz, i, md_dev+i);
-    sz+=sprintf (page+sz, "\n");
-  }
+		if (mddev->curr_resync)
+			sz += status_resync (page+sz, mddev);
+		else {
+			if (md_atomic_read(&mddev->resync_sem.count) != 1)
+				sz += sprintf(page + sz, " resync=DELAYED");
+		}
+		sz += sprintf(page + sz, "\n");
+	}
+	sz += status_unused (page + sz);
 
-  return (sz);
+	return (sz);
 }
 
-int register_md_personality (int p_num, struct md_personality *p)
+int register_md_personality (int pnum, mdk_personality_t *p)
 {
-  int i=(p_num >> PERSONALITY_SHIFT);
-
-  if (i >= MAX_PERSONALITY)
-    return -EINVAL;
+	if (pnum >= MAX_PERSONALITY)
+		return -EINVAL;
 
-  if (pers[i])
-    return -EBUSY;
+	if (pers[pnum])
+		return -EBUSY;
   
-  pers[i]=p;
-  printk ("%s personality registered\n", p->name);
-  return 0;
+	pers[pnum] = p;
+	printk(KERN_INFO "%s personality registered\n", p->name);
+	return 0;
 }
 
-int unregister_md_personality (int p_num)
+int unregister_md_personality (int pnum)
 {
-  int i=(p_num >> PERSONALITY_SHIFT);
-
-  if (i >= MAX_PERSONALITY)
-    return -EINVAL;
+	if (pnum >= MAX_PERSONALITY)
+		return -EINVAL;
 
-  printk ("%s personality unregistered\n", pers[i]->name);
-  pers[i]=NULL;
-  return 0;
+	printk(KERN_INFO "%s personality unregistered\n", pers[pnum]->name);
+	pers[pnum] = NULL;
+	return 0;
 } 
 
-static md_descriptor_t *get_spare(struct md_dev *mddev)
+static mdp_disk_t *get_spare(mddev_t *mddev)
 {
-	int i;
-	md_superblock_t *sb = mddev->sb;
-	md_descriptor_t *descriptor;
-	struct real_dev *realdev;
-	
-  	for (i = 0; i < mddev->nb_dev; i++) {
-  		realdev = &mddev->devices[i];
-		if (!realdev->sb)
+	mdp_super_t *sb = mddev->sb;
+	mdp_disk_t *disk;
+	mdk_rdev_t *rdev;
+	struct md_list_head *tmp;
+
+	ITERATE_RDEV(mddev,rdev,tmp) {
+		if (rdev->faulty)
+			continue;
+		if (!rdev->sb) {
+			MD_BUG();
 			continue;
-		descriptor = &sb->disks[realdev->sb->descriptor.number];
-		if (descriptor->state & (1 << MD_FAULTY_DEVICE))
+		}
+		disk = &sb->disks[rdev->desc_nr];
+		if (disk_faulty(disk)) {
+			MD_BUG();
 			continue;
-		if (descriptor->state & (1 << MD_ACTIVE_DEVICE))
+		}
+		if (disk_active(disk))
 			continue;
-		return descriptor;
+		return disk;
 	}
 	return NULL;
 }
 
+static int is_mddev_idle (mddev_t *mddev)
+{
+	mdk_rdev_t * rdev;
+	struct md_list_head *tmp;
+	int idle;
+	unsigned long curr_events;
+
+	idle = 1;
+	ITERATE_RDEV(mddev,rdev,tmp) {
+		curr_events = io_events[MAJOR(rdev->dev)];
+
+		if (curr_events != rdev->last_events) {
+//			printk("!I(%d)", curr_events-rdev->last_events);
+			rdev->last_events = curr_events;
+			idle = 0;
+		}
+	}
+	return idle;
+}
+
 /*
  * parallel resyncing thread. 
- *
- * FIXME: - make it abort with a dirty array on mdstop, now it just blocks
- *        - fix read error handing
  */
 
-int md_do_sync(struct md_dev *mddev)
+/*
+ * Determine correct block size for this device.
+ */
+unsigned int device_bsize (kdev_t dev)
+{
+	unsigned int i, correct_size;
+
+	correct_size = BLOCK_SIZE;
+	if (blksize_size[MAJOR(dev)]) {
+		i = blksize_size[MAJOR(dev)][MINOR(dev)];
+		if (i)
+			correct_size = i;
+	}
+
+	return correct_size;
+}
+
+static struct wait_queue *resync_wait = (struct wait_queue *)NULL;
+
+#define RA_ORDER (1)
+#define RA_PAGE_SIZE (PAGE_SIZE*(1<<RA_ORDER))
+#define MAX_NR_BLOCKS (RA_PAGE_SIZE/sizeof(struct buffer_head *))
+
+int md_do_sync(mddev_t *mddev, mdp_disk_t *spare)
 {
-        struct buffer_head *bh;
-	int max_blocks, blocksize, curr_bsize, percent=1, j;
-	kdev_t read_disk = MKDEV(MD_MAJOR, mddev - md_dev);
+	mddev_t *mddev2;
+        struct buffer_head **bh;
+	unsigned int max_blocks, blocksize, curr_bsize,
+		i, ii, j, k, chunk, window, nr_blocks, err, serialize;
+	kdev_t read_disk = mddev_to_kdev(mddev);
 	int major = MAJOR(read_disk), minor = MINOR(read_disk);
 	unsigned long starttime;
+	int max_read_errors = 2*MAX_NR_BLOCKS,
+		 max_write_errors = 2*MAX_NR_BLOCKS;
+	struct md_list_head *tmp;
+
+retry_alloc:
+	bh = (struct buffer_head **) md__get_free_pages(GFP_KERNEL, RA_ORDER);
+	if (!bh) {
+		printk(KERN_ERR
+		"could not alloc bh array for reconstruction ... retrying!\n");
+		goto retry_alloc;
+	}
+
+	err = down_interruptible(&mddev->resync_sem);
+	if (err)
+		goto out_nolock;
+
+recheck:
+	serialize = 0;
+	ITERATE_MDDEV(mddev2,tmp) {
+		if (mddev2 == mddev)
+			continue;
+		if (mddev2->curr_resync && match_mddev_units(mddev,mddev2)) {
+			printk(KERN_INFO "md: serializing resync, md%d has overlapping physical units with md%d!\n", mdidx(mddev), mdidx(mddev2));
+			serialize = 1;
+			break;
+		}
+	}
+	if (serialize) {
+		interruptible_sleep_on(&resync_wait);
+		if (md_signal_pending(current)) {
+			md_flush_signals();
+			err = -EINTR;
+			goto out;
+		}
+		goto recheck;
+	}
+
+	mddev->curr_resync = 1;
 
-	blocksize = blksize_size[major][minor];
+	blocksize = device_bsize(read_disk);
 	max_blocks = blk_size[major][minor] / (blocksize >> 10);
 
-	printk("... resync log\n");
-	printk(" ....   mddev->nb_dev: %d\n", mddev->nb_dev);
-	printk(" ....   raid array: %s\n", kdevname(read_disk));
-	printk(" ....   max_blocks: %d blocksize: %d\n", max_blocks, blocksize);
-	printk("md: syncing RAID array %s\n", kdevname(read_disk));
+	printk(KERN_INFO "md: syncing RAID array md%d\n", mdidx(mddev));
+	printk(KERN_INFO "md: minimum _guaranteed_ reconstruction speed: %d KB/sec.\n",
+						sysctl_speed_limit);
+	printk(KERN_INFO "md: using maximum available idle IO bandwith for reconstruction.\n");
+
+	/*
+	 * Resync has low priority.
+	 */
+	current->priority = 1;
+
+	is_mddev_idle(mddev); /* this also initializes IO event counters */
+	starttime = jiffies;
+	mddev->resync_start = starttime;
 
-	mddev->busy++;
+	/*
+	 * Tune reconstruction:
+	 */
+	window = md_maxreadahead[mdidx(mddev)]/1024;
+	nr_blocks = window / (blocksize >> 10);
+	if (!nr_blocks || (nr_blocks > MAX_NR_BLOCKS))
+		nr_blocks = MAX_NR_BLOCKS;
+	printk(KERN_INFO "md: using %dk window.\n",window);
 
-	starttime=jiffies;
-	for (j = 0; j < max_blocks; j++) {
+	for (j = 0; j < max_blocks; j += nr_blocks) {
 
+		if (j)
+			mddev->curr_resync = j;
 		/*
 		 * B careful. When some1 mounts a non-'blocksize' filesystem
 		 * then we get the blocksize changed right under us. Go deal
 		 * with it transparently, recalculate 'blocksize', 'j' and
 		 * 'max_blocks':
 		 */
-		curr_bsize = blksize_size[major][minor];
+		curr_bsize = device_bsize(read_disk);
 		if (curr_bsize != blocksize) {
-		diff_blocksize:
+			printk(KERN_INFO "md%d: blocksize changed\n",
+								mdidx(mddev));
+retry_read:
 			if (curr_bsize > blocksize)
 				/*
 				 * this is safe, rounds downwards.
@@ -1109,114 +3442,384 @@
 				j *= blocksize/curr_bsize;
 
 			blocksize = curr_bsize;
+			nr_blocks = window / (blocksize >> 10);
+			if (!nr_blocks || (nr_blocks > MAX_NR_BLOCKS))
+				nr_blocks = MAX_NR_BLOCKS;
 			max_blocks = blk_size[major][minor] / (blocksize >> 10);
-		}
-        	if ((bh = breada (read_disk, j, blocksize, j * blocksize,
-					max_blocks * blocksize)) != NULL) {
-			mark_buffer_dirty(bh, 1);
-			brelse(bh);
-		} else {
+			printk("nr_blocks changed to %d (blocksize %d, j %d, max_blocks %d)\n",
+					nr_blocks, blocksize, j, max_blocks);
 			/*
-			 * FIXME: Ugly, but set_blocksize() isnt safe ...
+			 * We will retry the current block-group
 			 */
-			curr_bsize = blksize_size[major][minor];
-			if (curr_bsize != blocksize)
-				goto diff_blocksize;
+		}
 
-			/*
-			 * It's a real read problem. FIXME, handle this
-			 * a better way.
-			 */
-			printk ( KERN_ALERT
-				 "read error, stopping reconstruction.\n");
-			mddev->busy--;
-			return 1;
+		/*
+		 * Cleanup routines expect this
+		 */
+		for (k = 0; k < nr_blocks; k++)
+			bh[k] = NULL;
+
+		chunk = nr_blocks;
+		if (chunk > max_blocks-j)
+			chunk = max_blocks-j;
+
+		/*
+		 * request buffer heads ...
+		 */
+		for (i = 0; i < chunk; i++) {
+			bh[i] = getblk (read_disk, j+i, blocksize);
+			if (!bh[i])
+				goto read_error;
+			if (!buffer_dirty(bh[i]))
+				mark_buffer_lowprio(bh[i]);
 		}
 
 		/*
-		 * Let's sleep some if we are faster than our speed limit:
+		 * read buffer heads ...
 		 */
-		while (blocksize*j/(jiffies-starttime+1)*HZ/1024 > SPEED_LIMIT)
-		{
-			current->state = TASK_INTERRUPTIBLE;
-			schedule_timeout(1);
+		ll_rw_block (READ, chunk, bh);
+		run_task_queue(&tq_disk);
+		
+		/*
+		 * verify that all of them are OK ...
+		 */
+		for (i = 0; i < chunk; i++) {
+			ii = chunk-i-1;
+			wait_on_buffer(bh[ii]);
+			if (!buffer_uptodate(bh[ii]))
+				goto read_error;
+		}
+
+retry_write:
+		for (i = 0; i < chunk; i++)
+			mark_buffer_dirty_lowprio(bh[i]);
+
+		ll_rw_block(WRITE, chunk, bh);
+		run_task_queue(&tq_disk);
+
+		for (i = 0; i < chunk; i++) {
+			ii = chunk-i-1;
+			wait_on_buffer(bh[ii]);
+
+			if (spare && disk_faulty(spare)) {
+				for (k = 0; k < chunk; k++)
+					brelse(bh[k]);
+				printk(" <SPARE FAILED!>\n ");
+				err = -EIO;
+				goto out;
+			}
+
+			if (!buffer_uptodate(bh[ii])) {
+				curr_bsize = device_bsize(read_disk);
+				if (curr_bsize != blocksize) {
+					printk(KERN_INFO
+						"md%d: blocksize changed during write\n",
+						mdidx(mddev));
+					for (k = 0; k < chunk; k++)
+						if (bh[k]) {
+							if (buffer_lowprio(bh[k]))
+								mark_buffer_clean(bh[k]);
+							brelse(bh[k]);
+						}
+					goto retry_read;
+				}
+				printk(" BAD WRITE %8d>\n", j);
+				/*
+				 * Ouch, write error, retry or bail out.
+				 */
+				if (max_write_errors) {
+					max_write_errors--;
+					printk ( KERN_WARNING "md%d: write error while reconstructing, at block %u(%d).\n", mdidx(mddev), j, blocksize);
+					goto retry_write;
+				}
+				printk ( KERN_ALERT
+				  "too many write errors, stopping reconstruction.\n");
+				for (k = 0; k < chunk; k++)
+					if (bh[k]) {
+						if (buffer_lowprio(bh[k]))
+							mark_buffer_clean(bh[k]);
+						brelse(bh[k]);
+					}
+				err = -EIO;
+				goto out;
+			}
 		}
 
 		/*
-		 * FIXME: put this status bar thing into /proc
+		 * This is the normal 'everything went OK' case
+		 * do a 'free-behind' logic, we sure dont need
+		 * this buffer if it was the only user.
 		 */
-		if (!(j%(max_blocks/100))) {
-			if (!(percent%10))
-				printk (" %03d%% done.\n",percent);
+		for (i = 0; i < chunk; i++)
+			if (buffer_dirty(bh[i]))
+				brelse(bh[i]);
 			else
-				printk (".");
-			percent++;
+				bforget(bh[i]);
+
+
+		if (md_signal_pending(current)) {
+			/*
+			 * got a signal, exit.
+			 */
+			mddev->curr_resync = 0;
+			printk("md_do_sync() got signal ... exiting\n");
+			md_flush_signals();
+			err = -EINTR;
+			goto out;
 		}
+
+		/*
+		 * this loop exits only if either when we are slower than
+		 * the 'hard' speed limit, or the system was IO-idle for
+		 * a jiffy.
+		 * the system might be non-idle CPU-wise, but we only care
+		 * about not overloading the IO subsystem. (things like an
+		 * e2fsck being done on the RAID array should execute fast)
+		 */
+repeat:
+		if (md_need_resched(current))
+			schedule();
+
+		if ((blocksize/1024)*j/((jiffies-starttime)/HZ + 1) + 1
+						> sysctl_speed_limit) {
+			current->priority = 1;
+
+			if (!is_mddev_idle(mddev)) {
+				current->state = TASK_INTERRUPTIBLE;
+				md_schedule_timeout(HZ/2);
+				if (!md_signal_pending(current))
+					goto repeat;
+			}
+		} else
+			current->priority = 40;
 	}
 	fsync_dev(read_disk);
-	printk("md: %s: sync done.\n", kdevname(read_disk));
-	mddev->busy--;
-	return 0;
+	printk(KERN_INFO "md: md%d: sync done.\n",mdidx(mddev));
+	err = 0;
+	/*
+	 * this also signals 'finished resyncing' to md_stop
+	 */
+out:
+	up(&mddev->resync_sem);
+out_nolock:
+	free_pages((unsigned long)bh, RA_ORDER);
+	mddev->curr_resync = 0;
+	wake_up(&resync_wait);
+	return err;
+
+read_error:
+	/*
+	 * set_blocksize() might change the blocksize. This
+	 * should not happen often, but it happens when eg.
+	 * someone mounts a filesystem that has non-1k
+	 * blocksize. set_blocksize() doesnt touch our
+	 * buffer, but to avoid aliasing problems we change
+	 * our internal blocksize too and retry the read.
+	 */
+	curr_bsize = device_bsize(read_disk);
+	if (curr_bsize != blocksize) {
+		printk(KERN_INFO "md%d: blocksize changed during read\n",
+			mdidx(mddev));
+		for (k = 0; k < chunk; k++)
+			if (bh[k]) {
+				if (buffer_lowprio(bh[k]))
+					mark_buffer_clean(bh[k]);
+				brelse(bh[k]);
+			}
+		goto retry_read;
+	}
+
+	/*
+	 * It's a real read problem. We retry and bail out
+	 * only if it's excessive.
+	 */
+	if (max_read_errors) {
+		max_read_errors--;
+		printk ( KERN_WARNING "md%d: read error while reconstructing, at block %u(%d).\n", mdidx(mddev), j, blocksize);
+		for (k = 0; k < chunk; k++)
+			if (bh[k]) {
+				if (buffer_lowprio(bh[k]))
+					mark_buffer_clean(bh[k]);
+				brelse(bh[k]);
+			}
+		goto retry_read;
+	}
+	printk ( KERN_ALERT "too many read errors, stopping reconstruction.\n");
+	for (k = 0; k < chunk; k++)
+		if (bh[k]) {
+			if (buffer_lowprio(bh[k]))
+				mark_buffer_clean(bh[k]);
+			brelse(bh[k]);
+		}
+	err = -EIO;
+	goto out;
 }
 
+#undef MAX_NR_BLOCKS
+
 /*
- * This is a kernel thread which: syncs a spare disk with the active array
+ * This is a kernel thread which syncs a spare disk with the active array
  *
  * the amount of foolproofing might seem to be a tad excessive, but an
  * early (not so error-safe) version of raid1syncd synced the first 0.5 gigs
  * of my root partition with the first 0.5 gigs of my /home partition ... so
  * i'm a bit nervous ;)
  */
-void mdsyncd (void *data)
+void md_do_recovery (void *data)
 {
-	int i;
-	struct md_dev *mddev;
-	md_superblock_t *sb;
-	md_descriptor_t *spare;
+	int err;
+	mddev_t *mddev;
+	mdp_super_t *sb;
+	mdp_disk_t *spare;
 	unsigned long flags;
+	struct md_list_head *tmp;
 
-	for (i = 0, mddev = md_dev; i < MAX_MD_DEV; i++, mddev++) {
-		if ((sb = mddev->sb) == NULL)
+	printk(KERN_INFO "md: recovery thread got woken up ...\n");
+restart:
+	ITERATE_MDDEV(mddev,tmp) {
+		sb = mddev->sb;
+		if (!sb)
+			continue;
+		if (mddev->recovery_running)
 			continue;
 		if (sb->active_disks == sb->raid_disks)
 			continue;
-		if (!sb->spare_disks)
+		if (!sb->spare_disks) {
+			printk(KERN_ERR "md%d: no spare disk to reconstruct array! -- continuing in degraded mode\n", mdidx(mddev));
 			continue;
+		}
+		/*
+		 * now here we get the spare and resync it.
+		 */
 		if ((spare = get_spare(mddev)) == NULL)
 			continue;
-		if (!mddev->pers->mark_spare)
+		printk(KERN_INFO "md%d: resyncing spare disk %s to replace failed disk\n", mdidx(mddev), partition_name(MKDEV(spare->major,spare->minor)));
+		if (!mddev->pers->diskop)
 			continue;
-		if (mddev->pers->mark_spare(mddev, spare, SPARE_WRITE))
+		if (mddev->pers->diskop(mddev, &spare, DISKOP_SPARE_WRITE))
 			continue;
-		if (md_do_sync(mddev) || (spare->state & (1 << MD_FAULTY_DEVICE))) {
-			mddev->pers->mark_spare(mddev, spare, SPARE_INACTIVE);
+		down(&mddev->recovery_sem);
+		mddev->recovery_running = 1;
+		err = md_do_sync(mddev, spare);
+		if (err == -EIO) {
+			printk(KERN_INFO "md%d: spare disk %s failed, skipping to next spare.\n", mdidx(mddev), partition_name(MKDEV(spare->major,spare->minor)));
+			if (!disk_faulty(spare)) {
+				mddev->pers->diskop(mddev,&spare,DISKOP_SPARE_INACTIVE);
+				mark_disk_faulty(spare);
+				mark_disk_nonsync(spare);
+				mark_disk_inactive(spare);
+				sb->spare_disks--;
+				sb->working_disks--;
+				sb->failed_disks++;
+			}
+		} else
+			if (disk_faulty(spare))
+				mddev->pers->diskop(mddev, &spare,
+						DISKOP_SPARE_INACTIVE);
+		if (err == -EINTR) {
+			/*
+			 * Recovery got interrupted ...
+			 * signal back that we have finished using the array.
+			 */
+			mddev->pers->diskop(mddev, &spare,
+							 DISKOP_SPARE_INACTIVE);
+			up(&mddev->recovery_sem);
+			mddev->recovery_running = 0;
 			continue;
+		} else {
+			mddev->recovery_running = 0;
+			up(&mddev->recovery_sem);
 		}
 		save_flags(flags);
 		cli();
-		mddev->pers->mark_spare(mddev, spare, SPARE_ACTIVE);
-		spare->state |= (1 << MD_SYNC_DEVICE);
-		spare->state |= (1 << MD_ACTIVE_DEVICE);
-		sb->spare_disks--;
-		sb->active_disks++;
-		mddev->sb_dirty = 1;
-		md_update_sb(mddev - md_dev);
+		if (!disk_faulty(spare)) {
+			/*
+			 * the SPARE_ACTIVE diskop possibly changes the
+			 * pointer too
+			 */
+			mddev->pers->diskop(mddev, &spare, DISKOP_SPARE_ACTIVE);
+			mark_disk_sync(spare);
+			mark_disk_active(spare);
+			sb->active_disks++;
+			sb->spare_disks--;
+		}
 		restore_flags(flags);
+		mddev->sb_dirty = 1;
+		md_update_sb(mddev);
+		goto restart;
 	}
+	printk(KERN_INFO "md: recovery thread finished ...\n");
 	
 }
 
+int md_notify_reboot(struct notifier_block *this,
+					unsigned long code, void *x)
+{
+	struct md_list_head *tmp;
+	mddev_t *mddev;
+
+	if ((code == MD_SYS_DOWN) || (code == MD_SYS_HALT)
+				  || (code == MD_SYS_POWER_OFF)) {
+
+		printk(KERN_INFO "stopping all md devices.\n");
+
+		ITERATE_MDDEV(mddev,tmp)
+			do_md_stop (mddev, 1);
+		/*
+		 * certain more exotic SCSI devices are known to be
+		 * volatile wrt too early system reboots. While the
+		 * right place to handle this issue is the given
+		 * driver, we do want to have a safe RAID driver ...
+		 */
+		md_mdelay(1000*1);
+	}
+	return NOTIFY_DONE;
+}
+
+struct notifier_block md_notifier = {
+	md_notify_reboot,
+	NULL,
+	0
+};
+
+md__initfunc(void raid_setup(char *str, int *ints))
+{
+	char tmpline[100];
+	int len, pos, nr, i;
+
+	len = strlen(str) + 1;
+	nr = 0;
+	pos = 0;
+
+	for (i = 0; i < len; i++) {
+		char c = str[i];
+
+		if (c == ',' || !c) {
+			tmpline[pos] = 0;
+			if (!strcmp(tmpline,"noautodetect"))
+				raid_setup_args.noautodetect = 1;
+			nr++;
+			pos = 0;
+			continue;
+		}
+		tmpline[pos] = c;
+		pos++;
+	}
+	raid_setup_args.set = 1;
+	return;
+}
+
 #ifdef CONFIG_MD_BOOT
 struct {
 	int set;
 	int ints[100];
 	char str[100];
-} md_setup_args __initdata = {
+} md_setup_args md__initdata = {
 	0,{0},{0}
 };
 
 /* called from init/main.c */
-__initfunc(void md_setup(char *str,int *ints))
+md__initfunc(void md_setup(char *str,int *ints))
 {
 	int i;
 	for(i=0;i<=ints[0];i++) {
@@ -1228,21 +3831,24 @@
 	return;
 }
 
-__initfunc(void do_md_setup(char *str,int *ints))
+md__initfunc(void do_md_setup(char *str,int *ints))
 {
-	int minor, pers, factor, fault;
+#if 0
+	int minor, pers, chunk_size, fault;
 	kdev_t dev;
 	int i=1;
 
+	printk("i plan to phase this out --mingo\n");
+
 	if(ints[0] < 4) {
-		printk ("md: Too few Arguments (%d).\n", ints[0]);
+		printk (KERN_WARNING "md: Too few Arguments (%d).\n", ints[0]);
 		return;
 	}
    
 	minor=ints[i++];
    
-	if (minor >= MAX_MD_DEV) {
-		printk ("md: Minor device number too high.\n");
+	if ((unsigned int)minor >= MAX_MD_DEVS) {
+		printk (KERN_WARNING "md: Minor device number too high.\n");
 		return;
 	}
 
@@ -1252,18 +3858,20 @@
 	case -1:
 #ifdef CONFIG_MD_LINEAR
 		pers = LINEAR;
-		printk ("md: Setting up md%d as linear device.\n",minor);
+		printk (KERN_INFO "md: Setting up md%d as linear device.\n",
+									minor);
 #else 
-	        printk ("md: Linear mode not configured." 
+	        printk (KERN_WARNING "md: Linear mode not configured." 
 			"Recompile the kernel with linear mode enabled!\n");
 #endif
 		break;
 	case 0:
 		pers = STRIPED;
 #ifdef CONFIG_MD_STRIPED
-		printk ("md: Setting up md%d as a striped device.\n",minor);
+		printk (KERN_INFO "md: Setting up md%d as a striped device.\n",
+								minor);
 #else 
-	        printk ("md: Striped mode not configured." 
+	        printk (KERN_WARNING "md: Striped mode not configured." 
 			"Recompile the kernel with striped mode enabled!\n");
 #endif
 		break;
@@ -1278,79 +3886,145 @@
 		break;
 */
 	default:	   
-		printk ("md: Unknown or not supported raid level %d.\n", ints[--i]);
+		printk (KERN_WARNING "md: Unknown or not supported raid level %d.\n", ints[--i]);
 		return;
 	}
 
-	if(pers) {
+	if (pers) {
 
-	  factor=ints[i++]; /* Chunksize  */
-	  fault =ints[i++]; /* Faultlevel */
+		chunk_size = ints[i++]; /* Chunksize  */
+		fault = ints[i++]; /* Faultlevel */
    
-	  pers=pers | factor | (fault << FAULT_SHIFT);   
+		pers = pers | chunk_size | (fault << FAULT_SHIFT);   
    
-	  while( str && (dev = name_to_kdev_t(str))) {
-	    do_md_add (minor, dev);
-	    if((str = strchr (str, ',')) != NULL)
-	      str++;
-	  }
+		while( str && (dev = name_to_kdev_t(str))) {
+			do_md_add (minor, dev);
+			if((str = strchr (str, ',')) != NULL)
+				str++;
+		}
 
-	  do_md_run (minor, pers);
-	  printk ("md: Loading md%d.\n",minor);
+		do_md_run (minor, pers);
+		printk (KERN_INFO "md: Loading md%d.\n",minor);
 	}
-   
+#endif
 }
 #endif
 
+void hsm_init (void);
+void translucent_init (void);
 void linear_init (void);
 void raid0_init (void);
 void raid1_init (void);
 void raid5_init (void);
 
-__initfunc(int md_init (void))
+md__initfunc(int md_init (void))
 {
-  printk ("md driver %d.%d.%d MAX_MD_DEV=%d, MAX_REAL=%d\n",
-    MD_MAJOR_VERSION, MD_MINOR_VERSION, MD_PATCHLEVEL_VERSION,
-    MAX_MD_DEV, MAX_REAL);
-
-  if (register_blkdev (MD_MAJOR, "md", &md_fops))
-  {
-    printk ("Unable to get major %d for md\n", MD_MAJOR);
-    return (-1);
-  }
-
-  blk_dev[MD_MAJOR].request_fn=DEVICE_REQUEST;
-  blk_dev[MD_MAJOR].current_request=NULL;
-  read_ahead[MD_MAJOR]=INT_MAX;
-  memset(md_dev, 0, MAX_MD_DEV * sizeof (struct md_dev));
-  md_gendisk.next=gendisk_head;
-
-  gendisk_head=&md_gendisk;
-
-#if SUPPORT_RECONSTRUCTION
-  if ((md_sync_thread = md_register_thread(mdsyncd, NULL)) == NULL)
-    printk("md: bug: md_sync_thread == NULL\n");
-#endif /* SUPPORT_RECONSTRUCTION */
+	static char * name = "mdrecoveryd";
+
+	printk (KERN_INFO "md driver %d.%d.%d MAX_MD_DEVS=%d, MAX_REAL=%d\n",
+			MD_MAJOR_VERSION, MD_MINOR_VERSION,
+			MD_PATCHLEVEL_VERSION, MAX_MD_DEVS, MAX_REAL);
+
+	if (register_blkdev (MD_MAJOR, "md", &md_fops))
+	{
+		printk (KERN_ALERT "Unable to get major %d for md\n", MD_MAJOR);
+		return (-1);
+	}
+
+	blk_dev[MD_MAJOR].request_fn = DEVICE_REQUEST;
+	blk_dev[MD_MAJOR].current_request = NULL;
+	read_ahead[MD_MAJOR] = INT_MAX;
+	md_gendisk.next = gendisk_head;
+
+	gendisk_head = &md_gendisk;
+
+	md_recovery_thread = md_register_thread(md_do_recovery, NULL, name);
+	if (!md_recovery_thread)
+		printk(KERN_ALERT "bug: couldn't allocate md_recovery_thread\n");
 
+	md_register_reboot_notifier(&md_notifier);
+	md_register_sysctl();
+
+#ifdef CONFIG_MD_HSM
+	hsm_init ();
+#endif
+#ifdef CONFIG_MD_TRANSLUCENT
+	translucent_init ();
+#endif
 #ifdef CONFIG_MD_LINEAR
-  linear_init ();
+	linear_init ();
 #endif
 #ifdef CONFIG_MD_STRIPED
-  raid0_init ();
+	raid0_init ();
 #endif
 #ifdef CONFIG_MD_MIRRORING
-  raid1_init ();
+	raid1_init ();
 #endif
 #ifdef CONFIG_MD_RAID5
-  raid5_init ();
+	raid5_init ();
+#endif
+#if defined(CONFIG_MD_RAID5) || defined(CONFIG_MD_RAID5_MODULE)
+        /*
+         * pick a XOR routine, runtime.
+         */
+	calibrate_xor_block();
 #endif
-  return (0);
+
+	return (0);
 }
 
 #ifdef CONFIG_MD_BOOT
-__initfunc(void md_setup_drive(void))
+md__initfunc(void md_setup_drive(void))
 {
 	if(md_setup_args.set)
 		do_md_setup(md_setup_args.str, md_setup_args.ints);
 }
 #endif
+
+MD_EXPORT_SYMBOL(md_size);
+MD_EXPORT_SYMBOL(register_md_personality);
+MD_EXPORT_SYMBOL(unregister_md_personality);
+MD_EXPORT_SYMBOL(partition_name);
+MD_EXPORT_SYMBOL(md_error);
+MD_EXPORT_SYMBOL(md_recover_arrays);
+MD_EXPORT_SYMBOL(md_register_thread);
+MD_EXPORT_SYMBOL(md_unregister_thread);
+MD_EXPORT_SYMBOL(md_update_sb);
+MD_EXPORT_SYMBOL(md_map);
+MD_EXPORT_SYMBOL(md_wakeup_thread);
+MD_EXPORT_SYMBOL(md_do_sync);
+MD_EXPORT_SYMBOL(md_print_devices);
+MD_EXPORT_SYMBOL(find_rdev_nr);
+MD_EXPORT_SYMBOL(md_check_ordering);
+MD_EXPORT_SYMBOL(md_interrupt_thread);
+MD_EXPORT_SYMBOL(mddev_map);
+
+#ifdef CONFIG_PROC_FS
+static struct proc_dir_entry proc_md = {
+	PROC_MD, 6, "mdstat",
+	S_IFREG | S_IRUGO, 1, 0, 0,
+	0, &proc_array_inode_operations,
+};
+#endif
+
+static void md_geninit (struct gendisk *gdisk)
+{
+	int i;
+  
+	for(i = 0; i < MAX_MD_DEVS; i++) {
+		md_blocksizes[i] = 1024;
+		md_maxreadahead[i] = MD_READAHEAD;
+		md_gendisk.part[i].start_sect = -1; /* avoid partition check */
+		md_gendisk.part[i].nr_sects = 0;
+	}
+
+	printk("md.c: sizeof(mdp_super_t) = %d\n", (int)sizeof(mdp_super_t));
+
+	blksize_size[MD_MAJOR] = md_blocksizes;
+	md_set_global_readahead(md_maxreadahead);
+
+#ifdef CONFIG_PROC_FS
+	proc_register(&proc_root, &proc_md);
+#endif
+}
+
diff -urN linux/drivers/block/raid0.c /tmp/linux/drivers/block/raid0.c
--- linux/drivers/block/raid0.c	Mon Sep  4 11:39:16 2000
+++ /tmp/linux/drivers/block/raid0.c	Fri Feb  2 19:06:00 2001
@@ -1,4 +1,3 @@
-
 /*
    raid0.c : Multiple Devices driver for Linux
              Copyright (C) 1994-96 Marc ZYNGIER
@@ -18,146 +17,201 @@
 */
 
 #include <linux/module.h>
-#include <linux/md.h>
-#include <linux/raid0.h>
-#include <linux/vmalloc.h>
+#include <linux/raid/raid0.h>
 
 #define MAJOR_NR MD_MAJOR
 #define MD_DRIVER
 #define MD_PERSONALITY
 
-static int create_strip_zones (int minor, struct md_dev *mddev)
+static int create_strip_zones (mddev_t *mddev)
 {
-  int i, j, c=0;
-  int current_offset=0;
-  struct real_dev *smallest_by_zone;
-  struct raid0_data *data=(struct raid0_data *) mddev->private;
-  
-  data->nr_strip_zones=1;
-  
-  for (i=1; i<mddev->nb_dev; i++)
-  {
-    for (j=0; j<i; j++)
-      if (mddev->devices[i].size==mddev->devices[j].size)
-      {
-	c=1;
-	break;
-      }
-
-    if (!c)
-      data->nr_strip_zones++;
-
-    c=0;
-  }
-
-  if ((data->strip_zone=vmalloc(sizeof(struct strip_zone)*data->nr_strip_zones)) == NULL)
-    return 1;
-
-  data->smallest=NULL;
-  
-  for (i=0; i<data->nr_strip_zones; i++)
-  {
-    data->strip_zone[i].dev_offset=current_offset;
-    smallest_by_zone=NULL;
-    c=0;
-
-    for (j=0; j<mddev->nb_dev; j++)
-      if (mddev->devices[j].size>current_offset)
-      {
-	data->strip_zone[i].dev[c++]=mddev->devices+j;
-	if (!smallest_by_zone ||
-	    smallest_by_zone->size > mddev->devices[j].size)
-	  smallest_by_zone=mddev->devices+j;
-      }
-
-    data->strip_zone[i].nb_dev=c;
-    data->strip_zone[i].size=(smallest_by_zone->size-current_offset)*c;
-
-    if (!data->smallest ||
-	data->smallest->size > data->strip_zone[i].size)
-      data->smallest=data->strip_zone+i;
-
-    data->strip_zone[i].zone_offset=i ? (data->strip_zone[i-1].zone_offset+
-					   data->strip_zone[i-1].size) : 0;
-    current_offset=smallest_by_zone->size;
-  }
-  return 0;
+	int i, c, j, j1, j2;
+	int current_offset, curr_zone_offset;
+	raid0_conf_t *conf = mddev_to_conf(mddev);
+	mdk_rdev_t *smallest, *rdev1, *rdev2, *rdev;
+ 
+	/*
+	 * The number of 'same size groups'
+	 */
+	conf->nr_strip_zones = 0;
+ 
+	ITERATE_RDEV_ORDERED(mddev,rdev1,j1) {
+		printk("raid0: looking at %s\n", partition_name(rdev1->dev));
+		c = 0;
+		ITERATE_RDEV_ORDERED(mddev,rdev2,j2) {
+			printk("raid0:   comparing %s(%d) with %s(%d)\n", partition_name(rdev1->dev), rdev1->size, partition_name(rdev2->dev), rdev2->size);
+			if (rdev2 == rdev1) {
+				printk("raid0:   END\n");
+				break;
+			}
+			if (rdev2->size == rdev1->size)
+			{
+				/*
+				 * Not unique, dont count it as a new
+				 * group
+				 */
+				printk("raid0:   EQUAL\n");
+				c = 1;
+				break;
+			}
+			printk("raid0:   NOT EQUAL\n");
+		}
+		if (!c) {
+			printk("raid0:   ==> UNIQUE\n");
+			conf->nr_strip_zones++;
+			printk("raid0: %d zones\n", conf->nr_strip_zones);
+		}
+	}
+		printk("raid0: FINAL %d zones\n", conf->nr_strip_zones);
+
+	conf->strip_zone = vmalloc(sizeof(struct strip_zone)*
+				conf->nr_strip_zones);
+	if (!conf->strip_zone)
+		return 1;
+
+
+	conf->smallest = NULL;
+	current_offset = 0;
+	curr_zone_offset = 0;
+
+	for (i = 0; i < conf->nr_strip_zones; i++)
+	{
+		struct strip_zone *zone = conf->strip_zone + i;
+
+		printk("zone %d\n", i);
+		zone->dev_offset = current_offset;
+		smallest = NULL;
+		c = 0;
+
+		ITERATE_RDEV_ORDERED(mddev,rdev,j) {
+
+			printk(" checking %s ...", partition_name(rdev->dev));
+			if (rdev->size > current_offset)
+			{
+				printk(" contained as device %d\n", c);
+				zone->dev[c] = rdev;
+				c++;
+				if (!smallest || (rdev->size <smallest->size)) {
+					smallest = rdev;
+					printk("  (%d) is smallest!.\n", rdev->size);
+				}
+			} else
+				printk(" nope.\n");
+		}
+
+		zone->nb_dev = c;
+		zone->size = (smallest->size - current_offset) * c;
+		printk(" zone->nb_dev: %d, size: %d\n",zone->nb_dev,zone->size);
+
+		if (!conf->smallest || (zone->size < conf->smallest->size))
+			conf->smallest = zone;
+
+		zone->zone_offset = curr_zone_offset;
+		curr_zone_offset += zone->size;
+
+		current_offset = smallest->size;
+		printk("current zone offset: %d\n", current_offset);
+	}
+	printk("done.\n");
+	return 0;
 }
 
-static int raid0_run (int minor, struct md_dev *mddev)
+static int raid0_run (mddev_t *mddev)
 {
-  int cur=0, i=0, size, zone0_size, nb_zone;
-  struct raid0_data *data;
-
-  MOD_INC_USE_COUNT;
+	int cur=0, i=0, size, zone0_size, nb_zone;
+	raid0_conf_t *conf;
 
-  if ((mddev->private=vmalloc (sizeof (struct raid0_data))) == NULL) return 1;
-  data=(struct raid0_data *) mddev->private;
-  
-  if (create_strip_zones (minor, mddev)) 
-  {
-  	vfree(data);
-  	return 1;
-  }
-
-  nb_zone=data->nr_zones=
-    md_size[minor]/data->smallest->size +
-    (md_size[minor]%data->smallest->size ? 1 : 0);
-
-  printk ("raid0 : Allocating %ld bytes for hash.\n",(long)sizeof(struct raid0_hash)*nb_zone);
-  if ((data->hash_table=vmalloc (sizeof (struct raid0_hash)*nb_zone)) == NULL)
-  {
-    vfree(data->strip_zone);
-    vfree(data);
-    return 1;
-  }
-  size=data->strip_zone[cur].size;
-
-  i=0;
-  while (cur<data->nr_strip_zones)
-  {
-    data->hash_table[i].zone0=data->strip_zone+cur;
-
-    if (size>=data->smallest->size)/* If we completely fill the slot */
-    {
-      data->hash_table[i++].zone1=NULL;
-      size-=data->smallest->size;
-
-      if (!size)
-      {
-	if (++cur==data->nr_strip_zones) continue;
-	size=data->strip_zone[cur].size;
-      }
-
-      continue;
-    }
-
-    if (++cur==data->nr_strip_zones) /* Last dev, set unit1 as NULL */
-    {
-      data->hash_table[i].zone1=NULL;
-      continue;
-    }
-
-    zone0_size=size;		/* Here, we use a 2nd dev to fill the slot */
-    size=data->strip_zone[cur].size;
-    data->hash_table[i++].zone1=data->strip_zone+cur;
-    size-=(data->smallest->size - zone0_size);
-  }
+	MOD_INC_USE_COUNT;
 
-  return (0);
+	conf = vmalloc(sizeof (raid0_conf_t));
+	if (!conf)
+		goto out;
+	mddev->private = (void *)conf;
+ 
+	if (md_check_ordering(mddev)) {
+		printk("raid0: disks are not ordered, aborting!\n");
+		goto out_free_conf;
+	}
+
+	if (create_strip_zones (mddev)) 
+		goto out_free_conf;
+
+	printk("raid0 : md_size is %d blocks.\n", md_size[mdidx(mddev)]);
+	printk("raid0 : conf->smallest->size is %d blocks.\n", conf->smallest->size);
+	nb_zone = md_size[mdidx(mddev)]/conf->smallest->size +
+			(md_size[mdidx(mddev)] % conf->smallest->size ? 1 : 0);
+	printk("raid0 : nb_zone is %d.\n", nb_zone);
+	conf->nr_zones = nb_zone;
+
+	printk("raid0 : Allocating %d bytes for hash.\n",
+				sizeof(struct raid0_hash)*nb_zone);
+
+	conf->hash_table = vmalloc (sizeof (struct raid0_hash)*nb_zone);
+	if (!conf->hash_table)
+		goto out_free_zone_conf;
+	size = conf->strip_zone[cur].size;
+
+	i = 0;
+	while (cur < conf->nr_strip_zones) {
+		conf->hash_table[i].zone0 = conf->strip_zone + cur;
+
+		/*
+		 * If we completely fill the slot
+		 */
+		if (size >= conf->smallest->size) {
+			conf->hash_table[i++].zone1 = NULL;
+			size -= conf->smallest->size;
+
+			if (!size) {
+				if (++cur == conf->nr_strip_zones)
+					continue;
+				size = conf->strip_zone[cur].size;
+			}
+			continue;
+		}
+		if (++cur == conf->nr_strip_zones) {
+			/*
+			 * Last dev, set unit1 as NULL
+			 */
+			conf->hash_table[i].zone1=NULL;
+			continue;
+		}
+
+		/*
+		 * Here we use a 2nd dev to fill the slot
+		 */
+		zone0_size = size;
+		size = conf->strip_zone[cur].size;
+		conf->hash_table[i++].zone1 = conf->strip_zone + cur;
+		size -= (conf->smallest->size - zone0_size);
+	}
+	return 0;
+
+out_free_zone_conf:
+	vfree(conf->strip_zone);
+	conf->strip_zone = NULL;
+
+out_free_conf:
+	vfree(conf);
+	mddev->private = NULL;
+out:
+	MOD_DEC_USE_COUNT;
+	return 1;
 }
 
-
-static int raid0_stop (int minor, struct md_dev *mddev)
+static int raid0_stop (mddev_t *mddev)
 {
-  struct raid0_data *data=(struct raid0_data *) mddev->private;
+	raid0_conf_t *conf = mddev_to_conf(mddev);
 
-  vfree (data->hash_table);
-  vfree (data->strip_zone);
-  vfree (data);
+	vfree (conf->hash_table);
+	conf->hash_table = NULL;
+	vfree (conf->strip_zone);
+	conf->strip_zone = NULL;
+	vfree (conf);
+	mddev->private = NULL;
 
-  MOD_DEC_USE_COUNT;
-  return 0;
+	MOD_DEC_USE_COUNT;
+	return 0;
 }
 
 /*
@@ -167,135 +221,140 @@
  * Of course, those facts may not be valid anymore (and surely won't...)
  * Hey guys, there's some work out there ;-)
  */
-static int raid0_map (struct md_dev *mddev, kdev_t *rdev,
+static int raid0_map (mddev_t *mddev, kdev_t dev, kdev_t *rdev,
 		      unsigned long *rsector, unsigned long size)
 {
-  struct raid0_data *data=(struct raid0_data *) mddev->private;
-  static struct raid0_hash *hash;
-  struct strip_zone *zone;
-  struct real_dev *tmp_dev;
-  int blk_in_chunk, factor, chunk, chunk_size;
-  long block, rblock;
-
-  factor=FACTOR(mddev);
-  chunk_size=(1UL << FACTOR_SHIFT(factor));
-  block=*rsector >> 1;
-  hash=data->hash_table+(block/data->smallest->size);
-
-  if (hash - data->hash_table > data->nr_zones) 
-  { 
-	  printk(KERN_DEBUG "raid0_map: invalid block %li\n", block);
-	  return -1;
-  }
-
-  /* Sanity check */
-  if ((chunk_size*2)<(*rsector % (chunk_size*2))+size)
-  {
-    printk ("raid0_convert : can't convert block across chunks or bigger than %dk %ld %ld\n", chunk_size, *rsector, size);
-    return (-1);
-  }
-  
-  if (block >= (hash->zone0->size +
-		hash->zone0->zone_offset))
-  {
-    if (!hash->zone1)
-    {
-      printk ("raid0_convert : hash->zone1==NULL for block %ld\n", block);
-      return (-1);
-    }
-    
-    zone=hash->zone1;
-  }
-  else
-    zone=hash->zone0;
+	raid0_conf_t *conf = mddev_to_conf(mddev);
+	struct raid0_hash *hash;
+	struct strip_zone *zone;
+	mdk_rdev_t *tmp_dev;
+	int blk_in_chunk, chunksize_bits, chunk, chunk_size;
+	long block, rblock;
+
+	chunk_size = mddev->param.chunk_size >> 10;
+	chunksize_bits = ffz(~chunk_size);
+	block = *rsector >> 1;
+	hash = conf->hash_table + block / conf->smallest->size;
+
+	if (hash - conf->hash_table > conf->nr_zones) {
+		printk(KERN_DEBUG "raid0_map: invalid block %lu\n", block);
+		return -1;
+	}
+
+	/* Sanity check */
+	if ((chunk_size * 2) < (*rsector % (chunk_size * 2)) + size)
+		goto bad_map;
+ 
+	if (!hash)
+		goto bad_hash;
+
+	if (!hash->zone0)
+		goto bad_zone0;
+ 
+	if (block >= (hash->zone0->size + hash->zone0->zone_offset)) {
+		if (!hash->zone1)
+			goto bad_zone1;
+		zone = hash->zone1;
+	} else
+		zone = hash->zone0;
     
-  blk_in_chunk=block & (chunk_size -1);
-  chunk=(block - zone->zone_offset) / (zone->nb_dev<<FACTOR_SHIFT(factor));
-  tmp_dev=zone->dev[(block >> FACTOR_SHIFT(factor)) % zone->nb_dev];
-  rblock=(chunk << FACTOR_SHIFT(factor)) + blk_in_chunk + zone->dev_offset;
+	blk_in_chunk = block & (chunk_size -1);
+	chunk = (block - zone->zone_offset) / (zone->nb_dev << chunksize_bits);
+	tmp_dev = zone->dev[(block >> chunksize_bits) % zone->nb_dev];
+	rblock = (chunk << chunksize_bits) + blk_in_chunk + zone->dev_offset;
   
-  *rdev=tmp_dev->dev;
-  *rsector=rblock<<1;
+	*rdev = tmp_dev->dev;
+	*rsector = rblock << 1;
 
-  return (0);
+	return 0;
+
+bad_map:
+	printk ("raid0_map bug: can't convert block across chunks or bigger than %dk %ld %ld\n", chunk_size, *rsector, size);
+	return -1;
+bad_hash:
+	printk("raid0_map bug: hash==NULL for block %ld\n", block);
+	return -1;
+bad_zone0:
+	printk ("raid0_map bug: hash->zone0==NULL for block %ld\n", block);
+	return -1;
+bad_zone1:
+	printk ("raid0_map bug: hash->zone1==NULL for block %ld\n", block);
+	return -1;
 }
 
 			   
-static int raid0_status (char *page, int minor, struct md_dev *mddev)
+static int raid0_status (char *page, mddev_t *mddev)
 {
-  int sz=0;
+	int sz = 0;
 #undef MD_DEBUG
 #ifdef MD_DEBUG
-  int j, k;
-  struct raid0_data *data=(struct raid0_data *) mddev->private;
+	int j, k;
+	raid0_conf_t *conf = mddev_to_conf(mddev);
   
-  sz+=sprintf (page+sz, "      ");
-  for (j=0; j<data->nr_zones; j++)
-  {
-    sz+=sprintf (page+sz, "[z%d",
-		 data->hash_table[j].zone0-data->strip_zone);
-    if (data->hash_table[j].zone1)
-      sz+=sprintf (page+sz, "/z%d] ",
-		   data->hash_table[j].zone1-data->strip_zone);
-    else
-      sz+=sprintf (page+sz, "] ");
-  }
+	sz += sprintf(page + sz, "      ");
+	for (j = 0; j < conf->nr_zones; j++) {
+		sz += sprintf(page + sz, "[z%d",
+				conf->hash_table[j].zone0 - conf->strip_zone);
+		if (conf->hash_table[j].zone1)
+			sz += sprintf(page+sz, "/z%d] ",
+				conf->hash_table[j].zone1 - conf->strip_zone);
+		else
+			sz += sprintf(page+sz, "] ");
+	}
   
-  sz+=sprintf (page+sz, "\n");
+	sz += sprintf(page + sz, "\n");
   
-  for (j=0; j<data->nr_strip_zones; j++)
-  {
-    sz+=sprintf (page+sz, "      z%d=[", j);
-    for (k=0; k<data->strip_zone[j].nb_dev; k++)
-      sz+=sprintf (page+sz, "%s/",
-		   partition_name(data->strip_zone[j].dev[k]->dev));
-    sz--;
-    sz+=sprintf (page+sz, "] zo=%d do=%d s=%d\n",
-		 data->strip_zone[j].zone_offset,
-		 data->strip_zone[j].dev_offset,
-		 data->strip_zone[j].size);
-  }
+	for (j = 0; j < conf->nr_strip_zones; j++) {
+		sz += sprintf(page + sz, "      z%d=[", j);
+		for (k = 0; k < conf->strip_zone[j].nb_dev; k++)
+			sz += sprintf (page+sz, "%s/", partition_name(
+				conf->strip_zone[j].dev[k]->dev));
+		sz--;
+		sz += sprintf (page+sz, "] zo=%d do=%d s=%d\n",
+				conf->strip_zone[j].zone_offset,
+				conf->strip_zone[j].dev_offset,
+				conf->strip_zone[j].size);
+	}
 #endif
-  sz+=sprintf (page+sz, " %dk chunks", 1<<FACTOR_SHIFT(FACTOR(mddev)));
-  return sz;
+	sz += sprintf(page + sz, " %dk chunks", mddev->param.chunk_size/1024);
+	return sz;
 }
 
-
-static struct md_personality raid0_personality=
+static mdk_personality_t raid0_personality=
 {
-  "raid0",
-  raid0_map,
-  NULL,				/* no special make_request */
-  NULL,				/* no special end_request */
-  raid0_run,
-  raid0_stop,
-  raid0_status,
-  NULL,				/* no ioctls */
-  0,
-  NULL,				/* no error_handler */
-  NULL,				/* hot_add_disk */
-  NULL,				/* hot_remove_disk */
-  NULL				/* mark_spare */
+	"raid0",
+	raid0_map,
+	NULL,				/* no special make_request */
+	NULL,				/* no special end_request */
+	raid0_run,
+	raid0_stop,
+	raid0_status,
+	NULL,				/* no ioctls */
+	0,
+	NULL,				/* no error_handler */
+	NULL,				/* no diskop */
+	NULL,				/* no stop resync */
+	NULL				/* no restart resync */
 };
 
-
 #ifndef MODULE
 
 void raid0_init (void)
 {
-  register_md_personality (RAID0, &raid0_personality);
+	register_md_personality (RAID0, &raid0_personality);
 }
 
 #else
 
 int init_module (void)
 {
-  return (register_md_personality (RAID0, &raid0_personality));
+	return (register_md_personality (RAID0, &raid0_personality));
 }
 
 void cleanup_module (void)
 {
-  unregister_md_personality (RAID0);
+	unregister_md_personality (RAID0);
 }
 
 #endif
+
diff -urN linux/drivers/block/raid1.c /tmp/linux/drivers/block/raid1.c
--- linux/drivers/block/raid1.c	Sun Dec 10 17:49:41 2000
+++ /tmp/linux/drivers/block/raid1.c	Fri Feb  2 19:06:00 2001
@@ -1,6 +1,6 @@
-/************************************************************************
+/*
  * raid1.c : Multiple Devices driver for Linux
- *           Copyright (C) 1996 Ingo Molnar, Miguel de Icaza, Gadi Oxman
+ * Copyright (C) 1996, 1997, 1998 Ingo Molnar, Miguel de Icaza, Gadi Oxman
  *
  * RAID-1 management functions.
  *
@@ -15,50 +15,55 @@
  */
 
 #include <linux/module.h>
-#include <linux/locks.h>
 #include <linux/malloc.h>
-#include <linux/md.h>
-#include <linux/raid1.h>
-#include <asm/bitops.h>
+#include <linux/raid/raid1.h>
 #include <asm/atomic.h>
 
 #define MAJOR_NR MD_MAJOR
 #define MD_DRIVER
 #define MD_PERSONALITY
 
-/*
- * The following can be used to debug the driver
- */
-/*#define RAID1_DEBUG*/
-#ifdef RAID1_DEBUG
-#define PRINTK(x)   do { printk x; } while (0);
-#else
-#define PRINTK(x)   do { ; } while (0);
-#endif
+#define MAX_LINEAR_SECTORS 128
 
 #define MAX(a,b)	((a) > (b) ? (a) : (b))
 #define MIN(a,b)	((a) < (b) ? (a) : (b))
 
-static struct md_personality raid1_personality;
-static struct md_thread *raid1_thread = NULL;
+static mdk_personality_t raid1_personality;
 struct buffer_head *raid1_retry_list = NULL;
 
-static int __raid1_map (struct md_dev *mddev, kdev_t *rdev,
+static void * raid1_kmalloc (int size)
+{
+	void * ptr;
+	/*
+	 * now we are rather fault tolerant than nice, but
+	 * there are a couple of places in the RAID code where we
+	 * simply can not afford to fail an allocation because
+	 * there is no failure return path (eg. make_request())
+	 */
+	while (!(ptr = kmalloc (sizeof (raid1_conf_t), GFP_BUFFER))) {
+		printk ("raid1: out of memory, retrying...\n");
+		current->state = TASK_UNINTERRUPTIBLE;
+		schedule_timeout(HZ/10);
+	}
+
+	memset(ptr, 0, size);
+	return ptr;
+}
+
+static int __raid1_map (mddev_t *mddev, kdev_t *rdev,
 		        unsigned long *rsector, unsigned long size)
 {
-	struct raid1_data *raid_conf = (struct raid1_data *) mddev->private;
-	int i, n = raid_conf->raid_disks;
+	raid1_conf_t *conf = mddev_to_conf(mddev);
+	int i, disks = MD_SB_DISKS;
 
 	/*
 	 * Later we do read balancing on the read side 
 	 * now we use the first available disk.
 	 */
 
-	PRINTK(("raid1_map().\n"));
-
-	for (i=0; i<n; i++) {
-		if (raid_conf->mirrors[i].operational) {
-			*rdev = raid_conf->mirrors[i].dev;
+	for (i = 0; i < disks; i++) {
+		if (conf->mirrors[i].operational) {
+			*rdev = conf->mirrors[i].dev;
 			return (0);
 		}
 	}
@@ -67,29 +72,29 @@
 	return (-1);
 }
 
-static int raid1_map (struct md_dev *mddev, kdev_t *rdev,
+static int raid1_map (mddev_t *mddev, kdev_t dev, kdev_t *rdev,
 		      unsigned long *rsector, unsigned long size)
 {
 	return 0;
 }
 
-void raid1_reschedule_retry (struct buffer_head *bh)
+static void raid1_reschedule_retry (struct buffer_head *bh)
 {
 	struct raid1_bh * r1_bh = (struct raid1_bh *)(bh->b_dev_id);
-
-	PRINTK(("raid1_reschedule_retry().\n"));
+	mddev_t *mddev = r1_bh->mddev;
+	raid1_conf_t *conf = mddev_to_conf(mddev);
 
 	r1_bh->next_retry = raid1_retry_list;
 	raid1_retry_list = bh;
-	md_wakeup_thread(raid1_thread);
+	md_wakeup_thread(conf->thread);
 }
 
 /*
- * raid1_end_buffer_io() is called when we have finished servicing a mirrored
+ * raid1_end_bh_io() is called when we have finished servicing a mirrored
  * operation and are ready to return a success/failure code to the buffer
  * cache layer.
  */
-static inline void raid1_end_buffer_io(struct raid1_bh *r1_bh, int uptodate)
+static void raid1_end_bh_io (struct raid1_bh *r1_bh, int uptodate)
 {
 	struct buffer_head *bh = r1_bh->master_bh;
 
@@ -97,8 +102,6 @@
 	kfree(r1_bh);
 }
 
-int raid1_one_error=0;
-
 void raid1_end_request (struct buffer_head *bh, int uptodate)
 {
 	struct raid1_bh * r1_bh = (struct raid1_bh *)(bh->b_dev_id);
@@ -106,12 +109,7 @@
 
 	save_flags(flags);
 	cli();
-	PRINTK(("raid1_end_request().\n"));
 
-	if (raid1_one_error) {
-		raid1_one_error=0;
-		uptodate=0;
-	}
 	/*
 	 * this branch is our 'one mirror IO has finished' event handler:
 	 */
@@ -136,15 +134,11 @@
 	 */
 
 	if ( (r1_bh->cmd == READ) || (r1_bh->cmd == READA) ) {
-
-		PRINTK(("raid1_end_request(), read branch.\n"));
-
 		/*
 		 * we have only one buffer_head on the read side
 		 */
 		if (uptodate) {
-			PRINTK(("raid1_end_request(), read branch, uptodate.\n"));
-			raid1_end_buffer_io(r1_bh, uptodate);
+			raid1_end_bh_io(r1_bh, uptodate);
 			restore_flags(flags);
 			return;
 		}
@@ -152,71 +146,56 @@
 		 * oops, read error:
 		 */
 		printk(KERN_ERR "raid1: %s: rescheduling block %lu\n", 
-				 kdevname(bh->b_dev), bh->b_blocknr);
-		raid1_reschedule_retry (bh);
+			 partition_name(bh->b_dev), bh->b_blocknr);
+		raid1_reschedule_retry(bh);
 		restore_flags(flags);
 		return;
 	}
 
 	/*
-	 * WRITE or WRITEA.
-	 */
-	PRINTK(("raid1_end_request(), write branch.\n"));
-
-	/*
+	 * WRITE:
+	 *
 	 * Let's see if all mirrored write operations have finished 
-	 * already [we have irqs off, so we can decrease]:
+	 * already.
 	 */
 
-	if (!--r1_bh->remaining) {
-		struct md_dev *mddev = r1_bh->mddev;
-		struct raid1_data *raid_conf = (struct raid1_data *) mddev->private;
-		int i, n = raid_conf->raid_disks;
-
-		PRINTK(("raid1_end_request(), remaining == 0.\n"));
+	if (atomic_dec_and_test(&r1_bh->remaining)) {
+		int i, disks = MD_SB_DISKS;
 
-		for ( i=0; i<n; i++)
-			if (r1_bh->mirror_bh[i]) kfree(r1_bh->mirror_bh[i]);
+		for ( i = 0; i < disks; i++)
+			if (r1_bh->mirror_bh[i])
+				kfree(r1_bh->mirror_bh[i]);
 
-		raid1_end_buffer_io(r1_bh, test_bit(BH_Uptodate, &r1_bh->state));
+		raid1_end_bh_io(r1_bh, test_bit(BH_Uptodate, &r1_bh->state));
 	}
-	else PRINTK(("raid1_end_request(), remaining == %u.\n", r1_bh->remaining));
 	restore_flags(flags);
 }
 
-/* This routine checks if the undelying device is an md device and in that
- * case it maps the blocks before putting the request on the queue
+/*
+ * This routine checks if the undelying device is an md device
+ * and in that case it maps the blocks before putting the
+ * request on the queue
  */
-static inline void
-map_and_make_request (int rw, struct buffer_head *bh)
+static void map_and_make_request (int rw, struct buffer_head *bh)
 {
 	if (MAJOR (bh->b_rdev) == MD_MAJOR)
-		md_map (MINOR (bh->b_rdev), &bh->b_rdev, &bh->b_rsector, bh->b_size >> 9);
+		md_map (bh->b_rdev, &bh->b_rdev,
+				&bh->b_rsector, bh->b_size >> 9);
 	clear_bit(BH_Lock, &bh->b_state);
 	make_request (MAJOR (bh->b_rdev), rw, bh);
 }
 	
-static int
-raid1_make_request (struct md_dev *mddev, int rw, struct buffer_head * bh)
+static int raid1_make_request (mddev_t *mddev, int rw,
+						 struct buffer_head * bh)
 {
-
-	struct raid1_data *raid_conf = (struct raid1_data *) mddev->private;
+	raid1_conf_t *conf = mddev_to_conf(mddev);
 	struct buffer_head *mirror_bh[MD_SB_DISKS], *bh_req;
 	struct raid1_bh * r1_bh;
-	int n = raid_conf->raid_disks, i, sum_bhs = 0, switch_disks = 0, sectors;
+	int disks = MD_SB_DISKS;
+	int i, sum_bhs = 0, switch_disks = 0, sectors, lowprio = 0;
 	struct mirror_info *mirror;
 
-	PRINTK(("raid1_make_request().\n"));
-
-	while (!( /* FIXME: now we are rather fault tolerant than nice */
-	r1_bh = kmalloc (sizeof (struct raid1_bh), GFP_BUFFER)
-	) )
-	{
-		printk ("raid1_make_request(#1): out of memory\n");
-		current->policy |= SCHED_YIELD;
-		schedule();
-	}
-	memset (r1_bh, 0, sizeof (struct raid1_bh));
+	r1_bh = raid1_kmalloc (sizeof (struct raid1_bh));
 
 /*
  * make_request() can abort the operation when READA or WRITEA are being
@@ -227,43 +206,65 @@
 	if (rw == READA) rw = READ;
 	if (rw == WRITEA) rw = WRITE;
 
-	if (rw == WRITE || rw == WRITEA)
-		mark_buffer_clean(bh);		/* Too early ? */
+	if (rw == WRITE) {
+		/*
+		 * Too early ?
+		 */
+		mark_buffer_clean(bh);
+		/*
+		 * not too early. we _first_ clean the bh, then we start
+		 * the IO, then when the IO has finished, we unlock the
+		 * bh and mark it uptodate. This way we do not miss the
+		 * case when the bh got dirty again during the IO.
+		 */
+	}
+
+	/*
+	 * special flag for 'lowprio' reconstruction requests ...
+	 */
+	if (buffer_lowprio(bh))
+		lowprio = 1;
 
 /*
- * i think the read and write branch should be separated completely, since we want
- * to do read balancing on the read side for example. Comments? :) --mingo
+ * i think the read and write branch should be separated completely,
+ * since we want to do read balancing on the read side for example.
+ * Comments? :) --mingo
  */
 
 	r1_bh->master_bh=bh;
 	r1_bh->mddev=mddev;
 	r1_bh->cmd = rw;
 
-	if (rw==READ || rw==READA) {
-		int last_used = raid_conf->last_used;
-		PRINTK(("raid1_make_request(), read branch.\n"));
-		mirror = raid_conf->mirrors + last_used;
+	if (rw==READ) {
+		int last_used = conf->last_used;
+
+		/*
+		 * read balancing logic:
+		 */
+		mirror = conf->mirrors + last_used;
 		bh->b_rdev = mirror->dev;
 		sectors = bh->b_size >> 9;
-		if (bh->b_blocknr * sectors == raid_conf->next_sect) {
-			raid_conf->sect_count += sectors;
-			if (raid_conf->sect_count >= mirror->sect_limit)
+
+		if (bh->b_blocknr * sectors == conf->next_sect) {
+			conf->sect_count += sectors;
+			if (conf->sect_count >= mirror->sect_limit)
 				switch_disks = 1;
 		} else
 			switch_disks = 1;
-		raid_conf->next_sect = (bh->b_blocknr + 1) * sectors;
-		if (switch_disks) {
-			PRINTK(("read-balancing: switching %d -> %d (%d sectors)\n", last_used, mirror->next, raid_conf->sect_count));
-			raid_conf->sect_count = 0;
-			last_used = raid_conf->last_used = mirror->next;
+		conf->next_sect = (bh->b_blocknr + 1) * sectors;
+		/*
+		 * Do not switch disks if full resync is in progress ...
+		 */
+		if (switch_disks && !conf->resync_mirrors) {
+			conf->sect_count = 0;
+			last_used = conf->last_used = mirror->next;
 			/*
-			 * Do not switch to write-only disks ... resyncing
-			 * is in progress
+			 * Do not switch to write-only disks ...
+			 * reconstruction is in progress
 			 */
-			while (raid_conf->mirrors[last_used].write_only)
-				raid_conf->last_used = raid_conf->mirrors[last_used].next;
+			while (conf->mirrors[last_used].write_only)
+				conf->last_used = conf->mirrors[last_used].next;
 		}
-		PRINTK (("raid1 read queue: %d %d\n", MAJOR (bh->b_rdev), MINOR (bh->b_rdev)));
 		bh_req = &r1_bh->bh_req;
 		memcpy(bh_req, bh, sizeof(*bh));
 		bh_req->b_end_io = raid1_end_request;
@@ -273,13 +274,12 @@
 	}
 
 	/*
-	 * WRITE or WRITEA.
+	 * WRITE:
 	 */
-	PRINTK(("raid1_make_request(n=%d), write branch.\n",n));
 
-	for (i = 0; i < n; i++) {
+	for (i = 0; i < disks; i++) {
 
-		if (!raid_conf->mirrors [i].operational) {
+		if (!conf->mirrors[i].operational) {
 			/*
 			 * the r1_bh->mirror_bh[i] pointer remains NULL
 			 */
@@ -287,89 +287,91 @@
 			continue;
 		}
 
+ 		/*
+ 		 * special case for reconstruction ...
+ 		 */
+ 		if (lowprio && (i == conf->last_used)) {
+ 			mirror_bh[i] = NULL;
+ 			continue;
+ 		}
+ 
+  	/*
+  	 * We should use a private pool (size depending on NR_REQUEST),
+  	 * to avoid writes filling up the memory with bhs
+  	 *
+ 	 * Such pools are much faster than kmalloc anyways (so we waste
+ 	 * almost nothing by not using the master bh when writing and
+ 	 * win alot of cleanness) but for now we are cool enough. --mingo
+ 	 *
+  	 * It's safe to sleep here, buffer heads cannot be used in a shared
+ 	 * manner in the write branch. Look how we lock the buffer at the
+ 	 * beginning of this function to grok the difference ;)
+  	 */
+ 		mirror_bh[i] = raid1_kmalloc(sizeof(struct buffer_head));
+  	/*
+  	 * prepare mirrored bh (fields ordered for max mem throughput):
+  	 */
+ 		mirror_bh[i]->b_blocknr    = bh->b_blocknr;
+ 		mirror_bh[i]->b_dev        = bh->b_dev;
+ 		mirror_bh[i]->b_rdev	   = conf->mirrors[i].dev;
+ 		mirror_bh[i]->b_rsector    = bh->b_rsector;
+ 		mirror_bh[i]->b_state      = (1<<BH_Req) | (1<<BH_Dirty);
+ 		if (lowprio)
+ 			mirror_bh[i]->b_state |= (1<<BH_LowPrio);
+ 
+ 		mirror_bh[i]->b_count      = 1;
+ 		mirror_bh[i]->b_size       = bh->b_size;
+ 		mirror_bh[i]->b_data       = bh->b_data;
+ 		mirror_bh[i]->b_list       = BUF_LOCKED;
+ 		mirror_bh[i]->b_end_io     = raid1_end_request;
+ 		mirror_bh[i]->b_dev_id     = r1_bh;
+  
+  		r1_bh->mirror_bh[i] = mirror_bh[i];
+  		sum_bhs++;
+	}
+
+	md_atomic_set(&r1_bh->remaining, sum_bhs);
+
 	/*
-	 * We should use a private pool (size depending on NR_REQUEST),
-	 * to avoid writes filling up the memory with bhs
-	 *
-	 * Such pools are much faster than kmalloc anyways (so we waste almost 
-	 * nothing by not using the master bh when writing and win alot of cleanness)
-	 *
-	 * but for now we are cool enough. --mingo
-	 *
-	 * It's safe to sleep here, buffer heads cannot be used in a shared
-	 * manner in the write branch. Look how we lock the buffer at the beginning
-	 * of this function to grok the difference ;)
-	 */
-		while (!( /* FIXME: now we are rather fault tolerant than nice */
-		mirror_bh[i] = kmalloc (sizeof (struct buffer_head), GFP_BUFFER)
-		) )
-		{
-			printk ("raid1_make_request(#2): out of memory\n");
-			current->policy |= SCHED_YIELD;
-			schedule();
-		}
-		memset (mirror_bh[i], 0, sizeof (struct buffer_head));
-
-	/*
-	 * prepare mirrored bh (fields ordered for max mem throughput):
-	 */
-		mirror_bh [i]->b_blocknr    = bh->b_blocknr;
-		mirror_bh [i]->b_dev        = bh->b_dev;
-		mirror_bh [i]->b_rdev 	    = raid_conf->mirrors [i].dev;
-		mirror_bh [i]->b_rsector    = bh->b_rsector;
-		mirror_bh [i]->b_state      = (1<<BH_Req) | (1<<BH_Dirty);
-		mirror_bh [i]->b_count      = 1;
-		mirror_bh [i]->b_size       = bh->b_size;
-		mirror_bh [i]->b_data       = bh->b_data;
-		mirror_bh [i]->b_list       = BUF_LOCKED;
-		mirror_bh [i]->b_end_io     = raid1_end_request;
-		mirror_bh [i]->b_dev_id     = r1_bh;
-
-		r1_bh->mirror_bh[i] = mirror_bh[i];
-		sum_bhs++;
-	}
-
-	r1_bh->remaining = sum_bhs;
-
-	PRINTK(("raid1_make_request(), write branch, sum_bhs=%d.\n",sum_bhs));
-
-	/*
-	 * We have to be a bit careful about the semaphore above, thats why we
-	 * start the requests separately. Since kmalloc() could fail, sleep and
-	 * make_request() can sleep too, this is the safer solution. Imagine,
-	 * end_request decreasing the semaphore before we could have set it up ...
-	 * We could play tricks with the semaphore (presetting it and correcting
-	 * at the end if sum_bhs is not 'n' but we have to do end_request by hand
-	 * if all requests finish until we had a chance to set up the semaphore
-	 * correctly ... lots of races).
-	 */
-	for (i = 0; i < n; i++)
-		if (mirror_bh [i] != NULL)
-			map_and_make_request (rw, mirror_bh [i]);
+	 * We have to be a bit careful about the semaphore above, thats
+	 * why we start the requests separately. Since kmalloc() could
+	 * fail, sleep and make_request() can sleep too, this is the
+	 * safer solution. Imagine, end_request decreasing the semaphore
+	 * before we could have set it up ... We could play tricks with
+	 * the semaphore (presetting it and correcting at the end if
+	 * sum_bhs is not 'n' but we have to do end_request by hand if
+	 * all requests finish until we had a chance to set up the
+	 * semaphore correctly ... lots of races).
+	 */
+	for (i = 0; i < disks; i++)
+		if (mirror_bh[i])
+			map_and_make_request(rw, mirror_bh[i]);
 
 	return (0);
 }
 			   
-static int raid1_status (char *page, int minor, struct md_dev *mddev)
+static int raid1_status (char *page, mddev_t *mddev)
 {
-	struct raid1_data *raid_conf = (struct raid1_data *) mddev->private;
+	raid1_conf_t *conf = mddev_to_conf(mddev);
 	int sz = 0, i;
 	
-	sz += sprintf (page+sz, " [%d/%d] [", raid_conf->raid_disks, raid_conf->working_disks);
-	for (i = 0; i < raid_conf->raid_disks; i++)
-		sz += sprintf (page+sz, "%s", raid_conf->mirrors [i].operational ? "U" : "_");
+	sz += sprintf (page+sz, " [%d/%d] [", conf->raid_disks,
+						 conf->working_disks);
+	for (i = 0; i < conf->raid_disks; i++)
+		sz += sprintf (page+sz, "%s",
+			conf->mirrors[i].operational ? "U" : "_");
 	sz += sprintf (page+sz, "]");
 	return sz;
 }
 
-static void raid1_fix_links (struct raid1_data *raid_conf, int failed_index)
+static void unlink_disk (raid1_conf_t *conf, int target)
 {
-	int disks = raid_conf->raid_disks;
-	int j;
+	int disks = MD_SB_DISKS;
+	int i;
 
-	for (j = 0; j < disks; j++)
-		if (raid_conf->mirrors [j].next == failed_index)
-			raid_conf->mirrors [j].next = raid_conf->mirrors [failed_index].next;
+	for (i = 0; i < disks; i++)
+		if (conf->mirrors[i].next == target)
+			conf->mirrors[i].next = conf->mirrors[target].next;
 }
 
 #define LAST_DISK KERN_ALERT \
@@ -388,48 +390,53 @@
 #define ALREADY_SYNCING KERN_INFO \
 "raid1: syncing already in progress.\n"
 
-static int raid1_error (struct md_dev *mddev, kdev_t dev)
+static void mark_disk_bad (mddev_t *mddev, int failed)
 {
-	struct raid1_data *raid_conf = (struct raid1_data *) mddev->private;
-	struct mirror_info *mirror;
-	md_superblock_t *sb = mddev->sb;
-	int disks = raid_conf->raid_disks;
-	int i;
+	raid1_conf_t *conf = mddev_to_conf(mddev);
+	struct mirror_info *mirror = conf->mirrors+failed;
+	mdp_super_t *sb = mddev->sb;
+
+	mirror->operational = 0;
+	unlink_disk(conf, failed);
+	mark_disk_faulty(sb->disks+mirror->number);
+	mark_disk_nonsync(sb->disks+mirror->number);
+	mark_disk_inactive(sb->disks+mirror->number);
+	sb->active_disks--;
+	sb->working_disks--;
+	sb->failed_disks++;
+	mddev->sb_dirty = 1;
+	md_wakeup_thread(conf->thread);
+	conf->working_disks--;
+	printk (DISK_FAILED, partition_name (mirror->dev),
+				 conf->working_disks);
+}
 
-	PRINTK(("raid1_error called\n"));
+static int raid1_error (mddev_t *mddev, kdev_t dev)
+{
+	raid1_conf_t *conf = mddev_to_conf(mddev);
+	struct mirror_info * mirrors = conf->mirrors;
+	int disks = MD_SB_DISKS;
+	int i;
 
-	if (raid_conf->working_disks == 1) {
+	if (conf->working_disks == 1) {
 		/*
 		 * Uh oh, we can do nothing if this is our last disk, but
 		 * first check if this is a queued request for a device
 		 * which has just failed.
 		 */
-		for (i = 0, mirror = raid_conf->mirrors; i < disks;
-				 i++, mirror++)
-			if (mirror->dev == dev && !mirror->operational)
+		for (i = 0; i < disks; i++) {
+			if (mirrors[i].dev==dev && !mirrors[i].operational)
 				return 0;
+		}
 		printk (LAST_DISK);
 	} else {
-		/* Mark disk as unusable */
-		for (i = 0, mirror = raid_conf->mirrors; i < disks;
-				 i++, mirror++) {
-			if (mirror->dev == dev && mirror->operational){
-				mirror->operational = 0;
-				raid1_fix_links (raid_conf, i);
-				sb->disks[mirror->number].state |=
-						(1 << MD_FAULTY_DEVICE);
-				sb->disks[mirror->number].state &=
-						~(1 << MD_SYNC_DEVICE);
-				sb->disks[mirror->number].state &=
-						~(1 << MD_ACTIVE_DEVICE);
-				sb->active_disks--;
-				sb->working_disks--;
-				sb->failed_disks++;
-				mddev->sb_dirty = 1;
-				md_wakeup_thread(raid1_thread);
-				raid_conf->working_disks--;
-				printk (DISK_FAILED, kdevname (dev),
-						raid_conf->working_disks);
+		/*
+		 * Mark disk as unusable
+		 */
+		for (i = 0; i < disks; i++) {
+			if (mirrors[i].dev==dev && mirrors[i].operational) {
+				mark_disk_bad (mddev, i);
+				break;
 			}
 		}
 	}
@@ -442,219 +449,396 @@
 #undef START_SYNCING
 
 /*
- * This is the personality-specific hot-addition routine
+ * Insert the spare disk into the drive-ring
  */
+static void link_disk(raid1_conf_t *conf, struct mirror_info *mirror)
+{
+	int j, next;
+	int disks = MD_SB_DISKS;
+	struct mirror_info *p = conf->mirrors;
 
-#define NO_SUPERBLOCK KERN_ERR \
-"raid1: cannot hot-add disk to the array with no RAID superblock\n"
+	for (j = 0; j < disks; j++, p++)
+		if (p->operational && !p->write_only) {
+			next = p->next;
+			p->next = mirror->raid_disk;
+			mirror->next = next;
+			return;
+		}
 
-#define WRONG_LEVEL KERN_ERR \
-"raid1: hot-add: level of disk is not RAID-1\n"
+	printk("raid1: bug: no read-operational devices\n");
+}
 
-#define HOT_ADD_SUCCEEDED KERN_INFO \
-"raid1: device %s hot-added\n"
+static void print_raid1_conf (raid1_conf_t *conf)
+{
+	int i;
+	struct mirror_info *tmp;
 
-static int raid1_hot_add_disk (struct md_dev *mddev, kdev_t dev)
+	printk("RAID1 conf printout:\n");
+	if (!conf) {
+		printk("(conf==NULL)\n");
+		return;
+	}
+	printk(" --- wd:%d rd:%d nd:%d\n", conf->working_disks,
+			 conf->raid_disks, conf->nr_disks);
+
+	for (i = 0; i < MD_SB_DISKS; i++) {
+		tmp = conf->mirrors + i;
+		printk(" disk %d, s:%d, o:%d, n:%d rd:%d us:%d dev:%s\n",
+			i, tmp->spare,tmp->operational,
+			tmp->number,tmp->raid_disk,tmp->used_slot,
+			partition_name(tmp->dev));
+	}
+}
+
+static int raid1_diskop(mddev_t *mddev, mdp_disk_t **d, int state)
 {
+	int err = 0;
+	int i, failed_disk=-1, spare_disk=-1, removed_disk=-1, added_disk=-1;
+	raid1_conf_t *conf = mddev->private;
+	struct mirror_info *tmp, *sdisk, *fdisk, *rdisk, *adisk;
 	unsigned long flags;
-	struct raid1_data *raid_conf = (struct raid1_data *) mddev->private;
-	struct mirror_info *mirror;
-	md_superblock_t *sb = mddev->sb;
-	struct real_dev * realdev;
-	int n;
+	mdp_super_t *sb = mddev->sb;
+	mdp_disk_t *failed_desc, *spare_desc, *added_desc;
+
+	save_flags(flags);
+	cli();
 
+	print_raid1_conf(conf);
 	/*
-	 * The device has its superblock already read and it was found
-	 * to be consistent for generic RAID usage.  Now we check whether
-	 * it's usable for RAID-1 hot addition.
+	 * find the disk ...
 	 */
+	switch (state) {
 
-	n = mddev->nb_dev++;
-	realdev = &mddev->devices[n];
-	if (!realdev->sb) {
-		printk (NO_SUPERBLOCK);
-		return -EINVAL;
-	}
-	if (realdev->sb->level != 1) {
-		printk (WRONG_LEVEL);
-		return -EINVAL;
+	case DISKOP_SPARE_ACTIVE:
+
+		/*
+		 * Find the failed disk within the RAID1 configuration ...
+		 * (this can only be in the first conf->working_disks part)
+		 */
+		for (i = 0; i < conf->raid_disks; i++) {
+			tmp = conf->mirrors + i;
+			if ((!tmp->operational && !tmp->spare) ||
+					!tmp->used_slot) {
+				failed_disk = i;
+				break;
+			}
+		}
+		/*
+		 * When we activate a spare disk we _must_ have a disk in
+		 * the lower (active) part of the array to replace. 
+		 */
+		if ((failed_disk == -1) || (failed_disk >= conf->raid_disks)) {
+			MD_BUG();
+			err = 1;
+			goto abort;
+		}
+		/* fall through */
+
+	case DISKOP_SPARE_WRITE:
+	case DISKOP_SPARE_INACTIVE:
+
+		/*
+		 * Find the spare disk ... (can only be in the 'high'
+		 * area of the array)
+		 */
+		for (i = conf->raid_disks; i < MD_SB_DISKS; i++) {
+			tmp = conf->mirrors + i;
+			if (tmp->spare && tmp->number == (*d)->number) {
+				spare_disk = i;
+				break;
+			}
+		}
+		if (spare_disk == -1) {
+			MD_BUG();
+			err = 1;
+			goto abort;
+		}
+		break;
+
+	case DISKOP_HOT_REMOVE_DISK:
+
+		for (i = 0; i < MD_SB_DISKS; i++) {
+			tmp = conf->mirrors + i;
+			if (tmp->used_slot && (tmp->number == (*d)->number)) {
+				if (tmp->operational) {
+					err = -EBUSY;
+					goto abort;
+				}
+				removed_disk = i;
+				break;
+			}
+		}
+		if (removed_disk == -1) {
+			MD_BUG();
+			err = 1;
+			goto abort;
+		}
+		break;
+
+	case DISKOP_HOT_ADD_DISK:
+
+		for (i = conf->raid_disks; i < MD_SB_DISKS; i++) {
+			tmp = conf->mirrors + i;
+			if (!tmp->used_slot) {
+				added_disk = i;
+				break;
+			}
+		}
+		if (added_disk == -1) {
+			MD_BUG();
+			err = 1;
+			goto abort;
+		}
+		break;
 	}
-	/* FIXME: are there other things left we could sanity-check? */
 
+	switch (state) {
 	/*
-	 * We have to disable interrupts, as our RAID-1 state is used
-	 * from irq handlers as well.
+	 * Switch the spare disk to write-only mode:
 	 */
-	save_flags(flags);
-	cli();
+	case DISKOP_SPARE_WRITE:
+		sdisk = conf->mirrors + spare_disk;
+		sdisk->operational = 1;
+		sdisk->write_only = 1;
+		break;
+	/*
+	 * Deactivate a spare disk:
+	 */
+	case DISKOP_SPARE_INACTIVE:
+		sdisk = conf->mirrors + spare_disk;
+		sdisk->operational = 0;
+		sdisk->write_only = 0;
+		break;
+	/*
+	 * Activate (mark read-write) the (now sync) spare disk,
+	 * which means we switch it's 'raid position' (->raid_disk)
+	 * with the failed disk. (only the first 'conf->nr_disks'
+	 * slots are used for 'real' disks and we must preserve this
+	 * property)
+	 */
+	case DISKOP_SPARE_ACTIVE:
 
-	raid_conf->raid_disks++;
-	mirror = raid_conf->mirrors+n;
+		sdisk = conf->mirrors + spare_disk;
+		fdisk = conf->mirrors + failed_disk;
 
-	mirror->number=n;
-	mirror->raid_disk=n;
-	mirror->dev=dev;
-	mirror->next=0; /* FIXME */
-	mirror->sect_limit=128;
-
-	mirror->operational=0;
-	mirror->spare=1;
-	mirror->write_only=0;
-
-	sb->disks[n].state |= (1 << MD_FAULTY_DEVICE);
-	sb->disks[n].state &= ~(1 << MD_SYNC_DEVICE);
-	sb->disks[n].state &= ~(1 << MD_ACTIVE_DEVICE);
-	sb->nr_disks++;
-	sb->spare_disks++;
+		spare_desc = &sb->disks[sdisk->number];
+		failed_desc = &sb->disks[fdisk->number];
 
-	restore_flags(flags);
+		if (spare_desc != *d) {
+			MD_BUG();
+			err = 1;
+			goto abort;
+		}
 
-	md_update_sb(MINOR(dev));
+		if (spare_desc->raid_disk != sdisk->raid_disk) {
+			MD_BUG();
+			err = 1;
+			goto abort;
+		}
+			
+		if (sdisk->raid_disk != spare_disk) {
+			MD_BUG();
+			err = 1;
+			goto abort;
+		}
 
-	printk (HOT_ADD_SUCCEEDED, kdevname(realdev->dev));
+		if (failed_desc->raid_disk != fdisk->raid_disk) {
+			MD_BUG();
+			err = 1;
+			goto abort;
+		}
 
-	return 0;
-}
+		if (fdisk->raid_disk != failed_disk) {
+			MD_BUG();
+			err = 1;
+			goto abort;
+		}
 
-#undef NO_SUPERBLOCK
-#undef WRONG_LEVEL
-#undef HOT_ADD_SUCCEEDED
+		/*
+		 * do the switch finally
+		 */
+		xchg_values(*spare_desc, *failed_desc);
+		xchg_values(*fdisk, *sdisk);
 
-/*
- * Insert the spare disk into the drive-ring
- */
-static void add_ring(struct raid1_data *raid_conf, struct mirror_info *mirror)
-{
-	int j, next;
-	struct mirror_info *p = raid_conf->mirrors;
+		/*
+		 * (careful, 'failed' and 'spare' are switched from now on)
+		 *
+		 * we want to preserve linear numbering and we want to
+		 * give the proper raid_disk number to the now activated
+		 * disk. (this means we switch back these values)
+		 */
+	
+		xchg_values(spare_desc->raid_disk, failed_desc->raid_disk);
+		xchg_values(sdisk->raid_disk, fdisk->raid_disk);
+		xchg_values(spare_desc->number, failed_desc->number);
+		xchg_values(sdisk->number, fdisk->number);
 
-	for (j = 0; j < raid_conf->raid_disks; j++, p++)
-		if (p->operational && !p->write_only) {
-			next = p->next;
-			p->next = mirror->raid_disk;
-			mirror->next = next;
-			return;
-		}
-	printk("raid1: bug: no read-operational devices\n");
-}
+		*d = failed_desc;
 
-static int raid1_mark_spare(struct md_dev *mddev, md_descriptor_t *spare,
-				int state)
-{
-	int i = 0, failed_disk = -1;
-	struct raid1_data *raid_conf = mddev->private;
-	struct mirror_info *mirror = raid_conf->mirrors;
-	md_descriptor_t *descriptor;
-	unsigned long flags;
+		if (sdisk->dev == MKDEV(0,0))
+			sdisk->used_slot = 0;
+		/*
+		 * this really activates the spare.
+		 */
+		fdisk->spare = 0;
+		fdisk->write_only = 0;
+		link_disk(conf, fdisk);
 
-	for (i = 0; i < MD_SB_DISKS; i++, mirror++) {
-		if (mirror->spare && mirror->number == spare->number)
-			goto found;
-	}
-	return 1;
-found:
-	for (i = 0, mirror = raid_conf->mirrors; i < raid_conf->raid_disks;
-								i++, mirror++)
-		if (!mirror->operational)
-			failed_disk = i;
+		/*
+		 * if we activate a spare, we definitely replace a
+		 * non-operational disk slot in the 'low' area of
+		 * the disk array.
+		 */
 
-	save_flags(flags);
-	cli();
-	switch (state) {
-		case SPARE_WRITE:
-			mirror->operational = 1;
-			mirror->write_only = 1;
-			raid_conf->raid_disks = MAX(raid_conf->raid_disks,
-							mirror->raid_disk + 1);
-			break;
-		case SPARE_INACTIVE:
-			mirror->operational = 0;
-			mirror->write_only = 0;
-			break;
-		case SPARE_ACTIVE:
-			mirror->spare = 0;
-			mirror->write_only = 0;
-			raid_conf->working_disks++;
-			add_ring(raid_conf, mirror);
-
-			if (failed_disk != -1) {
-				descriptor = &mddev->sb->disks[raid_conf->mirrors[failed_disk].number];
-				i = spare->raid_disk;
-				spare->raid_disk = descriptor->raid_disk;
-				descriptor->raid_disk = i;
-			}
-			break;
-		default:
-			printk("raid1_mark_spare: bug: state == %d\n", state);
-			restore_flags(flags);
-			return 1;
+		conf->working_disks++;
+
+		break;
+
+	case DISKOP_HOT_REMOVE_DISK:
+		rdisk = conf->mirrors + removed_disk;
+
+		if (rdisk->spare && (removed_disk < conf->raid_disks)) {
+			MD_BUG();	
+			err = 1;
+			goto abort;
+		}
+		rdisk->dev = MKDEV(0,0);
+		rdisk->used_slot = 0;
+		conf->nr_disks--;
+		break;
+
+	case DISKOP_HOT_ADD_DISK:
+		adisk = conf->mirrors + added_disk;
+		added_desc = *d;
+
+		if (added_disk != added_desc->number) {
+			MD_BUG();	
+			err = 1;
+			goto abort;
+		}
+
+		adisk->number = added_desc->number;
+		adisk->raid_disk = added_desc->raid_disk;
+		adisk->dev = MKDEV(added_desc->major,added_desc->minor);
+
+		adisk->operational = 0;
+		adisk->write_only = 0;
+		adisk->spare = 1;
+		adisk->used_slot = 1;
+		conf->nr_disks++;
+
+		break;
+
+	default:
+		MD_BUG();	
+		err = 1;
+		goto abort;
 	}
+abort:
 	restore_flags(flags);
-	return 0;
+	print_raid1_conf(conf);
+	return err;
 }
 
+
+#define IO_ERROR KERN_ALERT \
+"raid1: %s: unrecoverable I/O read error for block %lu\n"
+
+#define REDIRECT_SECTOR KERN_ERR \
+"raid1: %s: redirecting sector %lu to another mirror\n"
+
 /*
  * This is a kernel thread which:
  *
  *	1.	Retries failed read operations on working mirrors.
  *	2.	Updates the raid superblock when problems encounter.
  */
-void raid1d (void *data)
+static void raid1d (void *data)
 {
 	struct buffer_head *bh;
 	kdev_t dev;
 	unsigned long flags;
-	struct raid1_bh * r1_bh;
-	struct md_dev *mddev;
+	struct raid1_bh *r1_bh;
+	mddev_t *mddev;
 
-	PRINTK(("raid1d() active\n"));
-	save_flags(flags);
-	cli();
 	while (raid1_retry_list) {
+		save_flags(flags);
+		cli();
 		bh = raid1_retry_list;
 		r1_bh = (struct raid1_bh *)(bh->b_dev_id);
 		raid1_retry_list = r1_bh->next_retry;
 		restore_flags(flags);
 
-		mddev = md_dev + MINOR(bh->b_dev);
+		mddev = kdev_to_mddev(bh->b_dev);
 		if (mddev->sb_dirty) {
-			printk("dirty sb detected, updating.\n");
+			printk(KERN_INFO "dirty sb detected, updating.\n");
 			mddev->sb_dirty = 0;
-			md_update_sb(MINOR(bh->b_dev));
+			md_update_sb(mddev);
 		}
 		dev = bh->b_rdev;
-		__raid1_map (md_dev + MINOR(bh->b_dev), &bh->b_rdev, &bh->b_rsector, bh->b_size >> 9);
+		__raid1_map (mddev, &bh->b_rdev, &bh->b_rsector,
+							 bh->b_size >> 9);
 		if (bh->b_rdev == dev) {
-			printk (KERN_ALERT 
-					"raid1: %s: unrecoverable I/O read error for block %lu\n",
-						kdevname(bh->b_dev), bh->b_blocknr);
-			raid1_end_buffer_io(r1_bh, 0);
+			printk (IO_ERROR, partition_name(bh->b_dev), bh->b_blocknr);
+			raid1_end_bh_io(r1_bh, 0);
 		} else {
-			printk (KERN_ERR "raid1: %s: redirecting sector %lu to another mirror\n", 
-					  kdevname(bh->b_dev), bh->b_blocknr);
+			printk (REDIRECT_SECTOR,
+				partition_name(bh->b_dev), bh->b_blocknr);
 			map_and_make_request (r1_bh->cmd, bh);
 		}
-		cli();
 	}
-	restore_flags(flags);
+}
+#undef IO_ERROR
+#undef REDIRECT_SECTOR
+
+/*
+ * Private kernel thread to reconstruct mirrors after an unclean
+ * shutdown.
+ */
+static void raid1syncd (void *data)
+{
+        raid1_conf_t *conf = data;
+        mddev_t *mddev = conf->mddev;
+
+        if (!conf->resync_mirrors)
+                return;
+        if (conf->resync_mirrors == 2)
+                return;
+	down(&mddev->recovery_sem);
+        if (md_do_sync(mddev, NULL)) {
+		up(&mddev->recovery_sem);
+		return;
+	}
+	/*
+	 * Only if everything went Ok.
+	 */
+        conf->resync_mirrors = 0;
+	up(&mddev->recovery_sem);
 }
 
+
 /*
  * This will catch the scenario in which one of the mirrors was
  * mounted as a normal device rather than as a part of a raid set.
+ *
+ * check_consistency is very personality-dependent, eg. RAID5 cannot
+ * do this check, it uses another method.
  */
-static int __check_consistency (struct md_dev *mddev, int row)
+static int __check_consistency (mddev_t *mddev, int row)
 {
-	struct raid1_data *raid_conf = mddev->private;
+	raid1_conf_t *conf = mddev_to_conf(mddev);
+	int disks = MD_SB_DISKS;
 	kdev_t dev;
 	struct buffer_head *bh = NULL;
 	int i, rc = 0;
 	char *buffer = NULL;
 
-	for (i = 0; i < raid_conf->raid_disks; i++) {
-		if (!raid_conf->mirrors[i].operational)
+	for (i = 0; i < disks; i++) {
+		printk("(checking disk %d)\n",i);
+		if (!conf->mirrors[i].operational)
 			continue;
-		dev = raid_conf->mirrors[i].dev;
+		printk("(really checking disk %d)\n",i);
+		dev = conf->mirrors[i].dev;
 		set_blocksize(dev, 4096);
 		if ((bh = bread(dev, row / 4, 4096)) == NULL)
 			break;
@@ -683,167 +867,342 @@
 	return rc;
 }
 
-static int check_consistency (struct md_dev *mddev)
+static int check_consistency (mddev_t *mddev)
 {
-	int size = mddev->sb->size;
-	int row;
+	if (__check_consistency(mddev, 0))
+/*
+ * we do not do this currently, as it's perfectly possible to
+ * have an inconsistent array when it's freshly created. Only
+ * newly written data has to be consistent.
+ */
+		return 0;
 
-	for (row = 0; row < size; row += size / 8)
-		if (__check_consistency(mddev, row))
-			return 1;
 	return 0;
 }
 
-static int raid1_run (int minor, struct md_dev *mddev)
+#define INVALID_LEVEL KERN_WARNING \
+"raid1: md%d: raid level not set to mirroring (%d)\n"
+
+#define NO_SB KERN_ERR \
+"raid1: disabled mirror %s (couldn't access raid superblock)\n"
+
+#define ERRORS KERN_ERR \
+"raid1: disabled mirror %s (errors detected)\n"
+
+#define NOT_IN_SYNC KERN_ERR \
+"raid1: disabled mirror %s (not in sync)\n"
+
+#define INCONSISTENT KERN_ERR \
+"raid1: disabled mirror %s (inconsistent descriptor)\n"
+
+#define ALREADY_RUNNING KERN_ERR \
+"raid1: disabled mirror %s (mirror %d already operational)\n"
+
+#define OPERATIONAL KERN_INFO \
+"raid1: device %s operational as mirror %d\n"
+
+#define MEM_ERROR KERN_ERR \
+"raid1: couldn't allocate memory for md%d\n"
+
+#define SPARE KERN_INFO \
+"raid1: spare disk %s\n"
+
+#define NONE_OPERATIONAL KERN_ERR \
+"raid1: no operational mirrors for md%d\n"
+
+#define RUNNING_CKRAID KERN_ERR \
+"raid1: detected mirror differences -- running resync\n"
+
+#define ARRAY_IS_ACTIVE KERN_INFO \
+"raid1: raid set md%d active with %d out of %d mirrors\n"
+
+#define THREAD_ERROR KERN_ERR \
+"raid1: couldn't allocate thread for md%d\n"
+
+#define START_RESYNC KERN_WARNING \
+"raid1: raid set md%d not clean; reconstructing mirrors\n"
+
+static int raid1_run (mddev_t *mddev)
 {
-	struct raid1_data *raid_conf;
-	int i, j, raid_disk;
-	md_superblock_t *sb = mddev->sb;
-	md_descriptor_t *descriptor;
-	struct real_dev *realdev;
+	raid1_conf_t *conf;
+	int i, j, disk_idx;
+	struct mirror_info *disk;
+	mdp_super_t *sb = mddev->sb;
+	mdp_disk_t *descriptor;
+	mdk_rdev_t *rdev;
+	struct md_list_head *tmp;
+	int start_recovery = 0;
 
 	MOD_INC_USE_COUNT;
 
 	if (sb->level != 1) {
-		printk("raid1: %s: raid level not set to mirroring (%d)\n",
-				kdevname(MKDEV(MD_MAJOR, minor)), sb->level);
-		MOD_DEC_USE_COUNT;
-		return -EIO;
-	}
-	/****
-	 * copy the now verified devices into our private RAID1 bookkeeping
-	 * area. [whatever we allocate in raid1_run(), should be freed in
-	 * raid1_stop()]
+		printk(INVALID_LEVEL, mdidx(mddev), sb->level);
+		goto out;
+	}
+	/*
+	 * copy the already verified devices into our private RAID1
+	 * bookkeeping area. [whatever we allocate in raid1_run(),
+	 * should be freed in raid1_stop()]
 	 */
 
-	while (!( /* FIXME: now we are rather fault tolerant than nice */
-	mddev->private = kmalloc (sizeof (struct raid1_data), GFP_KERNEL)
-	) )
-	{
-		printk ("raid1_run(): out of memory\n");
-		current->policy |= SCHED_YIELD;
-		schedule();
-	}
-	raid_conf = mddev->private;
-	memset(raid_conf, 0, sizeof(*raid_conf));
-
-	PRINTK(("raid1_run(%d) called.\n", minor));
-
-  	for (i = 0; i < mddev->nb_dev; i++) {
-  		realdev = &mddev->devices[i];
-		if (!realdev->sb) {
-			printk(KERN_ERR "raid1: disabled mirror %s (couldn't access raid superblock)\n", kdevname(realdev->dev));
+	conf = raid1_kmalloc(sizeof(raid1_conf_t));
+	mddev->private = conf;
+	if (!conf) {
+		printk(MEM_ERROR, mdidx(mddev));
+		goto out;
+	}
+
+	ITERATE_RDEV(mddev,rdev,tmp) {
+		if (rdev->faulty) {
+			printk(ERRORS, partition_name(rdev->dev));
+		} else {
+			if (!rdev->sb) {
+				MD_BUG();
+				continue;
+			}
+		}
+		if (rdev->desc_nr == -1) {
+			MD_BUG();
 			continue;
 		}
-
-		/*
-		 * This is important -- we are using the descriptor on
-		 * the disk only to get a pointer to the descriptor on
-		 * the main superblock, which might be more recent.
-		 */
-		descriptor = &sb->disks[realdev->sb->descriptor.number];
-		if (descriptor->state & (1 << MD_FAULTY_DEVICE)) {
-			printk(KERN_ERR "raid1: disabled mirror %s (errors detected)\n", kdevname(realdev->dev));
+		descriptor = &sb->disks[rdev->desc_nr];
+		disk_idx = descriptor->raid_disk;
+		disk = conf->mirrors + disk_idx;
+
+		if (disk_faulty(descriptor)) {
+			disk->number = descriptor->number;
+			disk->raid_disk = disk_idx;
+			disk->dev = rdev->dev;
+			disk->sect_limit = MAX_LINEAR_SECTORS;
+			disk->operational = 0;
+			disk->write_only = 0;
+			disk->spare = 0;
+			disk->used_slot = 1;
 			continue;
 		}
-		if (descriptor->state & (1 << MD_ACTIVE_DEVICE)) {
-			if (!(descriptor->state & (1 << MD_SYNC_DEVICE))) {
-				printk(KERN_ERR "raid1: disabled mirror %s (not in sync)\n", kdevname(realdev->dev));
+		if (disk_active(descriptor)) {
+			if (!disk_sync(descriptor)) {
+				printk(NOT_IN_SYNC,
+					partition_name(rdev->dev));
 				continue;
 			}
-			raid_disk = descriptor->raid_disk;
-			if (descriptor->number > sb->nr_disks || raid_disk > sb->raid_disks) {
-				printk(KERN_ERR "raid1: disabled mirror %s (inconsistent descriptor)\n", kdevname(realdev->dev));
+			if ((descriptor->number > MD_SB_DISKS) ||
+					 (disk_idx > sb->raid_disks)) {
+
+				printk(INCONSISTENT,
+					partition_name(rdev->dev));
 				continue;
 			}
-			if (raid_conf->mirrors[raid_disk].operational) {
-				printk(KERN_ERR "raid1: disabled mirror %s (mirror %d already operational)\n", kdevname(realdev->dev), raid_disk);
+			if (disk->operational) {
+				printk(ALREADY_RUNNING,
+					partition_name(rdev->dev),
+					disk_idx);
 				continue;
 			}
-			printk(KERN_INFO "raid1: device %s operational as mirror %d\n", kdevname(realdev->dev), raid_disk);
-			raid_conf->mirrors[raid_disk].number = descriptor->number;
-			raid_conf->mirrors[raid_disk].raid_disk = raid_disk;
-			raid_conf->mirrors[raid_disk].dev = mddev->devices [i].dev;
-			raid_conf->mirrors[raid_disk].operational = 1;
-			raid_conf->mirrors[raid_disk].sect_limit = 128;
-			raid_conf->working_disks++;
+			printk(OPERATIONAL, partition_name(rdev->dev),
+ 					disk_idx);
+			disk->number = descriptor->number;
+			disk->raid_disk = disk_idx;
+			disk->dev = rdev->dev;
+			disk->sect_limit = MAX_LINEAR_SECTORS;
+			disk->operational = 1;
+			disk->write_only = 0;
+			disk->spare = 0;
+			disk->used_slot = 1;
+			conf->working_disks++;
 		} else {
 		/*
 		 * Must be a spare disk ..
 		 */
-			printk(KERN_INFO "raid1: spare disk %s\n", kdevname(realdev->dev));
-			raid_disk = descriptor->raid_disk;
-			raid_conf->mirrors[raid_disk].number = descriptor->number;
-			raid_conf->mirrors[raid_disk].raid_disk = raid_disk;
-			raid_conf->mirrors[raid_disk].dev = mddev->devices [i].dev;
-			raid_conf->mirrors[raid_disk].sect_limit = 128;
-
-			raid_conf->mirrors[raid_disk].operational = 0;
-			raid_conf->mirrors[raid_disk].write_only = 0;
-			raid_conf->mirrors[raid_disk].spare = 1;
-		}
-	}
-	if (!raid_conf->working_disks) {
-		printk(KERN_ERR "raid1: no operational mirrors for %s\n", kdevname(MKDEV(MD_MAJOR, minor)));
-		kfree(raid_conf);
-		mddev->private = NULL;
-		MOD_DEC_USE_COUNT;
-		return -EIO;
-	}
-
-	raid_conf->raid_disks = sb->raid_disks;
-	raid_conf->mddev = mddev;
-
-	for (j = 0; !raid_conf->mirrors[j].operational; j++);
-	raid_conf->last_used = j;
-	for (i = raid_conf->raid_disks - 1; i >= 0; i--) {
-		if (raid_conf->mirrors[i].operational) {
-			PRINTK(("raid_conf->mirrors[%d].next == %d\n", i, j));
-			raid_conf->mirrors[i].next = j;
+			printk(SPARE, partition_name(rdev->dev));
+			disk->number = descriptor->number;
+			disk->raid_disk = disk_idx;
+			disk->dev = rdev->dev;
+			disk->sect_limit = MAX_LINEAR_SECTORS;
+			disk->operational = 0;
+			disk->write_only = 0;
+			disk->spare = 1;
+			disk->used_slot = 1;
+		}
+	}
+	if (!conf->working_disks) {
+		printk(NONE_OPERATIONAL, mdidx(mddev));
+		goto out_free_conf;
+	}
+
+	conf->raid_disks = sb->raid_disks;
+	conf->nr_disks = sb->nr_disks;
+	conf->mddev = mddev;
+
+	for (i = 0; i < MD_SB_DISKS; i++) {
+		
+		descriptor = sb->disks+i;
+		disk_idx = descriptor->raid_disk;
+		disk = conf->mirrors + disk_idx;
+
+		if (disk_faulty(descriptor) && (disk_idx < conf->raid_disks) &&
+				!disk->used_slot) {
+
+			disk->number = descriptor->number;
+			disk->raid_disk = disk_idx;
+			disk->dev = MKDEV(0,0);
+
+			disk->operational = 0;
+			disk->write_only = 0;
+			disk->spare = 0;
+			disk->used_slot = 1;
+		}
+	}
+
+	/*
+	 * find the first working one and use it as a starting point
+	 * to read balancing.
+	 */
+	for (j = 0; !conf->mirrors[j].operational; j++)
+		/* nothing */;
+	conf->last_used = j;
+
+	/*
+	 * initialize the 'working disks' list.
+	 */
+	for (i = conf->raid_disks - 1; i >= 0; i--) {
+		if (conf->mirrors[i].operational) {
+			conf->mirrors[i].next = j;
 			j = i;
 		}
 	}
 
-	if (check_consistency(mddev)) {
-		printk(KERN_ERR "raid1: detected mirror differences -- run ckraid\n");
-		sb->state |= 1 << MD_SB_ERRORS;
-		kfree(raid_conf);
-		mddev->private = NULL;
-		MOD_DEC_USE_COUNT;
-		return -EIO;
+	if (conf->working_disks != sb->raid_disks) {
+		printk(KERN_ALERT "raid1: md%d, not all disks are operational -- trying to recover array\n", mdidx(mddev));
+		start_recovery = 1;
 	}
 
+	if (!start_recovery && (sb->state & (1 << MD_SB_CLEAN))) {
+		/*
+		 * we do sanity checks even if the device says
+		 * it's clean ...
+		 */
+		if (check_consistency(mddev)) {
+			printk(RUNNING_CKRAID);
+			sb->state &= ~(1 << MD_SB_CLEAN);
+		}
+	}
+
+	{
+		const char * name = "raid1d";
+
+		conf->thread = md_register_thread(raid1d, conf, name);
+		if (!conf->thread) {
+			printk(THREAD_ERROR, mdidx(mddev));
+			goto out_free_conf;
+        	}
+	}
+
+	if (!start_recovery && !(sb->state & (1 << MD_SB_CLEAN))) {
+		const char * name = "raid1syncd";
+
+		conf->resync_thread = md_register_thread(raid1syncd, conf,name);
+		if (!conf->resync_thread) {
+			printk(THREAD_ERROR, mdidx(mddev));
+			goto out_free_conf;
+		}
+
+		printk(START_RESYNC, mdidx(mddev));
+                conf->resync_mirrors = 1;
+                md_wakeup_thread(conf->resync_thread);
+        }
+
 	/*
 	 * Regenerate the "device is in sync with the raid set" bit for
 	 * each device.
 	 */
-	for (i = 0; i < sb->nr_disks ; i++) {
-		sb->disks[i].state &= ~(1 << MD_SYNC_DEVICE);
+	for (i = 0; i < MD_SB_DISKS; i++) {
+		mark_disk_nonsync(sb->disks+i);
 		for (j = 0; j < sb->raid_disks; j++) {
-			if (!raid_conf->mirrors[j].operational)
+			if (!conf->mirrors[j].operational)
 				continue;
-			if (sb->disks[i].number == raid_conf->mirrors[j].number)
-				sb->disks[i].state |= 1 << MD_SYNC_DEVICE;
+			if (sb->disks[i].number == conf->mirrors[j].number)
+				mark_disk_sync(sb->disks+i);
 		}
 	}
-	sb->active_disks = raid_conf->working_disks;
+	sb->active_disks = conf->working_disks;
 
-	printk("raid1: raid set %s active with %d out of %d mirrors\n", kdevname(MKDEV(MD_MAJOR, minor)), sb->active_disks, sb->raid_disks);
-	/* Ok, everything is just fine now */
-	return (0);
+	if (start_recovery)
+		md_recover_arrays();
+
+
+	printk(ARRAY_IS_ACTIVE, mdidx(mddev), sb->active_disks, sb->raid_disks);
+	/*
+	 * Ok, everything is just fine now
+	 */
+	return 0;
+
+out_free_conf:
+	kfree(conf);
+	mddev->private = NULL;
+out:
+	MOD_DEC_USE_COUNT;
+	return -EIO;
+}
+
+#undef INVALID_LEVEL
+#undef NO_SB
+#undef ERRORS
+#undef NOT_IN_SYNC
+#undef INCONSISTENT
+#undef ALREADY_RUNNING
+#undef OPERATIONAL
+#undef SPARE
+#undef NONE_OPERATIONAL
+#undef RUNNING_CKRAID
+#undef ARRAY_IS_ACTIVE
+
+static int raid1_stop_resync (mddev_t *mddev)
+{
+	raid1_conf_t *conf = mddev_to_conf(mddev);
+
+	if (conf->resync_thread) {
+		if (conf->resync_mirrors) {
+			conf->resync_mirrors = 2;
+			md_interrupt_thread(conf->resync_thread);
+			printk(KERN_INFO "raid1: mirror resync was not fully finished, restarting next time.\n");
+			return 1;
+		}
+		return 0;
+	}
+	return 0;
+}
+
+static int raid1_restart_resync (mddev_t *mddev)
+{
+	raid1_conf_t *conf = mddev_to_conf(mddev);
+
+	if (conf->resync_mirrors) {
+		if (!conf->resync_thread) {
+			MD_BUG();
+			return 0;
+		}
+		conf->resync_mirrors = 1;
+		md_wakeup_thread(conf->resync_thread);
+		return 1;
+	}
+	return 0;
 }
 
-static int raid1_stop (int minor, struct md_dev *mddev)
+static int raid1_stop (mddev_t *mddev)
 {
-	struct raid1_data *raid_conf = (struct raid1_data *) mddev->private;
+	raid1_conf_t *conf = mddev_to_conf(mddev);
 
-	kfree (raid_conf);
+	md_unregister_thread(conf->thread);
+	if (conf->resync_thread)
+		md_unregister_thread(conf->resync_thread);
+	kfree(conf);
 	mddev->private = NULL;
 	MOD_DEC_USE_COUNT;
 	return 0;
 }
 
-static struct md_personality raid1_personality=
+static mdk_personality_t raid1_personality=
 {
 	"raid1",
 	raid1_map,
@@ -855,15 +1214,13 @@
 	NULL,			/* no ioctls */
 	0,
 	raid1_error,
-	raid1_hot_add_disk,
-	/* raid1_hot_remove_drive */ NULL,
-	raid1_mark_spare
+	raid1_diskop,
+	raid1_stop_resync,
+	raid1_restart_resync
 };
 
 int raid1_init (void)
 {
-	if ((raid1_thread = md_register_thread(raid1d, NULL)) == NULL)
-		return -EBUSY;
 	return register_md_personality (RAID1, &raid1_personality);
 }
 
@@ -875,7 +1232,6 @@
 
 void cleanup_module (void)
 {
-	md_unregister_thread (raid1_thread);
 	unregister_md_personality (RAID1);
 }
 #endif
diff -urN linux/drivers/block/raid5.c /tmp/linux/drivers/block/raid5.c
--- linux/drivers/block/raid5.c	Fri May  8 01:17:13 1998
+++ /tmp/linux/drivers/block/raid5.c	Fri Feb  2 19:06:00 2001
@@ -1,4 +1,4 @@
-/*****************************************************************************
+/*
  * raid5.c : Multiple Devices driver for Linux
  *           Copyright (C) 1996, 1997 Ingo Molnar, Miguel de Icaza, Gadi Oxman
  *
@@ -14,16 +14,15 @@
  * Software Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
  */
 
+
 #include <linux/module.h>
 #include <linux/locks.h>
 #include <linux/malloc.h>
-#include <linux/md.h>
-#include <linux/raid5.h>
+#include <linux/raid/raid5.h>
 #include <asm/bitops.h>
 #include <asm/atomic.h>
-#include <asm/md.h>
 
-static struct md_personality raid5_personality;
+static mdk_personality_t raid5_personality;
 
 /*
  * Stripe cache
@@ -33,7 +32,7 @@
 #define HASH_PAGES_ORDER	0
 #define NR_HASH			(HASH_PAGES * PAGE_SIZE / sizeof(struct stripe_head *))
 #define HASH_MASK		(NR_HASH - 1)
-#define stripe_hash(raid_conf, sect, size)	((raid_conf)->stripe_hashtbl[((sect) / (size >> 9)) & HASH_MASK])
+#define stripe_hash(conf, sect, size)	((conf)->stripe_hashtbl[((sect) / (size >> 9)) & HASH_MASK])
 
 /*
  * The following can be used to debug the driver
@@ -46,6 +45,8 @@
 #define PRINTK(x)   do { ; } while (0)
 #endif
 
+static void print_raid5_conf (raid5_conf_t *conf);
+
 static inline int stripe_locked(struct stripe_head *sh)
 {
 	return test_bit(STRIPE_LOCKED, &sh->state);
@@ -61,32 +62,32 @@
  */
 static inline void lock_stripe(struct stripe_head *sh)
 {
-	struct raid5_data *raid_conf = sh->raid_conf;
-	if (!test_and_set_bit(STRIPE_LOCKED, &sh->state)) {
+	raid5_conf_t *conf = sh->raid_conf;
+	if (!md_test_and_set_bit(STRIPE_LOCKED, &sh->state)) {
 		PRINTK(("locking stripe %lu\n", sh->sector));
-		raid_conf->nr_locked_stripes++;
+		conf->nr_locked_stripes++;
 	}
 }
 
 static inline void unlock_stripe(struct stripe_head *sh)
 {
-	struct raid5_data *raid_conf = sh->raid_conf;
-	if (test_and_clear_bit(STRIPE_LOCKED, &sh->state)) {
+	raid5_conf_t *conf = sh->raid_conf;
+	if (md_test_and_clear_bit(STRIPE_LOCKED, &sh->state)) {
 		PRINTK(("unlocking stripe %lu\n", sh->sector));
-		raid_conf->nr_locked_stripes--;
+		conf->nr_locked_stripes--;
 		wake_up(&sh->wait);
 	}
 }
 
 static inline void finish_stripe(struct stripe_head *sh)
 {
-	struct raid5_data *raid_conf = sh->raid_conf;
+	raid5_conf_t *conf = sh->raid_conf;
 	unlock_stripe(sh);
 	sh->cmd = STRIPE_NONE;
 	sh->phase = PHASE_COMPLETE;
-	raid_conf->nr_pending_stripes--;
-	raid_conf->nr_cached_stripes++;
-	wake_up(&raid_conf->wait_for_stripe);
+	conf->nr_pending_stripes--;
+	conf->nr_cached_stripes++;
+	wake_up(&conf->wait_for_stripe);
 }
 
 void __wait_on_stripe(struct stripe_head *sh)
@@ -114,7 +115,7 @@
 		__wait_on_stripe(sh);
 }
 
-static inline void remove_hash(struct raid5_data *raid_conf, struct stripe_head *sh)
+static inline void remove_hash(raid5_conf_t *conf, struct stripe_head *sh)
 {
 	PRINTK(("remove_hash(), stripe %lu\n", sh->sector));
 
@@ -123,21 +124,22 @@
 			sh->hash_next->hash_pprev = sh->hash_pprev;
 		*sh->hash_pprev = sh->hash_next;
 		sh->hash_pprev = NULL;
-		raid_conf->nr_hashed_stripes--;
+		conf->nr_hashed_stripes--;
 	}
 }
 
-static inline void insert_hash(struct raid5_data *raid_conf, struct stripe_head *sh)
+static inline void insert_hash(raid5_conf_t *conf, struct stripe_head *sh)
 {
-	struct stripe_head **shp = &stripe_hash(raid_conf, sh->sector, sh->size);
+	struct stripe_head **shp = &stripe_hash(conf, sh->sector, sh->size);
 
-	PRINTK(("insert_hash(), stripe %lu, nr_hashed_stripes %d\n", sh->sector, raid_conf->nr_hashed_stripes));
+	PRINTK(("insert_hash(), stripe %lu, nr_hashed_stripes %d\n",
+			sh->sector, conf->nr_hashed_stripes));
 
 	if ((sh->hash_next = *shp) != NULL)
 		(*shp)->hash_pprev = &sh->hash_next;
 	*shp = sh;
 	sh->hash_pprev = shp;
-	raid_conf->nr_hashed_stripes++;
+	conf->nr_hashed_stripes++;
 }
 
 static struct buffer_head *get_free_buffer(struct stripe_head *sh, int b_size)
@@ -145,13 +147,15 @@
 	struct buffer_head *bh;
 	unsigned long flags;
 
-	save_flags(flags);
-	cli();
-	if ((bh = sh->buffer_pool) == NULL)
-		return NULL;
+	md_spin_lock_irqsave(&sh->stripe_lock, flags);
+	bh = sh->buffer_pool;
+	if (!bh)
+		goto out_unlock;
 	sh->buffer_pool = bh->b_next;
 	bh->b_size = b_size;
-	restore_flags(flags);
+out_unlock:
+	md_spin_unlock_irqrestore(&sh->stripe_lock, flags);
+
 	return bh;
 }
 
@@ -160,12 +164,14 @@
 	struct buffer_head *bh;
 	unsigned long flags;
 
-	save_flags(flags);
-	cli();
-	if ((bh = sh->bh_pool) == NULL)
-		return NULL;
+	md_spin_lock_irqsave(&sh->stripe_lock, flags);
+	bh = sh->bh_pool;
+	if (!bh)
+		goto out_unlock;
 	sh->bh_pool = bh->b_next;
-	restore_flags(flags);
+out_unlock:
+	md_spin_unlock_irqrestore(&sh->stripe_lock, flags);
+
 	return bh;
 }
 
@@ -173,54 +179,52 @@
 {
 	unsigned long flags;
 
-	save_flags(flags);
-	cli();
+	md_spin_lock_irqsave(&sh->stripe_lock, flags);
 	bh->b_next = sh->buffer_pool;
 	sh->buffer_pool = bh;
-	restore_flags(flags);
+	md_spin_unlock_irqrestore(&sh->stripe_lock, flags);
 }
 
 static void put_free_bh(struct stripe_head *sh, struct buffer_head *bh)
 {
 	unsigned long flags;
 
-	save_flags(flags);
-	cli();
+	md_spin_lock_irqsave(&sh->stripe_lock, flags);
 	bh->b_next = sh->bh_pool;
 	sh->bh_pool = bh;
-	restore_flags(flags);
+	md_spin_unlock_irqrestore(&sh->stripe_lock, flags);
 }
 
-static struct stripe_head *get_free_stripe(struct raid5_data *raid_conf)
+static struct stripe_head *get_free_stripe(raid5_conf_t *conf)
 {
 	struct stripe_head *sh;
 	unsigned long flags;
 
 	save_flags(flags);
 	cli();
-	if ((sh = raid_conf->free_sh_list) == NULL) {
+	if ((sh = conf->free_sh_list) == NULL) {
 		restore_flags(flags);
 		return NULL;
 	}
-	raid_conf->free_sh_list = sh->free_next;
-	raid_conf->nr_free_sh--;
-	if (!raid_conf->nr_free_sh && raid_conf->free_sh_list)
+	conf->free_sh_list = sh->free_next;
+	conf->nr_free_sh--;
+	if (!conf->nr_free_sh && conf->free_sh_list)
 		printk ("raid5: bug: free_sh_list != NULL, nr_free_sh == 0\n");
 	restore_flags(flags);
-	if (sh->hash_pprev || sh->nr_pending || sh->count)
+	if (sh->hash_pprev || md_atomic_read(&sh->nr_pending) || sh->count)
 		printk("get_free_stripe(): bug\n");
 	return sh;
 }
 
-static void put_free_stripe(struct raid5_data *raid_conf, struct stripe_head *sh)
+static void put_free_stripe(raid5_conf_t *conf, struct stripe_head *sh)
 {
 	unsigned long flags;
 
 	save_flags(flags);
 	cli();
-	sh->free_next = raid_conf->free_sh_list;
-	raid_conf->free_sh_list = sh;
-	raid_conf->nr_free_sh++;
+	sh->free_next = conf->free_sh_list;
+	conf->free_sh_list = sh;
+	conf->nr_free_sh++;
 	restore_flags(flags);
 }
 
@@ -324,8 +328,8 @@
 
 static void kfree_stripe(struct stripe_head *sh)
 {
-	struct raid5_data *raid_conf = sh->raid_conf;
-	int disks = raid_conf->raid_disks, j;
+	raid5_conf_t *conf = sh->raid_conf;
+	int disks = conf->raid_disks, j;
 
 	PRINTK(("kfree_stripe called, stripe %lu\n", sh->sector));
 	if (sh->phase != PHASE_COMPLETE || stripe_locked(sh) || sh->count) {
@@ -338,19 +342,19 @@
 		if (sh->bh_new[j] || sh->bh_copy[j])
 			printk("raid5: bug: sector %lu, new %p, copy %p\n", sh->sector, sh->bh_new[j], sh->bh_copy[j]);
 	}
-	remove_hash(raid_conf, sh);
-	put_free_stripe(raid_conf, sh);
+	remove_hash(conf, sh);
+	put_free_stripe(conf, sh);
 }
 
-static int shrink_stripe_cache(struct raid5_data *raid_conf, int nr)
+static int shrink_stripe_cache(raid5_conf_t *conf, int nr)
 {
 	struct stripe_head *sh;
 	int i, count = 0;
 
-	PRINTK(("shrink_stripe_cache called, %d/%d, clock %d\n", nr, raid_conf->nr_hashed_stripes, raid_conf->clock));
+	PRINTK(("shrink_stripe_cache called, %d/%d, clock %d\n", nr, conf->nr_hashed_stripes, conf->clock));
 	for (i = 0; i < NR_HASH; i++) {
 repeat:
-		sh = raid_conf->stripe_hashtbl[(i + raid_conf->clock) & HASH_MASK];
+		sh = conf->stripe_hashtbl[(i + conf->clock) & HASH_MASK];
 		for (; sh; sh = sh->hash_next) {
 			if (sh->phase != PHASE_COMPLETE)
 				continue;
@@ -360,30 +364,30 @@
 				continue;
 			kfree_stripe(sh);
 			if (++count == nr) {
-				PRINTK(("shrink completed, nr_hashed_stripes %d\n", raid_conf->nr_hashed_stripes));
-				raid_conf->clock = (i + raid_conf->clock) & HASH_MASK;
+				PRINTK(("shrink completed, nr_hashed_stripes %d\n", conf->nr_hashed_stripes));
+				conf->clock = (i + conf->clock) & HASH_MASK;
 				return nr;
 			}
 			goto repeat;
 		}
 	}
-	PRINTK(("shrink completed, nr_hashed_stripes %d\n", raid_conf->nr_hashed_stripes));
+	PRINTK(("shrink completed, nr_hashed_stripes %d\n", conf->nr_hashed_stripes));
 	return count;
 }
 
-static struct stripe_head *find_stripe(struct raid5_data *raid_conf, unsigned long sector, int size)
+static struct stripe_head *find_stripe(raid5_conf_t *conf, unsigned long sector, int size)
 {
 	struct stripe_head *sh;
 
-	if (raid_conf->buffer_size != size) {
-		PRINTK(("switching size, %d --> %d\n", raid_conf->buffer_size, size));
-		shrink_stripe_cache(raid_conf, raid_conf->max_nr_stripes);
-		raid_conf->buffer_size = size;
+	if (conf->buffer_size != size) {
+		PRINTK(("switching size, %d --> %d\n", conf->buffer_size, size));
+		shrink_stripe_cache(conf, conf->max_nr_stripes);
+		conf->buffer_size = size;
 	}
 
 	PRINTK(("find_stripe, sector %lu\n", sector));
-	for (sh = stripe_hash(raid_conf, sector, size); sh; sh = sh->hash_next)
-		if (sh->sector == sector && sh->raid_conf == raid_conf) {
+	for (sh = stripe_hash(conf, sector, size); sh; sh = sh->hash_next)
+		if (sh->sector == sector && sh->raid_conf == conf) {
 			if (sh->size == size) {
 				PRINTK(("found stripe %lu\n", sector));
 				return sh;
@@ -397,7 +401,7 @@
 	return NULL;
 }
 
-static int grow_stripes(struct raid5_data *raid_conf, int num, int priority)
+static int grow_stripes(raid5_conf_t *conf, int num, int priority)
 {
 	struct stripe_head *sh;
 
@@ -405,62 +409,64 @@
 		if ((sh = kmalloc(sizeof(struct stripe_head), priority)) == NULL)
 			return 1;
 		memset(sh, 0, sizeof(*sh));
-		if (grow_buffers(sh, 2 * raid_conf->raid_disks, PAGE_SIZE, priority)) {
-			shrink_buffers(sh, 2 * raid_conf->raid_disks);
+		sh->stripe_lock = MD_SPIN_LOCK_UNLOCKED;
+
+		if (grow_buffers(sh, 2 * conf->raid_disks, PAGE_SIZE, priority)) {
+			shrink_buffers(sh, 2 * conf->raid_disks);
 			kfree(sh);
 			return 1;
 		}
-		if (grow_bh(sh, raid_conf->raid_disks, priority)) {
-			shrink_buffers(sh, 2 * raid_conf->raid_disks);
-			shrink_bh(sh, raid_conf->raid_disks);
+		if (grow_bh(sh, conf->raid_disks, priority)) {
+			shrink_buffers(sh, 2 * conf->raid_disks);
+			shrink_bh(sh, conf->raid_disks);
 			kfree(sh);
 			return 1;
 		}
-		put_free_stripe(raid_conf, sh);
-		raid_conf->nr_stripes++;
+		put_free_stripe(conf, sh);
+		conf->nr_stripes++;
 	}
 	return 0;
 }
 
-static void shrink_stripes(struct raid5_data *raid_conf, int num)
+static void shrink_stripes(raid5_conf_t *conf, int num)
 {
 	struct stripe_head *sh;
 
 	while (num--) {
-		sh = get_free_stripe(raid_conf);
+		sh = get_free_stripe(conf);
 		if (!sh)
 			break;
-		shrink_buffers(sh, raid_conf->raid_disks * 2);
-		shrink_bh(sh, raid_conf->raid_disks);
+		shrink_buffers(sh, conf->raid_disks * 2);
+		shrink_bh(sh, conf->raid_disks);
 		kfree(sh);
-		raid_conf->nr_stripes--;
+		conf->nr_stripes--;
 	}
 }
 
-static struct stripe_head *kmalloc_stripe(struct raid5_data *raid_conf, unsigned long sector, int size)
+static struct stripe_head *kmalloc_stripe(raid5_conf_t *conf, unsigned long sector, int size)
 {
 	struct stripe_head *sh = NULL, *tmp;
 	struct buffer_head *buffer_pool, *bh_pool;
 
 	PRINTK(("kmalloc_stripe called\n"));
 
-	while ((sh = get_free_stripe(raid_conf)) == NULL) {
-		shrink_stripe_cache(raid_conf, raid_conf->max_nr_stripes / 8);
-		if ((sh = get_free_stripe(raid_conf)) != NULL)
+	while ((sh = get_free_stripe(conf)) == NULL) {
+		shrink_stripe_cache(conf, conf->max_nr_stripes / 8);
+		if ((sh = get_free_stripe(conf)) != NULL)
 			break;
-		if (!raid_conf->nr_pending_stripes)
+		if (!conf->nr_pending_stripes)
 			printk("raid5: bug: nr_free_sh == 0, nr_pending_stripes == 0\n");
-		md_wakeup_thread(raid_conf->thread);
+		md_wakeup_thread(conf->thread);
 		PRINTK(("waiting for some stripes to complete\n"));
-		sleep_on(&raid_conf->wait_for_stripe);
+		sleep_on(&conf->wait_for_stripe);
 	}
 
 	/*
 	 * The above might have slept, so perhaps another process
 	 * already created the stripe for us..
 	 */
-	if ((tmp = find_stripe(raid_conf, sector, size)) != NULL) { 
-		put_free_stripe(raid_conf, sh);
+	if ((tmp = find_stripe(conf, sector, size)) != NULL) { 
+		put_free_stripe(conf, sh);
 		wait_on_stripe(tmp);
 		return tmp;
 	}
@@ -472,25 +478,25 @@
 		sh->bh_pool = bh_pool;
 		sh->phase = PHASE_COMPLETE;
 		sh->cmd = STRIPE_NONE;
-		sh->raid_conf = raid_conf;
+		sh->raid_conf = conf;
 		sh->sector = sector;
 		sh->size = size;
-		raid_conf->nr_cached_stripes++;
-		insert_hash(raid_conf, sh);
+		conf->nr_cached_stripes++;
+		insert_hash(conf, sh);
 	} else printk("raid5: bug: kmalloc_stripe() == NULL\n");
 	return sh;
 }
 
-static struct stripe_head *get_stripe(struct raid5_data *raid_conf, unsigned long sector, int size)
+static struct stripe_head *get_stripe(raid5_conf_t *conf, unsigned long sector, int size)
 {
 	struct stripe_head *sh;
 
 	PRINTK(("get_stripe, sector %lu\n", sector));
-	sh = find_stripe(raid_conf, sector, size);
+	sh = find_stripe(conf, sector, size);
 	if (sh)
 		wait_on_stripe(sh);
 	else
-		sh = kmalloc_stripe(raid_conf, sector, size);
+		sh = kmalloc_stripe(conf, sector, size);
 	return sh;
 }
 
@@ -523,7 +529,7 @@
 	bh->b_end_io(bh, uptodate);
 	if (!uptodate)
 		printk(KERN_ALERT "raid5: %s: unrecoverable I/O error for "
-		       "block %lu\n", kdevname(bh->b_dev), bh->b_blocknr);
+		       "block %lu\n", partition_name(bh->b_dev), bh->b_blocknr);
 }
 
 static inline void raid5_mark_buffer_uptodate (struct buffer_head *bh, int uptodate)
@@ -537,36 +543,35 @@
 static void raid5_end_request (struct buffer_head * bh, int uptodate)
 {
 	struct stripe_head *sh = bh->b_dev_id;
-	struct raid5_data *raid_conf = sh->raid_conf;
-	int disks = raid_conf->raid_disks, i;
+	raid5_conf_t *conf = sh->raid_conf;
+	int disks = conf->raid_disks, i;
 	unsigned long flags;
 
 	PRINTK(("end_request %lu, nr_pending %d\n", sh->sector, sh->nr_pending));
-	save_flags(flags);
-	cli();
+	md_spin_lock_irqsave(&sh->stripe_lock, flags);
 	raid5_mark_buffer_uptodate(bh, uptodate);
-	--sh->nr_pending;
-	if (!sh->nr_pending) {
-		md_wakeup_thread(raid_conf->thread);
-		atomic_inc(&raid_conf->nr_handle);
+	if (atomic_dec_and_test(&sh->nr_pending)) {
+		md_wakeup_thread(conf->thread);
+		atomic_inc(&conf->nr_handle);
 	}
-	if (!uptodate)
+	if (!uptodate) {
 		md_error(bh->b_dev, bh->b_rdev);
-	if (raid_conf->failed_disks) {
+	}
+	if (conf->failed_disks) {
 		for (i = 0; i < disks; i++) {
-			if (raid_conf->disks[i].operational)
+			if (conf->disks[i].operational)
 				continue;
 			if (bh != sh->bh_old[i] && bh != sh->bh_req[i] && bh != sh->bh_copy[i])
 				continue;
-			if (bh->b_rdev != raid_conf->disks[i].dev)
+			if (bh->b_rdev != conf->disks[i].dev)
 				continue;
 			set_bit(STRIPE_ERROR, &sh->state);
 		}
 	}
-	restore_flags(flags);
+	md_spin_unlock_irqrestore(&sh->stripe_lock, flags);
 }
 
-static int raid5_map (struct md_dev *mddev, kdev_t *rdev,
+static int raid5_map (mddev_t *mddev, kdev_t dev, kdev_t *rdev,
 		      unsigned long *rsector, unsigned long size)
 {
 	/* No complex mapping used: the core of the work is done in the
@@ -577,11 +582,10 @@
 
 static void raid5_build_block (struct stripe_head *sh, struct buffer_head *bh, int i)
 {
-	struct raid5_data *raid_conf = sh->raid_conf;
-	struct md_dev *mddev = raid_conf->mddev;
-	int minor = (int) (mddev - md_dev);
+	raid5_conf_t *conf = sh->raid_conf;
+	mddev_t *mddev = conf->mddev;
 	char *b_data;
-	kdev_t dev = MKDEV(MD_MAJOR, minor);
+	kdev_t dev = mddev_to_kdev(mddev);
 	int block = sh->sector / (sh->size >> 9);
 
 	b_data = ((volatile struct buffer_head *) bh)->b_data;
@@ -589,7 +593,7 @@
 	init_buffer(bh, dev, block, raid5_end_request, sh);
 	((volatile struct buffer_head *) bh)->b_data = b_data;
 
-	bh->b_rdev	= raid_conf->disks[i].dev;
+	bh->b_rdev	= conf->disks[i].dev;
 	bh->b_rsector   = sh->sector;
 
 	bh->b_state	= (1 << BH_Req);
@@ -597,33 +601,62 @@
 	bh->b_list	= BUF_LOCKED;
 }
 
-static int raid5_error (struct md_dev *mddev, kdev_t dev)
+static int raid5_error (mddev_t *mddev, kdev_t dev)
 {
-	struct raid5_data *raid_conf = (struct raid5_data *) mddev->private;
-	md_superblock_t *sb = mddev->sb;
+	raid5_conf_t *conf = (raid5_conf_t *) mddev->private;
+	mdp_super_t *sb = mddev->sb;
 	struct disk_info *disk;
 	int i;
 
 	PRINTK(("raid5_error called\n"));
-	raid_conf->resync_parity = 0;
-	for (i = 0, disk = raid_conf->disks; i < raid_conf->raid_disks; i++, disk++)
+	conf->resync_parity = 0;
+	for (i = 0, disk = conf->disks; i < conf->raid_disks; i++, disk++) {
 		if (disk->dev == dev && disk->operational) {
 			disk->operational = 0;
-			sb->disks[disk->number].state |= (1 << MD_FAULTY_DEVICE);
-			sb->disks[disk->number].state &= ~(1 << MD_SYNC_DEVICE);
-			sb->disks[disk->number].state &= ~(1 << MD_ACTIVE_DEVICE);
+			mark_disk_faulty(sb->disks+disk->number);
+			mark_disk_nonsync(sb->disks+disk->number);
+			mark_disk_inactive(sb->disks+disk->number);
 			sb->active_disks--;
 			sb->working_disks--;
 			sb->failed_disks++;
 			mddev->sb_dirty = 1;
-			raid_conf->working_disks--;
-			raid_conf->failed_disks++;
-			md_wakeup_thread(raid_conf->thread);
+			conf->working_disks--;
+			conf->failed_disks++;
+			md_wakeup_thread(conf->thread);
 			printk (KERN_ALERT
-				"RAID5: Disk failure on %s, disabling device."
-				"Operation continuing on %d devices\n",
-				kdevname (dev), raid_conf->working_disks);
+				"raid5: Disk failure on %s, disabling device."
+				" Operation continuing on %d devices\n",
+				partition_name (dev), conf->working_disks);
+			return -EIO;
 		}
+	}
+	/*
+	 * handle errors in spares (during reconstruction)
+	 */
+	if (conf->spare) {
+		disk = conf->spare;
+		if (disk->dev == dev) {
+			printk (KERN_ALERT
+				"raid5: Disk failure on spare %s\n",
+				partition_name (dev));
+			if (!conf->spare->operational) {
+				MD_BUG();
+				return -EIO;
+			}
+			disk->operational = 0;
+			disk->write_only = 0;
+			conf->spare = NULL;
+			mark_disk_faulty(sb->disks+disk->number);
+			mark_disk_nonsync(sb->disks+disk->number);
+			mark_disk_inactive(sb->disks+disk->number);
+			sb->spare_disks--;
+			sb->working_disks--;
+			sb->failed_disks++;
+
+			return -EIO;
+		}
+	}
+	MD_BUG();
 	return 0;
 }	
 
@@ -634,12 +667,12 @@
 static inline unsigned long 
 raid5_compute_sector (int r_sector, unsigned int raid_disks, unsigned int data_disks,
 			unsigned int * dd_idx, unsigned int * pd_idx, 
-			struct raid5_data *raid_conf)
+			raid5_conf_t *conf)
 {
 	unsigned int  stripe;
 	int chunk_number, chunk_offset;
 	unsigned long new_sector;
-	int sectors_per_chunk = raid_conf->chunk_size >> 9;
+	int sectors_per_chunk = conf->chunk_size >> 9;
 
 	/* First compute the information on this sector */
 
@@ -662,9 +695,9 @@
 	/*
 	 * Select the parity disk based on the user selected algorithm.
 	 */
-	if (raid_conf->level == 4)
+	if (conf->level == 4)
 		*pd_idx = data_disks;
-	else switch (raid_conf->algorithm) {
+	else switch (conf->algorithm) {
 		case ALGORITHM_LEFT_ASYMMETRIC:
 			*pd_idx = data_disks - stripe % raid_disks;
 			if (*dd_idx >= *pd_idx)
@@ -684,7 +717,7 @@
 			*dd_idx = (*pd_idx + 1 + *dd_idx) % raid_disks;
 			break;
 		default:
-			printk ("raid5: unsupported algorithm %d\n", raid_conf->algorithm);
+			printk ("raid5: unsupported algorithm %d\n", conf->algorithm);
 	}
 
 	/*
@@ -705,16 +738,16 @@
 
 static unsigned long compute_blocknr(struct stripe_head *sh, int i)
 {
-	struct raid5_data *raid_conf = sh->raid_conf;
-	int raid_disks = raid_conf->raid_disks, data_disks = raid_disks - 1;
+	raid5_conf_t *conf = sh->raid_conf;
+	int raid_disks = conf->raid_disks, data_disks = raid_disks - 1;
 	unsigned long new_sector = sh->sector, check;
-	int sectors_per_chunk = raid_conf->chunk_size >> 9;
+	int sectors_per_chunk = conf->chunk_size >> 9;
 	unsigned long stripe = new_sector / sectors_per_chunk;
 	int chunk_offset = new_sector % sectors_per_chunk;
 	int chunk_number, dummy1, dummy2, dd_idx = i;
 	unsigned long r_sector, blocknr;
 
-	switch (raid_conf->algorithm) {
+	switch (conf->algorithm) {
 		case ALGORITHM_LEFT_ASYMMETRIC:
 		case ALGORITHM_RIGHT_ASYMMETRIC:
 			if (i > sh->pd_idx)
@@ -727,14 +760,14 @@
 			i -= (sh->pd_idx + 1);
 			break;
 		default:
-			printk ("raid5: unsupported algorithm %d\n", raid_conf->algorithm);
+			printk ("raid5: unsupported algorithm %d\n", conf->algorithm);
 	}
 
 	chunk_number = stripe * data_disks + i;
 	r_sector = chunk_number * sectors_per_chunk + chunk_offset;
 	blocknr = r_sector / (sh->size >> 9);
 
-	check = raid5_compute_sector (r_sector, raid_disks, data_disks, &dummy1, &dummy2, raid_conf);
+	check = raid5_compute_sector (r_sector, raid_disks, data_disks, &dummy1, &dummy2, conf);
 	if (check != sh->sector || dummy1 != dd_idx || dummy2 != sh->pd_idx) {
 		printk("compute_blocknr: map not correct\n");
 		return 0;
@@ -742,36 +775,11 @@
 	return blocknr;
 }
 
-#ifdef HAVE_ARCH_XORBLOCK
-static void xor_block(struct buffer_head *dest, struct buffer_head *source)
-{
-	__xor_block((char *) dest->b_data, (char *) source->b_data, dest->b_size);
-}
-#else
-static void xor_block(struct buffer_head *dest, struct buffer_head *source)
-{
-	long lines = dest->b_size / (sizeof (long)) / 8, i;
-	long *destp = (long *) dest->b_data, *sourcep = (long *) source->b_data;
-
-	for (i = lines; i > 0; i--) {
-		*(destp + 0) ^= *(sourcep + 0);
-		*(destp + 1) ^= *(sourcep + 1);
-		*(destp + 2) ^= *(sourcep + 2);
-		*(destp + 3) ^= *(sourcep + 3);
-		*(destp + 4) ^= *(sourcep + 4);
-		*(destp + 5) ^= *(sourcep + 5);
-		*(destp + 6) ^= *(sourcep + 6);
-		*(destp + 7) ^= *(sourcep + 7);
-		destp += 8;
-		sourcep += 8;
-	}
-}
-#endif
-
 static void compute_block(struct stripe_head *sh, int dd_idx)
 {
-	struct raid5_data *raid_conf = sh->raid_conf;
-	int i, disks = raid_conf->raid_disks;
+	raid5_conf_t *conf = sh->raid_conf;
+	int i, count, disks = conf->raid_disks;
+	struct buffer_head *bh_ptr[MAX_XOR_BLOCKS];
 
 	PRINTK(("compute_block, stripe %lu, idx %d\n", sh->sector, dd_idx));
 
@@ -780,69 +788,100 @@
 	raid5_build_block(sh, sh->bh_old[dd_idx], dd_idx);
 
 	memset(sh->bh_old[dd_idx]->b_data, 0, sh->size);
+	bh_ptr[0] = sh->bh_old[dd_idx];
+	count = 1;
 	for (i = 0; i < disks; i++) {
 		if (i == dd_idx)
 			continue;
 		if (sh->bh_old[i]) {
-			xor_block(sh->bh_old[dd_idx], sh->bh_old[i]);
-			continue;
-		} else
+			bh_ptr[count++] = sh->bh_old[i];
+		} else {
 			printk("compute_block() %d, stripe %lu, %d not present\n", dd_idx, sh->sector, i);
+		}
+		if (count == MAX_XOR_BLOCKS) {
+			xor_block(count, &bh_ptr[0]);
+			count = 1;
+		}
+	}
+	if(count != 1) {
+		xor_block(count, &bh_ptr[0]);
 	}
 	raid5_mark_buffer_uptodate(sh->bh_old[dd_idx], 1);
 }
 
 static void compute_parity(struct stripe_head *sh, int method)
 {
-	struct raid5_data *raid_conf = sh->raid_conf;
-	int i, pd_idx = sh->pd_idx, disks = raid_conf->raid_disks;
+	raid5_conf_t *conf = sh->raid_conf;
+	int i, pd_idx = sh->pd_idx, disks = conf->raid_disks, lowprio, count;
+	struct buffer_head *bh_ptr[MAX_XOR_BLOCKS];
 
 	PRINTK(("compute_parity, stripe %lu, method %d\n", sh->sector, method));
+	lowprio = 1;
 	for (i = 0; i < disks; i++) {
 		if (i == pd_idx || !sh->bh_new[i])
 			continue;
 		if (!sh->bh_copy[i])
 			sh->bh_copy[i] = raid5_kmalloc_buffer(sh, sh->size);
 		raid5_build_block(sh, sh->bh_copy[i], i);
+		if (!buffer_lowprio(sh->bh_new[i]))
+			lowprio = 0;
+		else
+			mark_buffer_lowprio(sh->bh_copy[i]);
 		mark_buffer_clean(sh->bh_new[i]);
 		memcpy(sh->bh_copy[i]->b_data, sh->bh_new[i]->b_data, sh->size);
 	}
 	if (sh->bh_copy[pd_idx] == NULL)
 		sh->bh_copy[pd_idx] = raid5_kmalloc_buffer(sh, sh->size);
 	raid5_build_block(sh, sh->bh_copy[pd_idx], sh->pd_idx);
+	if (lowprio)
+		mark_buffer_lowprio(sh->bh_copy[pd_idx]);
 
 	if (method == RECONSTRUCT_WRITE) {
 		memset(sh->bh_copy[pd_idx]->b_data, 0, sh->size);
+		bh_ptr[0] = sh->bh_copy[pd_idx];
+		count = 1;
 		for (i = 0; i < disks; i++) {
 			if (i == sh->pd_idx)
 				continue;
 			if (sh->bh_new[i]) {
-				xor_block(sh->bh_copy[pd_idx], sh->bh_copy[i]);
-				continue;
+				bh_ptr[count++] = sh->bh_copy[i];
+			} else if (sh->bh_old[i]) {
+				bh_ptr[count++] = sh->bh_old[i];
 			}
-			if (sh->bh_old[i]) {
-				xor_block(sh->bh_copy[pd_idx], sh->bh_old[i]);
-				continue;
+			if (count == MAX_XOR_BLOCKS) {
+				xor_block(count, &bh_ptr[0]);
+				count = 1;
 			}
 		}
+		if (count != 1) {
+			xor_block(count, &bh_ptr[0]);
+		}
 	} else if (method == READ_MODIFY_WRITE) {
 		memcpy(sh->bh_copy[pd_idx]->b_data, sh->bh_old[pd_idx]->b_data, sh->size);
+		bh_ptr[0] = sh->bh_copy[pd_idx];
+		count = 1;
 		for (i = 0; i < disks; i++) {
 			if (i == sh->pd_idx)
 				continue;
 			if (sh->bh_new[i] && sh->bh_old[i]) {
-				xor_block(sh->bh_copy[pd_idx], sh->bh_copy[i]);
-				xor_block(sh->bh_copy[pd_idx], sh->bh_old[i]);
-				continue;
+				bh_ptr[count++] = sh->bh_copy[i];
+				bh_ptr[count++] = sh->bh_old[i];
+			}
+			if (count >= (MAX_XOR_BLOCKS - 1)) {
+				xor_block(count, &bh_ptr[0]);
+				count = 1;
 			}
 		}
+		if (count != 1) {
+			xor_block(count, &bh_ptr[0]);
+		}
 	}
 	raid5_mark_buffer_uptodate(sh->bh_copy[pd_idx], 1);
 }
 
 static void add_stripe_bh (struct stripe_head *sh, struct buffer_head *bh, int dd_idx, int rw)
 {
-	struct raid5_data *raid_conf = sh->raid_conf;
+	raid5_conf_t *conf = sh->raid_conf;
 	struct buffer_head *bh_req;
 
 	if (sh->bh_new[dd_idx]) {
@@ -860,19 +899,22 @@
 	if (sh->phase == PHASE_COMPLETE && sh->cmd == STRIPE_NONE) {
 		sh->phase = PHASE_BEGIN;
 		sh->cmd = (rw == READ) ? STRIPE_READ : STRIPE_WRITE;
-		raid_conf->nr_pending_stripes++;
-		atomic_inc(&raid_conf->nr_handle);
+		conf->nr_pending_stripes++;
+		atomic_inc(&conf->nr_handle);
 	}
 	sh->bh_new[dd_idx] = bh;
 	sh->bh_req[dd_idx] = bh_req;
 	sh->cmd_new[dd_idx] = rw;
 	sh->new[dd_idx] = 1;
+
+	if (buffer_lowprio(bh))
+		mark_buffer_lowprio(bh_req);
 }
 
 static void complete_stripe(struct stripe_head *sh)
 {
-	struct raid5_data *raid_conf = sh->raid_conf;
-	int disks = raid_conf->raid_disks;
+	raid5_conf_t *conf = sh->raid_conf;
+	int disks = conf->raid_disks;
 	int i, new = 0;
 	
 	PRINTK(("complete_stripe %lu\n", sh->sector));
@@ -909,6 +951,22 @@
 	}
 }
 
+
+static int is_stripe_lowprio(struct stripe_head *sh, int disks)
+{
+	int i, lowprio = 1;
+
+	for (i = 0; i < disks; i++) {
+		if (sh->bh_new[i])
+			if (!buffer_lowprio(sh->bh_new[i]))
+				lowprio = 0;
+		if (sh->bh_old[i])
+			if (!buffer_lowprio(sh->bh_old[i]))
+				lowprio = 0;
+	}
+	return lowprio;
+}
+
 /*
  * handle_stripe() is our main logic routine. Note that:
  *
@@ -919,28 +977,27 @@
  * 2.	We should be careful to set sh->nr_pending whenever we sleep,
  *	to prevent re-entry of handle_stripe() for the same sh.
  *
- * 3.	raid_conf->failed_disks and disk->operational can be changed
+ * 3.	conf->failed_disks and disk->operational can be changed
  *	from an interrupt. This complicates things a bit, but it allows
  *	us to stop issuing requests for a failed drive as soon as possible.
  */
 static void handle_stripe(struct stripe_head *sh)
 {
-	struct raid5_data *raid_conf = sh->raid_conf;
-	struct md_dev *mddev = raid_conf->mddev;
-	int minor = (int) (mddev - md_dev);
+	raid5_conf_t *conf = sh->raid_conf;
+	mddev_t *mddev = conf->mddev;
 	struct buffer_head *bh;
-	int disks = raid_conf->raid_disks;
-	int i, nr = 0, nr_read = 0, nr_write = 0;
+	int disks = conf->raid_disks;
+	int i, nr = 0, nr_read = 0, nr_write = 0, lowprio;
 	int nr_cache = 0, nr_cache_other = 0, nr_cache_overwrite = 0, parity = 0;
 	int nr_failed_other = 0, nr_failed_overwrite = 0, parity_failed = 0;
 	int reading = 0, nr_writing = 0;
 	int method1 = INT_MAX, method2 = INT_MAX;
 	int block;
 	unsigned long flags;
-	int operational[MD_SB_DISKS], failed_disks = raid_conf->failed_disks;
+	int operational[MD_SB_DISKS], failed_disks = conf->failed_disks;
 
 	PRINTK(("handle_stripe(), stripe %lu\n", sh->sector));
-	if (sh->nr_pending) {
+	if (md_atomic_read(&sh->nr_pending)) {
 		printk("handle_stripe(), stripe %lu, io still pending\n", sh->sector);
 		return;
 	}
@@ -949,9 +1006,9 @@
 		return;
 	}
 
-	atomic_dec(&raid_conf->nr_handle);
+	atomic_dec(&conf->nr_handle);
 
-	if (test_and_clear_bit(STRIPE_ERROR, &sh->state)) {
+	if (md_test_and_clear_bit(STRIPE_ERROR, &sh->state)) {
 		printk("raid5: restarting stripe %lu\n", sh->sector);
 		sh->phase = PHASE_BEGIN;
 	}
@@ -969,11 +1026,11 @@
 	save_flags(flags);
 	cli();
 	for (i = 0; i < disks; i++) {
-		operational[i] = raid_conf->disks[i].operational;
-		if (i == sh->pd_idx && raid_conf->resync_parity)
+		operational[i] = conf->disks[i].operational;
+		if (i == sh->pd_idx && conf->resync_parity)
 			operational[i] = 0;
 	}
-	failed_disks = raid_conf->failed_disks;
+	failed_disks = conf->failed_disks;
 	restore_flags(flags);
 
 	if (failed_disks > 1) {
@@ -1017,7 +1074,7 @@
 	}
 
 	if (nr_write && nr_read)
-		printk("raid5: bug, nr_write == %d, nr_read == %d, sh->cmd == %d\n", nr_write, nr_read, sh->cmd);
+		printk("raid5: bug, nr_write ==`%d, nr_read == %d, sh->cmd == %d\n", nr_write, nr_read, sh->cmd);
 
 	if (nr_write) {
 		/*
@@ -1030,7 +1087,7 @@
 				if (sh->bh_new[i])
 					continue;
 				block = (int) compute_blocknr(sh, i);
-				bh = find_buffer(MKDEV(MD_MAJOR, minor), block, sh->size);
+				bh = find_buffer(mddev_to_kdev(mddev), block, sh->size);
 				if (bh && bh->b_count == 0 && buffer_dirty(bh) && !buffer_locked(bh)) {
 					PRINTK(("Whee.. sector %lu, index %d (%d) found in the buffer cache!\n", sh->sector, i, block));
 					add_stripe_bh(sh, bh, i, WRITE);
@@ -1064,21 +1121,22 @@
 
 		if (!method1 || !method2) {
 			lock_stripe(sh);
-			sh->nr_pending++;
+			lowprio = is_stripe_lowprio(sh, disks);
+			atomic_inc(&sh->nr_pending);
 			sh->phase = PHASE_WRITE;
 			compute_parity(sh, method1 <= method2 ? RECONSTRUCT_WRITE : READ_MODIFY_WRITE);
 			for (i = 0; i < disks; i++) {
-				if (!operational[i] && !raid_conf->spare && !raid_conf->resync_parity)
+				if (!operational[i] && !conf->spare && !conf->resync_parity)
 					continue;
 				if (i == sh->pd_idx || sh->bh_new[i])
 					nr_writing++;
 			}
 
-			sh->nr_pending = nr_writing;
-			PRINTK(("handle_stripe() %lu, writing back %d\n", sh->sector, sh->nr_pending));
+			md_atomic_set(&sh->nr_pending, nr_writing);
+			PRINTK(("handle_stripe() %lu, writing back %d\n", sh->sector, md_atomic_read(&sh->nr_pending)));
 
 			for (i = 0; i < disks; i++) {
-				if (!operational[i] && !raid_conf->spare && !raid_conf->resync_parity)
+				if (!operational[i] && !conf->spare && !conf->resync_parity)
 					continue;
 				bh = sh->bh_copy[i];
 				if (i != sh->pd_idx && ((bh == NULL) ^ (sh->bh_new[i] == NULL)))
@@ -1089,18 +1147,30 @@
 					bh->b_state |= (1<<BH_Dirty);
 					PRINTK(("making request for buffer %d\n", i));
 					clear_bit(BH_Lock, &bh->b_state);
-					if (!operational[i] && !raid_conf->resync_parity) {
-						bh->b_rdev = raid_conf->spare->dev;
-						make_request(MAJOR(raid_conf->spare->dev), WRITE, bh);
-					} else
-						make_request(MAJOR(raid_conf->disks[i].dev), WRITE, bh);
+					if (!operational[i] && !conf->resync_parity) {
+						bh->b_rdev = conf->spare->dev;
+						make_request(MAJOR(conf->spare->dev), WRITE, bh);
+					} else {
+#if 0
+						make_request(MAJOR(conf->disks[i].dev), WRITE, bh);
+#else
+						if (!lowprio || (i==sh->pd_idx))
+							make_request(MAJOR(conf->disks[i].dev), WRITE, bh);
+						else {
+							mark_buffer_clean(bh);
+							raid5_end_request(bh,1);
+							sh->new[i] = 0;
+						}
+#endif
+					}
 				}
 			}
 			return;
 		}
 
 		lock_stripe(sh);
-		sh->nr_pending++;
+		lowprio = is_stripe_lowprio(sh, disks);
+		atomic_inc(&sh->nr_pending);
 		if (method1 < method2) {
 			sh->write_method = RECONSTRUCT_WRITE;
 			for (i = 0; i < disks; i++) {
@@ -1110,6 +1180,8 @@
 					continue;
 				sh->bh_old[i] = raid5_kmalloc_buffer(sh, sh->size);
 				raid5_build_block(sh, sh->bh_old[i], i);
+				if (lowprio)
+					mark_buffer_lowprio(sh->bh_old[i]);
 				reading++;
 			}
 		} else {
@@ -1121,19 +1193,21 @@
 					continue;
 				sh->bh_old[i] = raid5_kmalloc_buffer(sh, sh->size);
 				raid5_build_block(sh, sh->bh_old[i], i);
+				if (lowprio)
+					mark_buffer_lowprio(sh->bh_old[i]);
 				reading++;
 			}
 		}
 		sh->phase = PHASE_READ_OLD;
-		sh->nr_pending = reading;
-		PRINTK(("handle_stripe() %lu, reading %d old buffers\n", sh->sector, sh->nr_pending));
+		md_atomic_set(&sh->nr_pending, reading);
+		PRINTK(("handle_stripe() %lu, reading %d old buffers\n", sh->sector, md_atomic_read(&sh->nr_pending)));
 		for (i = 0; i < disks; i++) {
 			if (!sh->bh_old[i])
 				continue;
 			if (buffer_uptodate(sh->bh_old[i]))
 				continue;
 		 	clear_bit(BH_Lock, &sh->bh_old[i]->b_state);
-			make_request(MAJOR(raid_conf->disks[i].dev), READ, sh->bh_old[i]);
+			make_request(MAJOR(conf->disks[i].dev), READ, sh->bh_old[i]);
 		}
 	} else {
 		/*
@@ -1141,7 +1215,8 @@
 		 */
 		method1 = nr_read - nr_cache_overwrite;
 		lock_stripe(sh);
-		sh->nr_pending++;
+		lowprio = is_stripe_lowprio(sh,disks);
+		atomic_inc(&sh->nr_pending);
 
 		PRINTK(("handle_stripe(), sector %lu, nr_read %d, nr_cache %d, method1 %d\n", sh->sector, nr_read, nr_cache, method1));
 		if (!method1 || (method1 == 1 && nr_cache == disks - 1)) {
@@ -1149,18 +1224,22 @@
 			for (i = 0; i < disks; i++) {
 				if (!sh->bh_new[i])
 					continue;
-				if (!sh->bh_old[i])
+				if (!sh->bh_old[i]) {
 					compute_block(sh, i);
+					if (lowprio)
+						mark_buffer_lowprio
+							(sh->bh_old[i]);
+				}
 				memcpy(sh->bh_new[i]->b_data, sh->bh_old[i]->b_data, sh->size);
 			}
-			sh->nr_pending--;
+			atomic_dec(&sh->nr_pending);
 			complete_stripe(sh);
 			return;
 		}
 		if (nr_failed_overwrite) {
 			sh->phase = PHASE_READ_OLD;
-			sh->nr_pending = (disks - 1) - nr_cache;
-			PRINTK(("handle_stripe() %lu, phase READ_OLD, pending %d\n", sh->sector, sh->nr_pending));
+			md_atomic_set(&sh->nr_pending, (disks - 1) - nr_cache);
+			PRINTK(("handle_stripe() %lu, phase READ_OLD, pending %d\n", sh->sector, md_atomic_read(&sh->nr_pending)));
 			for (i = 0; i < disks; i++) {
 				if (sh->bh_old[i])
 					continue;
@@ -1168,13 +1247,16 @@
 					continue;
 				sh->bh_old[i] = raid5_kmalloc_buffer(sh, sh->size);
 				raid5_build_block(sh, sh->bh_old[i], i);
+				if (lowprio)
+					mark_buffer_lowprio(sh->bh_old[i]);
 			 	clear_bit(BH_Lock, &sh->bh_old[i]->b_state);
-				make_request(MAJOR(raid_conf->disks[i].dev), READ, sh->bh_old[i]);
+				make_request(MAJOR(conf->disks[i].dev), READ, sh->bh_old[i]);
 			}
 		} else {
 			sh->phase = PHASE_READ;
-			sh->nr_pending = nr_read - nr_cache_overwrite;
-			PRINTK(("handle_stripe() %lu, phase READ, pending %d\n", sh->sector, sh->nr_pending));
+			md_atomic_set(&sh->nr_pending,
+				nr_read - nr_cache_overwrite);
+			PRINTK(("handle_stripe() %lu, phase READ, pending %d\n", sh->sector, md_atomic_read(&sh->nr_pending)));
 			for (i = 0; i < disks; i++) {
 				if (!sh->bh_new[i])
 					continue;
@@ -1182,16 +1264,16 @@
 					memcpy(sh->bh_new[i]->b_data, sh->bh_old[i]->b_data, sh->size);
 					continue;
 				}
-				make_request(MAJOR(raid_conf->disks[i].dev), READ, sh->bh_req[i]);
+				make_request(MAJOR(conf->disks[i].dev), READ, sh->bh_req[i]);
 			}
 		}
 	}
 }
 
-static int raid5_make_request (struct md_dev *mddev, int rw, struct buffer_head * bh)
+static int raid5_make_request (mddev_t *mddev, int rw, struct buffer_head * bh)
 {
-	struct raid5_data *raid_conf = (struct raid5_data *) mddev->private;
-	const unsigned int raid_disks = raid_conf->raid_disks;
+	raid5_conf_t *conf = (raid5_conf_t *) mddev->private;
+	const unsigned int raid_disks = conf->raid_disks;
 	const unsigned int data_disks = raid_disks - 1;
 	unsigned int  dd_idx, pd_idx;
 	unsigned long new_sector;
@@ -1202,15 +1284,15 @@
 	if (rw == WRITEA) rw = WRITE;
 
 	new_sector = raid5_compute_sector(bh->b_rsector, raid_disks, data_disks,
-						&dd_idx, &pd_idx, raid_conf);
+						&dd_idx, &pd_idx, conf);
 
 	PRINTK(("raid5_make_request, sector %lu\n", new_sector));
 repeat:
-	sh = get_stripe(raid_conf, new_sector, bh->b_size);
+	sh = get_stripe(conf, new_sector, bh->b_size);
 	if ((rw == READ && sh->cmd == STRIPE_WRITE) || (rw == WRITE && sh->cmd == STRIPE_READ)) {
 		PRINTK(("raid5: lock contention, rw == %d, sh->cmd == %d\n", rw, sh->cmd));
 		lock_stripe(sh);
-		if (!sh->nr_pending)
+		if (!md_atomic_read(&sh->nr_pending))
 			handle_stripe(sh);
 		goto repeat;
 	}
@@ -1221,24 +1303,24 @@
 		printk("raid5: bug: stripe->bh_new[%d], sector %lu exists\n", dd_idx, sh->sector);
 		printk("raid5: bh %p, bh_new %p\n", bh, sh->bh_new[dd_idx]);
 		lock_stripe(sh);
-		md_wakeup_thread(raid_conf->thread);
+		md_wakeup_thread(conf->thread);
 		wait_on_stripe(sh);
 		goto repeat;
 	}
 	add_stripe_bh(sh, bh, dd_idx, rw);
 
-	md_wakeup_thread(raid_conf->thread);
+	md_wakeup_thread(conf->thread);
 	return 0;
 }
 
 static void unplug_devices(struct stripe_head *sh)
 {
 #if 0
-	struct raid5_data *raid_conf = sh->raid_conf;
+	raid5_conf_t *conf = sh->raid_conf;
 	int i;
 
-	for (i = 0; i < raid_conf->raid_disks; i++)
-		unplug_device(blk_dev + MAJOR(raid_conf->disks[i].dev));
+	for (i = 0; i < conf->raid_disks; i++)
+		unplug_device(blk_dev + MAJOR(conf->disks[i].dev));
 #endif
 }
 
@@ -1252,8 +1334,8 @@
 static void raid5d (void *data)
 {
 	struct stripe_head *sh;
-	struct raid5_data *raid_conf = data;
-	struct md_dev *mddev = raid_conf->mddev;
+	raid5_conf_t *conf = data;
+	mddev_t *mddev = conf->mddev;
 	int i, handled = 0, unplug = 0;
 	unsigned long flags;
 
@@ -1261,47 +1343,47 @@
 
 	if (mddev->sb_dirty) {
 		mddev->sb_dirty = 0;
-		md_update_sb((int) (mddev - md_dev));
+		md_update_sb(mddev);
 	}
 	for (i = 0; i < NR_HASH; i++) {
 repeat:
-		sh = raid_conf->stripe_hashtbl[i];
+		sh = conf->stripe_hashtbl[i];
 		for (; sh; sh = sh->hash_next) {
-			if (sh->raid_conf != raid_conf)
+			if (sh->raid_conf != conf)
 				continue;
 			if (sh->phase == PHASE_COMPLETE)
 				continue;
-			if (sh->nr_pending)
+			if (md_atomic_read(&sh->nr_pending))
 				continue;
-			if (sh->sector == raid_conf->next_sector) {
-				raid_conf->sector_count += (sh->size >> 9);
-				if (raid_conf->sector_count >= 128)
+			if (sh->sector == conf->next_sector) {
+				conf->sector_count += (sh->size >> 9);
+				if (conf->sector_count >= 128)
 					unplug = 1;
 			} else
 				unplug = 1;
 			if (unplug) {
-				PRINTK(("unplugging devices, sector == %lu, count == %d\n", sh->sector, raid_conf->sector_count));
+				PRINTK(("unplugging devices, sector == %lu, count == %d\n", sh->sector, conf->sector_count));
 				unplug_devices(sh);
 				unplug = 0;
-				raid_conf->sector_count = 0;
+				conf->sector_count = 0;
 			}
-			raid_conf->next_sector = sh->sector + (sh->size >> 9);
+			conf->next_sector = sh->sector + (sh->size >> 9);
 			handled++;
 			handle_stripe(sh);
 			goto repeat;
 		}
 	}
-	if (raid_conf) {
-		PRINTK(("%d stripes handled, nr_handle %d\n", handled, atomic_read(&raid_conf->nr_handle)));
+	if (conf) {
+		PRINTK(("%d stripes handled, nr_handle %d\n", handled, md_atomic_read(&conf->nr_handle)));
 		save_flags(flags);
 		cli();
-		if (!atomic_read(&raid_conf->nr_handle))
-			clear_bit(THREAD_WAKEUP, &raid_conf->thread->flags);
+		if (!md_atomic_read(&conf->nr_handle))
+			clear_bit(THREAD_WAKEUP, &conf->thread->flags);
+		restore_flags(flags);
 	}
 	PRINTK(("--- raid5d inactive\n"));
 }
 
-#if SUPPORT_RECONSTRUCTION
 /*
  * Private kernel thread for parity reconstruction after an unclean
  * shutdown. Reconstruction on spare drives in case of a failed drive
@@ -1309,44 +1391,64 @@
  */
 static void raid5syncd (void *data)
 {
-	struct raid5_data *raid_conf = data;
-	struct md_dev *mddev = raid_conf->mddev;
+	raid5_conf_t *conf = data;
+	mddev_t *mddev = conf->mddev;
 
-	if (!raid_conf->resync_parity)
+	if (!conf->resync_parity)
+		return;
+	if (conf->resync_parity == 2)
+		return;
+	down(&mddev->recovery_sem);
+	if (md_do_sync(mddev,NULL)) {
+		up(&mddev->recovery_sem);
+		printk("raid5: resync aborted!\n");
 		return;
-	md_do_sync(mddev);
-	raid_conf->resync_parity = 0;
+	}
+	conf->resync_parity = 0;
+	up(&mddev->recovery_sem);
+	printk("raid5: resync finished.\n");
 }
-#endif /* SUPPORT_RECONSTRUCTION */
 
-static int __check_consistency (struct md_dev *mddev, int row)
+static int __check_consistency (mddev_t *mddev, int row)
 {
-	struct raid5_data *raid_conf = mddev->private;
+	raid5_conf_t *conf = mddev->private;
 	kdev_t dev;
 	struct buffer_head *bh[MD_SB_DISKS], tmp;
-	int i, rc = 0, nr = 0;
+	int i, rc = 0, nr = 0, count;
+	struct buffer_head *bh_ptr[MAX_XOR_BLOCKS];
 
-	if (raid_conf->working_disks != raid_conf->raid_disks)
+	if (conf->working_disks != conf->raid_disks)
 		return 0;
 	tmp.b_size = 4096;
 	if ((tmp.b_data = (char *) get_free_page(GFP_KERNEL)) == NULL)
 		return 0;
+	md_clear_page((unsigned long)tmp.b_data);
 	memset(bh, 0, MD_SB_DISKS * sizeof(struct buffer_head *));
-	for (i = 0; i < raid_conf->raid_disks; i++) {
-		dev = raid_conf->disks[i].dev;
+	for (i = 0; i < conf->raid_disks; i++) {
+		dev = conf->disks[i].dev;
 		set_blocksize(dev, 4096);
 		if ((bh[i] = bread(dev, row / 4, 4096)) == NULL)
 			break;
 		nr++;
 	}
-	if (nr == raid_conf->raid_disks) {
-		for (i = 1; i < nr; i++)
-			xor_block(&tmp, bh[i]);
+	if (nr == conf->raid_disks) {
+		bh_ptr[0] = &tmp;
+		count = 1;
+		for (i = 1; i < nr; i++) {
+			bh_ptr[count++] = bh[i];
+			if (count == MAX_XOR_BLOCKS) {
+				xor_block(count, &bh_ptr[0]);
+				count = 1;
+			}
+		}
+		if (count != 1) {
+			xor_block(count, &bh_ptr[0]);
+		}
 		if (memcmp(tmp.b_data, bh[0]->b_data, 4096))
 			rc = 1;
 	}
-	for (i = 0; i < raid_conf->raid_disks; i++) {
-		dev = raid_conf->disks[i].dev;
+	for (i = 0; i < conf->raid_disks; i++) {
+		dev = conf->disks[i].dev;
 		if (bh[i]) {
 			bforget(bh[i]);
 			bh[i] = NULL;
@@ -1358,285 +1460,607 @@
 	return rc;
 }
 
-static int check_consistency (struct md_dev *mddev)
+static int check_consistency (mddev_t *mddev)
 {
-	int size = mddev->sb->size;
-	int row;
+	if (__check_consistency(mddev, 0))
+/*
+ * We are not checking this currently, as it's legitimate to have
+ * an inconsistent array, at creation time.
+ */
+		return 0;
 
-	for (row = 0; row < size; row += size / 8)
-		if (__check_consistency(mddev, row))
-			return 1;
 	return 0;
 }
 
-static int raid5_run (int minor, struct md_dev *mddev)
+static int raid5_run (mddev_t *mddev)
 {
-	struct raid5_data *raid_conf;
+	raid5_conf_t *conf;
 	int i, j, raid_disk, memory;
-	md_superblock_t *sb = mddev->sb;
-	md_descriptor_t *descriptor;
-	struct real_dev *realdev;
+	mdp_super_t *sb = mddev->sb;
+	mdp_disk_t *desc;
+	mdk_rdev_t *rdev;
+	struct disk_info *disk;
+	struct md_list_head *tmp;
+	int start_recovery = 0;
 
 	MOD_INC_USE_COUNT;
 
 	if (sb->level != 5 && sb->level != 4) {
-		printk("raid5: %s: raid level not set to 4/5 (%d)\n", kdevname(MKDEV(MD_MAJOR, minor)), sb->level);
+		printk("raid5: md%d: raid level not set to 4/5 (%d)\n", mdidx(mddev), sb->level);
 		MOD_DEC_USE_COUNT;
 		return -EIO;
 	}
 
-	mddev->private = kmalloc (sizeof (struct raid5_data), GFP_KERNEL);
-	if ((raid_conf = mddev->private) == NULL)
+	mddev->private = kmalloc (sizeof (raid5_conf_t), GFP_KERNEL);
+	if ((conf = mddev->private) == NULL)
 		goto abort;
-	memset (raid_conf, 0, sizeof (*raid_conf));
-	raid_conf->mddev = mddev;
+	memset (conf, 0, sizeof (*conf));
+	conf->mddev = mddev;
 
-	if ((raid_conf->stripe_hashtbl = (struct stripe_head **) __get_free_pages(GFP_ATOMIC, HASH_PAGES_ORDER)) == NULL)
+	if ((conf->stripe_hashtbl = (struct stripe_head **) md__get_free_pages(GFP_ATOMIC, HASH_PAGES_ORDER)) == NULL)
 		goto abort;
-	memset(raid_conf->stripe_hashtbl, 0, HASH_PAGES * PAGE_SIZE);
+	memset(conf->stripe_hashtbl, 0, HASH_PAGES * PAGE_SIZE);
 
-	init_waitqueue(&raid_conf->wait_for_stripe);
-	PRINTK(("raid5_run(%d) called.\n", minor));
-
-  	for (i = 0; i < mddev->nb_dev; i++) {
-  		realdev = &mddev->devices[i];
-		if (!realdev->sb) {
-			printk(KERN_ERR "raid5: disabled device %s (couldn't access raid superblock)\n", kdevname(realdev->dev));
-			continue;
-		}
+	init_waitqueue(&conf->wait_for_stripe);
+	PRINTK(("raid5_run(md%d) called.\n", mdidx(mddev)));
 
+	ITERATE_RDEV(mddev,rdev,tmp) {
 		/*
 		 * This is important -- we are using the descriptor on
 		 * the disk only to get a pointer to the descriptor on
 		 * the main superblock, which might be more recent.
 		 */
-		descriptor = &sb->disks[realdev->sb->descriptor.number];
-		if (descriptor->state & (1 << MD_FAULTY_DEVICE)) {
-			printk(KERN_ERR "raid5: disabled device %s (errors detected)\n", kdevname(realdev->dev));
+		desc = sb->disks + rdev->desc_nr;
+		raid_disk = desc->raid_disk;
+		disk = conf->disks + raid_disk;
+
+		if (disk_faulty(desc)) {
+			printk(KERN_ERR "raid5: disabled device %s (errors detected)\n", partition_name(rdev->dev));
+			if (!rdev->faulty) {
+				MD_BUG();
+				goto abort;
+			}
+			disk->number = desc->number;
+			disk->raid_disk = raid_disk;
+			disk->dev = rdev->dev;
+
+			disk->operational = 0;
+			disk->write_only = 0;
+			disk->spare = 0;
+			disk->used_slot = 1;
 			continue;
 		}
-		if (descriptor->state & (1 << MD_ACTIVE_DEVICE)) {
-			if (!(descriptor->state & (1 << MD_SYNC_DEVICE))) {
-				printk(KERN_ERR "raid5: disabled device %s (not in sync)\n", kdevname(realdev->dev));
-				continue;
+		if (disk_active(desc)) {
+			if (!disk_sync(desc)) {
+				printk(KERN_ERR "raid5: disabled device %s (not in sync)\n", partition_name(rdev->dev));
+				MD_BUG();
+				goto abort;
 			}
-			raid_disk = descriptor->raid_disk;
-			if (descriptor->number > sb->nr_disks || raid_disk > sb->raid_disks) {
-				printk(KERN_ERR "raid5: disabled device %s (inconsistent descriptor)\n", kdevname(realdev->dev));
+			if (raid_disk > sb->raid_disks) {
+				printk(KERN_ERR "raid5: disabled device %s (inconsistent descriptor)\n", partition_name(rdev->dev));
 				continue;
 			}
-			if (raid_conf->disks[raid_disk].operational) {
-				printk(KERN_ERR "raid5: disabled device %s (device %d already operational)\n", kdevname(realdev->dev), raid_disk);
+			if (disk->operational) {
+				printk(KERN_ERR "raid5: disabled device %s (device %d already operational)\n", partition_name(rdev->dev), raid_disk);
 				continue;
 			}
-			printk(KERN_INFO "raid5: device %s operational as raid disk %d\n", kdevname(realdev->dev), raid_disk);
+			printk(KERN_INFO "raid5: device %s operational as raid disk %d\n", partition_name(rdev->dev), raid_disk);
 	
-			raid_conf->disks[raid_disk].number = descriptor->number;
-			raid_conf->disks[raid_disk].raid_disk = raid_disk;
-			raid_conf->disks[raid_disk].dev = mddev->devices[i].dev;
-			raid_conf->disks[raid_disk].operational = 1;
+			disk->number = desc->number;
+			disk->raid_disk = raid_disk;
+			disk->dev = rdev->dev;
+			disk->operational = 1;
+			disk->used_slot = 1;
 
-			raid_conf->working_disks++;
+			conf->working_disks++;
 		} else {
 			/*
 			 * Must be a spare disk ..
 			 */
-			printk(KERN_INFO "raid5: spare disk %s\n", kdevname(realdev->dev));
-			raid_disk = descriptor->raid_disk;
-			raid_conf->disks[raid_disk].number = descriptor->number;
-			raid_conf->disks[raid_disk].raid_disk = raid_disk;
-			raid_conf->disks[raid_disk].dev = mddev->devices [i].dev;
-
-			raid_conf->disks[raid_disk].operational = 0;
-			raid_conf->disks[raid_disk].write_only = 0;
-			raid_conf->disks[raid_disk].spare = 1;
-		}
-	}
-	raid_conf->raid_disks = sb->raid_disks;
-	raid_conf->failed_disks = raid_conf->raid_disks - raid_conf->working_disks;
-	raid_conf->mddev = mddev;
-	raid_conf->chunk_size = sb->chunk_size;
-	raid_conf->level = sb->level;
-	raid_conf->algorithm = sb->parity_algorithm;
-	raid_conf->max_nr_stripes = NR_STRIPES;
+			printk(KERN_INFO "raid5: spare disk %s\n", partition_name(rdev->dev));
+			disk->number = desc->number;
+			disk->raid_disk = raid_disk;
+			disk->dev = rdev->dev;
 
-	if (raid_conf->working_disks != sb->raid_disks && sb->state != (1 << MD_SB_CLEAN)) {
-		printk(KERN_ALERT "raid5: raid set %s not clean and not all disks are operational -- run ckraid\n", kdevname(MKDEV(MD_MAJOR, minor)));
-		goto abort;
+			disk->operational = 0;
+			disk->write_only = 0;
+			disk->spare = 1;
+			disk->used_slot = 1;
+		}
 	}
-	if (!raid_conf->chunk_size || raid_conf->chunk_size % 4) {
-		printk(KERN_ERR "raid5: invalid chunk size %d for %s\n", raid_conf->chunk_size, kdevname(MKDEV(MD_MAJOR, minor)));
+
+	for (i = 0; i < MD_SB_DISKS; i++) {
+		desc = sb->disks + i;
+		raid_disk = desc->raid_disk;
+		disk = conf->disks + raid_disk;
+
+		if (disk_faulty(desc) && (raid_disk < sb->raid_disks) &&
+			!conf->disks[raid_disk].used_slot) {
+
+			disk->number = desc->number;
+			disk->raid_disk = raid_disk;
+			disk->dev = MKDEV(0,0);
+
+			disk->operational = 0;
+			disk->write_only = 0;
+			disk->spare = 0;
+			disk->used_slot = 1;
+		}
+	}
+
+	conf->raid_disks = sb->raid_disks;
+	/*
+	 * 0 for a fully functional array, 1 for a degraded array.
+	 */
+	conf->failed_disks = conf->raid_disks - conf->working_disks;
+	conf->mddev = mddev;
+	conf->chunk_size = sb->chunk_size;
+	conf->level = sb->level;
+	conf->algorithm = sb->layout;
+	conf->max_nr_stripes = NR_STRIPES;
+
+#if 0
+	for (i = 0; i < conf->raid_disks; i++) {
+		if (!conf->disks[i].used_slot) {
+			MD_BUG();
+			goto abort;
+		}
+	}
+#endif
+	if (!conf->chunk_size || conf->chunk_size % 4) {
+		printk(KERN_ERR "raid5: invalid chunk size %d for md%d\n", conf->chunk_size, mdidx(mddev));
 		goto abort;
 	}
-	if (raid_conf->algorithm > ALGORITHM_RIGHT_SYMMETRIC) {
-		printk(KERN_ERR "raid5: unsupported parity algorithm %d for %s\n", raid_conf->algorithm, kdevname(MKDEV(MD_MAJOR, minor)));
+	if (conf->algorithm > ALGORITHM_RIGHT_SYMMETRIC) {
+		printk(KERN_ERR "raid5: unsupported parity algorithm %d for md%d\n", conf->algorithm, mdidx(mddev));
 		goto abort;
 	}
-	if (raid_conf->failed_disks > 1) {
-		printk(KERN_ERR "raid5: not enough operational devices for %s (%d/%d failed)\n", kdevname(MKDEV(MD_MAJOR, minor)), raid_conf->failed_disks, raid_conf->raid_disks);
+	if (conf->failed_disks > 1) {
+		printk(KERN_ERR "raid5: not enough operational devices for md%d (%d/%d failed)\n", mdidx(mddev), conf->failed_disks, conf->raid_disks);
 		goto abort;
 	}
 
-	if ((sb->state & (1 << MD_SB_CLEAN)) && check_consistency(mddev)) {
-		printk(KERN_ERR "raid5: detected raid-5 xor inconsistenty -- run ckraid\n");
-		sb->state |= 1 << MD_SB_ERRORS;
-		goto abort;
+	if (conf->working_disks != sb->raid_disks) {
+		printk(KERN_ALERT "raid5: md%d, not all disks are operational -- trying to recover array\n", mdidx(mddev));
+		start_recovery = 1;
 	}
 
-	if ((raid_conf->thread = md_register_thread(raid5d, raid_conf)) == NULL) {
-		printk(KERN_ERR "raid5: couldn't allocate thread for %s\n", kdevname(MKDEV(MD_MAJOR, minor)));
-		goto abort;
+	if (!start_recovery && (sb->state & (1 << MD_SB_CLEAN)) &&
+			check_consistency(mddev)) {
+		printk(KERN_ERR "raid5: detected raid-5 superblock xor inconsistency -- running resync\n");
+		sb->state &= ~(1 << MD_SB_CLEAN);
 	}
 
-#if SUPPORT_RECONSTRUCTION
-	if ((raid_conf->resync_thread = md_register_thread(raid5syncd, raid_conf)) == NULL) {
-		printk(KERN_ERR "raid5: couldn't allocate thread for %s\n", kdevname(MKDEV(MD_MAJOR, minor)));
-		goto abort;
+	{
+		const char * name = "raid5d";
+
+		conf->thread = md_register_thread(raid5d, conf, name);
+		if (!conf->thread) {
+			printk(KERN_ERR "raid5: couldn't allocate thread for md%d\n", mdidx(mddev));
+			goto abort;
+		}
 	}
-#endif /* SUPPORT_RECONSTRUCTION */
 
-	memory = raid_conf->max_nr_stripes * (sizeof(struct stripe_head) +
-		 raid_conf->raid_disks * (sizeof(struct buffer_head) +
+	memory = conf->max_nr_stripes * (sizeof(struct stripe_head) +
+		 conf->raid_disks * (sizeof(struct buffer_head) +
 		 2 * (sizeof(struct buffer_head) + PAGE_SIZE))) / 1024;
-	if (grow_stripes(raid_conf, raid_conf->max_nr_stripes, GFP_KERNEL)) {
+	if (grow_stripes(conf, conf->max_nr_stripes, GFP_KERNEL)) {
 		printk(KERN_ERR "raid5: couldn't allocate %dkB for buffers\n", memory);
-		shrink_stripes(raid_conf, raid_conf->max_nr_stripes);
+		shrink_stripes(conf, conf->max_nr_stripes);
 		goto abort;
 	} else
-		printk(KERN_INFO "raid5: allocated %dkB for %s\n", memory, kdevname(MKDEV(MD_MAJOR, minor)));
+		printk(KERN_INFO "raid5: allocated %dkB for md%d\n", memory, mdidx(mddev));
 
 	/*
 	 * Regenerate the "device is in sync with the raid set" bit for
 	 * each device.
 	 */
-	for (i = 0; i < sb->nr_disks ; i++) {
-		sb->disks[i].state &= ~(1 << MD_SYNC_DEVICE);
+	for (i = 0; i < MD_SB_DISKS ; i++) {
+		mark_disk_nonsync(sb->disks + i);
 		for (j = 0; j < sb->raid_disks; j++) {
-			if (!raid_conf->disks[j].operational)
+			if (!conf->disks[j].operational)
 				continue;
-			if (sb->disks[i].number == raid_conf->disks[j].number)
-				sb->disks[i].state |= 1 << MD_SYNC_DEVICE;
+			if (sb->disks[i].number == conf->disks[j].number)
+				mark_disk_sync(sb->disks + i);
 		}
 	}
-	sb->active_disks = raid_conf->working_disks;
+	sb->active_disks = conf->working_disks;
 
 	if (sb->active_disks == sb->raid_disks)
-		printk("raid5: raid level %d set %s active with %d out of %d devices, algorithm %d\n", raid_conf->level, kdevname(MKDEV(MD_MAJOR, minor)), sb->active_disks, sb->raid_disks, raid_conf->algorithm);
+		printk("raid5: raid level %d set md%d active with %d out of %d devices, algorithm %d\n", conf->level, mdidx(mddev), sb->active_disks, sb->raid_disks, conf->algorithm);
 	else
-		printk(KERN_ALERT "raid5: raid level %d set %s active with %d out of %d devices, algorithm %d\n", raid_conf->level, kdevname(MKDEV(MD_MAJOR, minor)), sb->active_disks, sb->raid_disks, raid_conf->algorithm);
+		printk(KERN_ALERT "raid5: raid level %d set md%d active with %d out of %d devices, algorithm %d\n", conf->level, mdidx(mddev), sb->active_disks, sb->raid_disks, conf->algorithm);
+
+	if (!start_recovery && ((sb->state & (1 << MD_SB_CLEAN))==0)) {
+		const char * name = "raid5syncd";
+
+		conf->resync_thread = md_register_thread(raid5syncd, conf,name);
+		if (!conf->resync_thread) {
+			printk(KERN_ERR "raid5: couldn't allocate thread for md%d\n", mdidx(mddev));
+			goto abort;
+		}
 
-	if ((sb->state & (1 << MD_SB_CLEAN)) == 0) {
-		printk("raid5: raid set %s not clean; re-constructing parity\n", kdevname(MKDEV(MD_MAJOR, minor)));
-		raid_conf->resync_parity = 1;
-#if SUPPORT_RECONSTRUCTION
-		md_wakeup_thread(raid_conf->resync_thread);
-#endif /* SUPPORT_RECONSTRUCTION */
+		printk("raid5: raid set md%d not clean; reconstructing parity\n", mdidx(mddev));
+		conf->resync_parity = 1;
+		md_wakeup_thread(conf->resync_thread);
 	}
 
+	print_raid5_conf(conf);
+	if (start_recovery)
+		md_recover_arrays();
+	print_raid5_conf(conf);
+
 	/* Ok, everything is just fine now */
 	return (0);
 abort:
-	if (raid_conf) {
-		if (raid_conf->stripe_hashtbl)
-			free_pages((unsigned long) raid_conf->stripe_hashtbl, HASH_PAGES_ORDER);
-		kfree(raid_conf);
+	if (conf) {
+		print_raid5_conf(conf);
+		if (conf->stripe_hashtbl)
+			free_pages((unsigned long) conf->stripe_hashtbl,
+							HASH_PAGES_ORDER);
+		kfree(conf);
 	}
 	mddev->private = NULL;
-	printk(KERN_ALERT "raid5: failed to run raid set %s\n", kdevname(MKDEV(MD_MAJOR, minor)));
+	printk(KERN_ALERT "raid5: failed to run raid set md%d\n", mdidx(mddev));
 	MOD_DEC_USE_COUNT;
 	return -EIO;
 }
 
-static int raid5_stop (int minor, struct md_dev *mddev)
+static int raid5_stop_resync (mddev_t *mddev)
+{
+	raid5_conf_t *conf = mddev_to_conf(mddev);
+	mdk_thread_t *thread = conf->resync_thread;
+
+	if (thread) {
+		if (conf->resync_parity) {
+			conf->resync_parity = 2;
+			md_interrupt_thread(thread);
+			printk(KERN_INFO "raid5: parity resync was not fully finished, restarting next time.\n");
+			return 1;
+		}
+		return 0;
+	}
+	return 0;
+}
+
+static int raid5_restart_resync (mddev_t *mddev)
 {
-	struct raid5_data *raid_conf = (struct raid5_data *) mddev->private;
+	raid5_conf_t *conf = mddev_to_conf(mddev);
 
-	shrink_stripe_cache(raid_conf, raid_conf->max_nr_stripes);
-	shrink_stripes(raid_conf, raid_conf->max_nr_stripes);
-	md_unregister_thread(raid_conf->thread);
-#if SUPPORT_RECONSTRUCTION
-	md_unregister_thread(raid_conf->resync_thread);
-#endif /* SUPPORT_RECONSTRUCTION */
-	free_pages((unsigned long) raid_conf->stripe_hashtbl, HASH_PAGES_ORDER);
-	kfree(raid_conf);
+	if (conf->resync_parity) {
+		if (!conf->resync_thread) {
+			MD_BUG();
+			return 0;
+		}
+		printk("raid5: waking up raid5resync.\n");
+		conf->resync_parity = 1;
+		md_wakeup_thread(conf->resync_thread);
+		return 1;
+	} else
+		printk("raid5: no restart-resync needed.\n");
+	return 0;
+}
+
+
+static int raid5_stop (mddev_t *mddev)
+{
+	raid5_conf_t *conf = (raid5_conf_t *) mddev->private;
+
+	shrink_stripe_cache(conf, conf->max_nr_stripes);
+	shrink_stripes(conf, conf->max_nr_stripes);
+	md_unregister_thread(conf->thread);
+	if (conf->resync_thread)
+		md_unregister_thread(conf->resync_thread);
+	free_pages((unsigned long) conf->stripe_hashtbl, HASH_PAGES_ORDER);
+	kfree(conf);
 	mddev->private = NULL;
 	MOD_DEC_USE_COUNT;
 	return 0;
 }
 
-static int raid5_status (char *page, int minor, struct md_dev *mddev)
+static int raid5_status (char *page, mddev_t *mddev)
 {
-	struct raid5_data *raid_conf = (struct raid5_data *) mddev->private;
-	md_superblock_t *sb = mddev->sb;
+	raid5_conf_t *conf = (raid5_conf_t *) mddev->private;
+	mdp_super_t *sb = mddev->sb;
 	int sz = 0, i;
 
-	sz += sprintf (page+sz, " level %d, %dk chunk, algorithm %d", sb->level, sb->chunk_size >> 10, sb->parity_algorithm);
-	sz += sprintf (page+sz, " [%d/%d] [", raid_conf->raid_disks, raid_conf->working_disks);
-	for (i = 0; i < raid_conf->raid_disks; i++)
-		sz += sprintf (page+sz, "%s", raid_conf->disks[i].operational ? "U" : "_");
+	sz += sprintf (page+sz, " level %d, %dk chunk, algorithm %d", sb->level, sb->chunk_size >> 10, sb->layout);
+	sz += sprintf (page+sz, " [%d/%d] [", conf->raid_disks, conf->working_disks);
+	for (i = 0; i < conf->raid_disks; i++)
+		sz += sprintf (page+sz, "%s", conf->disks[i].operational ? "U" : "_");
 	sz += sprintf (page+sz, "]");
 	return sz;
 }
 
-static int raid5_mark_spare(struct md_dev *mddev, md_descriptor_t *spare, int state)
+static void print_raid5_conf (raid5_conf_t *conf)
+{
+	int i;
+	struct disk_info *tmp;
+
+	printk("RAID5 conf printout:\n");
+	if (!conf) {
+		printk("(conf==NULL)\n");
+		return;
+	}
+	printk(" --- rd:%d wd:%d fd:%d\n", conf->raid_disks,
+		 conf->working_disks, conf->failed_disks);
+
+	for (i = 0; i < MD_SB_DISKS; i++) {
+		tmp = conf->disks + i;
+		printk(" disk %d, s:%d, o:%d, n:%d rd:%d us:%d dev:%s\n",
+			i, tmp->spare,tmp->operational,
+			tmp->number,tmp->raid_disk,tmp->used_slot,
+			partition_name(tmp->dev));
+	}
+}
+
+static int raid5_diskop(mddev_t *mddev, mdp_disk_t **d, int state)
 {
-	int i = 0, failed_disk = -1;
-	struct raid5_data *raid_conf = mddev->private;
-	struct disk_info *disk = raid_conf->disks;
+	int err = 0;
+	int i, failed_disk=-1, spare_disk=-1, removed_disk=-1, added_disk=-1;
+	raid5_conf_t *conf = mddev->private;
+	struct disk_info *tmp, *sdisk, *fdisk, *rdisk, *adisk;
 	unsigned long flags;
-	md_superblock_t *sb = mddev->sb;
-	md_descriptor_t *descriptor;
+	mdp_super_t *sb = mddev->sb;
+	mdp_disk_t *failed_desc, *spare_desc, *added_desc;
 
-	for (i = 0; i < MD_SB_DISKS; i++, disk++) {
-		if (disk->spare && disk->number == spare->number)
-			goto found;
-	}
-	return 1;
-found:
-	for (i = 0, disk = raid_conf->disks; i < raid_conf->raid_disks; i++, disk++)
-		if (!disk->operational)
-			failed_disk = i;
-	if (failed_disk == -1)
-		return 1;
 	save_flags(flags);
 	cli();
+
+	print_raid5_conf(conf);
+	/*
+	 * find the disk ...
+	 */
 	switch (state) {
-		case SPARE_WRITE:
-			disk->operational = 1;
-			disk->write_only = 1;
-			raid_conf->spare = disk;
-			break;
-		case SPARE_INACTIVE:
-			disk->operational = 0;
-			disk->write_only = 0;
-			raid_conf->spare = NULL;
-			break;
-		case SPARE_ACTIVE:
-			disk->spare = 0;
-			disk->write_only = 0;
 
-			descriptor = &sb->disks[raid_conf->disks[failed_disk].number];
-			i = spare->raid_disk;
-			disk->raid_disk = spare->raid_disk = descriptor->raid_disk;
-			if (disk->raid_disk != failed_disk)
-				printk("raid5: disk->raid_disk != failed_disk");
-			descriptor->raid_disk = i;
-
-			raid_conf->spare = NULL;
-			raid_conf->working_disks++;
-			raid_conf->failed_disks--;
-			raid_conf->disks[failed_disk] = *disk;
-			break;
-		default:
-			printk("raid5_mark_spare: bug: state == %d\n", state);
-			restore_flags(flags);
-			return 1;
+	case DISKOP_SPARE_ACTIVE:
+
+		/*
+		 * Find the failed disk within the RAID5 configuration ...
+		 * (this can only be in the first conf->raid_disks part)
+		 */
+		for (i = 0; i < conf->raid_disks; i++) {
+			tmp = conf->disks + i;
+			if ((!tmp->operational && !tmp->spare) ||
+					!tmp->used_slot) {
+				failed_disk = i;
+				break;
+			}
+		}
+		/*
+		 * When we activate a spare disk we _must_ have a disk in
+		 * the lower (active) part of the array to replace. 
+		 */
+		if ((failed_disk == -1) || (failed_disk >= conf->raid_disks)) {
+			MD_BUG();
+			err = 1;
+			goto abort;
+		}
+		/* fall through */
+
+	case DISKOP_SPARE_WRITE:
+	case DISKOP_SPARE_INACTIVE:
+
+		/*
+		 * Find the spare disk ... (can only be in the 'high'
+		 * area of the array)
+		 */
+		for (i = conf->raid_disks; i < MD_SB_DISKS; i++) {
+			tmp = conf->disks + i;
+			if (tmp->spare && tmp->number == (*d)->number) {
+				spare_disk = i;
+				break;
+			}
+		}
+		if (spare_disk == -1) {
+			MD_BUG();
+			err = 1;
+			goto abort;
+		}
+		break;
+
+	case DISKOP_HOT_REMOVE_DISK:
+
+		for (i = 0; i < MD_SB_DISKS; i++) {
+			tmp = conf->disks + i;
+			if (tmp->used_slot && (tmp->number == (*d)->number)) {
+				if (tmp->operational) {
+					err = -EBUSY;
+					goto abort;
+				}
+				removed_disk = i;
+				break;
+			}
+		}
+		if (removed_disk == -1) {
+			MD_BUG();
+			err = 1;
+			goto abort;
+		}
+		break;
+
+	case DISKOP_HOT_ADD_DISK:
+
+		for (i = conf->raid_disks; i < MD_SB_DISKS; i++) {
+			tmp = conf->disks + i;
+			if (!tmp->used_slot) {
+				added_disk = i;
+				break;
+			}
+		}
+		if (added_disk == -1) {
+			MD_BUG();
+			err = 1;
+			goto abort;
+		}
+		break;
+	}
+
+	switch (state) {
+	/*
+	 * Switch the spare disk to write-only mode:
+	 */
+	case DISKOP_SPARE_WRITE:
+		if (conf->spare) {
+			MD_BUG();
+			err = 1;
+			goto abort;
+		}
+		sdisk = conf->disks + spare_disk;
+		sdisk->operational = 1;
+		sdisk->write_only = 1;
+		conf->spare = sdisk;
+		break;
+	/*
+	 * Deactivate a spare disk:
+	 */
+	case DISKOP_SPARE_INACTIVE:
+		sdisk = conf->disks + spare_disk;
+		sdisk->operational = 0;
+		sdisk->write_only = 0;
+		/*
+		 * Was the spare being resynced?
+		 */
+		if (conf->spare == sdisk)
+			conf->spare = NULL;
+		break;
+	/*
+	 * Activate (mark read-write) the (now sync) spare disk,
+	 * which means we switch it's 'raid position' (->raid_disk)
+	 * with the failed disk. (only the first 'conf->raid_disks'
+	 * slots are used for 'real' disks and we must preserve this
+	 * property)
+	 */
+	case DISKOP_SPARE_ACTIVE:
+		if (!conf->spare) {
+			MD_BUG();
+			err = 1;
+			goto abort;
+		}
+		sdisk = conf->disks + spare_disk;
+		fdisk = conf->disks + failed_disk;
+
+		spare_desc = &sb->disks[sdisk->number];
+		failed_desc = &sb->disks[fdisk->number];
+
+		if (spare_desc != *d) {
+			MD_BUG();
+			err = 1;
+			goto abort;
+		}
+
+		if (spare_desc->raid_disk != sdisk->raid_disk) {
+			MD_BUG();
+			err = 1;
+			goto abort;
+		}
+			
+		if (sdisk->raid_disk != spare_disk) {
+			MD_BUG();
+			err = 1;
+			goto abort;
+		}
+
+		if (failed_desc->raid_disk != fdisk->raid_disk) {
+			MD_BUG();
+			err = 1;
+			goto abort;
+		}
+
+		if (fdisk->raid_disk != failed_disk) {
+			MD_BUG();
+			err = 1;
+			goto abort;
+		}
+
+		/*
+		 * do the switch finally
+		 */
+		xchg_values(*spare_desc, *failed_desc);
+		xchg_values(*fdisk, *sdisk);
+
+		/*
+		 * (careful, 'failed' and 'spare' are switched from now on)
+		 *
+		 * we want to preserve linear numbering and we want to
+		 * give the proper raid_disk number to the now activated
+		 * disk. (this means we switch back these values)
+		 */
+	
+		xchg_values(spare_desc->raid_disk, failed_desc->raid_disk);
+		xchg_values(sdisk->raid_disk, fdisk->raid_disk);
+		xchg_values(spare_desc->number, failed_desc->number);
+		xchg_values(sdisk->number, fdisk->number);
+
+		*d = failed_desc;
+
+		if (sdisk->dev == MKDEV(0,0))
+			sdisk->used_slot = 0;
+
+		/*
+		 * this really activates the spare.
+		 */
+		fdisk->spare = 0;
+		fdisk->write_only = 0;
+
+		/*
+		 * if we activate a spare, we definitely replace a
+		 * non-operational disk slot in the 'low' area of
+		 * the disk array.
+		 */
+		conf->failed_disks--;
+		conf->working_disks++;
+		conf->spare = NULL;
+
+		break;
+
+	case DISKOP_HOT_REMOVE_DISK:
+		rdisk = conf->disks + removed_disk;
+
+		if (rdisk->spare && (removed_disk < conf->raid_disks)) {
+			MD_BUG();	
+			err = 1;
+			goto abort;
+		}
+		rdisk->dev = MKDEV(0,0);
+		rdisk->used_slot = 0;
+
+		break;
+
+	case DISKOP_HOT_ADD_DISK:
+		adisk = conf->disks + added_disk;
+		added_desc = *d;
+
+		if (added_disk != added_desc->number) {
+			MD_BUG();	
+			err = 1;
+			goto abort;
+		}
+
+		adisk->number = added_desc->number;
+		adisk->raid_disk = added_desc->raid_disk;
+		adisk->dev = MKDEV(added_desc->major,added_desc->minor);
+
+		adisk->operational = 0;
+		adisk->write_only = 0;
+		adisk->spare = 1;
+		adisk->used_slot = 1;
+
+
+		break;
+
+	default:
+		MD_BUG();	
+		err = 1;
+		goto abort;
 	}
+abort:
 	restore_flags(flags);
-	return 0;
+	print_raid5_conf(conf);
+	return err;
 }
 
-static struct md_personality raid5_personality=
+static mdk_personality_t raid5_personality=
 {
 	"raid5",
 	raid5_map,
@@ -1648,14 +2072,19 @@
 	NULL,			/* no ioctls */
 	0,
 	raid5_error,
-	/* raid5_hot_add_disk, */ NULL,
-	/* raid1_hot_remove_drive */ NULL,
-	raid5_mark_spare
+	raid5_diskop,
+	raid5_stop_resync,
+	raid5_restart_resync
 };
 
 int raid5_init (void)
 {
-	return register_md_personality (RAID5, &raid5_personality);
+	int err;
+
+	err = register_md_personality (RAID5, &raid5_personality);
+	if (err)
+		return err;
+	return 0;
 }
 
 #ifdef MODULE
diff -urN linux/drivers/block/rd.c /tmp/linux/drivers/block/rd.c
--- linux/drivers/block/rd.c	Sun Dec 10 17:49:41 2000
+++ /tmp/linux/drivers/block/rd.c	Fri Feb  2 19:06:42 2001
@@ -173,7 +173,7 @@
 	if (CURRENT->cmd == READ) 
 		memset(CURRENT->buffer, 0, len); 
 	else	
-		set_bit(BH_Protected, &CURRENT->bh->b_state);
+		mark_buffer_protected(CURRENT->bh);
 
 	end_request(1);
 	goto repeat;
diff -urN linux/drivers/block/translucent.c /tmp/linux/drivers/block/translucent.c
--- linux/drivers/block/translucent.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/block/translucent.c	Fri Feb  2 19:06:00 2001
@@ -0,0 +1,136 @@
+/*
+   translucent.c : Translucent RAID driver for Linux
+              Copyright (C) 1998 Ingo Molnar
+
+   Translucent mode management functions.
+
+   This program is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 2, or (at your option)
+   any later version.
+   
+   You should have received a copy of the GNU General Public License
+   (for example /usr/src/linux/COPYING); if not, write to the Free
+   Software Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.  
+*/
+
+#include <linux/module.h>
+
+#include <linux/raid/md.h>
+#include <linux/malloc.h>
+
+#include <linux/raid/translucent.h>
+
+#define MAJOR_NR MD_MAJOR
+#define MD_DRIVER
+#define MD_PERSONALITY
+
+static int translucent_run (mddev_t *mddev)
+{
+	translucent_conf_t *conf;
+	mdk_rdev_t *rdev;
+	int i;
+
+	MOD_INC_USE_COUNT;
+
+	conf = kmalloc (sizeof (*conf), GFP_KERNEL);
+	if (!conf)
+		goto out;
+	mddev->private = conf;
+
+	if (mddev->nb_dev != 2) {
+		printk("translucent: this mode needs 2 disks, aborting!\n");
+		goto out;
+	}
+
+	if (md_check_ordering(mddev)) {
+		printk("translucent: disks are not ordered, aborting!\n");
+		goto out;
+	}
+
+	ITERATE_RDEV_ORDERED(mddev,rdev,i) {
+		dev_info_t *disk = conf->disks + i;
+
+		disk->dev = rdev->dev;
+		disk->size = rdev->size;
+	}
+
+	return 0;
+
+out:
+	if (conf)
+		kfree(conf);
+
+	MOD_DEC_USE_COUNT;
+	return 1;
+}
+
+static int translucent_stop (mddev_t *mddev)
+{
+	translucent_conf_t *conf = mddev_to_conf(mddev);
+  
+	kfree(conf);
+
+	MOD_DEC_USE_COUNT;
+
+	return 0;
+}
+
+
+static int translucent_map (mddev_t *mddev, kdev_t dev, kdev_t *rdev,
+		       unsigned long *rsector, unsigned long size)
+{
+	translucent_conf_t *conf = mddev_to_conf(mddev);
+  
+	*rdev = conf->disks[0].dev;
+
+	return 0;
+}
+
+static int translucent_status (char *page, mddev_t *mddev)
+{
+	int sz = 0;
+  
+	sz += sprintf(page+sz, " %d%% full", 10);
+	return sz;
+}
+
+
+static mdk_personality_t translucent_personality=
+{
+	"translucent",
+	translucent_map,
+	NULL,
+	NULL,
+	translucent_run,
+	translucent_stop,
+	translucent_status,
+	NULL,
+	0,
+	NULL,
+	NULL,
+	NULL,
+	NULL
+};
+
+#ifndef MODULE
+
+md__initfunc(void translucent_init (void))
+{
+	register_md_personality (TRANSLUCENT, &translucent_personality);
+}
+
+#else
+
+int init_module (void)
+{
+	return (register_md_personality (TRANSLUCENT, &translucent_personality));
+}
+
+void cleanup_module (void)
+{
+	unregister_md_personality (TRANSLUCENT);
+}
+
+#endif
+
diff -urN linux/drivers/block/xor.c /tmp/linux/drivers/block/xor.c
--- linux/drivers/block/xor.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/block/xor.c	Fri Feb  2 19:06:00 2001
@@ -0,0 +1,1894 @@
+/*
+ * xor.c : Multiple Devices driver for Linux
+ *
+ * Copyright (C) 1996, 1997, 1998, 1999 Ingo Molnar, Matti Aarnio, Jakub Jelinek
+ *
+ *
+ * optimized RAID-5 checksumming functions.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2, or (at your option)
+ * any later version.
+ *
+ * You should have received a copy of the GNU General Public License
+ * (for example /usr/src/linux/COPYING); if not, write to the Free
+ * Software Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#include <linux/module.h>
+#include <linux/raid/md.h>
+#ifdef __sparc_v9__
+#include <asm/head.h>
+#include <asm/asi.h>
+#include <asm/visasm.h>
+#endif
+
+/*
+ * we use the 'XOR function template' to register multiple xor
+ * functions runtime. The kernel measures their speed upon bootup
+ * and decides which one to use. (compile-time registration is
+ * not enough as certain CPU features like MMX can only be detected
+ * runtime)
+ *
+ * this architecture makes it pretty easy to add new routines
+ * that are faster on certain CPUs, without killing other CPU's
+ * 'native' routine. Although the current routines are belived
+ * to be the physically fastest ones on all CPUs tested, but
+ * feel free to prove me wrong and add yet another routine =B-)
+ * --mingo
+ */
+
+#define MAX_XOR_BLOCKS 5
+
+#define XOR_ARGS (unsigned int count, struct buffer_head **bh_ptr)
+
+typedef void (*xor_block_t) XOR_ARGS;
+xor_block_t xor_block = NULL;
+
+#ifndef __sparc_v9__
+
+struct xor_block_template;
+
+struct xor_block_template {
+	char * name;
+	xor_block_t xor_block;
+	int speed;
+	struct xor_block_template * next;
+};
+
+struct xor_block_template * xor_functions = NULL;
+
+#define XORBLOCK_TEMPLATE(x) \
+static void xor_block_##x XOR_ARGS; \
+static struct xor_block_template t_xor_block_##x = \
+				 { #x, xor_block_##x, 0, NULL }; \
+static void xor_block_##x XOR_ARGS
+
+#ifdef __i386__
+
+#ifdef CONFIG_X86_XMM
+/*
+ * Cache avoiding checksumming functions utilizing KNI instructions
+ * Copyright (C) 1999 Zach Brown (with obvious credit due Ingo)
+ */
+
+XORBLOCK_TEMPLATE(pIII_kni)
+{
+	char xmm_save[16*4];
+	int cr0;
+        int lines = (bh_ptr[0]->b_size>>8);
+
+	__asm__ __volatile__ ( 
+		"movl %%cr0,%0		;\n\t"
+		"clts			;\n\t"
+		"movups %%xmm0,(%1)	;\n\t"
+		"movups %%xmm1,0x10(%1)	;\n\t"
+		"movups %%xmm2,0x20(%1)	;\n\t"
+		"movups %%xmm3,0x30(%1)	;\n\t"
+		: "=r" (cr0)
+		: "r" (xmm_save) 
+		: "memory" );
+
+#define OFFS(x) "8*("#x"*2)"
+#define	PF0(x) \
+	"	prefetcht0  "OFFS(x)"(%1)   ;\n"
+#define LD(x,y) \
+        "       movaps   "OFFS(x)"(%1), %%xmm"#y"   ;\n"
+#define ST(x,y) \
+        "       movaps %%xmm"#y",   "OFFS(x)"(%1)   ;\n"
+#define PF1(x) \
+	"	prefetchnta "OFFS(x)"(%2)   ;\n"
+#define PF2(x) \
+	"	prefetchnta "OFFS(x)"(%3)   ;\n"
+#define PF3(x) \
+	"	prefetchnta "OFFS(x)"(%4)   ;\n"
+#define PF4(x) \
+	"	prefetchnta "OFFS(x)"(%5)   ;\n"
+#define PF5(x) \
+	"	prefetchnta "OFFS(x)"(%6)   ;\n"
+#define XO1(x,y) \
+        "       xorps   "OFFS(x)"(%2), %%xmm"#y"   ;\n"
+#define XO2(x,y) \
+        "       xorps   "OFFS(x)"(%3), %%xmm"#y"   ;\n"
+#define XO3(x,y) \
+        "       xorps   "OFFS(x)"(%4), %%xmm"#y"   ;\n"
+#define XO4(x,y) \
+        "       xorps   "OFFS(x)"(%5), %%xmm"#y"   ;\n"
+#define XO5(x,y) \
+        "       xorps   "OFFS(x)"(%6), %%xmm"#y"   ;\n"
+
+	switch(count) {
+		case 2:
+		        __asm__ __volatile__ (
+#undef BLOCK
+#define BLOCK(i) \
+		LD(i,0)					\
+			LD(i+1,1)			\
+		PF1(i)					\
+				PF1(i+2)		\
+				LD(i+2,2)		\
+					LD(i+3,3)	\
+		PF0(i+4)				\
+				PF0(i+6)		\
+		XO1(i,0)				\
+			XO1(i+1,1)			\
+				XO1(i+2,2)		\
+					XO1(i+3,3)	\
+		ST(i,0)					\
+			ST(i+1,1)			\
+				ST(i+2,2)		\
+					ST(i+3,3)	\
+
+
+		PF0(0)
+				PF0(2)
+
+	" .align 32,0x90		;\n"
+        " 1:                            ;\n"
+
+		BLOCK(0)
+		BLOCK(4)
+		BLOCK(8)
+		BLOCK(12)
+
+        "       addl $256, %1           ;\n"
+        "       addl $256, %2           ;\n"
+        "       decl %0                 ;\n"
+        "       jnz 1b                  ;\n"
+
+        		:
+			: "r" (lines),
+			  "r" (bh_ptr[0]->b_data),
+        		  "r" (bh_ptr[1]->b_data)
+		        : "memory" );
+			break;
+		case 3:
+		        __asm__ __volatile__ (
+#undef BLOCK
+#define BLOCK(i) \
+		PF1(i)					\
+				PF1(i+2)		\
+		LD(i,0)					\
+			LD(i+1,1)			\
+				LD(i+2,2)		\
+					LD(i+3,3)	\
+		PF2(i)					\
+				PF2(i+2)		\
+		PF0(i+4)				\
+				PF0(i+6)		\
+		XO1(i,0)				\
+			XO1(i+1,1)			\
+				XO1(i+2,2)		\
+					XO1(i+3,3)	\
+		XO2(i,0)				\
+			XO2(i+1,1)			\
+				XO2(i+2,2)		\
+					XO2(i+3,3)	\
+		ST(i,0)					\
+			ST(i+1,1)			\
+				ST(i+2,2)		\
+					ST(i+3,3)	\
+
+
+		PF0(0)
+				PF0(2)
+
+	" .align 32,0x90		;\n"
+        " 1:                            ;\n"
+
+		BLOCK(0)
+		BLOCK(4)
+		BLOCK(8)
+		BLOCK(12)
+
+        "       addl $256, %1           ;\n"
+        "       addl $256, %2           ;\n"
+        "       addl $256, %3           ;\n"
+        "       decl %0                 ;\n"
+        "       jnz 1b                  ;\n"
+        		:
+			: "r" (lines),
+			  "r" (bh_ptr[0]->b_data),
+        		  "r" (bh_ptr[1]->b_data),
+			  "r" (bh_ptr[2]->b_data)
+		        : "memory" );
+			break;
+		case 4:
+		        __asm__ __volatile__ (
+#undef BLOCK
+#define BLOCK(i) \
+		PF1(i)					\
+				PF1(i+2)		\
+		LD(i,0)					\
+			LD(i+1,1)			\
+				LD(i+2,2)		\
+					LD(i+3,3)	\
+		PF2(i)					\
+				PF2(i+2)		\
+		XO1(i,0)				\
+			XO1(i+1,1)			\
+				XO1(i+2,2)		\
+					XO1(i+3,3)	\
+		PF3(i)					\
+				PF3(i+2)		\
+		PF0(i+4)				\
+				PF0(i+6)		\
+		XO2(i,0)				\
+			XO2(i+1,1)			\
+				XO2(i+2,2)		\
+					XO2(i+3,3)	\
+		XO3(i,0)				\
+			XO3(i+1,1)			\
+				XO3(i+2,2)		\
+					XO3(i+3,3)	\
+		ST(i,0)					\
+			ST(i+1,1)			\
+				ST(i+2,2)		\
+					ST(i+3,3)	\
+
+
+		PF0(0)
+				PF0(2)
+
+	" .align 32,0x90		;\n"
+        " 1:                            ;\n"
+
+		BLOCK(0)
+		BLOCK(4)
+		BLOCK(8)
+		BLOCK(12)
+
+        "       addl $256, %1           ;\n"
+        "       addl $256, %2           ;\n"
+        "       addl $256, %3           ;\n"
+        "       addl $256, %4           ;\n"
+        "       decl %0                 ;\n"
+        "       jnz 1b                  ;\n"
+
+        		:
+			: "r" (lines),
+			  "r" (bh_ptr[0]->b_data),
+        		  "r" (bh_ptr[1]->b_data),
+			  "r" (bh_ptr[2]->b_data),
+			  "r" (bh_ptr[3]->b_data)
+		        : "memory" );
+			break;
+		case 5:
+		        __asm__ __volatile__ (
+#undef BLOCK
+#define BLOCK(i) \
+		PF1(i)					\
+				PF1(i+2)		\
+		LD(i,0)					\
+			LD(i+1,1)			\
+				LD(i+2,2)		\
+					LD(i+3,3)	\
+		PF2(i)					\
+				PF2(i+2)		\
+		XO1(i,0)				\
+			XO1(i+1,1)			\
+				XO1(i+2,2)		\
+					XO1(i+3,3)	\
+		PF3(i)					\
+				PF3(i+2)		\
+		XO2(i,0)				\
+			XO2(i+1,1)			\
+				XO2(i+2,2)		\
+					XO2(i+3,3)	\
+		PF4(i)					\
+				PF4(i+2)		\
+		PF0(i+4)				\
+				PF0(i+6)		\
+		XO3(i,0)				\
+			XO3(i+1,1)			\
+				XO3(i+2,2)		\
+					XO3(i+3,3)	\
+		XO4(i,0)				\
+			XO4(i+1,1)			\
+				XO4(i+2,2)		\
+					XO4(i+3,3)	\
+		ST(i,0)					\
+			ST(i+1,1)			\
+				ST(i+2,2)		\
+					ST(i+3,3)	\
+
+
+		PF0(0)
+				PF0(2)
+
+	" .align 32,0x90		;\n"
+        " 1:                            ;\n"
+
+		BLOCK(0)
+		BLOCK(4)
+		BLOCK(8)
+		BLOCK(12)
+
+        "       addl $256, %1           ;\n"
+        "       addl $256, %2           ;\n"
+        "       addl $256, %3           ;\n"
+        "       addl $256, %4           ;\n"
+        "       addl $256, %5           ;\n"
+        "       decl %0                 ;\n"
+        "       jnz 1b                  ;\n"
+
+        		:
+			: "r" (lines),
+			  "r" (bh_ptr[0]->b_data),
+        		  "r" (bh_ptr[1]->b_data),
+			  "r" (bh_ptr[2]->b_data),
+			  "r" (bh_ptr[3]->b_data),
+			  "r" (bh_ptr[4]->b_data)
+			: "memory");
+			break;
+	}
+
+	__asm__ __volatile__ ( 
+		"sfence			;\n\t"
+		"movups (%1),%%xmm0	;\n\t"
+		"movups 0x10(%1),%%xmm1	;\n\t"
+		"movups 0x20(%1),%%xmm2	;\n\t"
+		"movups 0x30(%1),%%xmm3	;\n\t"
+		"movl 	%0,%%cr0	;\n\t"
+		:
+		: "r" (cr0), "r" (xmm_save)
+		: "memory" );
+}
+
+#undef OFFS
+#undef LD
+#undef ST
+#undef PF0
+#undef PF1
+#undef PF2
+#undef PF3
+#undef PF4
+#undef PF5
+#undef XO1
+#undef XO2
+#undef XO3
+#undef XO4
+#undef XO5
+#undef BLOCK
+
+#endif /* CONFIG_X86_XMM */
+
+/*
+ * high-speed RAID5 checksumming functions utilizing MMX instructions
+ * Copyright (C) 1998 Ingo Molnar
+ */
+XORBLOCK_TEMPLATE(pII_mmx)
+{
+	char fpu_save[108];
+        int lines = (bh_ptr[0]->b_size>>7);
+
+	if (!(current->flags & PF_USEDFPU))
+		__asm__ __volatile__ ( " clts;\n");
+
+	__asm__ __volatile__ ( " fsave %0; fwait\n"::"m"(fpu_save[0]) );
+
+#define LD(x,y) \
+        "       movq   8*("#x")(%1), %%mm"#y"   ;\n"
+#define ST(x,y) \
+        "       movq %%mm"#y",   8*("#x")(%1)   ;\n"
+#define XO1(x,y) \
+        "       pxor   8*("#x")(%2), %%mm"#y"   ;\n"
+#define XO2(x,y) \
+        "       pxor   8*("#x")(%3), %%mm"#y"   ;\n"
+#define XO3(x,y) \
+        "       pxor   8*("#x")(%4), %%mm"#y"   ;\n"
+#define XO4(x,y) \
+        "       pxor   8*("#x")(%5), %%mm"#y"   ;\n"
+
+	switch(count) {
+		case 2:
+			__asm__ __volatile__ (
+#undef BLOCK
+#define BLOCK(i) \
+			LD(i,0)					\
+				LD(i+1,1)			\
+					LD(i+2,2)		\
+						LD(i+3,3)	\
+			XO1(i,0)				\
+			ST(i,0)					\
+				XO1(i+1,1)			\
+				ST(i+1,1)			\
+					XO1(i+2,2)		\
+					ST(i+2,2)		\
+						XO1(i+3,3)	\
+						ST(i+3,3)
+
+			" .align 32,0x90		;\n"
+  			" 1:                            ;\n"
+
+			BLOCK(0)
+			BLOCK(4)
+			BLOCK(8)
+			BLOCK(12)
+
+		        "       addl $128, %1         ;\n"
+		        "       addl $128, %2         ;\n"
+		        "       decl %0               ;\n"
+		        "       jnz 1b                ;\n"
+	        	:
+			: "r" (lines),
+			  "r" (bh_ptr[0]->b_data),
+			  "r" (bh_ptr[1]->b_data)
+			: "memory");
+			break;
+		case 3:
+			__asm__ __volatile__ (
+#undef BLOCK
+#define BLOCK(i) \
+			LD(i,0)					\
+				LD(i+1,1)			\
+					LD(i+2,2)		\
+						LD(i+3,3)	\
+			XO1(i,0)				\
+				XO1(i+1,1)			\
+					XO1(i+2,2)		\
+						XO1(i+3,3)	\
+			XO2(i,0)				\
+			ST(i,0)					\
+				XO2(i+1,1)			\
+				ST(i+1,1)			\
+					XO2(i+2,2)		\
+					ST(i+2,2)		\
+						XO2(i+3,3)	\
+						ST(i+3,3)
+
+			" .align 32,0x90		;\n"
+  			" 1:                            ;\n"
+
+			BLOCK(0)
+			BLOCK(4)
+			BLOCK(8)
+			BLOCK(12)
+
+		        "       addl $128, %1         ;\n"
+		        "       addl $128, %2         ;\n"
+		        "       addl $128, %3         ;\n"
+		        "       decl %0               ;\n"
+		        "       jnz 1b                ;\n"
+	        	:
+			: "r" (lines),
+			  "r" (bh_ptr[0]->b_data),
+			  "r" (bh_ptr[1]->b_data),
+			  "r" (bh_ptr[2]->b_data)
+			: "memory");
+			break;
+		case 4:
+			__asm__ __volatile__ (
+#undef BLOCK
+#define BLOCK(i) \
+			LD(i,0)					\
+				LD(i+1,1)			\
+					LD(i+2,2)		\
+						LD(i+3,3)	\
+			XO1(i,0)				\
+				XO1(i+1,1)			\
+					XO1(i+2,2)		\
+						XO1(i+3,3)	\
+			XO2(i,0)				\
+				XO2(i+1,1)			\
+					XO2(i+2,2)		\
+						XO2(i+3,3)	\
+			XO3(i,0)				\
+			ST(i,0)					\
+				XO3(i+1,1)			\
+				ST(i+1,1)			\
+					XO3(i+2,2)		\
+					ST(i+2,2)		\
+						XO3(i+3,3)	\
+						ST(i+3,3)
+
+			" .align 32,0x90		;\n"
+  			" 1:                            ;\n"
+
+			BLOCK(0)
+			BLOCK(4)
+			BLOCK(8)
+			BLOCK(12)
+
+		        "       addl $128, %1         ;\n"
+		        "       addl $128, %2         ;\n"
+		        "       addl $128, %3         ;\n"
+		        "       addl $128, %4         ;\n"
+		        "       decl %0               ;\n"
+		        "       jnz 1b                ;\n"
+	        	:
+			: "r" (lines),
+			  "r" (bh_ptr[0]->b_data),
+			  "r" (bh_ptr[1]->b_data),
+			  "r" (bh_ptr[2]->b_data),
+			  "r" (bh_ptr[3]->b_data)
+			: "memory");
+			break;
+		case 5:
+			__asm__ __volatile__ (
+#undef BLOCK
+#define BLOCK(i) \
+			LD(i,0)					\
+				LD(i+1,1)			\
+					LD(i+2,2)		\
+						LD(i+3,3)	\
+			XO1(i,0)				\
+				XO1(i+1,1)			\
+					XO1(i+2,2)		\
+						XO1(i+3,3)	\
+			XO2(i,0)				\
+				XO2(i+1,1)			\
+					XO2(i+2,2)		\
+						XO2(i+3,3)	\
+			XO3(i,0)				\
+				XO3(i+1,1)			\
+					XO3(i+2,2)		\
+						XO3(i+3,3)	\
+			XO4(i,0)				\
+			ST(i,0)					\
+				XO4(i+1,1)			\
+				ST(i+1,1)			\
+					XO4(i+2,2)		\
+					ST(i+2,2)		\
+						XO4(i+3,3)	\
+						ST(i+3,3)
+
+			" .align 32,0x90		;\n"
+  			" 1:                            ;\n"
+
+			BLOCK(0)
+			BLOCK(4)
+			BLOCK(8)
+			BLOCK(12)
+
+		        "       addl $128, %1         ;\n"
+		        "       addl $128, %2         ;\n"
+		        "       addl $128, %3         ;\n"
+		        "       addl $128, %4         ;\n"
+		        "       addl $128, %5         ;\n"
+		        "       decl %0               ;\n"
+		        "       jnz 1b                ;\n"
+	        	:
+			: "r" (lines),
+			  "r" (bh_ptr[0]->b_data),
+			  "r" (bh_ptr[1]->b_data),
+			  "r" (bh_ptr[2]->b_data),
+			  "r" (bh_ptr[3]->b_data),
+			  "r" (bh_ptr[4]->b_data)
+			: "memory");
+			break;
+	}
+
+	__asm__ __volatile__ ( " frstor %0;\n"::"m"(fpu_save[0]) );
+
+	if (!(current->flags & PF_USEDFPU))
+		stts();
+}
+
+#undef LD
+#undef XO1
+#undef XO2
+#undef XO3
+#undef XO4
+#undef ST
+#undef BLOCK
+
+XORBLOCK_TEMPLATE(p5_mmx)
+{
+	char fpu_save[108];
+        int lines = (bh_ptr[0]->b_size>>6);
+
+	if (!(current->flags & PF_USEDFPU))
+		__asm__ __volatile__ ( " clts;\n");
+
+	__asm__ __volatile__ ( " fsave %0; fwait\n"::"m"(fpu_save[0]) );
+
+	switch(count) {
+		case 2:
+		        __asm__ __volatile__ (
+
+			        " .align 32,0x90             ;\n"
+			        " 1:                         ;\n"
+			        "       movq   (%1), %%mm0   ;\n"
+			        "       movq  8(%1), %%mm1   ;\n"
+			        "       pxor   (%2), %%mm0   ;\n"
+			        "       movq 16(%1), %%mm2   ;\n"
+			        "       movq %%mm0,   (%1)   ;\n"
+			        "       pxor  8(%2), %%mm1   ;\n"
+			        "       movq 24(%1), %%mm3   ;\n"
+			        "       movq %%mm1,  8(%1)   ;\n"
+			        "       pxor 16(%2), %%mm2   ;\n"
+			        "       movq 32(%1), %%mm4   ;\n"
+			        "       movq %%mm2, 16(%1)   ;\n"
+			        "       pxor 24(%2), %%mm3   ;\n"
+			        "       movq 40(%1), %%mm5   ;\n"
+			        "       movq %%mm3, 24(%1)   ;\n"
+			        "       pxor 32(%2), %%mm4   ;\n"
+			        "       movq 48(%1), %%mm6   ;\n"
+			        "       movq %%mm4, 32(%1)   ;\n"
+			        "       pxor 40(%2), %%mm5   ;\n"
+			        "       movq 56(%1), %%mm7   ;\n"
+			        "       movq %%mm5, 40(%1)   ;\n"
+			        "       pxor 48(%2), %%mm6   ;\n"
+			        "       pxor 56(%2), %%mm7   ;\n"
+			        "       movq %%mm6, 48(%1)   ;\n"
+			        "       movq %%mm7, 56(%1)   ;\n"
+        
+			        "       addl $64, %1         ;\n"
+			        "       addl $64, %2         ;\n"
+			        "       decl %0              ;\n"
+			        "       jnz 1b               ;\n"
+
+			        : 
+			        : "r" (lines),
+				  "r" (bh_ptr[0]->b_data),
+				  "r" (bh_ptr[1]->b_data)
+			        : "memory" );
+			break;
+		case 3:
+			__asm__ __volatile__ (
+
+			        " .align 32,0x90             ;\n"
+			        " 1:                         ;\n"
+			        "       movq   (%1), %%mm0   ;\n"
+			        "       movq  8(%1), %%mm1   ;\n"
+			        "       pxor   (%2), %%mm0   ;\n"
+			        "       movq 16(%1), %%mm2   ;\n"
+			        "       pxor  8(%2), %%mm1   ;\n"
+			        "       pxor   (%3), %%mm0   ;\n"
+			        "       pxor 16(%2), %%mm2   ;\n"
+			        "       movq %%mm0,   (%1)   ;\n"
+			        "       pxor  8(%3), %%mm1   ;\n"
+			        "       pxor 16(%3), %%mm2   ;\n"
+			        "       movq 24(%1), %%mm3   ;\n"
+			        "       movq %%mm1,  8(%1)   ;\n"
+			        "       movq 32(%1), %%mm4   ;\n"
+			        "       movq 40(%1), %%mm5   ;\n"
+			        "       pxor 24(%2), %%mm3   ;\n"
+			        "       movq %%mm2, 16(%1)   ;\n"
+			        "       pxor 32(%2), %%mm4   ;\n"
+			        "       pxor 24(%3), %%mm3   ;\n"
+			        "       pxor 40(%2), %%mm5   ;\n"
+			        "       movq %%mm3, 24(%1)   ;\n"
+			        "       pxor 32(%3), %%mm4   ;\n"
+			        "       pxor 40(%3), %%mm5   ;\n"
+			        "       movq 48(%1), %%mm6   ;\n"
+			        "       movq %%mm4, 32(%1)   ;\n"
+			        "       movq 56(%1), %%mm7   ;\n"
+			        "       pxor 48(%2), %%mm6   ;\n"
+			        "       movq %%mm5, 40(%1)   ;\n"
+			        "       pxor 56(%2), %%mm7   ;\n"
+			        "       pxor 48(%3), %%mm6   ;\n"
+			        "       pxor 56(%3), %%mm7   ;\n"
+			        "       movq %%mm6, 48(%1)   ;\n"
+			        "       movq %%mm7, 56(%1)   ;\n"
+        
+			        "       addl $64, %1         ;\n"
+			        "       addl $64, %2         ;\n"
+			        "       addl $64, %3         ;\n"
+			        "       decl %0              ;\n"
+			        "       jnz 1b               ;\n"
+
+			        : 
+			        : "r" (lines),
+				  "r" (bh_ptr[0]->b_data),
+				  "r" (bh_ptr[1]->b_data),
+				  "r" (bh_ptr[2]->b_data)
+			        : "memory" );
+			break;
+		case 4:
+			__asm__ __volatile__ (
+
+			        " .align 32,0x90             ;\n"
+			        " 1:                         ;\n"
+			        "       movq   (%1), %%mm0   ;\n"
+			        "       movq  8(%1), %%mm1   ;\n"
+			        "       pxor   (%2), %%mm0   ;\n"
+			        "       movq 16(%1), %%mm2   ;\n"
+			        "       pxor  8(%2), %%mm1   ;\n"
+			        "       pxor   (%3), %%mm0   ;\n"
+			        "       pxor 16(%2), %%mm2   ;\n"
+			        "       pxor  8(%3), %%mm1   ;\n"
+			        "       pxor   (%4), %%mm0   ;\n"
+			        "       movq 24(%1), %%mm3   ;\n"
+			        "       pxor 16(%3), %%mm2   ;\n"
+			        "       pxor  8(%4), %%mm1   ;\n"
+			        "       movq %%mm0,   (%1)   ;\n"
+			        "       movq 32(%1), %%mm4   ;\n"
+			        "       pxor 24(%2), %%mm3   ;\n"
+			        "       pxor 16(%4), %%mm2   ;\n"
+			        "       movq %%mm1,  8(%1)   ;\n"
+			        "       movq 40(%1), %%mm5   ;\n"
+			        "       pxor 32(%2), %%mm4   ;\n"
+			        "       pxor 24(%3), %%mm3   ;\n"
+			        "       movq %%mm2, 16(%1)   ;\n"
+			        "       pxor 40(%2), %%mm5   ;\n"
+			        "       pxor 32(%3), %%mm4   ;\n"
+			        "       pxor 24(%4), %%mm3   ;\n"
+			        "       movq %%mm3, 24(%1)   ;\n"
+			        "       movq 56(%1), %%mm7   ;\n"
+			        "       movq 48(%1), %%mm6   ;\n"
+			        "       pxor 40(%3), %%mm5   ;\n"
+			        "       pxor 32(%4), %%mm4   ;\n"
+			        "       pxor 48(%2), %%mm6   ;\n"
+			        "       movq %%mm4, 32(%1)   ;\n"
+			        "       pxor 56(%2), %%mm7   ;\n"
+			        "       pxor 40(%4), %%mm5   ;\n"
+			        "       pxor 48(%3), %%mm6   ;\n"
+			        "       pxor 56(%3), %%mm7   ;\n"
+			        "       movq %%mm5, 40(%1)   ;\n"
+			        "       pxor 48(%4), %%mm6   ;\n"
+			        "       pxor 56(%4), %%mm7   ;\n"
+			        "       movq %%mm6, 48(%1)   ;\n"
+			        "       movq %%mm7, 56(%1)   ;\n"
+        
+			        "       addl $64, %1         ;\n"
+			        "       addl $64, %2         ;\n"
+			        "       addl $64, %3         ;\n"
+			        "       addl $64, %4         ;\n"
+			        "       decl %0              ;\n"
+			        "       jnz 1b               ;\n"
+
+			        : 
+			        : "r" (lines),
+				  "r" (bh_ptr[0]->b_data),
+				  "r" (bh_ptr[1]->b_data),
+				  "r" (bh_ptr[2]->b_data),
+				  "r" (bh_ptr[3]->b_data)
+			        : "memory" );
+			break;
+		case 5:
+			__asm__ __volatile__ (
+
+			        " .align 32,0x90             ;\n"
+			        " 1:                         ;\n"
+			        "       movq   (%1), %%mm0   ;\n"
+			        "       movq  8(%1), %%mm1   ;\n"
+			        "       pxor   (%2), %%mm0   ;\n"
+			        "       pxor  8(%2), %%mm1   ;\n"
+			        "       movq 16(%1), %%mm2   ;\n"
+			        "       pxor   (%3), %%mm0   ;\n"
+			        "       pxor  8(%3), %%mm1   ;\n"
+			        "       pxor 16(%2), %%mm2   ;\n"
+			        "       pxor   (%4), %%mm0   ;\n"
+			        "       pxor  8(%4), %%mm1   ;\n"
+			        "       pxor 16(%3), %%mm2   ;\n"
+			        "       movq 24(%1), %%mm3   ;\n"
+			        "       pxor   (%5), %%mm0   ;\n"
+			        "       pxor  8(%5), %%mm1   ;\n"
+			        "       movq %%mm0,   (%1)   ;\n"
+			        "       pxor 16(%4), %%mm2   ;\n"
+			        "       pxor 24(%2), %%mm3   ;\n"
+			        "       movq %%mm1,  8(%1)   ;\n"
+			        "       pxor 16(%5), %%mm2   ;\n"
+			        "       pxor 24(%3), %%mm3   ;\n"
+			        "       movq 32(%1), %%mm4   ;\n"
+			        "       movq %%mm2, 16(%1)   ;\n"
+			        "       pxor 24(%4), %%mm3   ;\n"
+			        "       pxor 32(%2), %%mm4   ;\n"
+			        "       movq 40(%1), %%mm5   ;\n"
+			        "       pxor 24(%5), %%mm3   ;\n"
+			        "       pxor 32(%3), %%mm4   ;\n"
+			        "       pxor 40(%2), %%mm5   ;\n"
+			        "       movq %%mm3, 24(%1)   ;\n"
+			        "       pxor 32(%4), %%mm4   ;\n"
+			        "       pxor 40(%3), %%mm5   ;\n"
+			        "       movq 48(%1), %%mm6   ;\n"
+			        "       movq 56(%1), %%mm7   ;\n"
+			        "       pxor 32(%5), %%mm4   ;\n"
+			        "       pxor 40(%4), %%mm5   ;\n"
+			        "       pxor 48(%2), %%mm6   ;\n"
+			        "       pxor 56(%2), %%mm7   ;\n"
+			        "       movq %%mm4, 32(%1)   ;\n"
+			        "       pxor 48(%3), %%mm6   ;\n"
+			        "       pxor 56(%3), %%mm7   ;\n"
+			        "       pxor 40(%5), %%mm5   ;\n"
+			        "       pxor 48(%4), %%mm6   ;\n"
+			        "       pxor 56(%4), %%mm7   ;\n"
+			        "       movq %%mm5, 40(%1)   ;\n"
+			        "       pxor 48(%5), %%mm6   ;\n"
+			        "       pxor 56(%5), %%mm7   ;\n"
+			        "       movq %%mm6, 48(%1)   ;\n"
+			        "       movq %%mm7, 56(%1)   ;\n"
+        
+			        "       addl $64, %1         ;\n"
+			        "       addl $64, %2         ;\n"
+			        "       addl $64, %3         ;\n"
+			        "       addl $64, %4         ;\n"
+			        "       addl $64, %5         ;\n"
+			        "       decl %0              ;\n"
+			        "       jnz 1b               ;\n"
+
+			        : 
+			        : "r" (lines),
+				  "r" (bh_ptr[0]->b_data),
+				  "r" (bh_ptr[1]->b_data),
+				  "r" (bh_ptr[2]->b_data),
+				  "r" (bh_ptr[3]->b_data),
+				  "r" (bh_ptr[4]->b_data)
+			        : "memory" );
+			break;
+	}
+
+	__asm__ __volatile__ ( " frstor %0;\n"::"m"(fpu_save[0]) );
+
+	if (!(current->flags & PF_USEDFPU))
+		stts();
+}
+#endif /* __i386__ */
+#endif /* !__sparc_v9__ */
+
+#ifdef __sparc_v9__
+/*
+ * High speed xor_block operation for RAID4/5 utilizing the
+ * UltraSparc Visual Instruction Set.
+ *
+ * Copyright (C) 1997, 1999 Jakub Jelinek (jj@ultra.linux.cz)
+ *
+ *	Requirements:
+ *	!(((long)dest | (long)sourceN) & (64 - 1)) &&
+ *	!(len & 127) && len >= 256
+ *
+ * It is done in pure assembly, as otherwise gcc makes it
+ * a non-leaf function, which is not what we want.
+ * Also, we don't measure the speeds as on other architectures,
+ * as the measuring routine does not take into account cold caches
+ * and the fact that xor_block_VIS bypasses the caches.
+ * xor_block_32regs might be 5% faster for count 2 if caches are hot
+ * and things just right (for count 3 VIS is about as fast as 32regs for
+ * hot caches and for count 4 and 5 VIS is faster by good margin always),
+ * but I think it is better not to pollute the caches.
+ * Actually, if I'd just fight for speed for hot caches, I could
+ * write a hybrid VIS/integer routine, which would do always two
+ * 64B blocks in VIS and two in IEUs, but I really care more about
+ * caches.
+ */
+extern void *VISenter(void);
+extern void xor_block_VIS XOR_ARGS;
+
+void __xor_block_VIS(void)
+{
+__asm__ ("
+	.globl xor_block_VIS
+xor_block_VIS:
+	ldx	[%%o1 + 0], %%o4
+	ldx	[%%o1 + 8], %%o3
+	ldx	[%%o4 + %1], %%g5
+	ldx	[%%o4 + %0], %%o4
+	ldx	[%%o3 + %0], %%o3
+	rd	%%fprs, %%o5
+	andcc	%%o5, %2, %%g0
+	be,pt	%%icc, 297f
+	 sethi	%%hi(%5), %%g1
+	jmpl	%%g1 + %%lo(%5), %%g7
+	 add	%%g7, 8, %%g7
+297:	wr	%%g0, %4, %%fprs
+	membar	#LoadStore|#StoreLoad|#StoreStore
+	sub	%%g5, 64, %%g5
+	ldda	[%%o4] %3, %%f0
+	ldda	[%%o3] %3, %%f16
+	cmp	%%o0, 4
+	bgeu,pt	%%xcc, 10f
+	 cmp	%%o0, 3
+	be,pn	%%xcc, 13f
+	 mov	-64, %%g1
+	sub	%%g5, 64, %%g5
+	rd	%%asi, %%g1
+	wr	%%g0, %3, %%asi
+
+2:	ldda	[%%o4 + 64] %%asi, %%f32
+	fxor	%%f0, %%f16, %%f16
+	fxor	%%f2, %%f18, %%f18
+	fxor	%%f4, %%f20, %%f20
+	fxor	%%f6, %%f22, %%f22
+	fxor	%%f8, %%f24, %%f24
+	fxor	%%f10, %%f26, %%f26
+	fxor	%%f12, %%f28, %%f28
+	fxor	%%f14, %%f30, %%f30
+	stda	%%f16, [%%o4] %3
+	ldda	[%%o3 + 64] %%asi, %%f48
+	ldda	[%%o4 + 128] %%asi, %%f0
+	fxor	%%f32, %%f48, %%f48
+	fxor	%%f34, %%f50, %%f50
+	add	%%o4, 128, %%o4
+	fxor	%%f36, %%f52, %%f52
+	add	%%o3, 128, %%o3
+	fxor	%%f38, %%f54, %%f54
+	subcc	%%g5, 128, %%g5
+	fxor	%%f40, %%f56, %%f56
+	fxor	%%f42, %%f58, %%f58
+	fxor	%%f44, %%f60, %%f60
+	fxor	%%f46, %%f62, %%f62
+	stda	%%f48, [%%o4 - 64] %%asi
+	bne,pt	%%xcc, 2b
+	 ldda	[%%o3] %3, %%f16
+
+	ldda	[%%o4 + 64] %%asi, %%f32
+	fxor	%%f0, %%f16, %%f16
+	fxor	%%f2, %%f18, %%f18
+	fxor	%%f4, %%f20, %%f20
+	fxor	%%f6, %%f22, %%f22
+	fxor	%%f8, %%f24, %%f24
+	fxor	%%f10, %%f26, %%f26
+	fxor	%%f12, %%f28, %%f28
+	fxor	%%f14, %%f30, %%f30
+	stda	%%f16, [%%o4] %3
+	ldda	[%%o3 + 64] %%asi, %%f48
+	membar	#Sync
+	fxor	%%f32, %%f48, %%f48
+	fxor	%%f34, %%f50, %%f50
+	fxor	%%f36, %%f52, %%f52
+	fxor	%%f38, %%f54, %%f54
+	fxor	%%f40, %%f56, %%f56
+	fxor	%%f42, %%f58, %%f58
+	fxor	%%f44, %%f60, %%f60
+	fxor	%%f46, %%f62, %%f62
+	stda	%%f48, [%%o4 + 64] %%asi
+	membar	#Sync|#StoreStore|#StoreLoad
+	wr	%%g0, 0, %%fprs
+	retl
+	 wr	%%g1, %%g0, %%asi
+
+13:	ldx	[%%o1 + 16], %%o2
+	ldx	[%%o2 + %0], %%o2
+
+3:	ldda	[%%o2] %3, %%f32
+	fxor	%%f0, %%f16, %%f48
+	fxor	%%f2, %%f18, %%f50
+	add	%%o4, 64, %%o4
+	fxor	%%f4, %%f20, %%f52
+	fxor	%%f6, %%f22, %%f54
+	add	%%o3, 64, %%o3
+	fxor	%%f8, %%f24, %%f56
+	fxor	%%f10, %%f26, %%f58
+	fxor	%%f12, %%f28, %%f60
+	fxor	%%f14, %%f30, %%f62
+	ldda	[%%o4] %3, %%f0
+	fxor	%%f48, %%f32, %%f48
+	fxor	%%f50, %%f34, %%f50
+	fxor	%%f52, %%f36, %%f52
+	fxor	%%f54, %%f38, %%f54
+	add	%%o2, 64, %%o2
+	fxor	%%f56, %%f40, %%f56
+	fxor	%%f58, %%f42, %%f58
+	subcc	%%g5, 64, %%g5
+	fxor	%%f60, %%f44, %%f60
+	fxor	%%f62, %%f46, %%f62
+	stda	%%f48, [%%o4 + %%g1] %3
+	bne,pt	%%xcc, 3b
+	 ldda	[%%o3] %3, %%f16
+
+	ldda	[%%o2] %3, %%f32
+	fxor	%%f0, %%f16, %%f48
+	fxor	%%f2, %%f18, %%f50
+	fxor	%%f4, %%f20, %%f52
+	fxor	%%f6, %%f22, %%f54
+	fxor	%%f8, %%f24, %%f56
+	fxor	%%f10, %%f26, %%f58
+	fxor	%%f12, %%f28, %%f60
+	fxor	%%f14, %%f30, %%f62
+	membar	#Sync
+	fxor	%%f48, %%f32, %%f48
+	fxor	%%f50, %%f34, %%f50
+	fxor	%%f52, %%f36, %%f52
+	fxor	%%f54, %%f38, %%f54
+	fxor	%%f56, %%f40, %%f56
+	fxor	%%f58, %%f42, %%f58
+	fxor	%%f60, %%f44, %%f60
+	fxor	%%f62, %%f46, %%f62
+	stda	%%f48, [%%o4] %3
+	membar	#Sync|#StoreStore|#StoreLoad
+	retl
+	 wr	%%g0, 0, %%fprs
+
+10:	cmp	%%o0, 5
+	be,pt	%%xcc, 15f
+	 mov	-64, %%g1
+
+14:	ldx	[%%o1 + 16], %%o2
+	ldx	[%%o1 + 24], %%o0
+	ldx	[%%o2 + %0], %%o2
+	ldx	[%%o0 + %0], %%o0
+
+4:	ldda	[%%o2] %3, %%f32
+	fxor	%%f0, %%f16, %%f16
+	fxor	%%f2, %%f18, %%f18
+	add	%%o4, 64, %%o4
+	fxor	%%f4, %%f20, %%f20
+	fxor	%%f6, %%f22, %%f22
+	add	%%o3, 64, %%o3
+	fxor	%%f8, %%f24, %%f24
+	fxor	%%f10, %%f26, %%f26
+	fxor	%%f12, %%f28, %%f28
+	fxor	%%f14, %%f30, %%f30
+	ldda	[%%o0] %3, %%f48
+	fxor	%%f16, %%f32, %%f32
+	fxor	%%f18, %%f34, %%f34
+	fxor	%%f20, %%f36, %%f36
+	fxor	%%f22, %%f38, %%f38
+	add	%%o2, 64, %%o2
+	fxor	%%f24, %%f40, %%f40
+	fxor	%%f26, %%f42, %%f42
+	fxor	%%f28, %%f44, %%f44
+	fxor	%%f30, %%f46, %%f46
+	ldda	[%%o4] %3, %%f0
+	fxor	%%f32, %%f48, %%f48
+	fxor	%%f34, %%f50, %%f50
+	fxor	%%f36, %%f52, %%f52
+	add	%%o0, 64, %%o0
+	fxor	%%f38, %%f54, %%f54
+	fxor	%%f40, %%f56, %%f56
+	fxor	%%f42, %%f58, %%f58
+	subcc	%%g5, 64, %%g5
+	fxor	%%f44, %%f60, %%f60
+	fxor	%%f46, %%f62, %%f62
+	stda	%%f48, [%%o4 + %%g1] %3
+	bne,pt	%%xcc, 4b
+	 ldda	[%%o3] %3, %%f16
+
+	ldda	[%%o2] %3, %%f32
+	fxor	%%f0, %%f16, %%f16
+	fxor	%%f2, %%f18, %%f18
+	fxor	%%f4, %%f20, %%f20
+	fxor	%%f6, %%f22, %%f22
+	fxor	%%f8, %%f24, %%f24
+	fxor	%%f10, %%f26, %%f26
+	fxor	%%f12, %%f28, %%f28
+	fxor	%%f14, %%f30, %%f30
+	ldda	[%%o0] %3, %%f48
+	fxor	%%f16, %%f32, %%f32
+	fxor	%%f18, %%f34, %%f34
+	fxor	%%f20, %%f36, %%f36
+	fxor	%%f22, %%f38, %%f38
+	fxor	%%f24, %%f40, %%f40
+	fxor	%%f26, %%f42, %%f42
+	fxor	%%f28, %%f44, %%f44
+	fxor	%%f30, %%f46, %%f46
+	membar	#Sync
+	fxor	%%f32, %%f48, %%f48
+	fxor	%%f34, %%f50, %%f50
+	fxor	%%f36, %%f52, %%f52
+	fxor	%%f38, %%f54, %%f54
+	fxor	%%f40, %%f56, %%f56
+	fxor	%%f42, %%f58, %%f58
+	fxor	%%f44, %%f60, %%f60
+	fxor	%%f46, %%f62, %%f62
+	stda	%%f48, [%%o4] %3
+	membar	#Sync|#StoreStore|#StoreLoad
+	retl
+	 wr	%%g0, 0, %%fprs
+
+15:	ldx	[%%o1 + 16], %%o2
+	ldx	[%%o1 + 24], %%o0
+	ldx	[%%o1 + 32], %%o1
+	ldx	[%%o2 + %0], %%o2
+	ldx	[%%o0 + %0], %%o0
+	ldx	[%%o1 + %0], %%o1
+
+5:	ldda	[%%o2] %3, %%f32
+	fxor	%%f0, %%f16, %%f48
+	fxor	%%f2, %%f18, %%f50
+	add	%%o4, 64, %%o4
+	fxor	%%f4, %%f20, %%f52
+	fxor	%%f6, %%f22, %%f54
+	add	%%o3, 64, %%o3
+	fxor	%%f8, %%f24, %%f56
+	fxor	%%f10, %%f26, %%f58
+	fxor	%%f12, %%f28, %%f60
+	fxor	%%f14, %%f30, %%f62
+	ldda	[%%o0] %3, %%f16
+	fxor	%%f48, %%f32, %%f48
+	fxor	%%f50, %%f34, %%f50
+	fxor	%%f52, %%f36, %%f52
+	fxor	%%f54, %%f38, %%f54
+	add	%%o2, 64, %%o2
+	fxor	%%f56, %%f40, %%f56
+	fxor	%%f58, %%f42, %%f58
+	fxor	%%f60, %%f44, %%f60
+	fxor	%%f62, %%f46, %%f62
+	ldda	[%%o1] %3, %%f32
+	fxor	%%f48, %%f16, %%f48
+	fxor	%%f50, %%f18, %%f50
+	add	%%o0, 64, %%o0
+	fxor	%%f52, %%f20, %%f52
+	fxor	%%f54, %%f22, %%f54
+	add	%%o1, 64, %%o1
+	fxor	%%f56, %%f24, %%f56
+	fxor	%%f58, %%f26, %%f58
+	fxor	%%f60, %%f28, %%f60
+	fxor	%%f62, %%f30, %%f62
+	ldda	[%%o4] %3, %%f0
+	fxor	%%f48, %%f32, %%f48
+	fxor	%%f50, %%f34, %%f50
+	fxor	%%f52, %%f36, %%f52
+	fxor	%%f54, %%f38, %%f54
+	fxor	%%f56, %%f40, %%f56
+	fxor	%%f58, %%f42, %%f58
+	subcc	%%g5, 64, %%g5
+	fxor	%%f60, %%f44, %%f60
+	fxor	%%f62, %%f46, %%f62
+	stda	%%f48, [%%o4 + %%g1] %3
+	bne,pt	%%xcc, 5b
+	 ldda	[%%o3] %3, %%f16
+
+	ldda	[%%o2] %3, %%f32
+	fxor	%%f0, %%f16, %%f48
+	fxor	%%f2, %%f18, %%f50
+	fxor	%%f4, %%f20, %%f52
+	fxor	%%f6, %%f22, %%f54
+	fxor	%%f8, %%f24, %%f56
+	fxor	%%f10, %%f26, %%f58
+	fxor	%%f12, %%f28, %%f60
+	fxor	%%f14, %%f30, %%f62
+	ldda	[%%o0] %3, %%f16
+	fxor	%%f48, %%f32, %%f48
+	fxor	%%f50, %%f34, %%f50
+	fxor	%%f52, %%f36, %%f52
+	fxor	%%f54, %%f38, %%f54
+	fxor	%%f56, %%f40, %%f56
+	fxor	%%f58, %%f42, %%f58
+	fxor	%%f60, %%f44, %%f60
+	fxor	%%f62, %%f46, %%f62
+	ldda	[%%o1] %3, %%f32
+	fxor	%%f48, %%f16, %%f48
+	fxor	%%f50, %%f18, %%f50
+	fxor	%%f52, %%f20, %%f52
+	fxor	%%f54, %%f22, %%f54
+	fxor	%%f56, %%f24, %%f56
+	fxor	%%f58, %%f26, %%f58
+	fxor	%%f60, %%f28, %%f60
+	fxor	%%f62, %%f30, %%f62
+	membar	#Sync
+	fxor	%%f48, %%f32, %%f48
+	fxor	%%f50, %%f34, %%f50
+	fxor	%%f52, %%f36, %%f52
+	fxor	%%f54, %%f38, %%f54
+	fxor	%%f56, %%f40, %%f56
+	fxor	%%f58, %%f42, %%f58
+	fxor	%%f60, %%f44, %%f60
+	fxor	%%f62, %%f46, %%f62
+	stda	%%f48, [%%o4] %3
+	membar	#Sync|#StoreStore|#StoreLoad
+	retl
+	 wr	%%g0, 0, %%fprs
+	" : :
+	"i" (&((struct buffer_head *)0)->b_data),
+	"i" (&((struct buffer_head *)0)->b_size),
+	"i" (FPRS_FEF|FPRS_DU), "i" (ASI_BLK_P),
+	"i" (FPRS_FEF), "i" (VISenter));
+}
+#endif /* __sparc_v9__ */
+
+#if defined(__sparc__) && !defined(__sparc_v9__)
+/*
+ * High speed xor_block operation for RAID4/5 utilizing the
+ * ldd/std SPARC instructions.
+ *
+ * Copyright (C) 1999 Jakub Jelinek (jj@ultra.linux.cz)
+ *
+ */
+
+XORBLOCK_TEMPLATE(SPARC)
+{
+	int size  = bh_ptr[0]->b_size;
+	int lines = size / (sizeof (long)) / 8, i;
+	long *destp   = (long *) bh_ptr[0]->b_data;
+	long *source1 = (long *) bh_ptr[1]->b_data;
+	long *source2, *source3, *source4;
+
+	switch (count) {
+	case 2:
+		for (i = lines; i > 0; i--) {
+		  __asm__ __volatile__("
+		  ldd [%0 + 0x00], %%g2
+		  ldd [%0 + 0x08], %%g4
+		  ldd [%0 + 0x10], %%o0
+		  ldd [%0 + 0x18], %%o2
+		  ldd [%1 + 0x00], %%o4
+		  ldd [%1 + 0x08], %%l0
+		  ldd [%1 + 0x10], %%l2
+		  ldd [%1 + 0x18], %%l4
+		  xor %%g2, %%o4, %%g2
+		  xor %%g3, %%o5, %%g3
+		  xor %%g4, %%l0, %%g4
+		  xor %%g5, %%l1, %%g5
+		  xor %%o0, %%l2, %%o0
+		  xor %%o1, %%l3, %%o1
+		  xor %%o2, %%l4, %%o2
+		  xor %%o3, %%l5, %%o3
+		  std %%g2, [%0 + 0x00]
+		  std %%g4, [%0 + 0x08]
+		  std %%o0, [%0 + 0x10]
+		  std %%o2, [%0 + 0x18]
+		  " : : "r" (destp), "r" (source1) : "g2", "g3", "g4", "g5", "o0", 
+		  "o1", "o2", "o3", "o4", "o5", "l0", "l1", "l2", "l3", "l4", "l5");
+		  destp += 8;
+		  source1 += 8;
+		}
+		break;
+	case 3:
+		source2 = (long *) bh_ptr[2]->b_data;
+		for (i = lines; i > 0; i--) {
+		  __asm__ __volatile__("
+		  ldd [%0 + 0x00], %%g2
+		  ldd [%0 + 0x08], %%g4
+		  ldd [%0 + 0x10], %%o0
+		  ldd [%0 + 0x18], %%o2
+		  ldd [%1 + 0x00], %%o4
+		  ldd [%1 + 0x08], %%l0
+		  ldd [%1 + 0x10], %%l2
+		  ldd [%1 + 0x18], %%l4
+		  xor %%g2, %%o4, %%g2
+		  xor %%g3, %%o5, %%g3
+		  ldd [%2 + 0x00], %%o4
+		  xor %%g4, %%l0, %%g4
+		  xor %%g5, %%l1, %%g5
+		  ldd [%2 + 0x08], %%l0
+		  xor %%o0, %%l2, %%o0
+		  xor %%o1, %%l3, %%o1
+		  ldd [%2 + 0x10], %%l2
+		  xor %%o2, %%l4, %%o2
+		  xor %%o3, %%l5, %%o3
+		  ldd [%2 + 0x18], %%l4
+		  xor %%g2, %%o4, %%g2
+		  xor %%g3, %%o5, %%g3
+		  xor %%g4, %%l0, %%g4
+		  xor %%g5, %%l1, %%g5
+		  xor %%o0, %%l2, %%o0
+		  xor %%o1, %%l3, %%o1
+		  xor %%o2, %%l4, %%o2
+		  xor %%o3, %%l5, %%o3
+		  std %%g2, [%0 + 0x00]
+		  std %%g4, [%0 + 0x08]
+		  std %%o0, [%0 + 0x10]
+		  std %%o2, [%0 + 0x18]
+		  " : : "r" (destp), "r" (source1), "r" (source2)
+		  : "g2", "g3", "g4", "g5", "o0", "o1", "o2", "o3", "o4", "o5",
+		  "l0", "l1", "l2", "l3", "l4", "l5");
+		  destp += 8;
+		  source1 += 8;
+		  source2 += 8;
+		}
+		break;
+	case 4:
+		source2 = (long *) bh_ptr[2]->b_data;
+		source3 = (long *) bh_ptr[3]->b_data;
+		for (i = lines; i > 0; i--) {
+		  __asm__ __volatile__("
+		  ldd [%0 + 0x00], %%g2
+		  ldd [%0 + 0x08], %%g4
+		  ldd [%0 + 0x10], %%o0
+		  ldd [%0 + 0x18], %%o2
+		  ldd [%1 + 0x00], %%o4
+		  ldd [%1 + 0x08], %%l0
+		  ldd [%1 + 0x10], %%l2
+		  ldd [%1 + 0x18], %%l4
+		  xor %%g2, %%o4, %%g2
+		  xor %%g3, %%o5, %%g3
+		  ldd [%2 + 0x00], %%o4
+		  xor %%g4, %%l0, %%g4
+		  xor %%g5, %%l1, %%g5
+		  ldd [%2 + 0x08], %%l0
+		  xor %%o0, %%l2, %%o0
+		  xor %%o1, %%l3, %%o1
+		  ldd [%2 + 0x10], %%l2
+		  xor %%o2, %%l4, %%o2
+		  xor %%o3, %%l5, %%o3
+		  ldd [%2 + 0x18], %%l4
+		  xor %%g2, %%o4, %%g2
+		  xor %%g3, %%o5, %%g3
+		  ldd [%3 + 0x00], %%o4
+		  xor %%g4, %%l0, %%g4
+		  xor %%g5, %%l1, %%g5
+		  ldd [%3 + 0x08], %%l0
+		  xor %%o0, %%l2, %%o0
+		  xor %%o1, %%l3, %%o1
+		  ldd [%3 + 0x10], %%l2
+		  xor %%o2, %%l4, %%o2
+		  xor %%o3, %%l5, %%o3
+		  ldd [%3 + 0x18], %%l4
+		  xor %%g2, %%o4, %%g2
+		  xor %%g3, %%o5, %%g3
+		  xor %%g4, %%l0, %%g4
+		  xor %%g5, %%l1, %%g5
+		  xor %%o0, %%l2, %%o0
+		  xor %%o1, %%l3, %%o1
+		  xor %%o2, %%l4, %%o2
+		  xor %%o3, %%l5, %%o3
+		  std %%g2, [%0 + 0x00]
+		  std %%g4, [%0 + 0x08]
+		  std %%o0, [%0 + 0x10]
+		  std %%o2, [%0 + 0x18]
+		  " : : "r" (destp), "r" (source1), "r" (source2), "r" (source3)
+		  : "g2", "g3", "g4", "g5", "o0", "o1", "o2", "o3", "o4", "o5",
+		  "l0", "l1", "l2", "l3", "l4", "l5");
+		  destp += 8;
+		  source1 += 8;
+		  source2 += 8;
+		  source3 += 8;
+		}
+		break;
+	case 5:
+		source2 = (long *) bh_ptr[2]->b_data;
+		source3 = (long *) bh_ptr[3]->b_data;
+		source4 = (long *) bh_ptr[4]->b_data;
+		for (i = lines; i > 0; i--) {
+		  __asm__ __volatile__("
+		  ldd [%0 + 0x00], %%g2
+		  ldd [%0 + 0x08], %%g4
+		  ldd [%0 + 0x10], %%o0
+		  ldd [%0 + 0x18], %%o2
+		  ldd [%1 + 0x00], %%o4
+		  ldd [%1 + 0x08], %%l0
+		  ldd [%1 + 0x10], %%l2
+		  ldd [%1 + 0x18], %%l4
+		  xor %%g2, %%o4, %%g2
+		  xor %%g3, %%o5, %%g3
+		  ldd [%2 + 0x00], %%o4
+		  xor %%g4, %%l0, %%g4
+		  xor %%g5, %%l1, %%g5
+		  ldd [%2 + 0x08], %%l0
+		  xor %%o0, %%l2, %%o0
+		  xor %%o1, %%l3, %%o1
+		  ldd [%2 + 0x10], %%l2
+		  xor %%o2, %%l4, %%o2
+		  xor %%o3, %%l5, %%o3
+		  ldd [%2 + 0x18], %%l4
+		  xor %%g2, %%o4, %%g2
+		  xor %%g3, %%o5, %%g3
+		  ldd [%3 + 0x00], %%o4
+		  xor %%g4, %%l0, %%g4
+		  xor %%g5, %%l1, %%g5
+		  ldd [%3 + 0x08], %%l0
+		  xor %%o0, %%l2, %%o0
+		  xor %%o1, %%l3, %%o1
+		  ldd [%3 + 0x10], %%l2
+		  xor %%o2, %%l4, %%o2
+		  xor %%o3, %%l5, %%o3
+		  ldd [%3 + 0x18], %%l4
+		  xor %%g2, %%o4, %%g2
+		  xor %%g3, %%o5, %%g3
+		  ldd [%4 + 0x00], %%o4
+		  xor %%g4, %%l0, %%g4
+		  xor %%g5, %%l1, %%g5
+		  ldd [%4 + 0x08], %%l0
+		  xor %%o0, %%l2, %%o0
+		  xor %%o1, %%l3, %%o1
+		  ldd [%4 + 0x10], %%l2
+		  xor %%o2, %%l4, %%o2
+		  xor %%o3, %%l5, %%o3
+		  ldd [%4 + 0x18], %%l4
+		  xor %%g2, %%o4, %%g2
+		  xor %%g3, %%o5, %%g3
+		  xor %%g4, %%l0, %%g4
+		  xor %%g5, %%l1, %%g5
+		  xor %%o0, %%l2, %%o0
+		  xor %%o1, %%l3, %%o1
+		  xor %%o2, %%l4, %%o2
+		  xor %%o3, %%l5, %%o3
+		  std %%g2, [%0 + 0x00]
+		  std %%g4, [%0 + 0x08]
+		  std %%o0, [%0 + 0x10]
+		  std %%o2, [%0 + 0x18]
+		  " : : "r" (destp), "r" (source1), "r" (source2), "r" (source3), "r" (source4)
+		  : "g2", "g3", "g4", "g5", "o0", "o1", "o2", "o3", "o4", "o5",
+		  "l0", "l1", "l2", "l3", "l4", "l5");
+		  destp += 8;
+		  source1 += 8;
+		  source2 += 8;
+		  source3 += 8;
+		  source4 += 8;
+		}
+		break;
+	}
+}
+#endif /* __sparc_v[78]__ */
+
+#ifndef __sparc_v9__
+
+/*
+ * this one works reasonably on any x86 CPU
+ * (send me an assembly version for inclusion if you can make it faster)
+ *
+ * this one is just as fast as written in pure assembly on x86.
+ * the reason for this separate version is that the
+ * fast open-coded xor routine "32reg" produces suboptimal code
+ * on x86, due to lack of registers.
+ */
+XORBLOCK_TEMPLATE(8regs)
+{
+	int len  = bh_ptr[0]->b_size;
+	long *destp   = (long *) bh_ptr[0]->b_data;
+	long *source1, *source2, *source3, *source4;
+	long lines = len / (sizeof (long)) / 8, i;
+
+	switch(count) {
+		case 2:
+			source1 = (long *) bh_ptr[1]->b_data;
+			for (i = lines; i > 0; i--) {
+				*(destp + 0) ^= *(source1 + 0);
+				*(destp + 1) ^= *(source1 + 1);
+				*(destp + 2) ^= *(source1 + 2);
+				*(destp + 3) ^= *(source1 + 3);
+				*(destp + 4) ^= *(source1 + 4);
+				*(destp + 5) ^= *(source1 + 5);
+				*(destp + 6) ^= *(source1 + 6);
+				*(destp + 7) ^= *(source1 + 7);
+				source1 += 8;
+				destp += 8;
+			}
+			break;
+		case 3:
+			source2 = (long *) bh_ptr[2]->b_data;
+			source1 = (long *) bh_ptr[1]->b_data;
+			for (i = lines; i > 0; i--) {
+				*(destp + 0) ^= *(source1 + 0);
+				*(destp + 0) ^= *(source2 + 0);
+				*(destp + 1) ^= *(source1 + 1);
+				*(destp + 1) ^= *(source2 + 1);
+				*(destp + 2) ^= *(source1 + 2);
+				*(destp + 2) ^= *(source2 + 2);
+				*(destp + 3) ^= *(source1 + 3);
+				*(destp + 3) ^= *(source2 + 3);
+				*(destp + 4) ^= *(source1 + 4);
+				*(destp + 4) ^= *(source2 + 4);
+				*(destp + 5) ^= *(source1 + 5);
+				*(destp + 5) ^= *(source2 + 5);
+				*(destp + 6) ^= *(source1 + 6);
+				*(destp + 6) ^= *(source2 + 6);
+				*(destp + 7) ^= *(source1 + 7);
+				*(destp + 7) ^= *(source2 + 7);
+				source1 += 8;
+				source2 += 8;
+				destp += 8;
+			}
+			break;
+		case 4:
+			source3 = (long *) bh_ptr[3]->b_data;
+			source2 = (long *) bh_ptr[2]->b_data;
+			source1 = (long *) bh_ptr[1]->b_data;
+			for (i = lines; i > 0; i--) {
+				*(destp + 0) ^= *(source1 + 0);
+				*(destp + 0) ^= *(source2 + 0);
+				*(destp + 0) ^= *(source3 + 0);
+				*(destp + 1) ^= *(source1 + 1);
+				*(destp + 1) ^= *(source2 + 1);
+				*(destp + 1) ^= *(source3 + 1);
+				*(destp + 2) ^= *(source1 + 2);
+				*(destp + 2) ^= *(source2 + 2);
+				*(destp + 2) ^= *(source3 + 2);
+				*(destp + 3) ^= *(source1 + 3);
+				*(destp + 3) ^= *(source2 + 3);
+				*(destp + 3) ^= *(source3 + 3);
+				*(destp + 4) ^= *(source1 + 4);
+				*(destp + 4) ^= *(source2 + 4);
+				*(destp + 4) ^= *(source3 + 4);
+				*(destp + 5) ^= *(source1 + 5);
+				*(destp + 5) ^= *(source2 + 5);
+				*(destp + 5) ^= *(source3 + 5);
+				*(destp + 6) ^= *(source1 + 6);
+				*(destp + 6) ^= *(source2 + 6);
+				*(destp + 6) ^= *(source3 + 6);
+				*(destp + 7) ^= *(source1 + 7);
+				*(destp + 7) ^= *(source2 + 7);
+				*(destp + 7) ^= *(source3 + 7);
+				source1 += 8;
+				source2 += 8;
+				source3 += 8;
+				destp += 8;
+			}
+			break;
+		case 5:
+			source4 = (long *) bh_ptr[4]->b_data;
+			source3 = (long *) bh_ptr[3]->b_data;
+			source2 = (long *) bh_ptr[2]->b_data;
+			source1 = (long *) bh_ptr[1]->b_data;
+			for (i = lines; i > 0; i--) {
+				*(destp + 0) ^= *(source1 + 0);
+				*(destp + 0) ^= *(source2 + 0);
+				*(destp + 0) ^= *(source3 + 0);
+				*(destp + 0) ^= *(source4 + 0);
+				*(destp + 1) ^= *(source1 + 1);
+				*(destp + 1) ^= *(source2 + 1);
+				*(destp + 1) ^= *(source3 + 1);
+				*(destp + 1) ^= *(source4 + 1);
+				*(destp + 2) ^= *(source1 + 2);
+				*(destp + 2) ^= *(source2 + 2);
+				*(destp + 2) ^= *(source3 + 2);
+				*(destp + 2) ^= *(source4 + 2);
+				*(destp + 3) ^= *(source1 + 3);
+				*(destp + 3) ^= *(source2 + 3);
+				*(destp + 3) ^= *(source3 + 3);
+				*(destp + 3) ^= *(source4 + 3);
+				*(destp + 4) ^= *(source1 + 4);
+				*(destp + 4) ^= *(source2 + 4);
+				*(destp + 4) ^= *(source3 + 4);
+				*(destp + 4) ^= *(source4 + 4);
+				*(destp + 5) ^= *(source1 + 5);
+				*(destp + 5) ^= *(source2 + 5);
+				*(destp + 5) ^= *(source3 + 5);
+				*(destp + 5) ^= *(source4 + 5);
+				*(destp + 6) ^= *(source1 + 6);
+				*(destp + 6) ^= *(source2 + 6);
+				*(destp + 6) ^= *(source3 + 6);
+				*(destp + 6) ^= *(source4 + 6);
+				*(destp + 7) ^= *(source1 + 7);
+				*(destp + 7) ^= *(source2 + 7);
+				*(destp + 7) ^= *(source3 + 7);
+				*(destp + 7) ^= *(source4 + 7);
+				source1 += 8;
+				source2 += 8;
+				source3 += 8;
+				source4 += 8;
+				destp += 8;
+			}
+			break;
+	}
+}
+
+/*
+ * platform independent RAID5 checksum calculation, this should
+ * be very fast on any platform that has a decent amount of
+ * registers. (32 or more)
+ */
+XORBLOCK_TEMPLATE(32regs)
+{
+	int size  = bh_ptr[0]->b_size;
+	int lines = size / (sizeof (long)) / 8, i;
+	long *destp   = (long *) bh_ptr[0]->b_data;
+	long *source1, *source2, *source3, *source4;
+	
+	  /* LOTS of registers available...
+	     We do explicite loop-unrolling here for code which
+	     favours RISC machines.  In fact this is almoast direct
+	     RISC assembly on Alpha and SPARC :-)  */
+
+
+	switch(count) {
+		case 2:
+			source1 = (long *) bh_ptr[1]->b_data;
+			for (i = lines; i > 0; i--) {
+	  			register long d0, d1, d2, d3, d4, d5, d6, d7;
+				d0 = destp[0];	/* Pull the stuff into registers	*/
+				d1 = destp[1];	/*  ... in bursts, if possible.		*/
+				d2 = destp[2];
+				d3 = destp[3];
+				d4 = destp[4];
+				d5 = destp[5];
+				d6 = destp[6];
+				d7 = destp[7];
+				d0 ^= source1[0];
+				d1 ^= source1[1];
+				d2 ^= source1[2];
+				d3 ^= source1[3];
+				d4 ^= source1[4];
+				d5 ^= source1[5];
+				d6 ^= source1[6];
+				d7 ^= source1[7];
+				destp[0] = d0;	/* Store the result (in burts)		*/
+				destp[1] = d1;
+				destp[2] = d2;
+				destp[3] = d3;
+				destp[4] = d4;	/* Store the result (in burts)		*/
+				destp[5] = d5;
+				destp[6] = d6;
+				destp[7] = d7;
+				source1 += 8;
+				destp += 8;
+			}
+			break;
+	  	case 3:
+			source2 = (long *) bh_ptr[2]->b_data;
+			source1 = (long *) bh_ptr[1]->b_data;
+			for (i = lines; i > 0; i--) {
+	  			register long d0, d1, d2, d3, d4, d5, d6, d7;
+				d0 = destp[0];	/* Pull the stuff into registers	*/
+				d1 = destp[1];	/*  ... in bursts, if possible.		*/
+				d2 = destp[2];
+				d3 = destp[3];
+				d4 = destp[4];
+				d5 = destp[5];
+				d6 = destp[6];
+				d7 = destp[7];
+				d0 ^= source1[0];
+				d1 ^= source1[1];
+				d2 ^= source1[2];
+				d3 ^= source1[3];
+				d4 ^= source1[4];
+				d5 ^= source1[5];
+				d6 ^= source1[6];
+				d7 ^= source1[7];
+				d0 ^= source2[0];
+				d1 ^= source2[1];
+				d2 ^= source2[2];
+				d3 ^= source2[3];
+				d4 ^= source2[4];
+				d5 ^= source2[5];
+				d6 ^= source2[6];
+				d7 ^= source2[7];
+				destp[0] = d0;	/* Store the result (in burts)		*/
+				destp[1] = d1;
+				destp[2] = d2;
+				destp[3] = d3;
+				destp[4] = d4;	/* Store the result (in burts)		*/
+				destp[5] = d5;
+				destp[6] = d6;
+				destp[7] = d7;
+				source1 += 8;
+				source2 += 8;
+				destp += 8;
+			}
+			break;
+		case 4:
+			source3 = (long *) bh_ptr[3]->b_data;
+			source2 = (long *) bh_ptr[2]->b_data;
+			source1 = (long *) bh_ptr[1]->b_data;
+			for (i = lines; i > 0; i--) {
+	  			register long d0, d1, d2, d3, d4, d5, d6, d7;
+				d0 = destp[0];	/* Pull the stuff into registers	*/
+				d1 = destp[1];	/*  ... in bursts, if possible.		*/
+				d2 = destp[2];
+				d3 = destp[3];
+				d4 = destp[4];
+				d5 = destp[5];
+				d6 = destp[6];
+				d7 = destp[7];
+				d0 ^= source1[0];
+				d1 ^= source1[1];
+				d2 ^= source1[2];
+				d3 ^= source1[3];
+				d4 ^= source1[4];
+				d5 ^= source1[5];
+				d6 ^= source1[6];
+				d7 ^= source1[7];
+				d0 ^= source2[0];
+				d1 ^= source2[1];
+				d2 ^= source2[2];
+				d3 ^= source2[3];
+				d4 ^= source2[4];
+				d5 ^= source2[5];
+				d6 ^= source2[6];
+				d7 ^= source2[7];
+				d0 ^= source3[0];
+				d1 ^= source3[1];
+				d2 ^= source3[2];
+				d3 ^= source3[3];
+				d4 ^= source3[4];
+				d5 ^= source3[5];
+				d6 ^= source3[6];
+				d7 ^= source3[7];
+				destp[0] = d0;	/* Store the result (in burts)		*/
+				destp[1] = d1;
+				destp[2] = d2;
+				destp[3] = d3;
+				destp[4] = d4;	/* Store the result (in burts)		*/
+				destp[5] = d5;
+				destp[6] = d6;
+				destp[7] = d7;
+				source1 += 8;
+				source2 += 8;
+				source3 += 8;
+				destp += 8;
+			}
+			break;
+		case 5:
+			source4 = (long *) bh_ptr[4]->b_data;
+			source3 = (long *) bh_ptr[3]->b_data;
+			source2 = (long *) bh_ptr[2]->b_data;
+			source1 = (long *) bh_ptr[1]->b_data;
+			for (i = lines; i > 0; i--) {
+	  			register long d0, d1, d2, d3, d4, d5, d6, d7;
+				d0 = destp[0];	/* Pull the stuff into registers	*/
+				d1 = destp[1];	/*  ... in bursts, if possible.		*/
+				d2 = destp[2];
+				d3 = destp[3];
+				d4 = destp[4];
+				d5 = destp[5];
+				d6 = destp[6];
+				d7 = destp[7];
+				d0 ^= source1[0];
+				d1 ^= source1[1];
+				d2 ^= source1[2];
+				d3 ^= source1[3];
+				d4 ^= source1[4];
+				d5 ^= source1[5];
+				d6 ^= source1[6];
+				d7 ^= source1[7];
+				d0 ^= source2[0];
+				d1 ^= source2[1];
+				d2 ^= source2[2];
+				d3 ^= source2[3];
+				d4 ^= source2[4];
+				d5 ^= source2[5];
+				d6 ^= source2[6];
+				d7 ^= source2[7];
+				d0 ^= source3[0];
+				d1 ^= source3[1];
+				d2 ^= source3[2];
+				d3 ^= source3[3];
+				d4 ^= source3[4];
+				d5 ^= source3[5];
+				d6 ^= source3[6];
+				d7 ^= source3[7];
+				d0 ^= source4[0];
+				d1 ^= source4[1];
+				d2 ^= source4[2];
+				d3 ^= source4[3];
+				d4 ^= source4[4];
+				d5 ^= source4[5];
+				d6 ^= source4[6];
+				d7 ^= source4[7];
+				destp[0] = d0;	/* Store the result (in burts)		*/
+				destp[1] = d1;
+				destp[2] = d2;
+				destp[3] = d3;
+				destp[4] = d4;	/* Store the result (in burts)		*/
+				destp[5] = d5;
+				destp[6] = d6;
+				destp[7] = d7;
+				source1 += 8;
+				source2 += 8;
+				source3 += 8;
+				source4 += 8;
+				destp += 8;
+			}
+			break;
+	}
+}
+
+/*
+ * (the -6*32 shift factor colors the cache)
+ */
+#define SIZE (PAGE_SIZE-6*32)
+
+static void xor_speed ( struct xor_block_template * func, 
+	struct buffer_head *b1, struct buffer_head *b2)
+{
+	int speed;
+	unsigned long now;
+	int i, count, max;
+	struct buffer_head *bh_ptr[6];
+
+	func->next = xor_functions;
+	xor_functions = func;
+	bh_ptr[0] = b1;
+	bh_ptr[1] = b2;
+
+	/*
+	 * count the number of XORs done during a whole jiffy.
+	 * calculate the speed of checksumming from this.
+	 * (we use a 2-page allocation to have guaranteed
+	 * color L1-cache layout)
+	 */
+	max = 0;
+	for (i = 0; i < 5; i++) {
+		now = jiffies;
+		count = 0;
+		while (jiffies == now) {
+			mb();
+			func->xor_block(2,bh_ptr);
+			mb();
+			count++;
+			mb();
+		}
+		if (count > max)
+			max = count;
+	}
+
+	speed = max * (HZ*SIZE/1024);
+	func->speed = speed;
+
+	printk( "   %-10s: %5d.%03d MB/sec\n", func->name,
+		speed / 1000, speed % 1000);
+}
+
+static inline void pick_fastest_function(void)
+{
+	struct xor_block_template *f, *fastest;
+
+	fastest = xor_functions;
+	for (f = fastest; f; f = f->next) {
+		if (f->speed > fastest->speed)
+			fastest = f;
+	}
+#ifdef CONFIG_X86_XMM 
+	if (boot_cpu_data.mmu_cr4_features & X86_CR4_OSXMMEXCPT) {
+		fastest = &t_xor_block_pIII_kni;
+	}
+#endif
+	xor_block = fastest->xor_block;
+	printk( "using fastest function: %s (%d.%03d MB/sec)\n", fastest->name,
+		fastest->speed / 1000, fastest->speed % 1000);
+}
+ 
+
+void calibrate_xor_block(void)
+{
+	struct buffer_head b1, b2;
+
+	memset(&b1,0,sizeof(b1));
+	b2 = b1;
+
+	b1.b_data = (char *) md__get_free_pages(GFP_KERNEL,2);
+	if (!b1.b_data) {
+		pick_fastest_function();
+		return;
+	}
+	b2.b_data = b1.b_data + 2*PAGE_SIZE + SIZE;
+
+	b1.b_size = SIZE;
+
+	printk(KERN_INFO "raid5: measuring checksumming speed\n");
+
+	sti(); /* should be safe */
+
+#if defined(__sparc__) && !defined(__sparc_v9__)
+	printk(KERN_INFO "raid5: trying high-speed SPARC checksum routine\n");
+	xor_speed(&t_xor_block_SPARC,&b1,&b2);
+#endif
+
+#ifdef CONFIG_X86_XMM 
+	if (boot_cpu_data.mmu_cr4_features & X86_CR4_OSXMMEXCPT) {
+		printk(KERN_INFO
+			"raid5: KNI detected, trying cache-avoiding KNI checksum routine\n");
+		/* we force the use of the KNI xor block because it
+			can write around l2.  we may also be able
+			to load into the l1 only depending on how
+			the cpu deals with a load to a line that is
+			being prefetched.
+		*/
+		xor_speed(&t_xor_block_pIII_kni,&b1,&b2);
+	}
+#endif /* CONFIG_X86_XMM */
+
+#ifdef __i386__
+
+	if (md_cpu_has_mmx()) {
+		printk(KERN_INFO
+			"raid5: MMX detected, trying high-speed MMX checksum routines\n");
+		xor_speed(&t_xor_block_pII_mmx,&b1,&b2);
+		xor_speed(&t_xor_block_p5_mmx,&b1,&b2);
+	}
+
+#endif /* __i386__ */
+	
+	
+	xor_speed(&t_xor_block_8regs,&b1,&b2);
+	xor_speed(&t_xor_block_32regs,&b1,&b2);
+
+	free_pages((unsigned long)b1.b_data,2);
+	pick_fastest_function();
+}
+
+#else /* __sparc_v9__ */
+
+void calibrate_xor_block(void)
+{
+	printk(KERN_INFO "raid5: using high-speed VIS checksum routine\n");
+	xor_block = xor_block_VIS;
+}
+
+#endif /* __sparc_v9__ */
+
+MD_EXPORT_SYMBOL(xor_block);
+
diff -urN linux/drivers/net/eepro100.c /tmp/linux/drivers/net/eepro100.c
--- linux/drivers/net/eepro100.c	Sun Dec 10 17:49:42 2000
+++ /tmp/linux/drivers/net/eepro100.c	Mon Apr 16 18:53:01 2001
@@ -1062,6 +1062,40 @@
 	/* Set the segment registers to '0'. */
 	wait_for_cmd_done(ioaddr + SCBCmd);
 	outl(0, ioaddr + SCBPointer);
+	/* 
+	 * Hi,
+	 *
+	 * On Thu, Jan 18, 2001 at 01:10:11PM +0100, Simon Huggins wrote:
+	 * > We have a server running 2.2.18 + RAID which has an eepro100 in it.
+	 * > It's connected to a Dlink DFE 816 100 16port 100baseTX hub.
+	 * > 
+	 * > When the machine boots we get a whole series of timeout errors.
+	 * > 
+	 * > Jan 18 11:58:09 miguet kernel: eepro100: cmd_wait for(0x70) timedout with(0x70)!
+	 * > Jan 18 11:58:09 miguet kernel: eepro100: cmd_wait for(0x10) timedout with(0x10)!
+	 * > Jan 18 11:58:09 miguet kernel: eepro100: cmd_wait for(0xffffff90) timedout with(0xffffff90)!
+	 * > Jan 18 11:58:38 miguet last message repeated 38 times
+	 * > Jan 18 11:58:50 miguet last message repeated 12 times
+	 *
+	 * Could you try to add
+	 *
+	 *     inl(ioaddr + SCBPointer);
+	 *     udelay(10);
+	 *
+	 *  before
+	 *
+	 *     outb(RxAddrLoad, ioaddr + SCBCmd);
+	 *
+	 *  in speedo_resume()?
+	 *
+	 *  These two line are a workaround for the RxAddrLoad timing bug, developed by
+	 *  Donald Becker.  wait_for_cmd_done timeouts may be related to this bug, too.
+	 *
+	 *  Best regards
+	 *  Andrey V. Savochkin
+	 */
+	inl(ioaddr + SCBPointer);
+	udelay(10);
 	outb(RxAddrLoad, ioaddr + SCBCmd);
 	wait_for_cmd_done(ioaddr + SCBCmd);
 	outb(CUCmdBase, ioaddr + SCBCmd);
diff -urN linux/drivers/scsi/Config.in /tmp/linux/drivers/scsi/Config.in
--- linux/drivers/scsi/Config.in	Sun Dec 10 17:49:42 2000
+++ /tmp/linux/drivers/scsi/Config.in	Fri Feb  2 19:06:14 2001
@@ -28,10 +28,8 @@
 dep_tristate 'Adaptec AHA1740 support' CONFIG_SCSI_AHA1740 $CONFIG_SCSI
 dep_tristate 'Adaptec AIC7xxx support' CONFIG_SCSI_AIC7XXX $CONFIG_SCSI
 if [ "$CONFIG_SCSI_AIC7XXX" != "n" ]; then
-    bool '   Enable Tagged Command Queueing (TCQ) by default' CONFIG_AIC7XXX_TCQ_ON_BY_DEFAULT
-    int  '   Maximum number of TCQ commands per device' CONFIG_AIC7XXX_CMDS_PER_DEVICE 8
-    bool '   Collect statistics to report in /proc' CONFIG_AIC7XXX_PROC_STATS N
-    int  '   Delay in seconds after SCSI bus reset' CONFIG_AIC7XXX_RESET_DELAY 5
+    int  '   Maximum number of TCQ commands per device' CONFIG_AIC7XXX_CMDS_PER_DEVICE 253
+    int  '   Initial bus reset delay in milli-seconds' CONFIG_AIC7XXX_RESET_DELAY 5000
 fi
 dep_tristate 'IBM ServeRAID support' CONFIG_SCSI_IPS $CONFIG_SCSI
 dep_tristate 'AdvanSys SCSI support' CONFIG_SCSI_ADVANSYS $CONFIG_SCSI
diff -urN linux/drivers/scsi/Makefile /tmp/linux/drivers/scsi/Makefile
--- linux/drivers/scsi/Makefile	Sun Dec 10 17:49:42 2000
+++ /tmp/linux/drivers/scsi/Makefile	Fri Feb  2 19:06:14 2001
@@ -716,6 +716,16 @@
 	$(LD) -r -o cpqfc.o cpqfcTSinit.o cpqfcTScontrol.o \
 	cpqfcTSi2c.o cpqfcTSworker.o $(CPQTRIG)
 
+aic7xxx-objs	:= aic7xxx/aic7xxx_linux.o aic7xxx/aic7xxx_linux_pci.o \
+		   aic7xxx/aic7xxx_proc.o aic7xxx/aic7770_linux.o \
+		   aic7xxx/aic7xxx.o aic7xxx/aic7xxx_pci.o \
+		   aic7xxx/aic7xxx_93cx6.o aic7xxx/aic7770.o
+
+aic7xxx.o: aic7xxx/aic7xxx_seq.h aic7xxx/aic7xxx_reg.h $(aic7xxx-objs) 
+	$(LD) -r -o $@ $(aic7xxx-objs)
+
+aic7xxx/aic7xxx_seq.h aic7xxx/aic7xxx_reg.h: aic7xxx/aic7xxx.seq aic7xxx/aic7xxx.reg
+	(cd aic7xxx && aicasm/aicasm -I. -r aic7xxx_reg.h -o aic7xxx_seq.h aic7xxx.seq)
 
 initio.o: ini9100u.c i91uscsi.c
 	$(CC) $(CFLAGS) -c ini9100u.c -o ini9100u.o
diff -urN linux/drivers/scsi/aic7xxx/aic7770.c /tmp/linux/drivers/scsi/aic7xxx/aic7770.c
--- linux/drivers/scsi/aic7xxx/aic7770.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aic7770.c	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,346 @@
+/*
+ * Product specific probe and attach routines for:
+ * 	27/284X and aic7770 motherboard SCSI controllers
+ *
+ * Copyright (c) 1994-1998, 2000, 2001 Justin T. Gibbs.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice immediately at the beginning of the file, without modification,
+ *    this list of conditions, and the following disclaimer.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $Id: //depot/src/aic7xxx/aic7770.c#6 $
+ *
+ * $FreeBSD: src/sys/dev/aic7xxx/aic7770.c,v 1.1 2000/09/16 20:02:27 gibbs Exp $
+ */
+
+#include "aic7xxx_osm.h"
+#include "aic7xxx_inline.h"
+#include "aic7xxx_93cx6.h"
+
+#define ID_AIC7770	0x04907770
+#define ID_AHA_274x	0x04907771
+#define ID_AHA_284xB	0x04907756 /* BIOS enabled */
+#define ID_AHA_284x	0x04907757 /* BIOS disabled*/
+
+static void aha2840_load_seeprom(struct ahc_softc *ahc);
+static ahc_device_setup_t ahc_aic7770_VL_setup;
+static ahc_device_setup_t ahc_aic7770_EISA_setup;;
+static ahc_device_setup_t ahc_aic7770_setup;
+
+
+struct aic7770_identity aic7770_ident_table [] =
+{
+	{
+		ID_AHA_274x,
+		0xFFFFFFFF,
+		"Adaptec 274X SCSI adapter",
+		ahc_aic7770_EISA_setup
+	},
+	{
+		ID_AHA_284xB,
+		0xFFFFFFFE,
+		"Adaptec 284X SCSI adapter",
+		ahc_aic7770_VL_setup
+	},
+	/* Generic chip probes for devices we don't know 'exactly' */
+	{
+		ID_AIC7770,
+		0xFFFFFFFF,
+		"Adaptec aic7770 SCSI adapter",
+		ahc_aic7770_EISA_setup
+	}
+};
+const int ahc_num_aic7770_devs = NUM_ELEMENTS(aic7770_ident_table);
+
+struct aic7770_identity *
+aic7770_find_device(uint32_t id)
+{
+	struct	aic7770_identity *entry;
+	int	i;
+
+	for (i = 0; i < ahc_num_aic7770_devs; i++) {
+		entry = &aic7770_ident_table[i];
+		if (entry->full_id == (id & entry->id_mask))
+			return (entry);
+	}
+	return (NULL);
+}
+
+int
+aic7770_config(struct ahc_softc *ahc, struct aic7770_identity *entry)
+{
+	struct	ahc_probe_config probe_config;
+	int	error;
+	u_int	hostconf;
+	u_int   irq;
+	u_int	intdef;
+	u_int	hcntrl;
+	int	shared;
+
+	ahc_init_probe_config(&probe_config);
+	error = entry->setup(ahc->dev_softc, &probe_config);
+	if (error != 0)
+		return (error);
+
+	error = aic7770_map_registers(ahc);
+	if (error != 0)
+		return (error);
+
+	/* Pause the card preseving the IRQ type */
+	hcntrl = ahc_inb(ahc, HCNTRL) & IRQMS;
+	ahc_outb(ahc, HCNTRL, hcntrl | PAUSE);
+	while ((ahc_inb(ahc, HCNTRL) & PAUSE) == 0)
+		;
+
+	/* Make sure we have a valid interrupt vector */
+	intdef = ahc_inb(ahc, INTDEF);
+	shared = (intdef & EDGE_TRIG) ? 0 : 1;
+	irq = intdef & VECTOR;
+	switch (irq) {
+	case 9:
+	case 10:
+	case 11:
+	case 12:
+	case 14:
+	case 15:
+		break;
+	default:
+		printf("aic7770_config: illegal irq setting %d\n", intdef);
+		return (ENXIO);
+	}
+
+	probe_config.description = entry->name;
+	error = ahc_softc_init(ahc, &probe_config);
+
+	error = aic7770_map_int(ahc, irq, shared);
+	if (error != 0)
+		return (error);
+
+	error = ahc_reset(ahc);
+	if (error != 0)
+		return (error);
+
+	switch (probe_config.chip & (AHC_EISA|AHC_VL)) {
+	case AHC_EISA:
+	{
+		u_int biosctrl;
+		u_int scsiconf;
+		u_int scsiconf1;
+
+		biosctrl = ahc_inb(ahc, HA_274_BIOSCTRL);
+		scsiconf = ahc_inb(ahc, SCSICONF);
+		scsiconf1 = ahc_inb(ahc, SCSICONF + 1);
+
+		/* Get the primary channel information */
+		if ((biosctrl & CHANNEL_B_PRIMARY) != 0)
+			ahc->flags |= AHC_CHANNEL_B_PRIMARY;
+
+		if ((biosctrl & BIOSMODE) == BIOSDISABLED) {
+			ahc->flags |= AHC_USEDEFAULTS;
+		} else {
+			if ((ahc->features & AHC_WIDE) != 0) {
+				ahc->our_id = scsiconf1 & HWSCSIID;
+				if (scsiconf & TERM_ENB)
+					ahc->flags |= AHC_TERM_ENB_A;
+			} else {
+				ahc->our_id = scsiconf & HSCSIID;
+				ahc->our_id_b = scsiconf1 & HSCSIID;
+				if (scsiconf & TERM_ENB)
+					ahc->flags |= AHC_TERM_ENB_A;
+				if (scsiconf1 & TERM_ENB)
+					ahc->flags |= AHC_TERM_ENB_B;
+			}
+		}
+		/*
+		 * We have no way to tell, so assume extended
+		 * translation is enabled.
+		 */
+		ahc->flags |= AHC_EXTENDED_TRANS_A|AHC_EXTENDED_TRANS_B;
+		break;
+	}
+	case AHC_VL:
+	{
+		aha2840_load_seeprom(ahc);
+		break;
+	}
+	default:
+		break;
+	}
+
+	/*
+	 * Ensure autoflush is enabled
+	 */
+	ahc_outb(ahc, SBLKCTL, ahc_inb(ahc, SBLKCTL) & ~AUTOFLUSHDIS);
+
+	/* Setup the FIFO threshold and the bus off time */
+	hostconf = ahc_inb(ahc, HOSTCONF);
+	ahc_outb(ahc, BUSSPD, hostconf & DFTHRSH);
+	ahc_outb(ahc, BUSTIME, (hostconf << 2) & BOFF);
+
+	/*
+	 * Generic aic7xxx initialization.
+	 */
+	error = ahc_init(ahc);
+	if (error != 0)
+		return (error);
+
+	/*
+	 * Link this softc in with all other ahc instances.
+	 */
+	ahc_softc_insert(ahc);
+
+	/*
+	 * Enable the board's BUS drivers
+	 */
+	ahc_outb(ahc, BCTL, ENABLE);
+
+	return (0);
+}
+
+/*
+ * Read the 284x SEEPROM.
+ */
+static void
+aha2840_load_seeprom(struct ahc_softc *ahc)
+{
+	struct	  seeprom_descriptor sd;
+	struct	  seeprom_config sc;
+	uint16_t  checksum = 0;
+	uint8_t   scsi_conf;
+	int	  have_seeprom;
+
+	sd.sd_ahc = ahc;
+	sd.sd_control_offset = SEECTL_2840;
+	sd.sd_status_offset = STATUS_2840;
+	sd.sd_dataout_offset = STATUS_2840;		
+	sd.sd_chip = C46;
+	sd.sd_MS = 0;
+	sd.sd_RDY = EEPROM_TF;
+	sd.sd_CS = CS_2840;
+	sd.sd_CK = CK_2840;
+	sd.sd_DO = DO_2840;
+	sd.sd_DI = DI_2840;
+
+	if (bootverbose)
+		printf("%s: Reading SEEPROM...", ahc_name(ahc));
+	have_seeprom = read_seeprom(&sd,
+				    (uint16_t *)&sc,
+				    /*start_addr*/0,
+				    sizeof(sc)/2);
+
+	if (have_seeprom) {
+		/* Check checksum */
+		int i;
+		int maxaddr = (sizeof(sc)/2) - 1;
+		uint16_t *scarray = (uint16_t *)&sc;
+
+		for (i = 0; i < maxaddr; i++)
+			checksum = checksum + scarray[i];
+		if (checksum != sc.checksum) {
+			if(bootverbose)
+				printf ("checksum error\n");
+			have_seeprom = 0;
+		} else if (bootverbose) {
+			printf("done.\n");
+		}
+	}
+
+	if (!have_seeprom) {
+		if (bootverbose)
+			printf("%s: No SEEPROM available\n", ahc_name(ahc));
+		ahc->flags |= AHC_USEDEFAULTS;
+	} else {
+		/*
+		 * Put the data we've collected down into SRAM
+		 * where ahc_init will find it.
+		 */
+		int i;
+		int max_targ = (ahc->features & AHC_WIDE) != 0 ? 16 : 8;
+		uint16_t discenable;
+
+		discenable = 0;
+		for (i = 0; i < max_targ; i++){
+	                uint8_t target_settings;
+			target_settings = (sc.device_flags[i] & CFXFER) << 4;
+			if (sc.device_flags[i] & CFSYNCH)
+				target_settings |= SOFS;
+			if (sc.device_flags[i] & CFWIDEB)
+				target_settings |= WIDEXFER;
+			if (sc.device_flags[i] & CFDISC)
+				discenable |= (0x01 << i);
+			ahc_outb(ahc, TARG_SCSIRATE + i, target_settings);
+		}
+		ahc_outb(ahc, DISC_DSB, ~(discenable & 0xff));
+		ahc_outb(ahc, DISC_DSB + 1, ~((discenable >> 8) & 0xff));
+
+		ahc->our_id = sc.brtime_id & CFSCSIID;
+
+		scsi_conf = (ahc->our_id & 0x7);
+		if (sc.adapter_control & CFSPARITY)
+			scsi_conf |= ENSPCHK;
+		if (sc.adapter_control & CFRESETB)
+			scsi_conf |= RESET_SCSI;
+
+		if (sc.bios_control & CF284XEXTEND)		
+			ahc->flags |= AHC_EXTENDED_TRANS_A;
+		/* Set SCSICONF info */
+		ahc_outb(ahc, SCSICONF, scsi_conf);
+
+		if (sc.adapter_control & CF284XSTERM)
+			ahc->flags |= AHC_TERM_ENB_A;
+	}
+}
+
+static int
+ahc_aic7770_VL_setup(ahc_dev_softc_t dev, struct ahc_probe_config *probe_config)
+{
+	int error;
+
+	error = ahc_aic7770_setup(dev, probe_config);
+	probe_config->chip |= AHC_VL;
+	return (error);
+}
+
+static int
+ahc_aic7770_EISA_setup(ahc_dev_softc_t dev,
+		       struct ahc_probe_config *probe_config)
+{
+	int error;
+
+	error = ahc_aic7770_setup(dev, probe_config);
+	probe_config->chip |= AHC_EISA;
+	return (error);
+}
+
+static int
+ahc_aic7770_setup(ahc_dev_softc_t dev, struct ahc_probe_config *probe_config)
+{
+	probe_config->channel = 'A';
+	probe_config->channel_b = 'B';
+	probe_config->chip = AHC_AIC7770;
+	probe_config->features = AHC_AIC7770_FE;
+	probe_config->bugs |= AHC_TMODE_WIDEODD_BUG;
+	probe_config->flags |= AHC_PAGESCBS;
+	return (0);
+}
diff -urN linux/drivers/scsi/aic7xxx/aic7770_linux.c /tmp/linux/drivers/scsi/aic7xxx/aic7770_linux.c
--- linux/drivers/scsi/aic7xxx/aic7770_linux.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aic7770_linux.c	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,146 @@
+/*
+ * Linux driver attachment glue for aic7770 based controllers.
+ *
+ * Copyright (c) 2000 Adaptec Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $Id: //depot/src/linux/drivers/scsi/aic7xxx/aic7770_linux.c#5 $
+ */
+
+#include "aic7xxx_osm.h"
+
+#define MINSLOT			1
+#define NUMSLOTS		16
+#define IDOFFSET		0x80
+
+int
+aic7770_linux_probe(Scsi_Host_Template *template)
+{
+#if defined(__i386__) || defined(__alpha__)
+	struct aic7770_identity *entry;
+	struct ahc_softc *ahc;
+	int i, slot;
+	int eisaBase;
+	int found;
+
+	if (aic7xxx_no_probe)
+		return (0);
+
+	eisaBase = 0x1000 + AHC_EISA_SLOT_OFFSET;
+	found = 0;
+	for (slot = 1; slot < NUMSLOTS; eisaBase+=0x1000, slot++) {
+		uint32_t eisa_id;
+		size_t	 id_size;
+
+		if (check_region(eisaBase, AHC_EISA_IOSIZE) != 0)
+			continue;
+
+		eisa_id = 0;
+		id_size = sizeof(eisa_id);
+		for (i = 0; i < 4; i++) {
+			/* VLcards require priming*/
+			outb(0x80 + i, eisaBase + IDOFFSET);
+			eisa_id |= inb(eisaBase + IDOFFSET + i)
+				   << ((id_size-i-1) * 8);
+		}
+		if (eisa_id & 0x80000000)
+			continue;  /* no EISA card in slot */
+
+		entry = aic7770_find_device(eisa_id);
+		if (entry != NULL) {
+			char	 buf[80];
+			char	*name;
+			int	 error;
+
+			/*
+			 * Allocate a softc for this card and
+			 * set it up for attachment by our
+			 * common detect routine.
+			 */
+			sprintf(buf, "ahc_eisa:%d", slot);
+			name = malloc(strlen(buf) + 1, M_DEVBUF, M_NOWAIT);
+			if (name == NULL)
+				break;
+			strcpy(name, buf);
+			ahc = ahc_alloc(template, name);
+			if (ahc == NULL) {
+				/*
+				 * If we can't allocate this one,
+				 * chances are we won't be able to
+				 * allocate future card structures.
+				 */
+				break;
+			}
+			ahc->tag = BUS_SPACE_PIO;
+			ahc->bsh.ioport = eisaBase;
+			error = aic7770_config(ahc, entry);
+			if (error != 0) {
+				ahc_free(ahc);
+				continue;
+			}
+			found++;
+		}
+	}
+	return (found);
+#else
+	return (0);
+#endif
+}
+
+int
+aic7770_map_registers(struct ahc_softc *ahc)
+{
+	/*
+	 * Lock out other contenders for our i/o space.
+	 */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,4,0)
+	request_region(ahc->bsh.ioport, AHC_EISA_IOSIZE, "aic7xxx");
+#else
+	if (request_region(ahc->bsh.ioport, AHC_EISA_IOSIZE, "aic7xxx") == 0)
+		return (ENOMEM);
+#endif
+
+	return (0);
+}
+
+int
+aic7770_map_int(struct ahc_softc *ahc, u_int irq, int shared)
+{
+	int error;
+
+	if (shared)
+		shared = SA_SHIRQ;
+
+	ahc->platform_data->irq = irq;
+	error = request_irq(ahc->platform_data->irq, aic7xxx_isr,
+			    SA_INTERRUPT|shared, "aic7xxx", ahc);
+	if (error < 0)
+		error = request_irq(ahc->platform_data->irq, aic7xxx_isr,
+				    shared, "aic7xxx", ahc);
+	
+	return (-error);
+}
diff -urN linux/drivers/scsi/aic7xxx/aic7xxx.c /tmp/linux/drivers/scsi/aic7xxx/aic7xxx.c
--- linux/drivers/scsi/aic7xxx/aic7xxx.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aic7xxx.c	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,6590 @@
+/*
+ * Core routines and tables shareable across OS platforms.
+ *
+ * Copyright (c) 1994-2001 Justin T. Gibbs.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $Id: //depot/src/aic7xxx/aic7xxx.c#27 $
+ *
+ * $FreeBSD: src/sys/dev/aic7xxx/aic7xxx.c,v 1.61 2000/11/13 03:35:43 gibbs Exp $
+ */
+
+#include "aic7xxx_osm.h"
+#include "aic7xxx_inline.h"
+#include "aicasm/aicasm_insformat.h"
+
+/****************************** Softc Data ************************************/
+struct ahc_softc_tailq ahc_tailq = TAILQ_HEAD_INITIALIZER(ahc_tailq);
+
+/***************************** Lookup Tables **********************************/
+char *ahc_chip_names[] =
+{
+	"NONE",
+	"aic7770",
+	"aic7850",
+	"aic7855",
+	"aic7859",
+	"aic7860",
+	"aic7870",
+	"aic7880",
+	"aic7895",
+	"aic7895C",
+	"aic7890/91",
+	"aic7896/97",
+	"aic7892",
+	"aic7899"
+};
+const u_int num_chip_names = NUM_ELEMENTS(ahc_chip_names);
+
+struct hard_error_entry hard_error[] = {
+	{ ILLHADDR,	"Illegal Host Access" },
+	{ ILLSADDR,	"Illegal Sequencer Address referrenced" },
+	{ ILLOPCODE,	"Illegal Opcode in sequencer program" },
+	{ SQPARERR,	"Sequencer Parity Error" },
+	{ DPARERR,	"Data-path Parity Error" },
+	{ MPARERR,	"Scratch or SCB Memory Parity Error" },
+	{ PCIERRSTAT,	"PCI Error detected" },
+	{ CIOPARERR,	"CIOBUS Parity Error" },
+};
+const u_int num_errors = NUM_ELEMENTS(hard_error);
+
+struct phase_table_entry phase_table[] =
+{
+	{ P_DATAOUT,	MSG_NOOP,		"in Data-out phase"	},
+	{ P_DATAIN,	MSG_INITIATOR_DET_ERR,	"in Data-in phase"	},
+	{ P_DATAOUT_DT,	MSG_NOOP,		"in DT Data-out phase"	},
+	{ P_DATAIN_DT,	MSG_INITIATOR_DET_ERR,	"in DT Data-in phase"	},
+	{ P_COMMAND,	MSG_NOOP,		"in Command phase"	},
+	{ P_MESGOUT,	MSG_NOOP,		"in Message-out phase"	},
+	{ P_STATUS,	MSG_INITIATOR_DET_ERR,	"in Status phase"	},
+	{ P_MESGIN,	MSG_PARITY_ERROR,	"in Message-in phase"	},
+	{ P_BUSFREE,	MSG_NOOP,		"while idle"		},
+	{ 0,		MSG_NOOP,		"in unknown phase"	}
+};
+
+/*
+ * In most cases we only wish to itterate over real phases, so
+ * exclude the last element from the count.
+ */
+const u_int num_phases = NUM_ELEMENTS(phase_table) - 1;
+
+/*
+ * Valid SCSIRATE values.  (p. 3-17)
+ * Provides a mapping of tranfer periods in ns to the proper value to
+ * stick in the scsixfer reg.
+ */
+struct ahc_syncrate ahc_syncrates[] =
+{
+      /* ultra2    fast/ultra  period     rate */
+	{ 0x42,      0x000,      9,      "80.0" },
+	{ 0x03,      0x000,     10,      "40.0" },
+	{ 0x04,      0x000,     11,      "33.0" },
+	{ 0x05,      0x100,     12,      "20.0" },
+	{ 0x06,      0x110,     15,      "16.0" },
+	{ 0x07,      0x120,     18,      "13.4" },
+	{ 0x08,      0x000,     25,      "10.0" },
+	{ 0x19,      0x010,     31,      "8.0"  },
+	{ 0x1a,      0x020,     37,      "6.67" },
+	{ 0x1b,      0x030,     43,      "5.7"  },
+	{ 0x1c,      0x040,     50,      "5.0"  },
+	{ 0x00,      0x050,     56,      "4.4"  },
+	{ 0x00,      0x060,     62,      "4.0"  },
+	{ 0x00,      0x070,     68,      "3.6"  },
+	{ 0x00,      0x000,      0,      NULL   }
+};
+
+/* Our Sequencer Program */
+#include "aic7xxx_seq.h"
+
+/**************************** Function Declarations ***************************/
+static struct tmode_tstate*
+			ahc_alloc_tstate(struct ahc_softc *ahc,
+					 u_int scsi_id, char channel);
+#ifdef AHC_TARGET_MODE
+static void		ahc_free_tstate(struct ahc_softc *ahc,
+					u_int scsi_id, char channel, int force);
+#endif
+static struct ahc_syncrate*
+			ahc_devlimited_syncrate(struct ahc_softc *ahc,
+					        struct ahc_initiator_tinfo *,
+						u_int *period,
+						u_int *ppr_options,
+						role_t role);
+static void		ahc_update_pending_syncrates(struct ahc_softc *ahc);
+static void		ahc_fetch_devinfo(struct ahc_softc *ahc,
+					  struct ahc_devinfo *devinfo);
+static void		ahc_scb_devinfo(struct ahc_softc *ahc,
+					struct ahc_devinfo *devinfo,
+					struct scb *scb);
+static void		ahc_setup_initiator_msgout(struct ahc_softc *ahc,
+						   struct ahc_devinfo *devinfo,
+						   struct scb *scb);
+static void		ahc_build_transfer_msg(struct ahc_softc *ahc,
+					       struct ahc_devinfo *devinfo);
+static void		ahc_construct_sdtr(struct ahc_softc *ahc,
+					   struct ahc_devinfo *devinfo,
+					   u_int period, u_int offset);
+static void		ahc_construct_wdtr(struct ahc_softc *ahc,
+					   struct ahc_devinfo *devinfo,
+					   u_int bus_width);
+static void		ahc_construct_ppr(struct ahc_softc *ahc,
+					  struct ahc_devinfo *devinfo,
+					  u_int period, u_int offset,
+					  u_int bus_width, u_int ppr_options);
+static void		ahc_clear_msg_state(struct ahc_softc *ahc);
+static void		ahc_handle_message_phase(struct ahc_softc *ahc);
+typedef enum {
+	AHCMSG_1B,
+	AHCMSG_2B,
+	AHCMSG_EXT
+} ahc_msgtype;
+static int		ahc_sent_msg(struct ahc_softc *ahc, ahc_msgtype type,
+				     u_int msgval, int full);
+static int		ahc_parse_msg(struct ahc_softc *ahc,
+				      struct ahc_devinfo *devinfo);
+static int		ahc_handle_msg_reject(struct ahc_softc *ahc,
+					      struct ahc_devinfo *devinfo);
+static void		ahc_handle_ign_wide_residue(struct ahc_softc *ahc,
+						struct ahc_devinfo *devinfo);
+static void		ahc_handle_devreset(struct ahc_softc *ahc,
+					    struct ahc_devinfo *devinfo,
+					    cam_status status, char *message,
+					    int verbose_level);
+
+static bus_dmamap_callback_t	ahc_dmamap_cb; 
+static void			ahc_build_free_scb_list(struct ahc_softc *ahc);
+static int			ahc_init_scbdata(struct ahc_softc *ahc);
+static void			ahc_fini_scbdata(struct ahc_softc *ahc);
+static void		ahc_qinfifo_requeue(struct ahc_softc *ahc,
+					    struct scb *prev_scb,
+					    struct scb *scb);
+static int		ahc_qinfifo_count(struct ahc_softc *ahc);
+static u_int		ahc_rem_scb_from_disc_list(struct ahc_softc *ahc,
+						   u_int prev, u_int scbptr);
+static void		ahc_add_curscb_to_free_list(struct ahc_softc *ahc);
+static u_int		ahc_rem_wscb(struct ahc_softc *ahc,
+				     u_int scbpos, u_int prev);
+static int		ahc_abort_scbs(struct ahc_softc *ahc, int target,
+				       char channel, int lun, u_int tag,
+				       role_t role, uint32_t status);
+static void		ahc_reset_current_bus(struct ahc_softc *ahc);
+static void		ahc_calc_residual(struct scb *scb);
+#ifdef AHC_DUMP_SEQ
+static void		ahc_dumpseq(struct ahc_softc *ahc);
+#endif
+static void		ahc_loadseq(struct ahc_softc *ahc);
+static int		ahc_check_patch(struct ahc_softc *ahc,
+					struct patch **start_patch,
+					u_int start_instr, u_int *skip_addr);
+static void		ahc_download_instr(struct ahc_softc *ahc,
+					   u_int instrptr, uint8_t *dconsts);
+#ifdef AHC_TARGET_MODE
+static void		ahc_queue_lstate_event(struct ahc_softc *ahc,
+					       struct tmode_lstate *lstate,
+					       u_int initiator_id,
+					       u_int event_type,
+					       u_int event_arg);
+static void		ahc_update_scsiid(struct ahc_softc *ahc,
+					  u_int targid_mask);
+static int		ahc_handle_target_cmd(struct ahc_softc *ahc,
+					      struct target_cmd *cmd);
+#endif
+/************************* Sequencer Execution Control ************************/
+/*
+ * Restart the sequencer program from address zero
+ */
+void
+restart_sequencer(struct ahc_softc *ahc)
+{
+
+	pause_sequencer(ahc);
+	ahc_outb(ahc, SCSISIGO, 0);		/* De-assert BSY */
+	ahc_outb(ahc, MSG_OUT, MSG_NOOP);	/* No message to send */
+	ahc_outb(ahc, SXFRCTL1, ahc_inb(ahc, SXFRCTL1) & ~BITBUCKET);
+
+	/*
+	 * Ensure that the sequencer's idea of TQINPOS
+	 * matches our own.  The sequencer increments TQINPOS
+	 * only after it sees a DMA complete and a reset could
+	 * occur before the increment leaving the kernel to believe
+	 * the command arrived but the sequencer to not.
+	 */
+	ahc_outb(ahc, TQINPOS, ahc->tqinfifonext);
+
+	/* Always allow reselection */
+	ahc_outb(ahc, SCSISEQ,
+		 ahc_inb(ahc, SCSISEQ_TEMPLATE) & (ENSELI|ENRSELI|ENAUTOATNP));
+	if ((ahc->features & AHC_CMD_CHAN) != 0) {
+		/* Ensure that no DMA operations are in progress */
+		ahc_outb(ahc, CCSCBCNT, 0);
+		ahc_outb(ahc, CCSGCTL, 0);
+		ahc_outb(ahc, CCSCBCTL, 0);
+	}
+	ahc_outb(ahc, MWI_RESIDUAL, 0);
+	ahc_outb(ahc, SEQCTL, FASTMODE);
+	ahc_outb(ahc, SEQADDR0, 0);
+	ahc_outb(ahc, SEQADDR1, 0);
+	unpause_sequencer(ahc);
+}
+
+/************************* Input/Output Queues ********************************/
+void
+ahc_run_qoutfifo(struct ahc_softc *ahc)
+{
+	struct scb *scb;
+	u_int  scb_index;
+
+	while (ahc->qoutfifo[ahc->qoutfifonext] != SCB_LIST_NULL) {
+
+		scb_index = ahc->qoutfifo[ahc->qoutfifonext];
+		if ((ahc->qoutfifonext & 0x03) == 0x03) {
+			u_int modnext;
+
+			/*
+			 * Clear 32bits of QOUTFIFO at a time
+			 * so that we don't clobber an incomming
+			 * byte DMA to the array on architectures
+			 * that only support 32bit load and store
+			 * operations.
+			 */
+			modnext = ahc->qoutfifonext & ~0x3;
+			*((uint32_t *)(&ahc->qoutfifo[modnext])) = 0xFFFFFFFFUL;
+		}
+		ahc->qoutfifonext++;
+
+		scb = ahc_lookup_scb(ahc, scb_index);
+		if (scb == NULL) {
+			printf("%s: WARNING no command for scb %d "
+			       "(cmdcmplt)\nQOUTPOS = %d\n",
+			       ahc_name(ahc), scb_index,
+			       ahc->qoutfifonext - 1);
+			continue;
+		}
+
+		/*
+		 * Save off the residual
+		 * if there is one.
+		 */
+		if (ahc_check_residual(scb) != 0)
+			ahc_calc_residual(scb);
+		else
+			ahc_set_residual(scb, 0);
+		ahc_done(ahc, scb);
+	}
+}
+
+void
+ahc_run_untagged_queues(struct ahc_softc *ahc)
+{
+	int i;
+
+	for (i = 0; i < 16; i++)
+		ahc_run_untagged_queue(ahc, &ahc->untagged_queues[i]);
+}
+
+void
+ahc_run_untagged_queue(struct ahc_softc *ahc, struct scb_tailq *queue)
+{
+	struct scb *scb;
+
+	if (ahc->untagged_queue_lock != 0)
+		return;
+
+	if ((scb = TAILQ_FIRST(queue)) != NULL
+	 && (scb->flags & SCB_ACTIVE) == 0) {
+		scb->flags |= SCB_ACTIVE;
+		ahc_queue_scb(ahc, scb);
+	}
+}
+
+/************************* Interrupt Handling *********************************/
+void
+ahc_handle_brkadrint(struct ahc_softc *ahc)
+{
+	/*
+	 * We upset the sequencer :-(
+	 * Lookup the error message
+	 */
+	int i, error, num_errors;
+
+	error = ahc_inb(ahc, ERROR);
+	num_errors =  sizeof(hard_error)/sizeof(hard_error[0]);
+	for (i = 0; error != 1 && i < num_errors; i++)
+		error >>= 1;
+	printf("%s: brkadrint, %s at seqaddr = 0x%x\n",
+	       ahc_name(ahc), hard_error[i].errmesg,
+	       ahc_inb(ahc, SEQADDR0) |
+	       (ahc_inb(ahc, SEQADDR1) << 8));
+
+	ahc_dump_card_state(ahc);
+
+	/* Tell everyone that this HBA is no longer availible */
+	ahc_abort_scbs(ahc, CAM_TARGET_WILDCARD, ALL_CHANNELS,
+		       CAM_LUN_WILDCARD, SCB_LIST_NULL, ROLE_UNKNOWN,
+		       CAM_NO_HBA);
+
+	/* Disable all interrupt sources by resetting the controller */
+	ahc_shutdown(ahc);
+}
+
+void
+ahc_handle_seqint(struct ahc_softc *ahc, u_int intstat)
+{
+	struct scb *scb;
+	struct ahc_devinfo devinfo;
+	
+	ahc_fetch_devinfo(ahc, &devinfo);
+
+	/*
+	 * Clear the upper byte that holds SEQINT status
+	 * codes and clear the SEQINT bit. We will unpause
+	 * the sequencer, if appropriate, after servicing
+	 * the request.
+	 */
+	ahc_outb(ahc, CLRINT, CLRSEQINT);
+	switch (intstat & SEQINT_MASK) {
+	case BAD_STATUS:
+	{
+		u_int  scb_index;
+		struct hardware_scb *hscb;
+
+		/*
+		 * Set the default return value to 0 (don't
+		 * send sense).  The sense code will change
+		 * this if needed.
+		 */
+		ahc_outb(ahc, RETURN_1, 0);
+
+		/*
+		 * The sequencer will notify us when a command
+		 * has an error that would be of interest to
+		 * the kernel.  This allows us to leave the sequencer
+		 * running in the common case of command completes
+		 * without error.  The sequencer will already have
+		 * dma'd the SCB back up to us, so we can reference
+		 * the in kernel copy directly.
+		 */
+		scb_index = ahc_inb(ahc, SCB_TAG);
+		scb = ahc_lookup_scb(ahc, scb_index);
+		if (scb == NULL) {
+			printf("%s:%c:%d: ahc_intr - referenced scb "
+			       "not valid during seqint 0x%x scb(%d)\n",
+			       ahc_name(ahc), devinfo.channel,
+			       devinfo.target, intstat, scb_index);
+			ahc_dump_card_state(ahc);
+			panic("for safety");
+			goto unpause;
+		}
+
+		hscb = scb->hscb; 
+
+		/* Don't want to clobber the original sense code */
+		if ((scb->flags & SCB_SENSE) != 0) {
+			/*
+			 * Clear the SCB_SENSE Flag and have
+			 * the sequencer do a normal command
+			 * complete.
+			 */
+			scb->flags &= ~SCB_SENSE;
+			ahc_set_transaction_status(scb, CAM_AUTOSENSE_FAIL);
+			break;
+		}
+		ahc_set_transaction_status(scb, CAM_SCSI_STATUS_ERROR);
+		/* Freeze the queue until the client sees the error. */
+		ahc_freeze_devq(ahc, scb);
+		ahc_freeze_scb(scb);
+		ahc_set_scsi_status(scb, hscb->shared_data.status.scsi_status);
+		switch (hscb->shared_data.status.scsi_status) {
+		case SCSI_STATUS_OK:
+			printf("%s: Interrupted for staus of 0???\n",
+			       ahc_name(ahc));
+			break;
+		case SCSI_STATUS_CMD_TERMINATED:
+		case SCSI_STATUS_CHECK_COND:
+#ifdef AHC_DEBUG
+			if (ahc_debug & AHC_SHOWSENSE) {
+				ahc_print_path(ahc, scb);
+				printf("SCB %d: requests Check Status\n",
+				       scb->hscb->tag);
+			}
+#endif
+
+			if (ahc_perform_autosense(scb)) {
+				struct ahc_dma_seg *sg;
+				struct scsi_sense *sc;
+				struct ahc_initiator_tinfo *targ_info;
+				struct tmode_tstate *tstate;
+				struct ahc_transinfo *tinfo;
+
+				targ_info =
+				    ahc_fetch_transinfo(ahc,
+							devinfo.channel,
+							devinfo.our_scsiid,
+							devinfo.target,
+							&tstate);
+				tinfo = &targ_info->current;
+				sg = scb->sg_list;
+				sc = (struct scsi_sense *)
+				     (&hscb->shared_data.cdb); 
+				/*
+				 * Save off the residual if there is one.
+				 */
+				if (ahc_check_residual(scb))
+					ahc_calc_residual(scb);
+				else
+					ahc_set_residual(scb, 0);
+#ifdef AHC_DEBUG
+				if (ahc_debug & AHC_SHOWSENSE) {
+					ahc_print_path(ahc, scb);
+					printf("Sending Sense\n");
+				}
+#endif
+				sg->addr = ahc_get_sense_bufaddr(ahc, scb);
+				sg->len = ahc_get_sense_bufsize(ahc, scb);
+				sg->len |= AHC_DMA_LAST_SEG;
+
+				/* Fixup byte order */
+				sg->addr = ahc_htole32(sg->addr);
+				sg->len = ahc_htole32(sg->len);
+
+				sc->opcode = REQUEST_SENSE;
+				sc->byte2 = 0;
+				if (tinfo->protocol_version <= SCSI_REV_2
+				 && SCB_GET_LUN(scb) < 8)
+					sc->byte2 = SCB_GET_LUN(scb) << 5;
+				sc->unused[0] = 0;
+				sc->unused[1] = 0;
+				sc->length = sg->len;
+				sc->control = 0;
+
+				/*
+				 * XXX Still true???
+				 * Would be nice to preserve DISCENB here,
+				 * but due to the way we manage busy targets,
+				 * we can't.
+				 */
+				hscb->control = 0;
+
+				/*
+				 * This request sense could be because the
+				 * the device lost power or in some other
+				 * way has lost our transfer negotiations.
+				 * Renegotiate if appropriate.  Unit attention
+				 * errors will be reported before any data
+				 * phases occur.
+				 */
+				if (ahc_get_residual(scb) 
+				 == ahc_get_transfer_length(scb)) {
+					ahc_update_target_msg_request(ahc,
+							      &devinfo,
+							      targ_info,
+							      /*force*/TRUE,
+							      /*paused*/TRUE);
+				}
+				hscb->cdb_len = sizeof(*sc);
+				hscb->dataptr = sg->addr; 
+				hscb->datacnt = sg->len;
+				hscb->sgptr = scb->sg_list_phys | SG_FULL_RESID;
+				hscb->sgptr = ahc_htole32(hscb->sgptr);
+				scb->sg_count = 1;
+				scb->flags |= SCB_SENSE;
+				ahc_qinfifo_requeue_tail(ahc, scb);
+				ahc_outb(ahc, RETURN_1, SEND_SENSE);
+#ifdef __FreeBSD__
+				/*
+				 * Ensure we have enough time to actually
+				 * retrieve the sense.
+				 */
+				untimeout(ahc_timeout, (caddr_t)scb,
+					  scb->io_ctx->ccb_h.timeout_ch);
+				scb->io_ctx->ccb_h.timeout_ch =
+				    timeout(ahc_timeout, (caddr_t)scb, 5 * hz);
+#endif
+			}
+			break;
+		default:
+			break;
+		}
+		break;
+	}
+	case NO_MATCH:
+	{
+		/* Ensure we don't leave the selection hardware on */
+		ahc_outb(ahc, SCSISEQ,
+			 ahc_inb(ahc, SCSISEQ) & (ENSELI|ENRSELI|ENAUTOATNP));
+
+		printf("%s:%c:%d: no active SCB for reconnecting "
+		       "target - issuing BUS DEVICE RESET\n",
+		       ahc_name(ahc), devinfo.channel, devinfo.target);
+		printf("SAVED_SCSIID == 0x%x, SAVED_LUN == 0x%x, "
+		       "ARG_1 == 0x%x ACCUM = 0x%x\n",
+		       ahc_inb(ahc, SAVED_SCSIID), ahc_inb(ahc, SAVED_LUN),
+		       ahc_inb(ahc, ARG_1), ahc_inb(ahc, ACCUM));
+		printf("SEQ_FLAGS == 0x%x, SCBPTR == 0x%x, BTT == 0x%x, "
+		       "SINDEX == 0x%x\n",
+		       ahc_inb(ahc, SEQ_FLAGS), ahc_inb(ahc, SCBPTR),
+		       ahc_index_busy_tcl(ahc,
+			    BUILD_TCL(ahc_inb(ahc, SAVED_SCSIID),
+				      ahc_inb(ahc, SAVED_LUN))),
+		       ahc_inb(ahc, SINDEX));
+		printf("SCSIID == 0x%x, SCB_SCSIID == 0x%x, SCB_LUN == 0x%x, "
+		       "SCB_TAG == 0x%x, SCB_CONTROL == 0x%x\n",
+		       ahc_inb(ahc, SCSIID), ahc_inb(ahc, SCB_SCSIID),
+		       ahc_inb(ahc, SCB_LUN), ahc_inb(ahc, SCB_TAG),
+		       ahc_inb(ahc, SCB_CONTROL));
+		printf("SCSIBUSL == 0x%x, SCSISIGI == 0x%x\n",
+		       ahc_inb(ahc, SCSIBUSL), ahc_inb(ahc, SCSISIGI));
+		printf("SXFRCTL0 == 0x%x\n", ahc_inb(ahc, SXFRCTL0));
+		printf("SEQCTL == 0x%x\n", ahc_inb(ahc, SEQCTL));
+		ahc_dump_card_state(ahc);
+		ahc->msgout_buf[0] = MSG_BUS_DEV_RESET;
+		ahc->msgout_len = 1;
+		ahc->msgout_index = 0;
+		ahc->msg_type = MSG_TYPE_INITIATOR_MSGOUT;
+		ahc_outb(ahc, MSG_OUT, HOST_MSG);
+		ahc_outb(ahc, SCSISIGO, ahc_inb(ahc, LASTPHASE) | ATNO);
+		break;
+	}
+	case SEND_REJECT: 
+	{
+		u_int rejbyte = ahc_inb(ahc, ACCUM);
+		printf("%s:%c:%d: Warning - unknown message received from "
+		       "target (0x%x).  Rejecting\n", 
+		       ahc_name(ahc), devinfo.channel, devinfo.target, rejbyte);
+		break; 
+	}
+	case NO_IDENT: 
+	{
+		/*
+		 * The reconnecting target either did not send an identify
+		 * message, or did, but we didn't find an SCB to match and
+		 * before it could respond to our ATN/abort, it hit a dataphase.
+		 * The only safe thing to do is to blow it away with a bus
+		 * reset.
+		 */
+		int found;
+
+		printf("%s:%c:%d: Target did not send an IDENTIFY message. "
+		       "LASTPHASE = 0x%x, SAVED_SCSIID == 0x%x\n",
+		       ahc_name(ahc), devinfo.channel, devinfo.target,
+		       ahc_inb(ahc, LASTPHASE), ahc_inb(ahc, SAVED_SCSIID));
+		found = ahc_reset_channel(ahc, devinfo.channel, 
+					  /*initiate reset*/TRUE);
+		printf("%s: Issued Channel %c Bus Reset. "
+		       "%d SCBs aborted\n", ahc_name(ahc), devinfo.channel,
+		       found);
+		return;
+	}
+	case IGN_WIDE_RES:
+		ahc_handle_ign_wide_residue(ahc, &devinfo);
+		break;
+	case BAD_PHASE:
+	{
+		u_int lastphase;
+
+		lastphase = ahc_inb(ahc, LASTPHASE);
+		printf("%s:%c:%d: unknown scsi bus phase %x, "
+		       "lastphase = 0x%x.  Attempting to continue\n",
+		       ahc_name(ahc), devinfo.channel, devinfo.target,
+		       lastphase, ahc_inb(ahc, SCSISIGI));
+		break;
+	}
+	case MISSED_BUSFREE:
+	{
+		u_int lastphase;
+
+		lastphase = ahc_inb(ahc, LASTPHASE);
+		printf("%s:%c:%d: Missed busfree. "
+		       "Lastphase = 0x%x, Curphase = 0x%x\n",
+		       ahc_name(ahc), devinfo.channel, devinfo.target,
+		       lastphase, ahc_inb(ahc, SCSISIGI));
+		restart_sequencer(ahc);
+		return;
+	}
+	case HOST_MSG_LOOP:
+	{
+		/*
+		 * The sequencer has encountered a message phase
+		 * that requires host assistance for completion.
+		 * While handling the message phase(s), we will be
+		 * notified by the sequencer after each byte is
+		 * transfered so we can track bus phase changes.
+		 *
+		 * If this is the first time we've seen a HOST_MSG_LOOP
+		 * interrupt, initialize the state of the host message
+		 * loop.
+		 */
+		if (ahc->msg_type == MSG_TYPE_NONE) {
+			u_int bus_phase;
+
+			bus_phase = ahc_inb(ahc, SCSISIGI) & PHASE_MASK;
+			if (bus_phase != P_MESGIN
+			 && bus_phase != P_MESGOUT) {
+				printf("ahc_intr: HOST_MSG_LOOP bad "
+				       "phase 0x%x\n",
+				      bus_phase);
+				/*
+				 * Probably transitioned to bus free before
+				 * we got here.  Just punt the message.
+				 */
+				ahc_clear_intstat(ahc);
+				restart_sequencer(ahc);
+				return;
+			}
+
+			if (devinfo.role == ROLE_INITIATOR) {
+				struct scb *scb;
+				u_int scb_index;
+
+				scb_index = ahc_inb(ahc, SCB_TAG);
+				scb = ahc_lookup_scb(ahc, scb_index);
+
+				if (scb == NULL)
+					panic("HOST_MSG_LOOP with "
+					      "invalid SCB %x\n", scb_index);
+
+				if (bus_phase == P_MESGOUT)
+					ahc_setup_initiator_msgout(ahc,
+								   &devinfo,
+								   scb);
+				else {
+					ahc->msg_type =
+					    MSG_TYPE_INITIATOR_MSGIN;
+					ahc->msgin_index = 0;
+				}
+			} else {
+				if (bus_phase == P_MESGOUT) {
+					ahc->msg_type =
+					    MSG_TYPE_TARGET_MSGOUT;
+					ahc->msgin_index = 0;
+				}
+#if AHC_TARGET_MODE
+				else 
+					ahc_setup_target_msgin(ahc, &devinfo);
+#endif
+			}
+		}
+
+		ahc_handle_message_phase(ahc);
+		break;
+	}
+	case PERR_DETECTED:
+	{
+		/*
+		 * If we've cleared the parity error interrupt
+		 * but the sequencer still believes that SCSIPERR
+		 * is true, it must be that the parity error is
+		 * for the currently presented byte on the bus,
+		 * and we are not in a phase (data-in) where we will
+		 * eventually ack this byte.  Ack the byte and
+		 * throw it away in the hope that the target will
+		 * take us to message out to deliver the appropriate
+		 * error message.
+		 */
+		if ((intstat & SCSIINT) == 0
+		 && (ahc_inb(ahc, SSTAT1) & SCSIPERR) != 0) {
+			u_int curphase;
+
+			/*
+			 * The hardware will only let you ack bytes
+			 * if the expected phase in SCSISIGO matches
+			 * the current phase.  Make sure this is
+			 * currently the case.
+			 */
+			curphase = ahc_inb(ahc, SCSISIGI) & PHASE_MASK;
+			ahc_outb(ahc, LASTPHASE, curphase);
+			ahc_outb(ahc, SCSISIGO, curphase);
+			ahc_inb(ahc, SCSIDATL);
+		}
+		break;
+	}
+	case DATA_OVERRUN:
+	{
+		/*
+		 * When the sequencer detects an overrun, it
+		 * places the controller in "BITBUCKET" mode
+		 * and allows the target to complete its transfer.
+		 * Unfortunately, none of the counters get updated
+		 * when the controller is in this mode, so we have
+		 * no way of knowing how large the overrun was.
+		 */
+		u_int scbindex = ahc_inb(ahc, SCB_TAG);
+		u_int lastphase = ahc_inb(ahc, LASTPHASE);
+		u_int i;
+
+		scb = ahc_lookup_scb(ahc, scbindex);
+		for (i = 0; i < num_phases; i++) {
+			if (lastphase == phase_table[i].phase)
+				break;
+		}
+		ahc_print_path(ahc, scb);
+		printf("data overrun detected %s."
+		       "  Tag == 0x%x.\n",
+		       phase_table[i].phasemsg,
+  		       scb->hscb->tag);
+		ahc_print_path(ahc, scb);
+		printf("%s seen Data Phase.  Length = %ld.  NumSGs = %d.\n",
+		       ahc_inb(ahc, SEQ_FLAGS) & DPHASE ? "Have" : "Haven't",
+		       ahc_get_transfer_length(scb), scb->sg_count);
+		if (scb->sg_count > 0) {
+			for (i = 0; i < scb->sg_count; i++) {
+				printf("sg[%d] - Addr 0x%x : Length %d\n",
+				       i,
+				       ahc_le32toh(scb->sg_list[i].addr),
+				       ahc_le32toh(scb->sg_list[i].len)
+				       & AHC_SG_LEN_MASK);
+			}
+		}
+		/*
+		 * Set this and it will take effect when the
+		 * target does a command complete.
+		 */
+		ahc_freeze_devq(ahc, scb);
+		ahc_set_transaction_status(scb, CAM_DATA_RUN_ERR);
+		ahc_freeze_scb(scb);
+		break;
+	}
+	case MKMSG_FAILED:
+	{
+		u_int scbindex;
+
+		printf("%s:%c:%d:%d: Attempt to issue message failed\n",
+		       ahc_name(ahc), devinfo.channel, devinfo.target,
+		       devinfo.lun);
+		scbindex = ahc_inb(ahc, SCB_TAG);
+		scb = ahc_lookup_scb(ahc, scbindex);
+		if (scb != NULL
+		 && (scb->flags & SCB_RECOVERY_SCB) != 0)
+			/*
+			 * Ensure that we didn't put a second instance of this
+			 * SCB into the QINFIFO.
+			 */
+			ahc_search_qinfifo(ahc, SCB_GET_TARGET(ahc, scb),
+					   SCB_GET_CHANNEL(ahc, scb),
+					   SCB_GET_LUN(scb), scb->hscb->tag,
+					   ROLE_INITIATOR, /*status*/0,
+					   SEARCH_REMOVE);
+		break;
+	}
+	case NO_FREE_SCB:
+	{
+		printf("%s: No free or disconnected SCBs\n", ahc_name(ahc));
+		ahc_dump_card_state(ahc);
+		panic("for safety");
+		break;
+	}
+	case SCB_MISMATCH:
+	{
+		u_int scbptr;
+
+		scbptr = ahc_inb(ahc, SCBPTR);
+		printf("Bogus TAG after DMA.  SCBPTR %d, tag %d, our tag %d\n",
+		       scbptr, ahc_inb(ahc, ARG_1),
+		       ahc->scb_data->hscbs[scbptr].tag);
+		ahc_dump_card_state(ahc);
+		panic("for saftey");
+		break;
+	}
+	case OUT_OF_RANGE:
+	{
+		printf("%s: BTT calculation out of range\n", ahc_name(ahc));
+		printf("SAVED_SCSIID == 0x%x, SAVED_LUN == 0x%x, "
+		       "ARG_1 == 0x%x ACCUM = 0x%x\n",
+		       ahc_inb(ahc, SAVED_SCSIID), ahc_inb(ahc, SAVED_LUN),
+		       ahc_inb(ahc, ARG_1), ahc_inb(ahc, ACCUM));
+		printf("SEQ_FLAGS == 0x%x, SCBPTR == 0x%x, BTT == 0x%x, "
+		       "SINDEX == 0x%x\n, A == 0x%x\n",
+		       ahc_inb(ahc, SEQ_FLAGS), ahc_inb(ahc, SCBPTR),
+		       ahc_index_busy_tcl(ahc,
+			    BUILD_TCL(ahc_inb(ahc, SAVED_SCSIID),
+				      ahc_inb(ahc, SAVED_LUN))),
+		       ahc_inb(ahc, SINDEX),
+		       ahc_inb(ahc, ACCUM));
+		printf("SCSIID == 0x%x, SCB_SCSIID == 0x%x, SCB_LUN == 0x%x, "
+		       "SCB_TAG == 0x%x, SCB_CONTROL == 0x%x\n",
+		       ahc_inb(ahc, SCSIID), ahc_inb(ahc, SCB_SCSIID),
+		       ahc_inb(ahc, SCB_LUN), ahc_inb(ahc, SCB_TAG),
+		       ahc_inb(ahc, SCB_CONTROL));
+		printf("SCSIBUSL == 0x%x, SCSISIGI == 0x%x\n",
+		       ahc_inb(ahc, SCSIBUSL), ahc_inb(ahc, SCSISIGI));
+		ahc_dump_card_state(ahc);
+		panic("for safety");
+		break;
+	}
+	default:
+		printf("ahc_intr: seqint, "
+		       "intstat == 0x%x, scsisigi = 0x%x\n",
+		       intstat, ahc_inb(ahc, SCSISIGI));
+		break;
+	}
+unpause:
+	/*
+	 *  The sequencer is paused immediately on
+	 *  a SEQINT, so we should restart it when
+	 *  we're done.
+	 */
+	unpause_sequencer(ahc);
+}
+
+void
+ahc_handle_scsiint(struct ahc_softc *ahc, u_int intstat)
+{
+	u_int	scb_index;
+	u_int	status0;
+	u_int	status;
+	struct	scb *scb;
+	char	cur_channel;
+	char	intr_channel;
+
+	/* Make sure the sequencer is in a safe location. */
+	ahc_clear_critical_section(ahc);
+
+	if ((ahc->features & AHC_TWIN) != 0
+	 && ((ahc_inb(ahc, SBLKCTL) & SELBUSB) != 0))
+		cur_channel = 'B';
+	else
+		cur_channel = 'A';
+	intr_channel = cur_channel;
+
+	if ((ahc->features & AHC_ULTRA2) != 0)
+		status0 = ahc_inb(ahc, SSTAT0) & IOERR;
+	else
+		status0 = 0;
+	status = ahc_inb(ahc, SSTAT1) & (SELTO|SCSIRSTI|BUSFREE|SCSIPERR);
+	if (status == 0 && status0 == 0) {
+		if ((ahc->features & AHC_TWIN) != 0) {
+			/* Try the other channel */
+		 	ahc_outb(ahc, SBLKCTL, ahc_inb(ahc, SBLKCTL) ^ SELBUSB);
+			status = ahc_inb(ahc, SSTAT1);
+		 	ahc_outb(ahc, SBLKCTL, ahc_inb(ahc, SBLKCTL) ^ SELBUSB);
+			intr_channel = (cur_channel == 'A') ? 'B' : 'A';
+		}
+		if (status == 0) {
+			printf("%s: Spurious SCSI interrupt\n", ahc_name(ahc));
+			ahc_outb(ahc, CLRINT, CLRSCSIINT);
+			unpause_sequencer(ahc);
+			return;
+		}
+	}
+
+	scb_index = ahc_inb(ahc, SCB_TAG);
+	scb = ahc_lookup_scb(ahc, scb_index);
+	if (scb != NULL
+	 && (ahc_inb(ahc, SEQ_FLAGS) & IDENTIFY_SEEN) == 0)
+		scb = NULL;
+
+	if ((ahc->features & AHC_ULTRA2) != 0
+		&& (status0 & IOERR) != 0) {
+		int now_lvd;
+
+		now_lvd = ahc_inb(ahc, SBLKCTL) & ENAB40;
+		printf("%s: Transceiver State Has Changed to %s mode\n",
+		       ahc_name(ahc), now_lvd ? "LVD" : "SE");
+		ahc_outb(ahc, CLRSINT0, CLRIOERR);
+		/*
+		 * When transitioning to SE mode, the reset line
+		 * glitches, triggering an arbitration bug in some
+		 * Ultra2 controllers.  This bug is cleared when we
+		 * assert the reset line.  Since a reset glitch has
+		 * already occurred with this transition and a
+		 * transceiver state change is handled just like
+		 * a bus reset anyway, asserting the reset line
+		 * ourselves is safe.
+		 */
+		ahc_reset_channel(ahc, intr_channel,
+				 /*Initiate Reset*/now_lvd == 0);
+	} else if ((status & SCSIRSTI) != 0) {
+		printf("%s: Someone reset channel %c\n",
+			ahc_name(ahc), intr_channel);
+		ahc_reset_channel(ahc, intr_channel, /*Initiate Reset*/FALSE);
+	} else if ((status & SCSIPERR) != 0) {
+		/*
+		 * Determine the bus phase and queue an appropriate message.
+		 * SCSIPERR is latched true as soon as a parity error
+		 * occurs.  If the sequencer acked the transfer that
+		 * caused the parity error and the currently presented
+		 * transfer on the bus has correct parity, SCSIPERR will
+		 * be cleared by CLRSCSIPERR.  Use this to determine if
+		 * we should look at the last phase the sequencer recorded,
+		 * or the current phase presented on the bus.
+		 */
+		u_int mesg_out;
+		u_int curphase;
+		u_int errorphase;
+		u_int lastphase;
+		u_int scsirate;
+		u_int i;
+		u_int sstat2;
+
+		lastphase = ahc_inb(ahc, LASTPHASE);
+		curphase = ahc_inb(ahc, SCSISIGI) & PHASE_MASK;
+		sstat2 = ahc_inb(ahc, SSTAT2);
+		ahc_outb(ahc, CLRSINT1, CLRSCSIPERR);
+		/*
+		 * For all phases save DATA, the sequencer won't
+		 * automatically ack a byte that has a parity error
+		 * in it.  So the only way that the current phase
+		 * could be 'data-in' is if the parity error is for
+		 * an already acked byte in the data phase.  During
+		 * synchronous data-in transfers, we may actually
+		 * ack bytes before latching the current phase in
+		 * LASTPHASE, leading to the discrepancy between
+		 * curphase and lastphase.
+		 */
+		if ((ahc_inb(ahc, SSTAT1) & SCSIPERR) != 0
+		 || curphase == P_DATAIN || curphase == P_DATAIN_DT)
+			errorphase = curphase;
+		else
+			errorphase = lastphase;
+
+		for (i = 0; i < num_phases; i++) {
+			if (errorphase == phase_table[i].phase)
+				break;
+		}
+		mesg_out = phase_table[i].mesg_out;
+		if (scb != NULL)
+			ahc_print_path(ahc, scb);
+		else
+			printf("%s:%c:%d: ", ahc_name(ahc),
+			       intr_channel,
+			       SCSIID_TARGET(ahc, ahc_inb(ahc, SAVED_SCSIID)));
+		scsirate = ahc_inb(ahc, SCSIRATE);
+		printf("parity error detected %s. "
+		       "SEQADDR(0x%x) SCSIRATE(0x%x)\n",
+		       phase_table[i].phasemsg,
+		       ahc_inb(ahc, SEQADDR0) | (ahc_inb(ahc, SEQADDR1) << 8),
+		       scsirate);
+
+		if ((ahc->features & AHC_DT) != 0) {
+
+			if ((sstat2 & CRCVALERR) != 0)
+				printf("\tCRC Value Mismatch\n");
+			if ((sstat2 & CRCENDERR) != 0)
+				printf("\tNo terminal CRC packet recevied\n");
+			if ((sstat2 & CRCREQERR) != 0)
+				printf("\tIllegal CRC packet request\n");
+			if ((sstat2 & DUAL_EDGE_ERR) != 0)
+				printf("\tUnexpected %sDT Data Phase\n",
+				       (scsirate & SINGLE_EDGE) ? "" : "non-");
+		}
+
+		/*
+		 * We've set the hardware to assert ATN if we   
+		 * get a parity error on "in" phases, so all we  
+		 * need to do is stuff the message buffer with
+		 * the appropriate message.  "In" phases have set
+		 * mesg_out to something other than MSG_NOP.
+		 */
+		if (mesg_out != MSG_NOOP) {
+			if (ahc->msg_type != MSG_TYPE_NONE)
+				ahc->send_msg_perror = TRUE;
+			else
+				ahc_outb(ahc, MSG_OUT, mesg_out);
+		}
+		ahc_outb(ahc, CLRINT, CLRSCSIINT);
+		unpause_sequencer(ahc);
+	} else if ((status & BUSFREE) != 0
+		&& (ahc_inb(ahc, SIMODE1) & ENBUSFREE) != 0) {
+		/*
+		 * First look at what phase we were last in.
+		 * If its message out, chances are pretty good
+		 * that the busfree was in response to one of
+		 * our abort requests.
+		 */
+		u_int lastphase = ahc_inb(ahc, LASTPHASE);
+		u_int saved_scsiid = ahc_inb(ahc, SAVED_SCSIID);
+		u_int saved_lun = ahc_inb(ahc, SAVED_LUN);
+		u_int target = SCSIID_TARGET(ahc, saved_scsiid);
+		u_int initiator_role_id = SCSIID_OUR_ID(saved_scsiid);
+		char channel = SCSIID_CHANNEL(ahc, saved_scsiid);
+		int printerror = 1;
+
+		ahc_outb(ahc, SCSISEQ,
+			 ahc_inb(ahc, SCSISEQ) & (ENSELI|ENRSELI|ENAUTOATNP));
+		if (lastphase == P_MESGOUT) {
+			struct ahc_devinfo devinfo;
+			u_int tag;
+
+			ahc_fetch_devinfo(ahc, &devinfo);
+			tag = SCB_LIST_NULL;
+			if (ahc_sent_msg(ahc, AHCMSG_1B, MSG_ABORT_TAG, TRUE)
+			 || ahc_sent_msg(ahc, AHCMSG_1B, MSG_ABORT, TRUE)) {
+				if (ahc->msgout_buf[ahc->msgout_index - 1]
+				 == MSG_ABORT_TAG)
+					tag = scb->hscb->tag;
+				ahc_print_path(ahc, scb);
+				printf("SCB %d - Abort %s Completed.\n",
+				       scb->hscb->tag, tag == SCB_LIST_NULL ?
+				       "" : "Tag");
+				ahc_abort_scbs(ahc, target, channel,
+					       saved_lun, tag,
+					       ROLE_INITIATOR,
+					       CAM_REQ_ABORTED);
+				printerror = 0;
+			} else if (ahc_sent_msg(ahc, AHCMSG_1B,
+						MSG_BUS_DEV_RESET, TRUE)) {
+				struct ahc_devinfo devinfo;
+#ifdef __FreeBSD__
+				/*
+				 * Don't mark the user's request for this BDR
+				 * as completing with CAM_BDR_SENT.  CAM3
+				 * specifies CAM_REQ_CMP.
+				 */
+				if (scb != NULL
+				 && scb->io_ctx->ccb_h.func_code== XPT_RESET_DEV
+				 && ahc_match_scb(ahc, scb, target, channel,
+						  CAM_LUN_WILDCARD,
+						  SCB_LIST_NULL,
+						  ROLE_INITIATOR)) {
+					ahc_set_transaction_status(scb, CAM_REQ_CMP);
+				}
+#endif
+				ahc_compile_devinfo(&devinfo,
+						    initiator_role_id,
+						    target,
+						    CAM_LUN_WILDCARD,
+						    channel,
+						    ROLE_INITIATOR);
+				ahc_handle_devreset(ahc, &devinfo,
+						    CAM_BDR_SENT,
+						    "Bus Device Reset",
+						    /*verbose_level*/0);
+				printerror = 0;
+			} else if (ahc_sent_msg(ahc, AHCMSG_EXT,
+						MSG_EXT_PPR, FALSE)) {
+				struct ahc_initiator_tinfo *tinfo;
+				struct tmode_tstate *tstate;
+
+				/*
+				 * PPR Rejected.  Try non-ppr negotiation
+				 * and retry command.
+				 */
+				tinfo = ahc_fetch_transinfo(ahc,
+							    devinfo.channel,
+							    devinfo.our_scsiid,
+							    devinfo.target,
+							    &tstate);
+				tinfo->current.transport_version = 2;
+				tinfo->goal.transport_version = 2;
+				tinfo->goal.ppr_options = 0;
+				ahc_qinfifo_requeue_tail(ahc, scb);
+				printerror = 0;
+			} else if (ahc_sent_msg(ahc, AHCMSG_EXT,
+						MSG_EXT_WDTR, FALSE)
+				|| ahc_sent_msg(ahc, AHCMSG_EXT,
+						MSG_EXT_SDTR, FALSE)) {
+				/*
+				 * Negotiation Rejected.  Go-async and
+				 * retry command.
+				 */
+				ahc_set_width(ahc, &devinfo,
+					      MSG_EXT_WDTR_BUS_8_BIT,
+					      AHC_TRANS_CUR|AHC_TRANS_GOAL,
+					      /*paused*/TRUE);
+				ahc_set_syncrate(ahc, &devinfo,
+						/*syncrate*/NULL,
+						/*period*/0, /*offset*/0,
+						/*ppr_options*/0,
+						AHC_TRANS_CUR|AHC_TRANS_GOAL,
+						/*paused*/TRUE);
+				ahc_qinfifo_requeue_tail(ahc, scb);
+				printerror = 0;
+			}
+		}
+		if (printerror != 0) {
+			u_int i;
+
+			if (scb != NULL) {
+				u_int tag;
+
+				if ((scb->hscb->control & TAG_ENB) != 0)
+					tag = scb->hscb->tag;
+				else
+					tag = SCB_LIST_NULL;
+				ahc_print_path(ahc, scb);
+				ahc_abort_scbs(ahc, target, channel,
+					       SCB_GET_LUN(scb), tag,
+					       ROLE_INITIATOR,
+					       CAM_UNEXP_BUSFREE);
+			} else {
+				/*
+				 * We had not fully identified this connection,
+				 * so we cannot abort anything.
+				 */
+				printf("%s: ", ahc_name(ahc));
+			}
+			for (i = 0; i < num_phases; i++) {
+				if (lastphase == phase_table[i].phase)
+					break;
+			}
+			printf("Unexpected busfree %s\n"
+			       "SEQADDR == 0x%x\n",
+			       phase_table[i].phasemsg, ahc_inb(ahc, SEQADDR0)
+				| (ahc_inb(ahc, SEQADDR1) << 8));
+		}
+		ahc_clear_msg_state(ahc);
+		ahc_outb(ahc, SIMODE1, ahc_inb(ahc, SIMODE1) & ~ENBUSFREE);
+		ahc_outb(ahc, CLRSINT1, CLRBUSFREE|CLRSCSIPERR);
+		ahc_outb(ahc, CLRINT, CLRSCSIINT);
+		restart_sequencer(ahc);
+	} else if ((status & SELTO) != 0) {
+		u_int scbptr;
+
+		scbptr = ahc_inb(ahc, WAITING_SCBH);
+		ahc_outb(ahc, SCBPTR, scbptr);
+		scb_index = ahc_inb(ahc, SCB_TAG);
+
+		scb = ahc_lookup_scb(ahc, scb_index);
+		if (scb == NULL) {
+			printf("%s: ahc_intr - referenced scb not "
+			       "valid during SELTO scb(%d, %d)\n",
+			       ahc_name(ahc), scbptr, scb_index);
+		} else {
+			ahc_set_transaction_status(scb, CAM_SEL_TIMEOUT);
+			ahc_freeze_devq(ahc, scb);
+		}
+		/* Stop the selection */
+		ahc_outb(ahc, SCSISEQ, 0);
+
+		/* No more pending messages */
+		ahc_clear_msg_state(ahc);
+
+		/* Clear interrupt state */
+		ahc_outb(ahc, CLRSINT1, CLRSELTIMEO|CLRBUSFREE|CLRSCSIPERR);
+
+		/*
+		 * Although the driver does not care about the
+		 * 'Selection in Progress' status bit, the busy
+		 * LED does.  SELINGO is only cleared by a sucessful
+		 * selection, so we must manually clear it to insure
+		 * the LED turns off just incase no future successful
+		 * selections occur (e.g. no devices on the bus).
+		 */
+		ahc_outb(ahc, CLRSINT0, CLRSELINGO);
+
+		ahc_outb(ahc, CLRINT, CLRSCSIINT);
+		restart_sequencer(ahc);
+	} else {
+		panic("%s: Missing case in ahc_handle_scsiint. status = %x\n",
+		      ahc_name(ahc), status);
+	}
+}
+
+#define AHC_MAX_STEPS 2000
+void
+ahc_clear_critical_section(struct ahc_softc *ahc)
+{
+	int	stepping;
+	int	steps;
+	u_int	simode0;
+	u_int	simode1;
+
+	if (ahc->num_critical_sections == 0)
+		return;
+
+	stepping = FALSE;
+	steps = 0;
+	simode0 = 0;
+	simode1 = 0;
+	for (;;) {
+		struct	cs *cs;
+		u_int	seqaddr;
+		u_int	i;
+
+		seqaddr = ahc_inb(ahc, SEQADDR0)
+			| (ahc_inb(ahc, SEQADDR1) << 8);
+
+		cs = ahc->critical_sections;
+		for (i = 0; i < ahc->num_critical_sections; i++, cs++) {
+			
+			if (cs->begin < seqaddr && cs->end >= seqaddr)
+				break;
+		}
+
+		if (i == ahc->num_critical_sections)
+			break;
+
+		if (steps > AHC_MAX_STEPS) {
+			printf("%s: Infinite loop in critical section\n",
+			       ahc_name(ahc));
+			ahc_dump_card_state(ahc);
+			panic("critical section loop");
+		}
+
+		steps++;
+		if (stepping == FALSE) {
+
+			/*
+			 * Disable all interrupt sources so that the
+			 * sequencer will not be stuck by a pausing
+			 * interrupt condition while we attempt to
+			 * leave a critical section.
+			 */
+			simode0 = ahc_inb(ahc, SIMODE0);
+			ahc_outb(ahc, SIMODE0, 0);
+			simode1 = ahc_inb(ahc, SIMODE1);
+			ahc_outb(ahc, SIMODE1, 0);
+			ahc_outb(ahc, CLRINT, CLRSCSIINT);
+			ahc_outb(ahc, SEQCTL, ahc_inb(ahc, SEQCTL) | STEP);
+			stepping = TRUE;
+		}
+		ahc_outb(ahc, HCNTRL, ahc->unpause);
+		do {
+			ahc_delay(200);
+		} while (!sequencer_paused(ahc));
+	}
+	if (stepping) {
+		ahc_outb(ahc, SIMODE0, simode0);
+		ahc_outb(ahc, SIMODE1, simode1);
+		ahc_outb(ahc, SEQCTL, ahc_inb(ahc, SEQCTL) & ~STEP);
+	}
+}
+
+/*
+ * Clear any pending interrupt status.
+ */
+void
+ahc_clear_intstat(struct ahc_softc *ahc)
+{
+	/* Clear any interrupt conditions this may have caused */
+	ahc_outb(ahc, CLRSINT1, CLRSELTIMEO|CLRATNO|CLRSCSIRSTI
+				|CLRBUSFREE|CLRSCSIPERR|CLRPHASECHG|
+				CLRREQINIT);
+	ahc_outb(ahc, CLRSINT0, CLRSELDO|CLRSELDI|CLRSELINGO);
+	ahc_outb(ahc, CLRINT, CLRSCSIINT);
+}
+
+/**************************** Debugging Routines ******************************/
+void
+ahc_print_scb(struct scb *scb)
+{
+	int i;
+
+	struct hardware_scb *hscb = scb->hscb;
+
+	printf("scb:%p control:0x%x scsiid:0x%x lun:%d cdb_len:%d\n",
+	       scb,
+	       hscb->control,
+	       hscb->scsiid,
+	       hscb->lun,
+	       hscb->cdb_len);
+	i = 0;
+	printf("Shared Data: %#02x %#02x %#02x %#02x\n",
+	       hscb->shared_data.cdb[i++],
+	       hscb->shared_data.cdb[i++],
+	       hscb->shared_data.cdb[i++],
+	       hscb->shared_data.cdb[i++]);
+	printf("             %#02x %#02x %#02x %#02x\n",
+	       hscb->shared_data.cdb[i++],
+	       hscb->shared_data.cdb[i++],
+	       hscb->shared_data.cdb[i++],
+	       hscb->shared_data.cdb[i++]);
+	printf("             %#02x %#02x %#02x %#02x\n",
+	       hscb->shared_data.cdb[i++],
+	       hscb->shared_data.cdb[i++],
+	       hscb->shared_data.cdb[i++],
+	       hscb->shared_data.cdb[i++]);
+	printf("        dataptr:%#x datacnt:%#x sgptr:%#x tag:%#x\n",
+		ahc_le32toh(hscb->dataptr),
+		ahc_le32toh(hscb->datacnt),
+		ahc_le32toh(hscb->sgptr),
+		hscb->tag);
+	if (scb->sg_count > 0) {
+		for (i = 0; i < scb->sg_count; i++) {
+			printf("sg[%d] - Addr 0x%x : Length %d\n",
+			       i,
+			       ahc_le32toh(scb->sg_list[i].addr),
+			       ahc_le32toh(scb->sg_list[i].len));
+		}
+	}
+}
+
+/************************* Transfer Negotiation *******************************/
+/*
+ * Allocate per target mode instance (ID we respond to as a target)
+ * transfer negotiation data structures.
+ */
+static struct tmode_tstate *
+ahc_alloc_tstate(struct ahc_softc *ahc, u_int scsi_id, char channel)
+{
+	struct tmode_tstate *master_tstate;
+	struct tmode_tstate *tstate;
+	int i;
+
+	master_tstate = ahc->enabled_targets[ahc->our_id];
+	if (channel == 'B') {
+		scsi_id += 8;
+		master_tstate = ahc->enabled_targets[ahc->our_id_b + 8];
+	}
+	if (ahc->enabled_targets[scsi_id] != NULL
+	 && ahc->enabled_targets[scsi_id] != master_tstate)
+		panic("%s: ahc_alloc_tstate - Target already allocated",
+		      ahc_name(ahc));
+	tstate = malloc(sizeof(*tstate), M_DEVBUF, M_NOWAIT);
+	if (tstate == NULL)
+		return (NULL);
+
+	/*
+	 * If we have allocated a master tstate, copy user settings from
+	 * the master tstate (taken from SRAM or the EEPROM) for this
+	 * channel, but reset our current and goal settings to async/narrow
+	 * until an initiator talks to us.
+	 */
+	if (master_tstate != NULL) {
+		memcpy(tstate, master_tstate, sizeof(*tstate));
+		memset(tstate->enabled_luns, 0, sizeof(tstate->enabled_luns));
+		tstate->ultraenb = 0;
+		for (i = 0; i < 16; i++) {
+			memset(&tstate->transinfo[i].current, 0,
+			      sizeof(tstate->transinfo[i].current));
+			memset(&tstate->transinfo[i].goal, 0,
+			      sizeof(tstate->transinfo[i].goal));
+		}
+	} else
+		memset(tstate, 0, sizeof(*tstate));
+	ahc->enabled_targets[scsi_id] = tstate;
+	return (tstate);
+}
+
+#ifdef AHC_TARGET_MODE
+/*
+ * Free per target mode instance (ID we respond to as a target)
+ * transfer negotiation data structures.
+ */
+static void
+ahc_free_tstate(struct ahc_softc *ahc, u_int scsi_id, char channel, int force)
+{
+	struct tmode_tstate *tstate;
+
+	/* Don't clean up the entry for our initiator role */
+	if ((ahc->flags & AHC_INITIATORROLE) != 0
+	 && ((channel == 'B' && scsi_id == ahc->our_id_b)
+	  || (channel == 'A' && scsi_id == ahc->our_id))
+	 && force == FALSE)
+		return;
+
+	if (channel == 'B')
+		scsi_id += 8;
+	tstate = ahc->enabled_targets[scsi_id];
+	if (tstate != NULL)
+		free(tstate, M_DEVBUF);
+	ahc->enabled_targets[scsi_id] = NULL;
+}
+#endif
+
+/*
+ * Called when we have an active connection to a target on the bus,
+ * this function finds the nearest syncrate to the input period limited
+ * by the capabilities of the bus connectivity of and sync settings for
+ * the target.
+ */
+struct ahc_syncrate *
+ahc_devlimited_syncrate(struct ahc_softc *ahc,
+			struct ahc_initiator_tinfo *tinfo,
+			u_int *period, u_int *ppr_options, role_t role) {
+	struct	ahc_transinfo *transinfo;
+	u_int	maxsync;
+
+	if ((ahc->features & AHC_ULTRA2) != 0) {
+		if ((ahc_inb(ahc, SBLKCTL) & ENAB40) != 0
+		 && (ahc_inb(ahc, SSTAT2) & EXP_ACTIVE) == 0) {
+			maxsync = AHC_SYNCRATE_DT;
+		} else {
+			maxsync = AHC_SYNCRATE_ULTRA;
+			/* Can't do DT on an SE bus */
+			*ppr_options &= ~MSG_EXT_PPR_DT_REQ;
+		}
+	} else if ((ahc->features & AHC_ULTRA) != 0) {
+		maxsync = AHC_SYNCRATE_ULTRA;
+	} else {
+		maxsync = AHC_SYNCRATE_FAST;
+	}
+	/*
+	 * Never allow a value higher than our current goal
+	 * period otherwise we may allow a target initiated
+	 * negotiation to go above the limit as set by the
+	 * user.  In the case of an initiator initiated
+	 * sync negotiation, we limit based on the user
+	 * setting.  This allows the system to still accept
+	 * incoming negotiations even if target initiated
+	 * negotiation is not performed.
+	 */
+	if (role == ROLE_TARGET)
+		transinfo = &tinfo->user;
+	else 
+		transinfo = &tinfo->goal;
+	*ppr_options &= transinfo->ppr_options;
+	if (transinfo->period == 0) {
+		*period = 0;
+		*ppr_options = 0;
+		return (NULL);
+	}
+	*period = MAX(*period, transinfo->period);
+	return (ahc_find_syncrate(ahc, period, ppr_options, maxsync));
+}
+
+/*
+ * Look up the valid period to SCSIRATE conversion in our table.
+ * Return the period and offset that should be sent to the target
+ * if this was the beginning of an SDTR.
+ */
+struct ahc_syncrate *
+ahc_find_syncrate(struct ahc_softc *ahc, u_int *period,
+		  u_int *ppr_options, u_int maxsync)
+{
+	struct ahc_syncrate *syncrate;
+
+	if ((ahc->features & AHC_DT) == 0)
+		*ppr_options &= ~MSG_EXT_PPR_DT_REQ;
+	
+	for (syncrate = &ahc_syncrates[maxsync];
+	     syncrate->rate != NULL;
+	     syncrate++) {
+
+		/*
+		 * The Ultra2 table doesn't go as low
+		 * as for the Fast/Ultra cards.
+		 */
+		if ((ahc->features & AHC_ULTRA2) != 0
+		 && (syncrate->sxfr_u2 == 0))
+			break;
+
+		/* Skip any DT entries if DT is not available */
+		if ((*ppr_options & MSG_EXT_PPR_DT_REQ) == 0
+		 && (syncrate->sxfr_u2 & DT_SXFR) != 0)
+			continue;
+
+		if (*period <= syncrate->period) {
+			/*
+			 * When responding to a target that requests
+			 * sync, the requested rate may fall between
+			 * two rates that we can output, but still be
+			 * a rate that we can receive.  Because of this,
+			 * we want to respond to the target with
+			 * the same rate that it sent to us even
+			 * if the period we use to send data to it
+			 * is lower.  Only lower the response period
+			 * if we must.
+			 */
+			if (syncrate == &ahc_syncrates[maxsync])
+				*period = syncrate->period;
+
+			/*
+			 * At some speeds, we only support
+			 * ST transfers.
+			 */
+		 	if ((syncrate->sxfr_u2 & ST_SXFR) != 0)
+				*ppr_options &= ~MSG_EXT_PPR_DT_REQ;
+			break;
+		}
+	}
+
+	if ((*period == 0)
+	 || (syncrate->rate == NULL)
+	 || ((ahc->features & AHC_ULTRA2) != 0
+	  && (syncrate->sxfr_u2 == 0))) {
+		/* Use asynchronous transfers. */
+		*period = 0;
+		syncrate = NULL;
+		*ppr_options &= ~MSG_EXT_PPR_DT_REQ;
+	}
+	return (syncrate);
+}
+
+/*
+ * Convert from an entry in our syncrate table to the SCSI equivalent
+ * sync "period" factor.
+ */
+u_int
+ahc_find_period(struct ahc_softc *ahc, u_int scsirate, u_int maxsync)
+{
+	struct ahc_syncrate *syncrate;
+
+	if ((ahc->features & AHC_ULTRA2) != 0)
+		scsirate &= SXFR_ULTRA2;
+	else
+		scsirate &= SXFR;
+
+	syncrate = &ahc_syncrates[maxsync];
+	while (syncrate->rate != NULL) {
+
+		if ((ahc->features & AHC_ULTRA2) != 0) {
+			if (syncrate->sxfr_u2 == 0)
+				break;
+			else if (scsirate == (syncrate->sxfr_u2 & SXFR_ULTRA2))
+				return (syncrate->period);
+		} else if (scsirate == (syncrate->sxfr & SXFR)) {
+				return (syncrate->period);
+		}
+		syncrate++;
+	}
+	return (0); /* async */
+}
+
+/*
+ * Truncate the given synchronous offset to a value the
+ * current adapter type and syncrate are capable of.
+ */
+void
+ahc_validate_offset(struct ahc_softc *ahc,
+		    struct ahc_initiator_tinfo *tinfo,
+		    struct ahc_syncrate *syncrate,
+		    u_int *offset, int wide, role_t role)
+{
+	u_int maxoffset;
+
+	/* Limit offset to what we can do */
+	if (syncrate == NULL) {
+		maxoffset = 0;
+	} else if ((ahc->features & AHC_ULTRA2) != 0) {
+		maxoffset = MAX_OFFSET_ULTRA2;
+	} else {
+		if (wide)
+			maxoffset = MAX_OFFSET_16BIT;
+		else
+			maxoffset = MAX_OFFSET_8BIT;
+	}
+	*offset = MIN(*offset, maxoffset);
+	if (tinfo != NULL) {
+		if (role == ROLE_TARGET)
+			*offset = MIN(*offset, tinfo->user.offset);
+		else
+			*offset = MIN(*offset, tinfo->goal.offset);
+	}
+}
+
+/*
+ * Truncate the given transfer width parameter to a value the
+ * current adapter type is capable of.
+ */
+void
+ahc_validate_width(struct ahc_softc *ahc, struct ahc_initiator_tinfo *tinfo,
+		   u_int *bus_width, role_t role)
+{
+	switch (*bus_width) {
+	default:
+		if (ahc->features & AHC_WIDE) {
+			/* Respond Wide */
+			*bus_width = MSG_EXT_WDTR_BUS_16_BIT;
+			break;
+		}
+		/* FALLTHROUGH */
+	case MSG_EXT_WDTR_BUS_8_BIT:
+		*bus_width = MSG_EXT_WDTR_BUS_8_BIT;
+		break;
+	}
+	if (tinfo != NULL) {
+		if (role == ROLE_TARGET)
+			*bus_width = MIN(tinfo->user.width, *bus_width);
+		else
+			*bus_width = MIN(tinfo->goal.width, *bus_width);
+	}
+}
+
+/*
+ * Update the bitmask of targets for which the controller should
+ * negotiate with at the next convenient oportunity.  This currently
+ * means the next time we send the initial identify messages for
+ * a new transaction.
+ */
+void
+ahc_update_target_msg_request(struct ahc_softc *ahc,
+			      struct ahc_devinfo *devinfo,
+			      struct ahc_initiator_tinfo *tinfo,
+			      int force, int paused)
+{
+	u_int targ_msg_req_orig;
+
+	targ_msg_req_orig = ahc->targ_msg_req;
+	if (tinfo->current.period != tinfo->goal.period
+	 || tinfo->current.width != tinfo->goal.width
+	 || tinfo->current.offset != tinfo->goal.offset
+	 || tinfo->current.ppr_options != tinfo->goal.ppr_options
+	 || (force
+	  && (tinfo->goal.period != 0
+	   || tinfo->goal.width != MSG_EXT_WDTR_BUS_8_BIT
+	   || tinfo->goal.ppr_options != 0)))
+		ahc->targ_msg_req |= devinfo->target_mask;
+	else
+		ahc->targ_msg_req &= ~devinfo->target_mask;
+
+	if (ahc->targ_msg_req != targ_msg_req_orig) {
+		/* Update the message request bit for this target */
+		if (!paused)
+			pause_sequencer(ahc);
+
+		ahc_outb(ahc, TARGET_MSG_REQUEST,
+			 ahc->targ_msg_req & 0xFF);
+		ahc_outb(ahc, TARGET_MSG_REQUEST + 1,
+			 (ahc->targ_msg_req >> 8) & 0xFF);
+
+		if (!paused)
+			unpause_sequencer(ahc);
+	}
+}
+
+/*
+ * Update the user/goal/current tables of synchronous negotiation
+ * parameters as well as, in the case of a current or active update,
+ * any data structures on the host controller.  In the case of an
+ * active update, the specified target is currently talking to us on
+ * the bus, so the transfer parameter update must take effect
+ * immediately.
+ */
+void
+ahc_set_syncrate(struct ahc_softc *ahc, struct ahc_devinfo *devinfo,
+		 struct ahc_syncrate *syncrate, u_int period,
+		 u_int offset, u_int ppr_options, u_int type, int paused)
+{
+	struct	ahc_initiator_tinfo *tinfo;
+	struct	tmode_tstate *tstate;
+	u_int	old_period;
+	u_int	old_offset;
+	u_int	old_ppr;
+	int	active = (type & AHC_TRANS_ACTIVE) == AHC_TRANS_ACTIVE;
+
+	if (syncrate == NULL) {
+		period = 0;
+		offset = 0;
+	}
+
+	tinfo = ahc_fetch_transinfo(ahc, devinfo->channel, devinfo->our_scsiid,
+				    devinfo->target, &tstate);
+	old_period = tinfo->current.period;
+	old_offset = tinfo->current.offset;
+	old_ppr	   = tinfo->current.ppr_options;
+
+	if ((type & AHC_TRANS_CUR) != 0
+	 && (old_period != period
+	  || old_offset != offset
+	  || old_ppr != ppr_options)) {
+		u_int	scsirate;
+
+		scsirate = tinfo->scsirate;
+		if ((ahc->features & AHC_ULTRA2) != 0) {
+
+			scsirate &= ~(SXFR_ULTRA2|SINGLE_EDGE|ENABLE_CRC);
+			if (syncrate != NULL) {
+				scsirate |= syncrate->sxfr_u2;
+				if ((ppr_options & MSG_EXT_PPR_DT_REQ) != 0)
+					scsirate |= ENABLE_CRC;
+				else
+					scsirate |= SINGLE_EDGE;
+			}
+		} else {
+
+			scsirate &= ~(SXFR|SOFS);
+			/*
+			 * Ensure Ultra mode is set properly for
+			 * this target.
+			 */
+			tstate->ultraenb &= ~devinfo->target_mask;
+			if (syncrate != NULL) {
+				if (syncrate->sxfr & ULTRA_SXFR) {
+					tstate->ultraenb |=
+						devinfo->target_mask;
+				}
+				scsirate |= syncrate->sxfr & SXFR;
+				scsirate |= offset & SOFS;
+			}
+			if (active) {
+				u_int sxfrctl0;
+
+				sxfrctl0 = ahc_inb(ahc, SXFRCTL0);
+				sxfrctl0 &= ~FAST20;
+				if (tstate->ultraenb & devinfo->target_mask)
+					sxfrctl0 |= FAST20;
+				ahc_outb(ahc, SXFRCTL0, sxfrctl0);
+			}
+		}
+		if (active) {
+			ahc_outb(ahc, SCSIRATE, scsirate);
+			if ((ahc->features & AHC_ULTRA2) != 0)
+				ahc_outb(ahc, SCSIOFFSET, offset);
+		}
+
+		tinfo->scsirate = scsirate;
+		tinfo->current.period = period;
+		tinfo->current.offset = offset;
+		tinfo->current.ppr_options = ppr_options;
+
+		/* Update the syncrates in any pending scbs */
+		ahc_update_pending_syncrates(ahc);
+
+		ahc_send_async(ahc, devinfo->channel, devinfo->target,
+			       CAM_LUN_WILDCARD, AC_TRANSFER_NEG);
+		if (bootverbose) {
+			if (offset != 0) {
+				printf("%s: target %d synchronous at %sMHz%s, "
+				       "offset = 0x%x\n", ahc_name(ahc),
+				       devinfo->target, syncrate->rate,
+				       (ppr_options & MSG_EXT_PPR_DT_REQ)
+				       ? " DT" : "", offset);
+			} else {
+				printf("%s: target %d using "
+				       "asynchronous transfers\n",
+				       ahc_name(ahc), devinfo->target);
+			}
+		}
+	}
+
+	if ((type & AHC_TRANS_GOAL) != 0) {
+		tinfo->goal.period = period;
+		tinfo->goal.offset = offset;
+		tinfo->goal.ppr_options = ppr_options;
+	}
+
+	if ((type & AHC_TRANS_USER) != 0) {
+		tinfo->user.period = period;
+		tinfo->user.offset = offset;
+		tinfo->user.ppr_options = ppr_options;
+	}
+
+	ahc_update_target_msg_request(ahc, devinfo, tinfo,
+				      /*force*/FALSE,
+				      paused);
+}
+
+/*
+ * Update the user/goal/current tables of wide negotiation
+ * parameters as well as, in the case of a current or active update,
+ * any data structures on the host controller.  In the case of an
+ * active update, the specified target is currently talking to us on
+ * the bus, so the transfer parameter update must take effect
+ * immediately.
+ */
+void
+ahc_set_width(struct ahc_softc *ahc, struct ahc_devinfo *devinfo,
+	      u_int width, u_int type, int paused)
+{
+	struct ahc_initiator_tinfo *tinfo;
+	struct tmode_tstate *tstate;
+	u_int  oldwidth;
+	int    active = (type & AHC_TRANS_ACTIVE) == AHC_TRANS_ACTIVE;
+
+	tinfo = ahc_fetch_transinfo(ahc, devinfo->channel, devinfo->our_scsiid,
+				    devinfo->target, &tstate);
+	oldwidth = tinfo->current.width;
+
+	if ((type & AHC_TRANS_CUR) != 0 && oldwidth != width) {
+		u_int	scsirate;
+
+		scsirate =  tinfo->scsirate;
+		scsirate &= ~WIDEXFER;
+		if (width == MSG_EXT_WDTR_BUS_16_BIT)
+			scsirate |= WIDEXFER;
+
+		tinfo->scsirate = scsirate;
+
+		if (active)
+			ahc_outb(ahc, SCSIRATE, scsirate);
+
+		tinfo->current.width = width;
+
+		ahc_send_async(ahc, devinfo->channel, devinfo->target,
+			       CAM_LUN_WILDCARD, AC_TRANSFER_NEG);
+		if (bootverbose) {
+			printf("%s: target %d using %dbit transfers\n",
+			       ahc_name(ahc), devinfo->target,
+			       8 * (0x01 << width));
+		}
+	}
+	if ((type & AHC_TRANS_GOAL) != 0)
+		tinfo->goal.width = width;
+	if ((type & AHC_TRANS_USER) != 0)
+		tinfo->user.width = width;
+
+	ahc_update_target_msg_request(ahc, devinfo, tinfo,
+				      /*force*/FALSE, paused);
+}
+
+/*
+ * Update the current state of tagged queuing for a given target.
+ */
+void
+ahc_set_tags(struct ahc_softc *ahc, struct ahc_devinfo *devinfo, int enable)
+{
+	struct ahc_initiator_tinfo *tinfo;
+	struct tmode_tstate *tstate;
+	uint16_t orig_tagenable;
+
+	tinfo = ahc_fetch_transinfo(ahc, devinfo->channel, devinfo->our_scsiid,
+				    devinfo->target, &tstate);
+
+	orig_tagenable = tstate->tagenable;
+	if (enable)
+		tstate->tagenable |= devinfo->target_mask;
+	else
+		tstate->tagenable &= ~devinfo->target_mask;
+
+	if (orig_tagenable != tstate->tagenable) {
+		ahc_platform_set_tags(ahc, devinfo, enable);
+		ahc_send_async(ahc, devinfo->channel, devinfo->target,
+			       devinfo->lun, AC_TRANSFER_NEG);
+	}
+
+}
+
+/*
+ * When the transfer settings for a connection change, update any
+ * in-transit SCBs to contain the new data so the hardware will
+ * be set correctly during future (re)selections.
+ */
+static void
+ahc_update_pending_syncrates(struct ahc_softc *ahc)
+{
+	struct	scb *pending_scb;
+	int	pending_scb_count;
+	int	i;
+	u_int	saved_scbptr;
+
+	/*
+	 * Traverse the pending SCB list and ensure that all of the
+	 * SCBs there have the proper settings.
+	 */
+	pending_scb_count = 0;
+	LIST_FOREACH(pending_scb, &ahc->pending_scbs, pending_links) {
+		struct ahc_devinfo devinfo;
+		struct hardware_scb *pending_hscb;
+		struct ahc_initiator_tinfo *tinfo;
+		struct tmode_tstate *tstate;
+		
+		ahc_scb_devinfo(ahc, &devinfo, pending_scb);
+		tinfo = ahc_fetch_transinfo(ahc, devinfo.channel,
+					    devinfo.our_scsiid,
+					    devinfo.target, &tstate);
+		pending_hscb = pending_scb->hscb;
+		pending_hscb->control &= ~ULTRAENB;
+		if ((tstate->ultraenb & devinfo.target_mask) != 0)
+			pending_hscb->control |= ULTRAENB;
+		pending_hscb->scsirate = tinfo->scsirate;
+		pending_hscb->scsioffset = tinfo->current.offset;
+		pending_scb_count++;
+	}
+
+	if (pending_scb_count == 0)
+		return;
+
+	saved_scbptr = ahc_inb(ahc, SCBPTR);
+	/* Ensure that the hscbs down on the card match the new information */
+	for (i = 0; i < ahc->scb_data->maxhscbs; i++) {
+		struct	hardware_scb *pending_hscb;
+		u_int	control;
+		u_int	scb_tag;
+
+		ahc_outb(ahc, SCBPTR, i);
+		scb_tag = ahc_inb(ahc, SCB_TAG);
+		pending_scb = ahc_lookup_scb(ahc, scb_tag);
+		if (pending_scb == NULL)
+			continue;
+
+		pending_hscb = pending_scb->hscb;
+		control = ahc_inb(ahc, SCB_CONTROL);
+		control &= ~ULTRAENB;
+		if ((pending_hscb->control & ULTRAENB) != 0)
+			control |= ULTRAENB;
+		ahc_outb(ahc, SCB_CONTROL, control);
+		ahc_outb(ahc, SCB_SCSIRATE, pending_hscb->scsirate);
+		ahc_outb(ahc, SCB_SCSIOFFSET, pending_hscb->scsioffset);
+	}
+	ahc_outb(ahc, SCBPTR, saved_scbptr);
+}
+
+/**************************** Pathing Information *****************************/
+static void
+ahc_fetch_devinfo(struct ahc_softc *ahc, struct ahc_devinfo *devinfo)
+{
+	u_int	saved_scsiid;
+	role_t	role;
+	int	our_id;
+
+	if (ahc_inb(ahc, SSTAT0) & TARGET)
+		role = ROLE_TARGET;
+	else
+		role = ROLE_INITIATOR;
+
+	if (role == ROLE_TARGET
+	 && (ahc->features & AHC_MULTI_TID) != 0
+	 && (ahc_inb(ahc, SEQ_FLAGS) & CMDPHASE_PENDING) != 0) {
+		/* We were selected, so pull our id from TARGIDIN */
+		our_id = ahc_inb(ahc, TARGIDIN) & OID;
+	} else if ((ahc->features & AHC_ULTRA2) != 0)
+		our_id = ahc_inb(ahc, SCSIID_ULTRA2) & OID;
+	else
+		our_id = ahc_inb(ahc, SCSIID) & OID;
+
+	saved_scsiid = ahc_inb(ahc, SAVED_SCSIID);
+	ahc_compile_devinfo(devinfo,
+			    our_id,
+			    SCSIID_TARGET(ahc, saved_scsiid),
+			    ahc_inb(ahc, SAVED_LUN),
+			    SCSIID_CHANNEL(ahc, saved_scsiid),
+			    role);
+}
+
+void
+ahc_compile_devinfo(struct ahc_devinfo *devinfo, u_int our_id, u_int target,
+		    u_int lun, char channel, role_t role)
+{
+	devinfo->our_scsiid = our_id;
+	devinfo->target = target;
+	devinfo->lun = lun;
+	devinfo->target_offset = target;
+	devinfo->channel = channel;
+	devinfo->role = role;
+	if (channel == 'B')
+		devinfo->target_offset += 8;
+	devinfo->target_mask = (0x01 << devinfo->target_offset);
+}
+
+static void
+ahc_scb_devinfo(struct ahc_softc *ahc, struct ahc_devinfo *devinfo,
+		struct scb *scb)
+{
+	role_t	role;
+	int	our_id;
+
+	our_id = SCSIID_OUR_ID(scb->hscb->scsiid);
+	role = ROLE_INITIATOR;
+	if ((scb->hscb->control & TARGET_SCB) != 0)
+		role = ROLE_TARGET;
+	ahc_compile_devinfo(devinfo, our_id, SCB_GET_TARGET(ahc, scb),
+			    SCB_GET_LUN(scb), SCB_GET_CHANNEL(ahc, scb), role);
+}
+
+
+/************************ Message Phase Processing ****************************/
+/*
+ * When an initiator transaction with the MK_MESSAGE flag either reconnects
+ * or enters the initial message out phase, we are interrupted.  Fill our
+ * outgoing message buffer with the appropriate message and beging handing
+ * the message phase(s) manually.
+ */
+static void
+ahc_setup_initiator_msgout(struct ahc_softc *ahc, struct ahc_devinfo *devinfo,
+			   struct scb *scb)
+{
+	/*
+	 * To facilitate adding multiple messages together,
+	 * each routine should increment the index and len
+	 * variables instead of setting them explicitly.
+	 */
+	ahc->msgout_index = 0;
+	ahc->msgout_len = 0;
+
+	if ((scb->flags & SCB_DEVICE_RESET) == 0
+	 && ahc_inb(ahc, MSG_OUT) == MSG_IDENTIFYFLAG) {
+		u_int identify_msg;
+
+		identify_msg = MSG_IDENTIFYFLAG | SCB_GET_LUN(scb);
+		if ((scb->hscb->control & DISCENB) != 0)
+			identify_msg |= MSG_IDENTIFY_DISCFLAG;
+		ahc->msgout_buf[ahc->msgout_index++] = identify_msg;
+		ahc->msgout_len++;
+
+		if ((scb->hscb->control & TAG_ENB) != 0) {
+			ahc->msgout_buf[ahc->msgout_index++] =
+			    scb->hscb->control & (TAG_ENB|SCB_TAG_TYPE);
+			ahc->msgout_buf[ahc->msgout_index++] = scb->hscb->tag;
+			ahc->msgout_len += 2;
+		}
+	}
+
+	if (scb->flags & SCB_DEVICE_RESET) {
+		ahc->msgout_buf[ahc->msgout_index++] = MSG_BUS_DEV_RESET;
+		ahc->msgout_len++;
+		ahc_print_path(ahc, scb);
+		printf("Bus Device Reset Message Sent\n");
+	} else if ((scb->flags & SCB_ABORT) != 0) {
+		if ((scb->hscb->control & TAG_ENB) != 0)
+			ahc->msgout_buf[ahc->msgout_index++] = MSG_ABORT_TAG;
+		else
+			ahc->msgout_buf[ahc->msgout_index++] = MSG_ABORT;
+		ahc->msgout_len++;
+		ahc_print_path(ahc, scb);
+		printf("Abort Message Sent\n");
+	} else if ((ahc->targ_msg_req & devinfo->target_mask) != 0
+		|| (scb->flags & SCB_NEGOTIATE) != 0) {
+		ahc_build_transfer_msg(ahc, devinfo);
+	} else {
+		printf("ahc_intr: AWAITING_MSG for an SCB that "
+		       "does not have a waiting message\n");
+		printf("SCSIID = %x, target_mask = %x\n", scb->hscb->scsiid,
+		       devinfo->target_mask);
+		panic("SCB = %d, SCB Control = %x, MSG_OUT = %x "
+		      "SCB flags = %x", scb->hscb->tag, scb->hscb->control,
+		      ahc_inb(ahc, MSG_OUT), scb->flags);
+	}
+
+	/*
+	 * Clear the MK_MESSAGE flag from the SCB so we aren't
+	 * asked to send this message again.
+	 */
+	ahc_outb(ahc, SCB_CONTROL, ahc_inb(ahc, SCB_CONTROL) & ~MK_MESSAGE);
+	ahc->msgout_index = 0;
+	ahc->msg_type = MSG_TYPE_INITIATOR_MSGOUT;
+}
+
+/*
+ * Build an appropriate transfer negotiation message for the
+ * currently active target.
+ */
+static void
+ahc_build_transfer_msg(struct ahc_softc *ahc, struct ahc_devinfo *devinfo)
+{
+	/*
+	 * We need to initiate transfer negotiations.
+	 * If our current and goal settings are identical,
+	 * we want to renegotiate due to a check condition.
+	 */
+	struct	ahc_initiator_tinfo *tinfo;
+	struct	tmode_tstate *tstate;
+	struct	ahc_syncrate *rate;
+	int	dowide;
+	int	dosync;
+	int	doppr;
+	int	use_ppr;
+	u_int	period;
+	u_int	ppr_options;
+	u_int	offset;
+
+	tinfo = ahc_fetch_transinfo(ahc, devinfo->channel, devinfo->our_scsiid,
+				    devinfo->target, &tstate);
+	dowide = tinfo->current.width != tinfo->goal.width;
+	dosync = tinfo->current.period != tinfo->goal.period;
+	doppr = tinfo->current.ppr_options != tinfo->goal.ppr_options;
+
+	if (!dowide && !dosync && !doppr) {
+		dowide = tinfo->goal.width != MSG_EXT_WDTR_BUS_8_BIT;
+		dosync = tinfo->goal.period != 0;
+		doppr = tinfo->goal.ppr_options != 0;
+	}
+
+	if (!dowide && !dosync && !doppr) {
+		panic("ahc_intr: AWAITING_MSG for negotiation, "
+		      "but no negotiation needed\n");	
+	}
+
+	use_ppr = (tinfo->current.transport_version >= 3) || doppr;
+	/* Target initiated PPR is not allowed in the SCSI spec */
+	if (devinfo->role == ROLE_TARGET)
+		use_ppr = 0;
+
+	/*
+	 * Both the PPR message and SDTR message require the
+	 * goal syncrate to be limited to what the target device
+	 * is capable of handling (based on whether an LVD->SE
+	 * expander is on the bus), so combine these two cases.
+	 * Regardless, guarantee that if we are using WDTR and SDTR
+	 * messages that WDTR comes first.
+	 */
+	if (use_ppr || (dosync && !dowide)) {
+
+		period = tinfo->goal.period;
+		ppr_options = tinfo->goal.ppr_options;
+		if (use_ppr == 0)
+			ppr_options = 0;
+		rate = ahc_devlimited_syncrate(ahc, tinfo, &period,
+					       &ppr_options, devinfo->role);
+		offset = tinfo->goal.offset;
+		ahc_validate_offset(ahc, tinfo, rate, &offset,
+				    use_ppr ? tinfo->goal.width
+					    : tinfo->current.width,
+				    devinfo->role);
+		if (use_ppr) {
+			ahc_construct_ppr(ahc, devinfo, period, offset,
+					  tinfo->goal.width, ppr_options);
+		} else {
+			ahc_construct_sdtr(ahc, devinfo, period, offset);
+		}
+	} else {
+		ahc_construct_wdtr(ahc, devinfo, tinfo->goal.width);
+	}
+}
+
+/*
+ * Build a synchronous negotiation message in our message
+ * buffer based on the input parameters.
+ */
+static void
+ahc_construct_sdtr(struct ahc_softc *ahc, struct ahc_devinfo *devinfo,
+		   u_int period, u_int offset)
+{
+	ahc->msgout_buf[ahc->msgout_index++] = MSG_EXTENDED;
+	ahc->msgout_buf[ahc->msgout_index++] = MSG_EXT_SDTR_LEN;
+	ahc->msgout_buf[ahc->msgout_index++] = MSG_EXT_SDTR;
+	ahc->msgout_buf[ahc->msgout_index++] = period;
+	ahc->msgout_buf[ahc->msgout_index++] = offset;
+	ahc->msgout_len += 5;
+	if (bootverbose) {
+		printf("(%s:%c:%d:%d): Sending SDTR period %x, offset %x\n",
+		       ahc_name(ahc), devinfo->channel, devinfo->target,
+		       devinfo->lun, period, offset);
+	}
+}
+
+/*
+ * Build a wide negotiateion message in our message
+ * buffer based on the input parameters.
+ */
+static void
+ahc_construct_wdtr(struct ahc_softc *ahc, struct ahc_devinfo *devinfo,
+		   u_int bus_width)
+{
+	ahc->msgout_buf[ahc->msgout_index++] = MSG_EXTENDED;
+	ahc->msgout_buf[ahc->msgout_index++] = MSG_EXT_WDTR_LEN;
+	ahc->msgout_buf[ahc->msgout_index++] = MSG_EXT_WDTR;
+	ahc->msgout_buf[ahc->msgout_index++] = bus_width;
+	ahc->msgout_len += 4;
+	if (bootverbose) {
+		printf("(%s:%c:%d:%d): Sending WDTR %x\n",
+		       ahc_name(ahc), devinfo->channel, devinfo->target,
+		       devinfo->lun, bus_width);
+	}
+}
+
+/*
+ * Build a parallel protocol request message in our message
+ * buffer based on the input parameters.
+ */
+static void
+ahc_construct_ppr(struct ahc_softc *ahc, struct ahc_devinfo *devinfo,
+		  u_int period, u_int offset, u_int bus_width,
+		  u_int ppr_options)
+{
+	ahc->msgout_buf[ahc->msgout_index++] = MSG_EXTENDED;
+	ahc->msgout_buf[ahc->msgout_index++] = MSG_EXT_PPR_LEN;
+	ahc->msgout_buf[ahc->msgout_index++] = MSG_EXT_PPR;
+	ahc->msgout_buf[ahc->msgout_index++] = period;
+	ahc->msgout_buf[ahc->msgout_index++] = 0;
+	ahc->msgout_buf[ahc->msgout_index++] = offset;
+	ahc->msgout_buf[ahc->msgout_index++] = bus_width;
+	ahc->msgout_buf[ahc->msgout_index++] = ppr_options;
+	ahc->msgout_len += 8;
+	if (bootverbose) {
+		printf("(%s:%c:%d:%d): Sending PPR bus_width %x, period %x, "
+		       "offset %x, ppr_options %x\n", ahc_name(ahc),
+		       devinfo->channel, devinfo->target, devinfo->lun,
+		       bus_width, period, offset, ppr_options);
+	}
+}
+
+/*
+ * Clear any active message state.
+ */
+static void
+ahc_clear_msg_state(struct ahc_softc *ahc)
+{
+	ahc->msgout_len = 0;
+	ahc->msgin_index = 0;
+	ahc->msg_type = MSG_TYPE_NONE;
+	if ((ahc_inb(ahc, SCSISIGI) & ATNI) == 0) {
+		/*
+		 * The target didn't care to respond to our
+		 * message request, so clear ATN.
+		 */
+		ahc_outb(ahc, CLRSINT1, CLRATNO);
+	}
+	ahc_outb(ahc, MSG_OUT, MSG_NOOP);
+}
+
+/*
+ * Manual message loop handler.
+ */
+static void
+ahc_handle_message_phase(struct ahc_softc *ahc)
+{ 
+	struct	ahc_devinfo devinfo;
+	u_int	bus_phase;
+	int	end_session;
+
+	ahc_fetch_devinfo(ahc, &devinfo);
+	end_session = FALSE;
+	bus_phase = ahc_inb(ahc, SCSISIGI) & PHASE_MASK;
+
+reswitch:
+	switch (ahc->msg_type) {
+	case MSG_TYPE_INITIATOR_MSGOUT:
+	{
+		int lastbyte;
+		int phasemis;
+		int msgdone;
+
+		if (ahc->msgout_len == 0)
+			panic("HOST_MSG_LOOP interrupt with no active message");
+
+		phasemis = bus_phase != P_MESGOUT;
+		if (phasemis) {
+			if (bus_phase == P_MESGIN) {
+				/*
+				 * Change gears and see if
+				 * this messages is of interest to
+				 * us or should be passed back to
+				 * the sequencer.
+				 */
+				ahc_outb(ahc, CLRSINT1, CLRATNO);
+				ahc->send_msg_perror = FALSE;
+				ahc->msg_type = MSG_TYPE_INITIATOR_MSGIN;
+				ahc->msgin_index = 0;
+				goto reswitch;
+			}
+			end_session = TRUE;
+			break;
+		}
+
+		if (ahc->send_msg_perror) {
+			ahc_outb(ahc, CLRSINT1, CLRATNO);
+			ahc_outb(ahc, CLRSINT1, CLRREQINIT);
+			ahc_outb(ahc, SCSIDATL, MSG_PARITY_ERROR);
+			break;
+		}
+
+		msgdone	= ahc->msgout_index == ahc->msgout_len;
+		if (msgdone) {
+			/*
+			 * The target has requested a retry.
+			 * Re-assert ATN, reset our message index to
+			 * 0, and try again.
+			 */
+			ahc->msgout_index = 0;
+			ahc_outb(ahc, SCSISIGO, ahc_inb(ahc, SCSISIGO) | ATNO);
+		}
+
+		lastbyte = ahc->msgout_index == (ahc->msgout_len - 1);
+		if (lastbyte) {
+			/* Last byte is signified by dropping ATN */
+			ahc_outb(ahc, CLRSINT1, CLRATNO);
+		}
+
+		/*
+		 * Clear our interrupt status and present
+		 * the next byte on the bus.
+		 */
+		ahc_outb(ahc, CLRSINT1, CLRREQINIT);
+		ahc_outb(ahc, SCSIDATL, ahc->msgout_buf[ahc->msgout_index++]);
+		break;
+	}
+	case MSG_TYPE_INITIATOR_MSGIN:
+	{
+		int phasemis;
+		int message_done;
+
+		phasemis = bus_phase != P_MESGIN;
+
+		if (phasemis) {
+			ahc->msgin_index = 0;
+			if (bus_phase == P_MESGOUT
+			 && (ahc->send_msg_perror == TRUE
+			  || (ahc->msgout_len != 0
+			   && ahc->msgout_index == 0))) {
+				ahc->msg_type = MSG_TYPE_INITIATOR_MSGOUT;
+				goto reswitch;
+			}
+			end_session = TRUE;
+			break;
+		}
+
+		/* Pull the byte in without acking it */
+		ahc->msgin_buf[ahc->msgin_index] = ahc_inb(ahc, SCSIBUSL);
+
+		message_done = ahc_parse_msg(ahc, &devinfo);
+
+		if (message_done) {
+			/*
+			 * Clear our incoming message buffer in case there
+			 * is another message following this one.
+			 */
+			ahc->msgin_index = 0;
+
+			/*
+			 * If this message illicited a response,
+			 * assert ATN so the target takes us to the
+			 * message out phase.
+			 */
+			if (ahc->msgout_len != 0)
+				ahc_outb(ahc, SCSISIGO,
+					 ahc_inb(ahc, SCSISIGO) | ATNO);
+		} else 
+			ahc->msgin_index++;
+
+		/* Ack the byte */
+		ahc_outb(ahc, CLRSINT1, CLRREQINIT);
+		ahc_inb(ahc, SCSIDATL);
+		break;
+	}
+	case MSG_TYPE_TARGET_MSGIN:
+	{
+		int msgdone;
+		int msgout_request;
+
+		if (ahc->msgout_len == 0)
+			panic("Target MSGIN with no active message");
+
+		/*
+		 * If we interrupted a mesgout session, the initiator
+		 * will not know this until our first REQ.  So, we
+		 * only honor mesgout requests after we've sent our
+		 * first byte.
+		 */
+		if ((ahc_inb(ahc, SCSISIGI) & ATNI) != 0
+		 && ahc->msgout_index > 0)
+			msgout_request = TRUE;
+		else
+			msgout_request = FALSE;
+
+		if (msgout_request) {
+
+			/*
+			 * Change gears and see if
+			 * this messages is of interest to
+			 * us or should be passed back to
+			 * the sequencer.
+			 */
+			ahc->msg_type = MSG_TYPE_TARGET_MSGOUT;
+			ahc_outb(ahc, SCSISIGO, P_MESGOUT | BSYO);
+			ahc->msgin_index = 0;
+			/* Dummy read to REQ for first byte */
+			ahc_inb(ahc, SCSIDATL);
+			ahc_outb(ahc, SXFRCTL0,
+				 ahc_inb(ahc, SXFRCTL0) | SPIOEN);
+			break;
+		}
+
+		msgdone = ahc->msgout_index == ahc->msgout_len;
+		if (msgdone) {
+			ahc_outb(ahc, SXFRCTL0,
+				 ahc_inb(ahc, SXFRCTL0) & ~SPIOEN);
+			end_session = TRUE;
+			break;
+		}
+
+		/*
+		 * Present the next byte on the bus.
+		 */
+		ahc_outb(ahc, SXFRCTL0, ahc_inb(ahc, SXFRCTL0) | SPIOEN);
+		ahc_outb(ahc, SCSIDATL, ahc->msgout_buf[ahc->msgout_index++]);
+		break;
+	}
+	case MSG_TYPE_TARGET_MSGOUT:
+	{
+		int lastbyte;
+		int msgdone;
+
+		/*
+		 * The initiator signals that this is
+		 * the last byte by dropping ATN.
+		 */
+		lastbyte = (ahc_inb(ahc, SCSISIGI) & ATNI) == 0;
+
+		/*
+		 * Read the latched byte, but turn off SPIOEN first
+		 * so that we don't inadvertantly cause a REQ for the
+		 * next byte.
+		 */
+		ahc_outb(ahc, SXFRCTL0, ahc_inb(ahc, SXFRCTL0) & ~SPIOEN);
+		ahc->msgin_buf[ahc->msgin_index] = ahc_inb(ahc, SCSIDATL);
+		msgdone = ahc_parse_msg(ahc, &devinfo);
+		if (msgdone == MSGLOOP_TERMINATED) {
+			/*
+			 * The message is *really* done in that it caused
+			 * us to go to bus free.  The sequencer has already
+			 * been reset at this point, so pull the ejection
+			 * handle.
+			 */
+			return;
+		}
+		
+		ahc->msgin_index++;
+
+		/*
+		 * XXX Read spec about initiator dropping ATN too soon
+		 *     and use msgdone to detect it.
+		 */
+		if (msgdone == MSGLOOP_MSGCOMPLETE) {
+			ahc->msgin_index = 0;
+
+			/*
+			 * If this message illicited a response, transition
+			 * to the Message in phase and send it.
+			 */
+			if (ahc->msgout_len != 0) {
+				ahc_outb(ahc, SCSISIGO, P_MESGIN | BSYO);
+				ahc_outb(ahc, SXFRCTL0,
+					 ahc_inb(ahc, SXFRCTL0) | SPIOEN);
+				ahc->msg_type = MSG_TYPE_TARGET_MSGIN;
+				ahc->msgin_index = 0;
+				break;
+			}
+		}
+
+		if (lastbyte)
+			end_session = TRUE;
+		else {
+			/* Ask for the next byte. */
+			ahc_outb(ahc, SXFRCTL0,
+				 ahc_inb(ahc, SXFRCTL0) | SPIOEN);
+		}
+
+		break;
+	}
+	default:
+		panic("Unknown REQINIT message type");
+	}
+
+	if (end_session) {
+		ahc_clear_msg_state(ahc);
+		ahc_outb(ahc, RETURN_1, EXIT_MSG_LOOP);
+	} else
+		ahc_outb(ahc, RETURN_1, CONT_MSG_LOOP);
+}
+
+/*
+ * See if we sent a particular extended message to the target.
+ * If "full" is true, return true only if the target saw the full
+ * message.  If "full" is false, return true if the target saw at
+ * least the first byte of the message.
+ */
+static int
+ahc_sent_msg(struct ahc_softc *ahc, ahc_msgtype type, u_int msgval, int full)
+{
+	int found;
+	u_int index;
+
+	found = FALSE;
+	index = 0;
+
+	while (index < ahc->msgout_len) {
+		if (ahc->msgout_buf[index] == MSG_EXTENDED) {
+			u_int end_index;
+
+			end_index = index + 1 + ahc->msgout_buf[index + 1];
+			if (ahc->msgout_buf[index+2] == msgval
+			 && type == AHCMSG_EXT) {
+
+				if (full) {
+					if (ahc->msgout_index > end_index)
+						found = TRUE;
+				} else if (ahc->msgout_index > index)
+					found = TRUE;
+			}
+			index = end_index;
+		} else if (ahc->msgout_buf[index] >= MSG_SIMPLE_Q_TAG
+			&& ahc->msgout_buf[index] <= MSG_IGN_WIDE_RESIDUE) {
+
+			/* Skip tag type and tag id or residue param*/
+			index += 2;
+		} else {
+			/* Single byte message */
+			if (type == AHCMSG_1B
+			 && ahc->msgout_buf[index] == msgval
+			 && ahc->msgout_index > index)
+				found = TRUE;
+			index++;
+		}
+
+		if (found)
+			break;
+	}
+	return (found);
+}
+
+/*
+ * Wait for a complete incomming message, parse it, and respond accordingly.
+ */
+static int
+ahc_parse_msg(struct ahc_softc *ahc, struct ahc_devinfo *devinfo)
+{
+	struct	ahc_initiator_tinfo *tinfo;
+	struct	tmode_tstate *tstate;
+	int	reject;
+	int	done;
+	int	response;
+	u_int	targ_scsirate;
+
+	done = MSGLOOP_IN_PROG;
+	response = FALSE;
+	reject = FALSE;
+	tinfo = ahc_fetch_transinfo(ahc, devinfo->channel, devinfo->our_scsiid,
+				    devinfo->target, &tstate);
+	targ_scsirate = tinfo->scsirate;
+
+	/*
+	 * Parse as much of the message as is availible,
+	 * rejecting it if we don't support it.  When
+	 * the entire message is availible and has been
+	 * handled, return MSGLOOP_MSGCOMPLETE, indicating
+	 * that we have parsed an entire message.
+	 *
+	 * In the case of extended messages, we accept the length
+	 * byte outright and perform more checking once we know the
+	 * extended message type.
+	 */
+	switch (ahc->msgin_buf[0]) {
+	case MSG_MESSAGE_REJECT:
+		response = ahc_handle_msg_reject(ahc, devinfo);
+		/* FALLTHROUGH */
+	case MSG_NOOP:
+		done = MSGLOOP_MSGCOMPLETE;
+		break;
+	case MSG_EXTENDED:
+	{
+		/* Wait for enough of the message to begin validation */
+		if (ahc->msgin_index < 2)
+			break;
+		switch (ahc->msgin_buf[2]) {
+		case MSG_EXT_SDTR:
+		{
+			struct	 ahc_syncrate *syncrate;
+			u_int	 period;
+			u_int	 ppr_options;
+			u_int	 offset;
+			u_int	 saved_offset;
+			
+			if (ahc->msgin_buf[1] != MSG_EXT_SDTR_LEN) {
+				reject = TRUE;
+				break;
+			}
+
+			/*
+			 * Wait until we have both args before validating
+			 * and acting on this message.
+			 *
+			 * Add one to MSG_EXT_SDTR_LEN to account for
+			 * the extended message preamble.
+			 */
+			if (ahc->msgin_index < (MSG_EXT_SDTR_LEN + 1))
+				break;
+
+			period = ahc->msgin_buf[3];
+			ppr_options = 0;
+			saved_offset = offset = ahc->msgin_buf[4];
+			syncrate = ahc_devlimited_syncrate(ahc, tinfo, &period,
+							   &ppr_options,
+							   devinfo->role);
+			ahc_validate_offset(ahc, tinfo, syncrate, &offset,
+					    targ_scsirate & WIDEXFER,
+					    devinfo->role);
+			if (bootverbose) {
+				printf("(%s:%c:%d:%d): Received "
+				       "SDTR period %x, offset %x\n\t"
+				       "Filtered to period %x, offset %x\n",
+				       ahc_name(ahc), devinfo->channel,
+				       devinfo->target, devinfo->lun,
+				       ahc->msgin_buf[3], saved_offset,
+				       period, offset);
+			}
+			ahc_set_syncrate(ahc, devinfo, 
+					 syncrate, period,
+					 offset, ppr_options,
+					 AHC_TRANS_ACTIVE|AHC_TRANS_GOAL,
+					 /*paused*/TRUE);
+
+			/*
+			 * See if we initiated Sync Negotiation
+			 * and didn't have to fall down to async
+			 * transfers.
+			 */
+			if (ahc_sent_msg(ahc, AHCMSG_EXT, MSG_EXT_SDTR, TRUE)) {
+				/* We started it */
+				if (saved_offset != offset) {
+					/* Went too low - force async */
+					reject = TRUE;
+				}
+			} else {
+				/*
+				 * Send our own SDTR in reply
+				 */
+				if (bootverbose) {
+					printf("(%s:%c:%d:%d): Target "
+					       "Initiated SDTR\n",
+					       ahc_name(ahc), devinfo->channel,
+					       devinfo->target, devinfo->lun);
+				}
+				ahc->msgout_index = 0;
+				ahc->msgout_len = 0;
+				ahc_construct_sdtr(ahc, devinfo,
+						   period, offset);
+				ahc->msgout_index = 0;
+				response = TRUE;
+			}
+			done = MSGLOOP_MSGCOMPLETE;
+			break;
+		}
+		case MSG_EXT_WDTR:
+		{
+			u_int bus_width;
+			u_int saved_width;
+			u_int sending_reply;
+
+			sending_reply = FALSE;
+			if (ahc->msgin_buf[1] != MSG_EXT_WDTR_LEN) {
+				reject = TRUE;
+				break;
+			}
+
+			/*
+			 * Wait until we have our arg before validating
+			 * and acting on this message.
+			 *
+			 * Add one to MSG_EXT_WDTR_LEN to account for
+			 * the extended message preamble.
+			 */
+			if (ahc->msgin_index < (MSG_EXT_WDTR_LEN + 1))
+				break;
+
+			bus_width = ahc->msgin_buf[3];
+			saved_width = bus_width;
+			ahc_validate_width(ahc, tinfo, &bus_width,
+					   devinfo->role);
+			if (bootverbose) {
+				printf("(%s:%c:%d:%d): Received WDTR "
+				       "%x filtered to %x\n",
+				       ahc_name(ahc), devinfo->channel,
+				       devinfo->target, devinfo->lun,
+				       saved_width, bus_width);
+			}
+
+			if (ahc_sent_msg(ahc, AHCMSG_EXT, MSG_EXT_WDTR, TRUE)) {
+				/*
+				 * Don't send a WDTR back to the
+				 * target, since we asked first.
+				 * If the width went higher than our
+				 * request, reject it.
+				 */
+				if (saved_width > bus_width) {
+					reject = TRUE;
+					printf("(%s:%c:%d:%d): requested %dBit "
+					       "transfers.  Rejecting...\n",
+					       ahc_name(ahc), devinfo->channel,
+					       devinfo->target, devinfo->lun,
+					       8 * (0x01 << bus_width));
+					bus_width = 0;
+				}
+			} else {
+				/*
+				 * Send our own WDTR in reply
+				 */
+				if (bootverbose) {
+					printf("(%s:%c:%d:%d): Target "
+					       "Initiated WDTR\n",
+					       ahc_name(ahc), devinfo->channel,
+					       devinfo->target, devinfo->lun);
+				}
+				ahc->msgout_index = 0;
+				ahc->msgout_len = 0;
+				ahc_construct_wdtr(ahc, devinfo, bus_width);
+				ahc->msgout_index = 0;
+				response = TRUE;
+				sending_reply = TRUE;
+			}
+			ahc_set_width(ahc, devinfo, bus_width,
+				      AHC_TRANS_ACTIVE|AHC_TRANS_GOAL,
+				      /*paused*/TRUE);
+			/* After a wide message, we are async */
+			ahc_set_syncrate(ahc, devinfo,
+					 /*syncrate*/NULL, /*period*/0,
+					 /*offset*/0, /*ppr_options*/0,
+					 AHC_TRANS_ACTIVE, /*paused*/TRUE);
+			if (sending_reply == FALSE && reject == FALSE) {
+
+				if (tinfo->goal.period) {
+					ahc->msgout_index = 0;
+					ahc->msgout_len = 0;
+					ahc_build_transfer_msg(ahc, devinfo);
+					ahc->msgout_index = 0;
+					response = TRUE;
+				}
+			}
+			done = MSGLOOP_MSGCOMPLETE;
+			break;
+		}
+		case MSG_EXT_PPR:
+		{
+			struct	ahc_syncrate *syncrate;
+			u_int	period;
+			u_int	offset;
+			u_int	bus_width;
+			u_int	ppr_options;
+			u_int	saved_width;
+			u_int	saved_offset;
+			u_int	saved_ppr_options;
+
+			if (ahc->msgin_buf[1] != MSG_EXT_PPR_LEN) {
+				reject = TRUE;
+				break;
+			}
+
+			/*
+			 * Wait until we have all args before validating
+			 * and acting on this message.
+			 *
+			 * Add one to MSG_EXT_PPR_LEN to account for
+			 * the extended message preamble.
+			 */
+			if (ahc->msgin_index < (MSG_EXT_PPR_LEN + 1))
+				break;
+
+			period = ahc->msgin_buf[3];
+			offset = ahc->msgin_buf[5];
+			bus_width = ahc->msgin_buf[6];
+			saved_width = bus_width;
+			ppr_options = ahc->msgin_buf[7];
+			/*
+			 * According to the spec, a DT only
+			 * period factor with no DT option
+			 * set implies async.
+			 */
+			if ((ppr_options & MSG_EXT_PPR_DT_REQ) == 0
+			 && period == 9)
+				offset = 0;
+			saved_ppr_options = ppr_options;
+			saved_offset = offset;
+
+			/*
+			 * Mask out any options we don't support
+			 * on any controller.  Transfer options are
+			 * only available if we are negotiating wide.
+			 */
+			ppr_options &= MSG_EXT_PPR_DT_REQ;
+			if (bus_width == 0)
+				ppr_options = 0;
+
+			ahc_validate_width(ahc, tinfo, &bus_width,
+					   devinfo->role);
+			syncrate = ahc_devlimited_syncrate(ahc, tinfo, &period,
+							   &ppr_options,
+							   devinfo->role);
+			ahc_validate_offset(ahc, tinfo, syncrate,
+					    &offset, bus_width,
+					    devinfo->role);
+
+			if (ahc_sent_msg(ahc, AHCMSG_EXT, MSG_EXT_PPR, TRUE)) {
+				/*
+				 * If we are unable to do any of the
+				 * requested options (we went too low),
+				 * then we'll have to reject the message.
+				 */
+				if (saved_width > bus_width
+				 || saved_offset != offset
+				 || saved_ppr_options != ppr_options) {
+					reject = TRUE;
+					period = 0;
+					offset = 0;
+					bus_width = 0;
+					ppr_options = 0;
+					syncrate = NULL;
+				}
+			} else {
+				if (devinfo->role != ROLE_TARGET)
+					printf("(%s:%c:%d:%d): Target "
+					       "Initiated PPR\n",
+					       ahc_name(ahc), devinfo->channel,
+					       devinfo->target, devinfo->lun);
+				else
+					printf("(%s:%c:%d:%d): Initiator "
+					       "Initiated PPR\n",
+					       ahc_name(ahc), devinfo->channel,
+					       devinfo->target, devinfo->lun);
+				ahc->msgout_index = 0;
+				ahc->msgout_len = 0;
+				ahc_construct_ppr(ahc, devinfo, period, offset,
+						  bus_width, ppr_options);
+				ahc->msgout_index = 0;
+				response = TRUE;
+			}
+			if (bootverbose) {
+				printf("(%s:%c:%d:%d): Received PPR width %x, "
+				       "period %x, offset %x,options %x\n"
+				       "\tFiltered to width %x, period %x, "
+				       "offset %x, options %x\n",
+				       ahc_name(ahc), devinfo->channel,
+				       devinfo->target, devinfo->lun,
+				       ahc->msgin_buf[3], saved_width,
+				       saved_offset, saved_ppr_options,
+				       bus_width, period, offset, ppr_options);
+			}
+			ahc_set_width(ahc, devinfo, bus_width,
+				      AHC_TRANS_ACTIVE|AHC_TRANS_GOAL,
+				      /*paused*/TRUE);
+			ahc_set_syncrate(ahc, devinfo,
+					 syncrate, period,
+					 offset, ppr_options,
+					 AHC_TRANS_ACTIVE|AHC_TRANS_GOAL,
+					 /*paused*/TRUE);
+			done = MSGLOOP_MSGCOMPLETE;
+			break;
+		}
+		default:
+			/* Unknown extended message.  Reject it. */
+			reject = TRUE;
+			break;
+		}
+		break;
+	}
+	case MSG_BUS_DEV_RESET:
+		ahc_handle_devreset(ahc, devinfo,
+				    CAM_BDR_SENT,
+				    "Bus Device Reset Received",
+				    /*verbose_level*/0);
+		restart_sequencer(ahc);
+		done = MSGLOOP_TERMINATED;
+		break;
+	case MSG_ABORT_TAG:
+	case MSG_ABORT:
+	case MSG_CLEAR_QUEUE:
+#ifdef AHC_TARGET_MODE
+		/* Target mode messages */
+		if (devinfo->role != ROLE_TARGET) {
+			reject = TRUE;
+			break;
+		}
+		ahc_abort_scbs(ahc, devinfo->target, devinfo->channel,
+			       devinfo->lun,
+			       ahc->msgin_buf[0] == MSG_ABORT_TAG
+						  ? SCB_LIST_NULL
+						  : ahc_inb(ahc, INITIATOR_TAG),
+			       ROLE_TARGET, CAM_REQ_ABORTED);
+
+		tstate = ahc->enabled_targets[devinfo->our_scsiid];
+		if (tstate != NULL) {
+			struct tmode_lstate* lstate;
+
+			lstate = tstate->enabled_luns[devinfo->lun];
+			if (lstate != NULL) {
+				ahc_queue_lstate_event(ahc, lstate,
+						       devinfo->our_scsiid,
+						       ahc->msgin_buf[0],
+						       /*arg*/0);
+				ahc_send_lstate_events(ahc, lstate);
+			}
+		}
+		done = MSGLOOP_MSGCOMPLETE;
+		break;
+#endif
+	case MSG_TERM_IO_PROC:
+	default:
+		reject = TRUE;
+		break;
+	}
+
+	if (reject) {
+		/*
+		 * Setup to reject the message.
+		 */
+		ahc->msgout_index = 0;
+		ahc->msgout_len = 1;
+		ahc->msgout_buf[0] = MSG_MESSAGE_REJECT;
+		done = MSGLOOP_MSGCOMPLETE;
+		response = TRUE;
+	}
+
+	if (done != MSGLOOP_IN_PROG && !response)
+		/* Clear the outgoing message buffer */
+		ahc->msgout_len = 0;
+
+	return (done);
+}
+
+/*
+ * Process a message reject message.
+ */
+static int
+ahc_handle_msg_reject(struct ahc_softc *ahc, struct ahc_devinfo *devinfo)
+{
+	/*
+	 * What we care about here is if we had an
+	 * outstanding SDTR or WDTR message for this
+	 * target.  If we did, this is a signal that
+	 * the target is refusing negotiation.
+	 */
+	struct scb *scb;
+	struct ahc_initiator_tinfo *tinfo;
+	struct tmode_tstate *tstate;
+	u_int scb_index;
+	u_int last_msg;
+	int   response = 0;
+
+	scb_index = ahc_inb(ahc, SCB_TAG);
+	scb = ahc_lookup_scb(ahc, scb_index);
+	tinfo = ahc_fetch_transinfo(ahc, devinfo->channel,
+				    devinfo->our_scsiid,
+				    devinfo->target, &tstate);
+	/* Might be necessary */
+	last_msg = ahc_inb(ahc, LAST_MSG);
+
+	if (ahc_sent_msg(ahc, AHCMSG_EXT, MSG_EXT_PPR, /*full*/FALSE)) {
+		/*
+		 * Target does not support the PPR message.
+		 * Attempt to negotiate SPI-2 style.
+		 */
+		if (bootverbose) {
+			printf("(%s:%c:%d:%d): PPR Rejected. "
+			       "Trying WDTR/SDTR\n",
+			       ahc_name(ahc), devinfo->channel,
+			       devinfo->target, devinfo->lun);
+		}
+		tinfo->goal.ppr_options = 0;
+		tinfo->current.transport_version = 2;
+		tinfo->goal.transport_version = 2;
+		ahc->msgout_index = 0;
+		ahc->msgout_len = 0;
+		ahc_build_transfer_msg(ahc, devinfo);
+		ahc->msgout_index = 0;
+		response = 1;
+	} else if (ahc_sent_msg(ahc, AHCMSG_EXT, MSG_EXT_WDTR, /*full*/FALSE)) {
+
+		/* note 8bit xfers */
+		printf("(%s:%c:%d:%d): refuses WIDE negotiation.  Using "
+		       "8bit transfers\n", ahc_name(ahc),
+		       devinfo->channel, devinfo->target, devinfo->lun);
+		ahc_set_width(ahc, devinfo, MSG_EXT_WDTR_BUS_8_BIT,
+			      AHC_TRANS_ACTIVE|AHC_TRANS_GOAL,
+			      /*paused*/TRUE);
+		/*
+		 * No need to clear the sync rate.  If the target
+		 * did not accept the command, our syncrate is
+		 * unaffected.  If the target started the negotiation,
+		 * but rejected our response, we already cleared the
+		 * sync rate before sending our WDTR.
+		 */
+		if (tinfo->goal.period) {
+
+			/* Start the sync negotiation */
+			ahc->msgout_index = 0;
+			ahc->msgout_len = 0;
+			ahc_build_transfer_msg(ahc, devinfo);
+			ahc->msgout_index = 0;
+			response = 1;
+		}
+	} else if (ahc_sent_msg(ahc, AHCMSG_EXT, MSG_EXT_SDTR, /*full*/FALSE)) {
+		/* note asynch xfers and clear flag */
+		ahc_set_syncrate(ahc, devinfo, /*syncrate*/NULL, /*period*/0,
+				 /*offset*/0, /*ppr_options*/0,
+				 AHC_TRANS_ACTIVE|AHC_TRANS_GOAL,
+				 /*paused*/TRUE);
+		printf("(%s:%c:%d:%d): refuses synchronous negotiation. "
+		       "Using asynchronous transfers\n",
+		       ahc_name(ahc), devinfo->channel,
+		       devinfo->target, devinfo->lun);
+	} else if ((scb->hscb->control & MSG_SIMPLE_Q_TAG) != 0) {
+
+		printf("(%s:%c:%d:%d): refuses tagged commands.  Performing "
+		       "non-tagged I/O\n", ahc_name(ahc),
+		       devinfo->channel, devinfo->target, devinfo->lun);
+		ahc_set_tags(ahc, devinfo, FALSE);
+
+		/*
+		 * Resend the identify for this CCB as the target
+		 * may believe that the selection is invalid otherwise.
+		 */
+		ahc_outb(ahc, SCB_CONTROL,
+			 ahc_inb(ahc, SCB_CONTROL) & ~MSG_SIMPLE_Q_TAG);
+	 	scb->hscb->control &= ~MSG_SIMPLE_Q_TAG;
+		ahc_set_transaction_tag(scb, /*enabled*/FALSE,
+					/*type*/MSG_SIMPLE_Q_TAG);
+		ahc_outb(ahc, MSG_OUT, MSG_IDENTIFYFLAG);
+		ahc_outb(ahc, SCSISIGO, ahc_inb(ahc, SCSISIGO) | ATNO);
+
+		/*
+		 * This transaction is now at the head of
+		 * the untagged queue for this target.
+		 */
+		if ((ahc->flags & AHC_SCB_BTT) == 0) {
+			struct scb_tailq *untagged_q;
+
+			untagged_q = &(ahc->untagged_queues[devinfo->target]);
+			TAILQ_INSERT_HEAD(untagged_q, scb, links.tqe);
+			scb->flags |= SCB_UNTAGGEDQ;
+		}
+		ahc_busy_tcl(ahc, BUILD_TCL(scb->hscb->scsiid, devinfo->lun),
+			     scb->hscb->tag);
+
+		/*
+		 * Requeue all tagged commands for this target
+		 * currently in our posession so they can be
+		 * converted to untagged commands.
+		 */
+		ahc_search_qinfifo(ahc, SCB_GET_TARGET(ahc, scb),
+				   SCB_GET_CHANNEL(ahc, scb),
+				   SCB_GET_LUN(scb), /*tag*/SCB_LIST_NULL,
+				   ROLE_INITIATOR, CAM_REQUEUE_REQ,
+				   SEARCH_COMPLETE);
+	} else {
+		/*
+		 * Otherwise, we ignore it.
+		 */
+		printf("%s:%c:%d: Message reject for %x -- ignored\n",
+		       ahc_name(ahc), devinfo->channel, devinfo->target,
+		       last_msg);
+	}
+	return (response);
+}
+
+/*
+ * Process an ingnore wide residue message.
+ */
+static void
+ahc_handle_ign_wide_residue(struct ahc_softc *ahc, struct ahc_devinfo *devinfo)
+{
+	u_int scb_index;
+	struct scb *scb;
+
+	scb_index = ahc_inb(ahc, SCB_TAG);
+	scb = ahc_lookup_scb(ahc, scb_index);
+	/*
+	 * XXX Actually check data direction in the sequencer?
+	 * Perhaps add datadir to some spare bits in the hscb?
+	 */
+	if ((ahc_inb(ahc, SEQ_FLAGS) & DPHASE) == 0
+	 || ahc_get_transfer_dir(scb) != CAM_DIR_IN) {
+		/*
+		 * Ignore the message if we haven't
+		 * seen an appropriate data phase yet.
+		 */
+	} else {
+		/*
+		 * If the residual occurred on the last
+		 * transfer and the transfer request was
+		 * expected to end on an odd count, do
+		 * nothing.  Otherwise, subtract a byte
+		 * and update the residual count accordingly.
+		 */
+		uint32_t sgptr;
+
+		sgptr = ahc_inb(ahc, SCB_RESIDUAL_SGPTR);
+		if ((sgptr & SG_LIST_NULL) != 0
+		 && ahc_inb(ahc, DATA_COUNT_ODD) == 1) {
+			/*
+			 * If the residual occurred on the last
+			 * transfer and the transfer request was
+			 * expected to end on an odd count, do
+			 * nothing.
+			 */
+		} else {
+			struct ahc_dma_seg *sg;
+			uint32_t data_cnt;
+			uint32_t data_addr;
+
+			/* Pull in the rest of the sgptr */
+			sgptr |= (ahc_inb(ahc, SCB_RESIDUAL_SGPTR + 3) << 24)
+			      | (ahc_inb(ahc, SCB_RESIDUAL_SGPTR + 2) << 16)
+			      | (ahc_inb(ahc, SCB_RESIDUAL_SGPTR + 1) << 8);
+			sgptr &= SG_PTR_MASK;
+			data_cnt = (ahc_inb(ahc, SCB_RESIDUAL_DATACNT+2) << 16)
+				 | (ahc_inb(ahc, SCB_RESIDUAL_DATACNT+1) << 8)
+				 | (ahc_inb(ahc, SCB_RESIDUAL_DATACNT));
+
+			data_addr = (ahc_inb(ahc, SHADDR + 3) << 24)
+				  | (ahc_inb(ahc, SHADDR + 2) << 16)
+				  | (ahc_inb(ahc, SHADDR + 1) << 8)
+				  | (ahc_inb(ahc, SHADDR));
+
+			data_cnt += 1;
+			data_addr -= 1;
+
+			sg = ahc_sg_bus_to_virt(scb, sgptr);
+			/*
+			 * The residual sg ptr points to the next S/G
+			 * to load so we must go back one.
+			 */
+			sg--;
+			if (sg != scb->sg_list
+			 && (sg->len & AHC_SG_LEN_MASK) < data_cnt) {
+
+				sg--;
+				data_cnt = 1 | (sg->len & AHC_DMA_LAST_SEG);
+				data_addr = sg->addr
+					  + (sg->len & AHC_SG_LEN_MASK) - 1;
+
+				/*
+				 * Increment sg so it points to the
+				 * "next" sg.
+				 */
+				sg++;
+				sgptr = ahc_sg_virt_to_bus(scb, sg);
+				ahc_outb(ahc, SCB_RESIDUAL_SGPTR + 3,
+					 sgptr >> 24);
+				ahc_outb(ahc, SCB_RESIDUAL_SGPTR + 2,
+					 sgptr >> 16);
+				ahc_outb(ahc, SCB_RESIDUAL_SGPTR + 1,
+					 sgptr >> 8);
+				ahc_outb(ahc, SCB_RESIDUAL_SGPTR, sgptr);
+			}
+
+/* XXX What about high address byte??? */
+			ahc_outb(ahc, SCB_RESIDUAL_DATACNT + 3, data_cnt >> 24);
+			ahc_outb(ahc, SCB_RESIDUAL_DATACNT + 2, data_cnt >> 16);
+			ahc_outb(ahc, SCB_RESIDUAL_DATACNT + 1, data_cnt >> 8);
+			ahc_outb(ahc, SCB_RESIDUAL_DATACNT, data_cnt);
+
+/* XXX Perhaps better to just keep the saved address in sram */
+			if ((ahc->features & AHC_ULTRA2) != 0) {
+				ahc_outb(ahc, HADDR + 3, data_addr >> 24);
+				ahc_outb(ahc, HADDR + 2, data_addr >> 16);
+				ahc_outb(ahc, HADDR + 1, data_addr >> 8);
+				ahc_outb(ahc, HADDR, data_addr);
+				ahc_outb(ahc, DFCNTRL, PRELOADEN);
+				ahc_outb(ahc, SXFRCTL0,
+					 ahc_inb(ahc, SXFRCTL0) | CLRCHN);
+			} else {
+				ahc_outb(ahc, HADDR + 3, data_addr >> 24);
+				ahc_outb(ahc, HADDR + 2, data_addr >> 16);
+				ahc_outb(ahc, HADDR + 1, data_addr >> 8);
+				ahc_outb(ahc, HADDR, data_addr);
+			}
+		}
+	}
+}
+
+/*
+ * Handle the effects of issuing a bus device reset message.
+ */
+static void
+ahc_handle_devreset(struct ahc_softc *ahc, struct ahc_devinfo *devinfo,
+		    cam_status status, char *message, int verbose_level)
+{
+#ifdef AHC_TARGET_MODE
+	struct tmode_tstate* tstate;
+	u_int lun;
+#endif
+	int found;
+
+	found = ahc_abort_scbs(ahc, devinfo->target, devinfo->channel,
+			       CAM_LUN_WILDCARD, SCB_LIST_NULL, devinfo->role,
+			       status);
+
+#ifdef AHC_TARGET_MODE
+	/*
+	 * Send an immediate notify ccb to all target mord peripheral
+	 * drivers affected by this action.
+	 */
+	tstate = ahc->enabled_targets[devinfo->our_scsiid];
+	if (tstate != NULL) {
+		for (lun = 0; lun < AHC_NUM_LUNS; lun++) {
+			struct tmode_lstate* lstate;
+
+			lstate = tstate->enabled_luns[lun];
+			if (lstate == NULL)
+				continue;
+
+			ahc_queue_lstate_event(ahc, lstate, devinfo->our_scsiid,
+					       MSG_BUS_DEV_RESET, /*arg*/0);
+			ahc_send_lstate_events(ahc, lstate);
+		}
+	}
+#endif
+
+	/*
+	 * Go back to async/narrow transfers and renegotiate.
+	 */
+	ahc_set_width(ahc, devinfo, MSG_EXT_WDTR_BUS_8_BIT,
+		      AHC_TRANS_CUR, /*paused*/TRUE);
+	ahc_set_syncrate(ahc, devinfo, /*syncrate*/NULL,
+			 /*period*/0, /*offset*/0, /*ppr_options*/0,
+			 AHC_TRANS_CUR, /*paused*/TRUE);
+	
+	ahc_send_async(ahc, devinfo->channel, devinfo->target,
+		       CAM_LUN_WILDCARD, AC_SENT_BDR);
+
+	if (message != NULL
+	 && (verbose_level <= bootverbose))
+		printf("%s: %s on %c:%d. %d SCBs aborted\n", ahc_name(ahc),
+		       message, devinfo->channel, devinfo->target, found);
+}
+
+#ifdef AHC_TARGET_MODE
+void
+ahc_setup_target_msgin(struct ahc_softc *ahc, struct ahc_devinfo *devinfo)
+{
+	/*              
+	 * To facilitate adding multiple messages together,
+	 * each routine should increment the index and len
+	 * variables instead of setting them explicitly.
+	 */             
+	ahc->msgout_index = 0;
+	ahc->msgout_len = 0;
+
+	if ((ahc->targ_msg_req & devinfo->target_mask) != 0)
+		ahc_build_transfer_msg(ahc, devinfo);
+	else
+		panic("ahc_intr: AWAITING target message with no message");
+
+	ahc->msgout_index = 0;
+	ahc->msg_type = MSG_TYPE_TARGET_MSGIN;
+}
+#endif
+/**************************** Initialization **********************************/
+/*
+ * Allocate a controller structure for a new device
+ * and perform initial initializion.
+ */
+struct ahc_softc *
+ahc_alloc(void *platform_arg, char *name)
+{
+	struct  ahc_softc *ahc;
+	int	i;
+
+#ifndef	__FreeBSD__
+	ahc = malloc(sizeof(*ahc), M_DEVBUF, M_NOWAIT);
+	if (!ahc) {
+		printf("aic7xxx: cannot malloc softc!\n");
+		free(name, M_DEVBUF);
+		return NULL;
+	}
+#else
+	ahc = device_get_softc((device_t)platform_arg);
+#endif
+	memset(ahc, 0, sizeof(*ahc));
+	LIST_INIT(&ahc->pending_scbs);
+	/* We don't know or unit number until the OSM sets it */
+	ahc->name = name;
+	for (i = 0; i < 16; i++)
+		TAILQ_INIT(&ahc->untagged_queues[i]);
+	if (ahc_platform_alloc(ahc, platform_arg) != 0) {
+		ahc_free(ahc);
+		ahc = NULL;
+	}
+	return (ahc);
+}
+
+int
+ahc_softc_init(struct ahc_softc *ahc, struct ahc_probe_config *config)
+{
+
+	ahc->chip = config->chip;
+	ahc->features = config->features;
+	ahc->bugs = config->bugs;
+	ahc->flags = config->flags;
+	ahc->channel = config->channel; 
+	ahc->unpause = (ahc_inb(ahc, HCNTRL) & IRQMS) | INTEN;
+	ahc->description = config->description;
+	/* The IRQMS bit is only valid on VL and EISA chips */
+	if ((ahc->chip & AHC_PCI) != 0)
+		ahc->unpause &= ~IRQMS;
+	ahc->pause = ahc->unpause | PAUSE; 
+	/* XXX The shared scb data stuff should be depricated */
+	if (ahc->scb_data == NULL) {
+		ahc->scb_data = malloc(sizeof(*ahc->scb_data),
+				       M_DEVBUF, M_NOWAIT);
+		if (ahc->scb_data == NULL)
+			return (ENOMEM);
+		memset(ahc->scb_data, 0, sizeof(*ahc->scb_data));
+	}
+
+	return (0);
+}
+
+void
+ahc_softc_insert(struct ahc_softc *ahc)
+{
+	struct ahc_softc *list_ahc;
+
+#ifdef AHC_SUPPORT_PCI
+	/*
+	 * Second Function PCI devices need to inherit some
+	 * settings from function 0.  We assume that function 0
+	 * will always be found prior to function 1.
+	 */
+	if ((ahc->chip & AHC_BUS_MASK) == AHC_PCI
+	 && ahc_get_pci_function(ahc->dev_softc) == 1) {
+		TAILQ_FOREACH(list_ahc, &ahc_tailq, links) {
+			ahc_dev_softc_t list_pci;
+			ahc_dev_softc_t pci;
+
+			list_pci = list_ahc->dev_softc;
+			pci = ahc->dev_softc;
+			if (ahc_get_pci_bus(list_pci) == ahc_get_pci_bus(pci)
+			 && ahc_get_pci_slot(list_pci) == ahc_get_pci_slot(pci)
+			 && ahc_get_pci_function(list_pci) == 0) {
+				ahc->flags &= ~AHC_BIOS_ENABLED; 
+				ahc->flags |=
+				    list_ahc->flags & AHC_BIOS_ENABLED;
+				ahc->flags &= ~AHC_CHANNEL_B_PRIMARY; 
+				ahc->flags |=
+				    list_ahc->flags & AHC_CHANNEL_B_PRIMARY;
+				break;
+			}
+		}
+	}
+#endif
+
+	/*
+	 * Insertion sort into our list of softcs.
+	 */
+	list_ahc = TAILQ_FIRST(&ahc_tailq);
+	while (list_ahc != NULL
+	    && ahc_softc_comp(list_ahc, ahc) <= 0)
+		list_ahc = TAILQ_NEXT(list_ahc, links);
+	if (list_ahc != NULL)
+		TAILQ_INSERT_BEFORE(list_ahc, ahc, links);
+	else
+		TAILQ_INSERT_TAIL(&ahc_tailq, ahc, links);
+	ahc->init_level++;
+}
+
+void
+ahc_set_unit(struct ahc_softc *ahc, int unit)
+{
+	ahc->unit = unit;
+}
+
+void
+ahc_set_name(struct ahc_softc *ahc, char *name)
+{
+	if (ahc->name != NULL)
+		free(ahc->name, M_DEVBUF);
+	ahc->name = name;
+}
+
+void
+ahc_free(struct ahc_softc *ahc)
+{
+	int i;
+
+	ahc_fini_scbdata(ahc);
+	switch (ahc->init_level) {
+	default:
+	case 5:
+		ahc_shutdown(ahc);
+		TAILQ_REMOVE(&ahc_tailq, ahc, links);
+		/* FALLTHROUGH */
+	case 4:
+		ahc_dmamap_unload(ahc, ahc->shared_data_dmat,
+				  ahc->shared_data_dmamap);
+		/* FALLTHROUGH */
+	case 3:
+		ahc_dmamem_free(ahc, ahc->shared_data_dmat, ahc->qoutfifo,
+				ahc->shared_data_dmamap);
+		ahc_dmamap_destroy(ahc, ahc->shared_data_dmat,
+				   ahc->shared_data_dmamap);
+		/* FALLTHROUGH */
+	case 2:
+		ahc_dma_tag_destroy(ahc, ahc->shared_data_dmat);
+	case 1:
+#ifndef __linux__
+		ahc_dma_tag_destroy(ahc, ahc->buffer_dmat);
+#endif
+		break;
+	}
+
+	ahc_platform_free(ahc);
+	for (i = 0; i < AHC_NUM_TARGETS; i++) {
+		struct tmode_tstate *tstate;
+
+		tstate = ahc->enabled_targets[i];
+		if (tstate != NULL) {
+#if AHC_TARGET_MODE
+			int j;
+
+			for (j = 0; j < AHC_NUM_LUNS; j++) {
+				struct tmode_lstate *lstate;
+
+				lstate = tstate->enabled_luns[j];
+				if (lstate != NULL) {
+					xpt_free_path(lstate->path);
+					free(lstate, M_DEVBUF);
+				}
+			}
+#endif
+			free(tstate, M_DEVBUF);
+		}
+	}
+#if AHC_TARGET_MODE
+	if (ahc->black_hole != NULL) {
+		xpt_free_path(ahc->black_hole->path);
+		free(ahc->black_hole, M_DEVBUF);
+	}
+#endif
+	if (ahc->name != NULL)
+		free(ahc->name, M_DEVBUF);
+#ifndef __FreeBSD__
+	free(ahc, M_DEVBUF);
+#endif
+	return;
+}
+
+void
+ahc_shutdown(void *arg)
+{
+	struct	ahc_softc *ahc;
+	int	i;
+
+	ahc = (struct ahc_softc *)arg;
+
+	/* This will reset most registers to 0, but not all */
+	ahc_reset(ahc);
+	ahc_outb(ahc, SCSISEQ, 0);
+	ahc_outb(ahc, SXFRCTL0, 0);
+	ahc_outb(ahc, DSPCISTATUS, 0);
+
+	for (i = TARG_SCSIRATE; i < HA_274_BIOSCTRL; i++)
+		ahc_outb(ahc, i, 0);
+}
+
+/*
+ * Reset the controller and record some information about it
+ * that is only availabel just after a reset.
+ */
+int
+ahc_reset(struct ahc_softc *ahc)
+{
+	u_int	sblkctl;
+	u_int	sxfrctl1_a, sxfrctl1_b;
+	int	wait;
+	
+	/*
+	 * Preserve the value of the SXFRCTL1 register for all channels.
+	 * It contains settings that affect termination and we don't want
+	 * to disturb the integrity of the bus.
+	 */
+	pause_sequencer(ahc);
+	sxfrctl1_b = 0;
+	if ((ahc->chip & AHC_CHIPID_MASK) == AHC_AIC7770) {
+		u_int sblkctl;
+
+		/*
+		 * Save channel B's settings in case this chip
+		 * is setup for TWIN channel operation.
+		 */
+		sblkctl = ahc_inb(ahc, SBLKCTL);
+		ahc_outb(ahc, SBLKCTL, sblkctl | SELBUSB);
+		sxfrctl1_b = ahc_inb(ahc, SXFRCTL1);
+		ahc_outb(ahc, SBLKCTL, sblkctl & ~SELBUSB);
+	}
+	sxfrctl1_a = ahc_inb(ahc, SXFRCTL1);
+
+	ahc_outb(ahc, HCNTRL, CHIPRST | ahc->pause);
+
+	/*
+	 * Ensure that the reset has finished
+	 */
+	wait = 1000;
+	do {
+		ahc_delay(1000);
+	} while (--wait && !(ahc_inb(ahc, HCNTRL) & CHIPRSTACK));
+
+	if (wait == 0) {
+		printf("%s: WARNING - Failed chip reset!  "
+		       "Trying to initialize anyway.\n", ahc_name(ahc));
+	}
+	ahc_outb(ahc, HCNTRL, ahc->pause);
+
+	/* Determine channel configuration */
+	sblkctl = ahc_inb(ahc, SBLKCTL) & (SELBUSB|SELWIDE);
+	/* No Twin Channel PCI cards */
+	if ((ahc->chip & AHC_PCI) != 0)
+		sblkctl &= ~SELBUSB;
+	switch (sblkctl) {
+	case 0:
+		/* Single Narrow Channel */
+		break;
+	case 2:
+		/* Wide Channel */
+		ahc->features |= AHC_WIDE;
+		break;
+	case 8:
+		/* Twin Channel */
+		ahc->features |= AHC_TWIN;
+		break;
+	default:
+		printf(" Unsupported adapter type.  Ignoring\n");
+		return(-1);
+	}
+
+	/*
+	 * Reload sxfrctl1.
+	 *
+	 * We must always initialize STPWEN to 1 before we
+	 * restore the saved values.  STPWEN is initialized
+	 * to a tri-state condition which can only be cleared
+	 * by turning it on.
+	 */
+	if ((ahc->features & AHC_TWIN) != 0) {
+		u_int sblkctl;
+
+		sblkctl = ahc_inb(ahc, SBLKCTL);
+		ahc_outb(ahc, SBLKCTL, sblkctl | SELBUSB);
+		ahc_outb(ahc, SXFRCTL1, sxfrctl1_b);
+		ahc_outb(ahc, SBLKCTL, sblkctl & ~SELBUSB);
+	}
+	ahc_outb(ahc, SXFRCTL1, sxfrctl1_a);
+
+#ifdef AHC_DUMP_SEQ
+	if (ahc->init_level == 0)
+		ahc_dumpseq(ahc);
+#endif
+
+	return (0);
+}
+
+/*
+ * Determine the number of SCBs available on the controller
+ */
+int
+ahc_probe_scbs(struct ahc_softc *ahc) {
+	int i;
+
+	for (i = 0; i < AHC_SCB_MAX; i++) {
+
+		ahc_outb(ahc, SCBPTR, i);
+		ahc_outb(ahc, SCB_BASE, i);
+		if (ahc_inb(ahc, SCB_BASE) != i)
+			break;
+		ahc_outb(ahc, SCBPTR, 0);
+		if (ahc_inb(ahc, SCB_BASE) != 0)
+			break;
+	}
+	return (i);
+}
+
+void
+ahc_init_probe_config(struct ahc_probe_config *probe_config)
+{
+	probe_config->description = NULL;
+	probe_config->channel = 'A';
+	probe_config->channel_b = 'B';
+	probe_config->chip = AHC_NONE;
+	probe_config->features = AHC_FENONE;
+	probe_config->bugs = AHC_BUGNONE;
+	probe_config->flags = AHC_FNONE;
+}
+
+static void
+ahc_dmamap_cb(void *arg, bus_dma_segment_t *segs, int nseg, int error) 
+{
+	bus_addr_t *baddr;
+
+	baddr = (bus_addr_t *)arg;
+	*baddr = segs->ds_addr;
+}
+
+static void
+ahc_build_free_scb_list(struct ahc_softc *ahc)
+{
+	int i;
+
+	for (i = 0; i < ahc->scb_data->maxhscbs; i++) {
+		ahc_outb(ahc, SCBPTR, i);
+
+		/* Clear the control byte. */
+		ahc_outb(ahc, SCB_CONTROL, 0);
+
+		/* Set the next pointer */
+		if ((ahc->flags & AHC_PAGESCBS) != 0)
+			ahc_outb(ahc, SCB_NEXT, i+1);
+		else 
+			ahc_outb(ahc, SCB_NEXT, SCB_LIST_NULL);
+
+		/* Make the tag number invalid */
+		ahc_outb(ahc, SCB_TAG, SCB_LIST_NULL);
+	}
+
+	/* Make sure that the last SCB terminates the free list */
+	ahc_outb(ahc, SCBPTR, i-1);
+	ahc_outb(ahc, SCB_NEXT, SCB_LIST_NULL);
+
+	/* Ensure we clear the 0 SCB's control byte. */
+	ahc_outb(ahc, SCBPTR, 0);
+	ahc_outb(ahc, SCB_CONTROL, 0);
+}
+
+static int
+ahc_init_scbdata(struct ahc_softc *ahc)
+{
+	struct scb_data *scb_data;
+
+	scb_data = ahc->scb_data;
+	SLIST_INIT(&scb_data->free_scbs);
+	SLIST_INIT(&scb_data->sg_maps);
+
+	/* Allocate SCB resources */
+	scb_data->scbarray =
+	    (struct scb *)malloc(sizeof(struct scb) * AHC_SCB_MAX,
+				 M_DEVBUF, M_NOWAIT);
+	if (scb_data->scbarray == NULL)
+		return (ENOMEM);
+	memset(scb_data->scbarray, 0, sizeof(struct scb) * AHC_SCB_MAX);
+
+	/* Determine the number of hardware SCBs and initialize them */
+
+	scb_data->maxhscbs = ahc_probe_scbs(ahc);
+	if ((ahc->flags & AHC_PAGESCBS) != 0) {
+		/* SCB 0 heads the free list */
+		ahc_outb(ahc, FREE_SCBH, 0);
+	} else {
+		ahc_outb(ahc, FREE_SCBH, SCB_LIST_NULL);
+	}
+
+	if (ahc->scb_data->maxhscbs == 0) {
+		printf("%s: No SCB space found\n", ahc_name(ahc));
+		return (ENXIO);
+	}
+
+	ahc_build_free_scb_list(ahc);
+
+	/*
+	 * Create our DMA tags.  These tags define the kinds of device
+	 * accessible memory allocations and memory mappings we will
+	 * need to perform during normal operation.
+	 *
+	 * Unless we need to further restrict the allocation, we rely
+	 * on the restrictions of the parent dmat, hence the common
+	 * use of MAXADDR and MAXSIZE.
+	 */
+
+	/* DMA tag for our hardware scb structures */
+	if (ahc_dma_tag_create(ahc, ahc->parent_dmat, /*alignment*/1,
+			       /*boundary*/0, /*lowaddr*/BUS_SPACE_MAXADDR,
+			       /*highaddr*/BUS_SPACE_MAXADDR,
+			       /*filter*/NULL, /*filterarg*/NULL,
+			       AHC_SCB_MAX * sizeof(struct hardware_scb),
+			       /*nsegments*/1,
+			       /*maxsegsz*/BUS_SPACE_MAXSIZE_32BIT,
+			       /*flags*/0, &scb_data->hscb_dmat) != 0) {
+		goto error_exit;
+	}
+
+	scb_data->init_level++;
+
+	/* Allocation for our ccbs */
+	if (ahc_dmamem_alloc(ahc, scb_data->hscb_dmat,
+			     (void **)&scb_data->hscbs,
+			     BUS_DMA_NOWAIT, &scb_data->hscb_dmamap) != 0) {
+		goto error_exit;
+	}
+
+	scb_data->init_level++;
+
+	/* And permanently map them */
+	ahc_dmamap_load(ahc, scb_data->hscb_dmat, scb_data->hscb_dmamap,
+			scb_data->hscbs,
+			AHC_SCB_MAX * sizeof(struct hardware_scb),
+			ahc_dmamap_cb, &scb_data->hscb_busaddr, /*flags*/0);
+
+	scb_data->init_level++;
+
+	/* DMA tag for our sense buffers */
+	if (ahc_dma_tag_create(ahc, ahc->parent_dmat, /*alignment*/1,
+			       /*boundary*/0, /*lowaddr*/BUS_SPACE_MAXADDR,
+			       /*highaddr*/BUS_SPACE_MAXADDR,
+			       /*filter*/NULL, /*filterarg*/NULL,
+			       AHC_SCB_MAX * sizeof(struct scsi_sense_data),
+			       /*nsegments*/1,
+			       /*maxsegsz*/BUS_SPACE_MAXSIZE_32BIT,
+			       /*flags*/0, &scb_data->sense_dmat) != 0) {
+		goto error_exit;
+	}
+
+	scb_data->init_level++;
+
+	/* Allocate them */
+	if (ahc_dmamem_alloc(ahc, scb_data->sense_dmat,
+			     (void **)&scb_data->sense,
+			     BUS_DMA_NOWAIT, &scb_data->sense_dmamap) != 0) {
+		goto error_exit;
+	}
+
+	scb_data->init_level++;
+
+	/* And permanently map them */
+	ahc_dmamap_load(ahc, scb_data->sense_dmat, scb_data->sense_dmamap,
+			scb_data->sense,
+			AHC_SCB_MAX * sizeof(struct scsi_sense_data),
+			ahc_dmamap_cb, &scb_data->sense_busaddr, /*flags*/0);
+
+	scb_data->init_level++;
+
+	/* DMA tag for our S/G structures.  We allocate in page sized chunks */
+	if (ahc_dma_tag_create(ahc, ahc->parent_dmat, /*alignment*/1,
+			       /*boundary*/0, /*lowaddr*/BUS_SPACE_MAXADDR,
+			       /*highaddr*/BUS_SPACE_MAXADDR,
+			       /*filter*/NULL, /*filterarg*/NULL,
+			       PAGE_SIZE, /*nsegments*/1,
+			       /*maxsegsz*/BUS_SPACE_MAXSIZE_32BIT,
+			       /*flags*/0, &scb_data->sg_dmat) != 0) {
+		goto error_exit;
+	}
+
+	scb_data->init_level++;
+
+	/* Perform initial CCB allocation */
+	memset(scb_data->hscbs, 0, AHC_SCB_MAX * sizeof(struct hardware_scb));
+	ahc_alloc_scbs(ahc);
+
+	if (scb_data->numscbs == 0) {
+		printf("%s: ahc_init_scbdata - "
+		       "Unable to allocate initial scbs\n",
+		       ahc_name(ahc));
+		goto error_exit;
+	}
+
+	/*
+	 * Tell the sequencer which SCB will be the next one it receives.
+	 */
+	ahc->next_queued_scb = ahc_get_scb(ahc);
+	ahc_outb(ahc, NEXT_QUEUED_SCB, ahc->next_queued_scb->hscb->tag);
+
+	/*
+	 * Note that we were successfull
+	 */
+	return (0); 
+
+error_exit:
+
+	return (ENOMEM);
+}
+
+static void
+ahc_fini_scbdata(struct ahc_softc *ahc)
+{
+	struct scb_data *scb_data;
+
+	scb_data = ahc->scb_data;
+	if (scb_data == NULL)
+		return;
+
+	switch (scb_data->init_level) {
+	default:
+	case 7:
+	{
+		struct sg_map_node *sg_map;
+
+		while ((sg_map = SLIST_FIRST(&scb_data->sg_maps))!= NULL) {
+			SLIST_REMOVE_HEAD(&scb_data->sg_maps, links);
+			ahc_dmamap_unload(ahc, scb_data->sg_dmat,
+					  sg_map->sg_dmamap);
+			ahc_dmamem_free(ahc, scb_data->sg_dmat,
+					sg_map->sg_vaddr,
+					sg_map->sg_dmamap);
+			free(sg_map, M_DEVBUF);
+		}
+		ahc_dma_tag_destroy(ahc, scb_data->sg_dmat);
+	}
+	case 6:
+		ahc_dmamap_unload(ahc, scb_data->sense_dmat,
+				  scb_data->sense_dmamap);
+	case 5:
+		ahc_dmamem_free(ahc, scb_data->sense_dmat, scb_data->sense,
+				scb_data->sense_dmamap);
+		ahc_dmamap_destroy(ahc, scb_data->sense_dmat,
+				   scb_data->sense_dmamap);
+	case 4:
+		ahc_dma_tag_destroy(ahc, scb_data->sense_dmat);
+	case 3:
+		ahc_dmamap_unload(ahc, scb_data->hscb_dmat,
+				  scb_data->hscb_dmamap);
+	case 2:
+		ahc_dmamem_free(ahc, scb_data->hscb_dmat, scb_data->hscbs,
+				scb_data->hscb_dmamap);
+		ahc_dmamap_destroy(ahc, scb_data->hscb_dmat,
+				   scb_data->hscb_dmamap);
+	case 1:
+		ahc_dma_tag_destroy(ahc, scb_data->hscb_dmat);
+		break;
+	case 0:
+		break;
+	}
+	if (scb_data->scbarray != NULL)
+		free(scb_data->scbarray, M_DEVBUF);
+}
+
+void
+ahc_alloc_scbs(struct ahc_softc *ahc)
+{
+	struct scb_data *scb_data;
+	struct scb *next_scb;
+	struct sg_map_node *sg_map;
+	bus_addr_t physaddr;
+	struct ahc_dma_seg *segs;
+	int newcount;
+	int i;
+
+	scb_data = ahc->scb_data;
+	if (scb_data->numscbs >= AHC_SCB_MAX)
+		/* Can't allocate any more */
+		return;
+
+	next_scb = &scb_data->scbarray[scb_data->numscbs];
+
+	sg_map = malloc(sizeof(*sg_map), M_DEVBUF, M_NOWAIT);
+
+	if (sg_map == NULL)
+		return;
+
+	/* Allocate S/G space for the next batch of SCBS */
+	if (ahc_dmamem_alloc(ahc, scb_data->sg_dmat,
+			     (void **)&sg_map->sg_vaddr,
+			     BUS_DMA_NOWAIT, &sg_map->sg_dmamap) != 0) {
+		free(sg_map, M_DEVBUF);
+		return;
+	}
+
+	SLIST_INSERT_HEAD(&scb_data->sg_maps, sg_map, links);
+
+	ahc_dmamap_load(ahc, scb_data->sg_dmat, sg_map->sg_dmamap,
+			sg_map->sg_vaddr, PAGE_SIZE, ahc_dmamap_cb,
+			&sg_map->sg_physaddr, /*flags*/0);
+
+	segs = sg_map->sg_vaddr;
+	physaddr = sg_map->sg_physaddr;
+
+	newcount = (PAGE_SIZE / (AHC_NSEG * sizeof(struct ahc_dma_seg)));
+	for (i = 0; scb_data->numscbs < AHC_SCB_MAX && i < newcount; i++) {
+		struct scb_platform_data *pdata;
+#ifndef __linux__
+		int error;
+#endif
+		pdata = (struct scb_platform_data *)malloc(sizeof(*pdata),
+							   M_DEVBUF, M_NOWAIT);
+		if (pdata == NULL)
+			break;
+		next_scb->platform_data = pdata;
+		next_scb->sg_list = segs;
+		/*
+		 * The sequencer always starts with the second entry.
+		 * The first entry is embedded in the scb.
+		 */
+		next_scb->sg_list_phys = physaddr + sizeof(struct ahc_dma_seg);
+		next_scb->ahc_softc = ahc;
+		next_scb->flags = SCB_FREE;
+#ifndef __linux__
+		error = ahc_dmamap_create(ahc, ahc->buffer_dmat, /*flags*/0,
+					  &next_scb->dmamap);
+		if (error != 0)
+			break;
+#endif
+		next_scb->hscb = &scb_data->hscbs[scb_data->numscbs];
+		next_scb->hscb->tag = ahc->scb_data->numscbs;
+		SLIST_INSERT_HEAD(&ahc->scb_data->free_scbs,
+				  next_scb, links.sle);
+		segs += AHC_NSEG;
+		physaddr += (AHC_NSEG * sizeof(struct ahc_dma_seg));
+		next_scb++;
+		ahc->scb_data->numscbs++;
+	}
+}
+
+void
+ahc_controller_info(struct ahc_softc *ahc, char *buf)
+{
+	int len;
+
+	len = sprintf(buf, "%s: ", ahc_chip_names[ahc->chip & AHC_CHIPID_MASK]);
+	buf += len;
+	if ((ahc->features & AHC_TWIN) != 0)
+ 		len = sprintf(buf, "Twin Channel, A SCSI Id=%d, "
+			      "B SCSI Id=%d, primary %c, ",
+			      ahc->our_id, ahc->our_id_b,
+			      ahc->flags & AHC_CHANNEL_B_PRIMARY ? 'B': 'A');
+	else {
+		const char *type;
+
+		if ((ahc->features & AHC_WIDE) != 0) {
+			type = "Wide";
+		} else {
+			type = "Single";
+		}
+		len = sprintf(buf, "%s Channel %c, SCSI Id=%d, ",
+			      type, ahc->channel, ahc->our_id);
+	}
+	buf += len;
+
+	if ((ahc->flags & AHC_PAGESCBS) != 0)
+		sprintf(buf, "%d/%d SCBs",
+			ahc->scb_data->maxhscbs, AHC_SCB_MAX);
+	else
+		sprintf(buf, "%d SCBs", ahc->scb_data->maxhscbs);
+}
+
+/*
+ * Start the board, ready for normal operation
+ */
+int
+ahc_init(struct ahc_softc *ahc)
+{
+	int	 max_targ;
+	int	 i;
+	int	 term;
+	u_int	 scsi_conf;
+	u_int	 scsiseq_template;
+	u_int	 ultraenb;
+	u_int	 discenable;
+	u_int	 tagenable;
+	size_t	 driver_data_size;
+	uint32_t physaddr;
+
+#ifdef AHC_DEBUG_SEQUENCER
+	ahc->flags |= AHC_SEQUENCER_DEBUG;
+#endif
+
+#ifdef AHC_PRINT_SRAM
+	printf("Scratch Ram:");
+	for (i = 0x20; i < 0x5f; i++) {
+		if (((i % 8) == 0) && (i != 0)) {
+			printf ("\n              ");
+		}
+		printf (" 0x%x", ahc_inb(ahc, i));
+	}
+	if ((ahc->features & AHC_MORE_SRAM) != 0) {
+		for (i = 0x70; i < 0x7f; i++) {
+			if (((i % 8) == 0) && (i != 0)) {
+				printf ("\n              ");
+			}
+			printf (" 0x%x", ahc_inb(ahc, i));
+		}
+	}
+	printf ("\n");
+#endif
+	max_targ = 15;
+
+	/*
+	 * Assume we have a board at this stage and it has been reset.
+	 */
+	if ((ahc->flags & AHC_USEDEFAULTS) != 0)
+		ahc->our_id = ahc->our_id_b = 7;
+	
+	/*
+	 * Default to allowing initiator operations.
+	 */
+	ahc->flags |= AHC_INITIATORROLE;
+
+	/*
+	 * Only allow target mode features if this unit has them enabled.
+	 */
+	if ((AHC_TMODE_ENABLE & (0x1 << ahc->unit)) == 0)
+		ahc->features &= ~AHC_TARGETMODE;
+
+#ifndef __linux__
+	/* DMA tag for mapping buffers into device visible space. */
+	if (ahc_dma_tag_create(ahc, ahc->parent_dmat, /*alignment*/1,
+			       /*boundary*/0, /*lowaddr*/BUS_SPACE_MAXADDR,
+			       /*highaddr*/BUS_SPACE_MAXADDR,
+			       /*filter*/NULL, /*filterarg*/NULL,
+			       /*maxsize*/MAXBSIZE, /*nsegments*/AHC_NSEG,
+			       /*maxsegsz*/AHC_MAXTRANSFER_SIZE,
+			       /*flags*/BUS_DMA_ALLOCNOW,
+			       &ahc->buffer_dmat) != 0) {
+		return (ENOMEM);
+	}
+#endif
+
+	ahc->init_level++;
+
+	/*
+	 * DMA tag for our command fifos and other data in system memory
+	 * the card's sequencer must be able to access.  For initiator
+	 * roles, we need to allocate space for the the qinfifo and qoutfifo.
+	 * The qinfifo and qoutfifo are composed of 256 1 byte elements. 
+	 * When providing for the target mode role, we must additionally
+	 * provide space for the incoming target command fifo and an extra
+	 * byte to deal with a dma bug in some chip versions.
+	 */
+	driver_data_size = 2 * 256 * sizeof(uint8_t);
+	if ((ahc->features & AHC_TARGETMODE) != 0)
+		driver_data_size += AHC_TMODE_CMDS * sizeof(struct target_cmd)
+				 + /*DMA WideOdd Bug Buffer*/1;
+	if (ahc_dma_tag_create(ahc, ahc->parent_dmat, /*alignment*/1,
+			       /*boundary*/0, /*lowaddr*/BUS_SPACE_MAXADDR,
+			       /*highaddr*/BUS_SPACE_MAXADDR,
+			       /*filter*/NULL, /*filterarg*/NULL,
+			       driver_data_size,
+			       /*nsegments*/1,
+			       /*maxsegsz*/BUS_SPACE_MAXSIZE_32BIT,
+			       /*flags*/0, &ahc->shared_data_dmat) != 0) {
+		return (ENOMEM);
+	}
+
+	ahc->init_level++;
+
+	/* Allocation of driver data */
+	if (ahc_dmamem_alloc(ahc, ahc->shared_data_dmat,
+			     (void **)&ahc->qoutfifo,
+			     BUS_DMA_NOWAIT, &ahc->shared_data_dmamap) != 0) {
+		return (ENOMEM);
+	}
+
+	ahc->init_level++;
+
+	/* And permanently map it in */
+	ahc_dmamap_load(ahc, ahc->shared_data_dmat, ahc->shared_data_dmamap,
+			ahc->qoutfifo, driver_data_size, ahc_dmamap_cb,
+			&ahc->shared_data_busaddr, /*flags*/0);
+
+	if ((ahc->features & AHC_TARGETMODE) != 0) {
+		ahc->targetcmds = (struct target_cmd *)ahc->qoutfifo;
+		ahc->qoutfifo = (uint8_t *)&ahc->targetcmds[AHC_TMODE_CMDS];
+		ahc->dma_bug_buf = ahc->shared_data_busaddr
+				 + driver_data_size - 1;
+		/* All target command blocks start out invalid. */
+		for (i = 0; i < AHC_TMODE_CMDS; i++)
+			ahc->targetcmds[i].cmd_valid = 0;
+		ahc->tqinfifonext = 1;
+		ahc_outb(ahc, KERNEL_TQINPOS, ahc->tqinfifonext - 1);
+		ahc_outb(ahc, TQINPOS, ahc->tqinfifonext);
+		ahc->qoutfifo = (uint8_t *)&ahc->targetcmds[256];
+	}
+	ahc->qinfifo = &ahc->qoutfifo[256];
+
+	ahc->init_level++;
+
+	/* Allocate SCB data now that buffer_dmat is initialized */
+	if (ahc->scb_data->maxhscbs == 0)
+		if (ahc_init_scbdata(ahc) != 0)
+			return (ENOMEM);
+
+	/*
+	 * Allocate a tstate to house information for our
+	 * initiator presence on the bus as well as the user
+	 * data for any target mode initiator.
+	 */
+	if (ahc_alloc_tstate(ahc, ahc->our_id, 'A') == NULL) {
+		printf("%s: unable to allocate tmode_tstate.  "
+		       "Failing attach\n", ahc_name(ahc));
+		return (-1);
+	}
+
+	if ((ahc->features & AHC_TWIN) != 0) {
+		if (ahc_alloc_tstate(ahc, ahc->our_id_b, 'B') == NULL) {
+			printf("%s: unable to allocate tmode_tstate.  "
+			       "Failing attach\n", ahc_name(ahc));
+			return (-1);
+		}
+	}
+
+	ahc_outb(ahc, SEQ_FLAGS, 0);
+
+	if (ahc->scb_data->maxhscbs < AHC_SCB_MAX) {
+		ahc->flags |= AHC_PAGESCBS;
+	} else {
+		ahc->flags &= ~AHC_PAGESCBS;
+	}
+
+#ifdef AHC_DEBUG
+	if (ahc_debug & AHC_SHOWMISC) {
+		printf("%s: hardware scb %d bytes; kernel scb %d bytes; "
+		       "ahc_dma %d bytes\n",
+			ahc_name(ahc),
+			sizeof(struct hardware_scb),
+			sizeof(struct scb),
+			sizeof(struct ahc_dma_seg));
+	}
+#endif /* AHC_DEBUG */
+
+	/* Set the SCSI Id, SXFRCTL0, SXFRCTL1, and SIMODE1, for both channels*/
+	if (ahc->features & AHC_TWIN) {
+
+		/*
+		 * The device is gated to channel B after a chip reset,
+		 * so set those values first
+		 */
+		ahc_outb(ahc, SBLKCTL, ahc_inb(ahc, SBLKCTL) | SELBUSB);
+		term = (ahc->flags & AHC_TERM_ENB_B) != 0 ? STPWEN : 0;
+		ahc_outb(ahc, SCSIID, ahc->our_id_b);
+		scsi_conf = ahc_inb(ahc, SCSICONF + 1);
+		ahc_outb(ahc, SXFRCTL1, (scsi_conf & (ENSPCHK|STIMESEL))
+					|term|ahc->seltime_b|ENSTIMER|ACTNEGEN);
+		if ((ahc->features & AHC_ULTRA2) != 0)
+			ahc_outb(ahc, SIMODE0, ahc_inb(ahc, SIMODE0)|ENIOERR);
+		ahc_outb(ahc, SIMODE1, ENSELTIMO|ENSCSIRST|ENSCSIPERR);
+		ahc_outb(ahc, SXFRCTL0, DFON|SPIOEN);
+
+		if ((scsi_conf & RESET_SCSI) != 0
+		 && (ahc->flags & AHC_INITIATORROLE) != 0)
+			ahc->flags |= AHC_RESET_BUS_B;
+
+		/* Select Channel A */
+		ahc_outb(ahc, SBLKCTL, ahc_inb(ahc, SBLKCTL) & ~SELBUSB);
+	}
+	term = (ahc->flags & AHC_TERM_ENB_A) != 0 ? STPWEN : 0;
+	if ((ahc->features & AHC_ULTRA2) != 0)
+		ahc_outb(ahc, SCSIID_ULTRA2, ahc->our_id);
+	else
+		ahc_outb(ahc, SCSIID, ahc->our_id);
+	scsi_conf = ahc_inb(ahc, SCSICONF);
+	ahc_outb(ahc, SXFRCTL1, (scsi_conf & (ENSPCHK|STIMESEL))
+				|term|ahc->seltime
+				|ENSTIMER|ACTNEGEN);
+	if ((ahc->features & AHC_ULTRA2) != 0)
+		ahc_outb(ahc, SIMODE0, ahc_inb(ahc, SIMODE0)|ENIOERR);
+	ahc_outb(ahc, SIMODE1, ENSELTIMO|ENSCSIRST|ENSCSIPERR);
+	ahc_outb(ahc, SXFRCTL0, DFON|SPIOEN);
+
+	if ((scsi_conf & RESET_SCSI) != 0
+	 && (ahc->flags & AHC_INITIATORROLE) != 0)
+		ahc->flags |= AHC_RESET_BUS_A;
+
+	/*
+	 * Look at the information that board initialization or
+	 * the board bios has left us.
+	 */
+	ultraenb = 0;	
+	tagenable = ALL_TARGETS_MASK;
+
+	/* Grab the disconnection disable table and invert it for our needs */
+	if (ahc->flags & AHC_USEDEFAULTS) {
+		printf("%s: Host Adapter Bios disabled.  Using default SCSI "
+			"device parameters\n", ahc_name(ahc));
+		ahc->flags |= AHC_EXTENDED_TRANS_A|AHC_EXTENDED_TRANS_B|
+			      AHC_TERM_ENB_A|AHC_TERM_ENB_B;
+		discenable = ALL_TARGETS_MASK;
+		if ((ahc->features & AHC_ULTRA) != 0)
+			ultraenb = ALL_TARGETS_MASK;
+	} else {
+		discenable = ~((ahc_inb(ahc, DISC_DSB + 1) << 8)
+			   | ahc_inb(ahc, DISC_DSB));
+		if ((ahc->features & (AHC_ULTRA|AHC_ULTRA2)) != 0)
+			ultraenb = (ahc_inb(ahc, ULTRA_ENB + 1) << 8)
+				      | ahc_inb(ahc, ULTRA_ENB);
+	}
+
+	if ((ahc->features & (AHC_WIDE|AHC_TWIN)) == 0)
+		max_targ = 7;
+
+	for (i = 0; i <= max_targ; i++) {
+		struct ahc_initiator_tinfo *tinfo;
+		struct tmode_tstate *tstate;
+		u_int our_id;
+		u_int target_id;
+		char channel;
+
+		channel = 'A';
+		our_id = ahc->our_id;
+		target_id = i;
+		if (i > 7 && (ahc->features & AHC_TWIN) != 0) {
+			channel = 'B';
+			our_id = ahc->our_id_b;
+			target_id = i % 8;
+		}
+		tinfo = ahc_fetch_transinfo(ahc, channel, our_id,
+					    target_id, &tstate);
+		/* Default to async narrow across the board */
+		memset(tinfo, 0, sizeof(*tinfo));
+		if (ahc->flags & AHC_USEDEFAULTS) {
+			if ((ahc->features & AHC_WIDE) != 0)
+				tinfo->user.width = MSG_EXT_WDTR_BUS_16_BIT;
+
+			/*
+			 * These will be truncated when we determine the
+			 * connection type we have with the target.
+			 */
+			tinfo->user.period = ahc_syncrates->period;
+			tinfo->user.offset = ~0;
+		} else {
+			u_int scsirate;
+			uint16_t mask;
+
+			/* Take the settings leftover in scratch RAM. */
+			scsirate = ahc_inb(ahc, TARG_SCSIRATE + i);
+			mask = (0x01 << i);
+			if ((ahc->features & AHC_ULTRA2) != 0) {
+				u_int offset;
+				u_int maxsync;
+
+				if ((scsirate & SOFS) == 0x0F) {
+					/*
+					 * Haven't negotiated yet,
+					 * so the format is different.
+					 */
+					scsirate = (scsirate & SXFR) >> 4
+						 | (ultraenb & mask)
+						  ? 0x08 : 0x0
+						 | (scsirate & WIDEXFER);
+					offset = MAX_OFFSET_ULTRA2;
+				} else
+					offset = ahc_inb(ahc, TARG_OFFSET + i);
+				maxsync = AHC_SYNCRATE_ULTRA2;
+				if ((ahc->features & AHC_DT) != 0)
+					maxsync = AHC_SYNCRATE_DT;
+				tinfo->user.period =
+				    ahc_find_period(ahc, scsirate, maxsync);
+				if (offset == 0)
+					tinfo->user.period = 0;
+				else
+					tinfo->user.offset = ~0;
+				if ((scsirate & SXFR_ULTRA2) <= 8/*10MHz*/
+				 && (ahc->features & AHC_DT) != 0)
+					tinfo->user.ppr_options =
+					    MSG_EXT_PPR_DT_REQ;
+			} else if ((scsirate & SOFS) != 0) {
+				if ((scsirate & SXFR) == 0x40
+				 && (ultraenb & mask) != 0) {
+					/* Treat 10MHz as a non-ultra speed */
+					scsirate &= ~SXFR;
+				 	ultraenb &= ~mask;
+				}
+				tinfo->user.period = 
+				    ahc_find_period(ahc, scsirate,
+						    (ultraenb & mask)
+						   ? AHC_SYNCRATE_ULTRA
+						   : AHC_SYNCRATE_FAST);
+				if (tinfo->user.period != 0)
+					tinfo->user.offset = ~0;
+			}
+			if ((scsirate & WIDEXFER) != 0
+			 && (ahc->features & AHC_WIDE) != 0)
+				tinfo->user.width = MSG_EXT_WDTR_BUS_16_BIT;
+			tinfo->user.protocol_version = 4;
+			if ((ahc->features & AHC_DT) != 0)
+				tinfo->user.transport_version = 3;
+			else
+				tinfo->user.transport_version = 2;
+			tinfo->goal.protocol_version = 2;
+			tinfo->goal.transport_version = 2;
+			tinfo->current.protocol_version = 2;
+			tinfo->current.transport_version = 2;
+		}
+		tstate->ultraenb = ultraenb;
+		tstate->discenable = discenable;
+		tstate->tagenable = 0; /* Wait until the XPT says its okay */
+	}
+	ahc->user_discenable = discenable;
+	ahc->user_tagenable = tagenable;
+
+	/* There are no untagged SCBs active yet. */
+	for (i = 0; i < 16; i++) {
+		ahc_unbusy_tcl(ahc, BUILD_TCL(i << 4, 0));
+		if ((ahc->flags & AHC_SCB_BTT) != 0) {
+			int lun;
+
+			/*
+			 * The SCB based BTT allows an entry per
+			 * target and lun pair.
+			 */
+			for (lun = 1; lun < AHC_NUM_LUNS; lun++)
+				ahc_unbusy_tcl(ahc, BUILD_TCL(i << 4, lun));
+		}
+	}
+
+	/* All of our queues are empty */
+	for (i = 0; i < 256; i++)
+		ahc->qoutfifo[i] = SCB_LIST_NULL;
+
+	for (i = 0; i < 256; i++)
+		ahc->qinfifo[i] = SCB_LIST_NULL;
+
+	if ((ahc->features & AHC_MULTI_TID) != 0) {
+		ahc_outb(ahc, TARGID, 0);
+		ahc_outb(ahc, TARGID + 1, 0);
+	}
+
+	/*
+	 * Tell the sequencer where it can find our arrays in memory.
+	 */
+	physaddr = ahc->scb_data->hscb_busaddr;
+	ahc_outb(ahc, HSCB_ADDR, physaddr & 0xFF);
+	ahc_outb(ahc, HSCB_ADDR + 1, (physaddr >> 8) & 0xFF);
+	ahc_outb(ahc, HSCB_ADDR + 2, (physaddr >> 16) & 0xFF);
+	ahc_outb(ahc, HSCB_ADDR + 3, (physaddr >> 24) & 0xFF);
+
+	physaddr = ahc->shared_data_busaddr;
+	ahc_outb(ahc, SHARED_DATA_ADDR, physaddr & 0xFF);
+	ahc_outb(ahc, SHARED_DATA_ADDR + 1, (physaddr >> 8) & 0xFF);
+	ahc_outb(ahc, SHARED_DATA_ADDR + 2, (physaddr >> 16) & 0xFF);
+	ahc_outb(ahc, SHARED_DATA_ADDR + 3, (physaddr >> 24) & 0xFF);
+
+	/*
+	 * Initialize the group code to command length table.
+	 * This overrides the values in TARG_SCSIRATE, so only
+	 * setup the table after we have processed that information.
+	 */
+	ahc_outb(ahc, CMDSIZE_TABLE, 5);
+	ahc_outb(ahc, CMDSIZE_TABLE + 1, 9);
+	ahc_outb(ahc, CMDSIZE_TABLE + 2, 9);
+	ahc_outb(ahc, CMDSIZE_TABLE + 3, 0);
+	ahc_outb(ahc, CMDSIZE_TABLE + 4, 15);
+	ahc_outb(ahc, CMDSIZE_TABLE + 5, 11);
+	ahc_outb(ahc, CMDSIZE_TABLE + 6, 0);
+	ahc_outb(ahc, CMDSIZE_TABLE + 7, 0);
+		
+	/* Tell the sequencer of our initial queue positions */
+	ahc_outb(ahc, KERNEL_QINPOS, 0);
+	ahc_outb(ahc, QINPOS, 0);
+	ahc_outb(ahc, QOUTPOS, 0);
+
+	/* Don't have any special messages to send to targets */
+	ahc_outb(ahc, TARGET_MSG_REQUEST, 0);
+	ahc_outb(ahc, TARGET_MSG_REQUEST + 1, 0);
+
+	/*
+	 * Use the built in queue management registers
+	 * if they are available.
+	 */
+	if ((ahc->features & AHC_QUEUE_REGS) != 0) {
+		ahc_outb(ahc, QOFF_CTLSTA, SCB_QSIZE_256);
+		ahc_outb(ahc, SDSCB_QOFF, 0);
+		ahc_outb(ahc, SNSCB_QOFF, 0);
+		ahc_outb(ahc, HNSCB_QOFF, 0);
+	}
+
+
+	/* We don't have any waiting selections */
+	ahc_outb(ahc, WAITING_SCBH, SCB_LIST_NULL);
+
+	/* Our disconnection list is empty too */
+	ahc_outb(ahc, DISCONNECTED_SCBH, SCB_LIST_NULL);
+
+	/* Message out buffer starts empty */
+	ahc_outb(ahc, MSG_OUT, MSG_NOOP);
+
+	/*
+	 * Setup the allowed SCSI Sequences based on operational mode.
+	 * If we are a target, we'll enalbe select in operations once
+	 * we've had a lun enabled.
+	 */
+	scsiseq_template = ENSELO|ENAUTOATNO|ENAUTOATNP;
+	if ((ahc->flags & AHC_INITIATORROLE) != 0)
+		scsiseq_template |= ENRSELI;
+	ahc_outb(ahc, SCSISEQ_TEMPLATE, scsiseq_template);
+
+	/*
+	 * Load the Sequencer program and Enable the adapter
+	 * in "fast" mode.
+	 */
+	if (bootverbose)
+		printf("%s: Downloading Sequencer Program...",
+		       ahc_name(ahc));
+
+	ahc_loadseq(ahc);
+
+	if ((ahc->features & AHC_ULTRA2) != 0) {
+		int wait;
+
+		/*
+		 * Wait for up to 500ms for our transceivers
+		 * to settle.  If the adapter does not have
+		 * a cable attached, the tranceivers may
+		 * never settle, so don't complain if we
+		 * fail here.
+		 */
+		pause_sequencer(ahc);
+		for (wait = 5000;
+		     (ahc_inb(ahc, SBLKCTL) & (ENAB40|ENAB20)) == 0 && wait;
+		     wait--)
+			ahc_delay(100);
+		unpause_sequencer(ahc);
+	}
+	return (0);
+}
+
+/*
+ * Ensure that the card is paused in a location
+ * outside of all critical sections and that all
+ * pending work is completed prior to returning.
+ * This routine should only be called from outside
+ * an interrupt context.
+ */
+void
+ahc_pause_and_flushwork(struct ahc_softc *ahc)
+{
+	int intstat;
+	int maxloops;
+
+	maxloops = 1000;
+	ahc->flags |= AHC_ALL_INTERRUPTS;
+	intstat = 0;
+	do {
+		ahc_intr(ahc);
+		pause_sequencer(ahc);
+		ahc_clear_critical_section(ahc);
+		if (intstat == 0xFF && (ahc->features & AHC_REMOVABLE) != 0)
+			break;
+		maxloops--;
+	} while (((intstat = ahc_inb(ahc, INTSTAT)) & INT_PEND) && --maxloops);
+	if (maxloops == 0) {
+		printf("Infinite interrupt loop, INTSTAT = %x",
+		      ahc_inb(ahc, INTSTAT));
+	}
+	ahc_platform_flushwork(ahc);
+	ahc->flags &= ~AHC_ALL_INTERRUPTS;
+}
+
+int
+ahc_suspend(struct ahc_softc *ahc)
+{
+	uint8_t *ptr;
+	int	 i;
+
+	ahc_pause_and_flushwork(ahc);
+
+	if (LIST_FIRST(&ahc->pending_scbs) != NULL)
+		return (EBUSY);
+
+#if AHC_TARGET_MODE
+	/*
+	 * XXX What about ATIOs that have not yet been serviced?
+	 * Perhaps we should just refuse to be suspended if we
+	 * are acting in a target role.
+	 */
+	if (ahc->pending_device != NULL)
+		return (EBUSY);
+#endif
+
+	/* Save volatile registers */
+	if ((ahc->features & AHC_TWIN) != 0) {
+		ahc_outb(ahc, SBLKCTL, ahc_inb(ahc, SBLKCTL) | SELBUSB);
+		ahc->suspend_state.channel[1].scsiseq = ahc_inb(ahc, SCSISEQ);
+		ahc->suspend_state.channel[1].sxfrctl0 = ahc_inb(ahc, SXFRCTL0);
+		ahc->suspend_state.channel[1].sxfrctl1 = ahc_inb(ahc, SXFRCTL1);
+		ahc->suspend_state.channel[1].simode0 = ahc_inb(ahc, SIMODE0);
+		ahc->suspend_state.channel[1].simode1 = ahc_inb(ahc, SIMODE1);
+		ahc->suspend_state.channel[1].seltimer = ahc_inb(ahc, SELTIMER);
+		ahc->suspend_state.channel[1].seqctl = ahc_inb(ahc, SEQCTL);
+		ahc_outb(ahc, SBLKCTL, ahc_inb(ahc, SBLKCTL) & ~SELBUSB);
+	}
+	ahc->suspend_state.channel[0].scsiseq = ahc_inb(ahc, SCSISEQ);
+	ahc->suspend_state.channel[0].sxfrctl0 = ahc_inb(ahc, SXFRCTL0);
+	ahc->suspend_state.channel[0].sxfrctl1 = ahc_inb(ahc, SXFRCTL1);
+	ahc->suspend_state.channel[0].simode0 = ahc_inb(ahc, SIMODE0);
+	ahc->suspend_state.channel[0].simode1 = ahc_inb(ahc, SIMODE1);
+	ahc->suspend_state.channel[0].seltimer = ahc_inb(ahc, SELTIMER);
+	ahc->suspend_state.channel[0].seqctl = ahc_inb(ahc, SEQCTL);
+
+	if ((ahc->chip & AHC_PCI) != 0) {
+		ahc->suspend_state.dscommand0 = ahc_inb(ahc, DSCOMMAND0);
+		ahc->suspend_state.dspcistatus = ahc_inb(ahc, DSPCISTATUS);
+	}
+
+	if ((ahc->features & AHC_DT) != 0) {
+		u_int sfunct;
+
+		sfunct = ahc_inb(ahc, SFUNCT) & ~ALT_MODE;
+		ahc_outb(ahc, SFUNCT, sfunct | ALT_MODE);
+		ahc->suspend_state.optionmode = ahc_inb(ahc, OPTIONMODE);
+		ahc_outb(ahc, SFUNCT, sfunct);
+		ahc->suspend_state.crccontrol1 = ahc_inb(ahc, CRCCONTROL1);
+	}
+
+	if ((ahc->features & AHC_MULTI_FUNC) != 0)
+		ahc->suspend_state.scbbaddr = ahc_inb(ahc, SCBBADDR);
+
+	if ((ahc->features & AHC_ULTRA2) != 0)
+		ahc->suspend_state.dff_thrsh = ahc_inb(ahc, DFF_THRSH);
+
+	ptr = ahc->suspend_state.scratch_ram;
+	for (i = 0; i < 64; i++)
+		*ptr++ = ahc_inb(ahc, SRAM_BASE + i);
+
+	if ((ahc->features & AHC_MORE_SRAM) != 0) {
+		for (i = 0; i < 16; i++)
+			*ptr++ = ahc_inb(ahc, TARG_OFFSET + i);
+	}
+
+	ptr = ahc->suspend_state.btt;
+	if ((ahc->flags & AHC_SCB_BTT) != 0) {
+		for (i = 0;i < AHC_NUM_TARGETS; i++) {
+			int j;
+
+			for (j = 0;j < AHC_NUM_LUNS; j++) {
+				u_int tcl;
+
+				tcl = BUILD_TCL(i << 4, j);
+				*ptr = ahc_index_busy_tcl(ahc, tcl);
+			}
+		}
+	}
+	ahc_shutdown(ahc);
+	return (0);
+}
+
+int
+ahc_resume(struct ahc_softc *ahc)
+{
+	uint8_t *ptr;
+	int	 i;
+
+	ahc_reset(ahc);
+
+	ahc_build_free_scb_list(ahc);
+
+	/* Restore volatile registers */
+	if ((ahc->features & AHC_TWIN) != 0) {
+		ahc_outb(ahc, SBLKCTL, ahc_inb(ahc, SBLKCTL) | SELBUSB);
+		ahc_outb(ahc, SCSIID, ahc->our_id);
+		ahc_outb(ahc, SCSISEQ, ahc->suspend_state.channel[1].scsiseq);
+		ahc_outb(ahc, SXFRCTL0, ahc->suspend_state.channel[1].sxfrctl0);
+		ahc_outb(ahc, SXFRCTL1, ahc->suspend_state.channel[1].sxfrctl1);
+		ahc_outb(ahc, SIMODE0, ahc->suspend_state.channel[1].simode0);
+		ahc_outb(ahc, SIMODE1, ahc->suspend_state.channel[1].simode1);
+		ahc_outb(ahc, SELTIMER, ahc->suspend_state.channel[1].seltimer);
+		ahc_outb(ahc, SEQCTL, ahc->suspend_state.channel[1].seqctl);
+		ahc_outb(ahc, SBLKCTL, ahc_inb(ahc, SBLKCTL) & ~SELBUSB);
+	}
+	ahc_outb(ahc, SCSISEQ, ahc->suspend_state.channel[0].scsiseq);
+	ahc_outb(ahc, SXFRCTL0, ahc->suspend_state.channel[0].sxfrctl0);
+	ahc_outb(ahc, SXFRCTL1, ahc->suspend_state.channel[0].sxfrctl1);
+	ahc_outb(ahc, SIMODE0, ahc->suspend_state.channel[0].simode0);
+	ahc_outb(ahc, SIMODE1, ahc->suspend_state.channel[0].simode1);
+	ahc_outb(ahc, SELTIMER, ahc->suspend_state.channel[0].seltimer);
+	ahc_outb(ahc, SEQCTL, ahc->suspend_state.channel[0].seqctl);
+	if ((ahc->features & AHC_ULTRA2) != 0)
+		ahc_outb(ahc, SCSIID_ULTRA2, ahc->our_id);
+	else
+		ahc_outb(ahc, SCSIID, ahc->our_id);
+
+	if ((ahc->chip & AHC_PCI) != 0) {
+		ahc_outb(ahc, DSCOMMAND0, ahc->suspend_state.dscommand0);
+		ahc_outb(ahc, DSPCISTATUS, ahc->suspend_state.dspcistatus);
+	}
+
+	if ((ahc->features & AHC_DT) != 0) {
+		u_int sfunct;
+
+		sfunct = ahc_inb(ahc, SFUNCT) & ~ALT_MODE;
+		ahc_outb(ahc, SFUNCT, sfunct | ALT_MODE);
+		ahc_outb(ahc, OPTIONMODE, ahc->suspend_state.optionmode);
+		ahc_outb(ahc, SFUNCT, sfunct);
+		ahc_outb(ahc, CRCCONTROL1, ahc->suspend_state.crccontrol1);
+	}
+
+	if ((ahc->features & AHC_MULTI_FUNC) != 0)
+		ahc_outb(ahc, SCBBADDR, ahc->suspend_state.scbbaddr);
+
+	if ((ahc->features & AHC_ULTRA2) != 0)
+		ahc_outb(ahc, DFF_THRSH, ahc->suspend_state.dff_thrsh);
+
+	ptr = ahc->suspend_state.scratch_ram;
+	for (i = 0; i < 64; i++)
+		ahc_outb(ahc, SRAM_BASE + i, *ptr++);
+
+	if ((ahc->features & AHC_MORE_SRAM) != 0) {
+		for (i = 0; i < 16; i++)
+			ahc_outb(ahc, TARG_OFFSET + i, *ptr++);
+	}
+
+	ptr = ahc->suspend_state.btt;
+	if ((ahc->flags & AHC_SCB_BTT) != 0) {
+		for (i = 0;i < AHC_NUM_TARGETS; i++) {
+			int j;
+
+			for (j = 0;j < AHC_NUM_LUNS; j++) {
+				u_int tcl;
+
+				tcl = BUILD_TCL(i << 4, j);
+				ahc_busy_tcl(ahc, tcl, *ptr);
+			}
+		}
+	}
+	return (0);
+}
+
+/************************** Busy Target Table *********************************/
+/*
+ * Return the untagged transaction id for a given target/channel lun.
+ * Optionally, clear the entry.
+ */
+u_int
+ahc_index_busy_tcl(struct ahc_softc *ahc, u_int tcl)
+{
+	u_int scbid;
+	u_int target_offset;
+
+	if ((ahc->flags & AHC_SCB_BTT) != 0) {
+		u_int saved_scbptr;
+		
+		saved_scbptr = ahc_inb(ahc, SCBPTR);
+		ahc_outb(ahc, SCBPTR, TCL_LUN(tcl));
+		scbid = ahc_inb(ahc, SCB_64_BTT + TCL_TARGET_OFFSET(tcl));
+		ahc_outb(ahc, SCBPTR, saved_scbptr);
+	} else {
+		target_offset = TCL_TARGET_OFFSET(tcl);
+		scbid = ahc_inb(ahc, BUSY_TARGETS + target_offset);
+	}
+
+	return (scbid);
+}
+
+void
+ahc_unbusy_tcl(struct ahc_softc *ahc, u_int tcl)
+{
+	u_int target_offset;
+
+	if ((ahc->flags & AHC_SCB_BTT) != 0) {
+		u_int saved_scbptr;
+		
+		saved_scbptr = ahc_inb(ahc, SCBPTR);
+		ahc_outb(ahc, SCBPTR, TCL_LUN(tcl));
+		ahc_outb(ahc, SCB_64_BTT+TCL_TARGET_OFFSET(tcl), SCB_LIST_NULL);
+		ahc_outb(ahc, SCBPTR, saved_scbptr);
+	} else {
+		target_offset = TCL_TARGET_OFFSET(tcl);
+		ahc_outb(ahc, BUSY_TARGETS + target_offset, SCB_LIST_NULL);
+	}
+}
+
+void
+ahc_busy_tcl(struct ahc_softc *ahc, u_int tcl, u_int scbid)
+{
+	u_int target_offset;
+
+	if ((ahc->flags & AHC_SCB_BTT) != 0) {
+		u_int saved_scbptr;
+		
+		saved_scbptr = ahc_inb(ahc, SCBPTR);
+		ahc_outb(ahc, SCBPTR, TCL_LUN(tcl));
+		ahc_outb(ahc, SCB_64_BTT + TCL_TARGET_OFFSET(tcl), scbid);
+		ahc_outb(ahc, SCBPTR, saved_scbptr);
+	} else {
+		target_offset = TCL_TARGET_OFFSET(tcl);
+		ahc_outb(ahc, BUSY_TARGETS + target_offset, scbid);
+	}
+}
+
+/************************** SCB and SCB queue management **********************/
+int
+ahc_match_scb(struct ahc_softc *ahc, struct scb *scb, int target,
+	      char channel, int lun, u_int tag, role_t role)
+{
+	int targ = SCB_GET_TARGET(ahc, scb);
+	char chan = SCB_GET_CHANNEL(ahc, scb);
+	int slun = SCB_GET_LUN(scb);
+	int match;
+
+	match = ((chan == channel) || (channel == ALL_CHANNELS));
+	if (match != 0)
+		match = ((targ == target) || (target == CAM_TARGET_WILDCARD));
+	if (match != 0)
+		match = ((lun == slun) || (lun == CAM_LUN_WILDCARD));
+	if (match != 0) {
+#if AHC_TARGET_MODE
+		int group;
+
+		group = XPT_FC_GROUP(scb->io_ctx->ccb_h.func_code);
+		if (role == ROLE_INITIATOR) {
+			match = (group == XPT_FC_GROUP_COMMON)
+			      && ((tag == scb->hscb->tag)
+			       || (tag == SCB_LIST_NULL));
+		} else if (role == ROLE_TARGET) {
+			match = (group == XPT_FC_GROUP_TMODE)
+			      && ((tag == scb->io_ctx->csio.tag_id)
+			       || (tag == SCB_LIST_NULL));
+		}
+#else /* !AHC_TARGET_MODE */
+		match = ((tag == scb->hscb->tag) || (tag == SCB_LIST_NULL));
+#endif /* AHC_TARGET_MODE */
+	}
+
+	return match;
+}
+
+void
+ahc_freeze_devq(struct ahc_softc *ahc, struct scb *scb)
+{
+	int	target;
+	char	channel;
+	int	lun;
+
+	target = SCB_GET_TARGET(ahc, scb);
+	lun = SCB_GET_LUN(scb);
+	channel = SCB_GET_CHANNEL(ahc, scb);
+	
+	ahc_search_qinfifo(ahc, target, channel, lun,
+			   /*tag*/SCB_LIST_NULL, ROLE_UNKNOWN,
+			   CAM_REQUEUE_REQ, SEARCH_COMPLETE);
+
+	ahc_platform_freeze_devq(ahc, scb);
+}
+
+void
+ahc_qinfifo_requeue_tail(struct ahc_softc *ahc, struct scb *scb)
+{
+	struct scb *prev_scb;
+
+	prev_scb = NULL;
+	if (ahc_qinfifo_count(ahc) != 0) {
+		u_int prev_tag;
+		uint8_t prev_pos;
+
+		prev_pos = ahc->qinfifonext - 1;
+		prev_tag = ahc->qinfifo[prev_pos];
+		prev_scb = ahc_lookup_scb(ahc, prev_tag);
+	}
+	ahc_qinfifo_requeue(ahc, prev_scb, scb);
+	if ((ahc->features & AHC_QUEUE_REGS) != 0) {
+		ahc_outb(ahc, HNSCB_QOFF, ahc->qinfifonext);
+	} else {
+		ahc_outb(ahc, KERNEL_QINPOS, ahc->qinfifonext);
+	}
+}
+
+static void
+ahc_qinfifo_requeue(struct ahc_softc *ahc, struct scb *prev_scb,
+		    struct scb *scb)
+{
+	if (prev_scb == NULL)
+		ahc_outb(ahc, NEXT_QUEUED_SCB, scb->hscb->tag);
+	else
+		prev_scb->hscb->next = scb->hscb->tag;
+	ahc->qinfifo[ahc->qinfifonext++] = scb->hscb->tag;
+	scb->hscb->next = ahc->next_queued_scb->hscb->tag;
+}
+
+static int
+ahc_qinfifo_count(struct ahc_softc *ahc)
+{
+	u_int8_t qinpos;
+	u_int8_t diff;
+
+	if ((ahc->features & AHC_QUEUE_REGS) != 0) {
+		qinpos = ahc_inb(ahc, SNSCB_QOFF);
+		ahc_outb(ahc, SNSCB_QOFF, qinpos);
+	} else
+		qinpos = ahc_inb(ahc, QINPOS);
+	diff = ahc->qinfifonext - qinpos;
+	return (diff);
+}
+
+int
+ahc_search_qinfifo(struct ahc_softc *ahc, int target, char channel,
+		   int lun, u_int tag, role_t role, uint32_t status,
+		   ahc_search_action action)
+{
+	struct	scb *scb;
+	struct	scb *prev_scb;
+	uint8_t qinstart;
+	uint8_t qinpos;
+	uint8_t qintail;
+	uint8_t next, prev;
+	uint8_t curscbptr;
+	int	found;
+	int	maxtarget;
+	int	i;
+	int	have_qregs;
+
+	qintail = ahc->qinfifonext;
+	have_qregs = (ahc->features & AHC_QUEUE_REGS) != 0;
+	if (have_qregs) {
+		qinstart = ahc_inb(ahc, SNSCB_QOFF);
+		ahc_outb(ahc, SNSCB_QOFF, qinstart);
+	} else
+		qinstart = ahc_inb(ahc, QINPOS);
+	qinpos = qinstart;
+	next = ahc_inb(ahc, NEXT_QUEUED_SCB);
+	found = 0;
+	prev_scb = NULL;
+
+	if (action == SEARCH_COMPLETE) {
+		/*
+		 * Don't attempt to run any queued untagged transactions
+		 * until we are done with the abort process.
+		 */
+		ahc_freeze_untagged_queues(ahc);
+	}
+
+	/*
+	 * Start with an empty queue.  Entries that are not chosen
+	 * for removal will be re-added to the queue as we go.
+	 */
+	ahc->qinfifonext = qinpos;
+	ahc_outb(ahc, NEXT_QUEUED_SCB, ahc->next_queued_scb->hscb->tag);
+
+	while (qinpos != qintail) {
+		scb = ahc_lookup_scb(ahc, ahc->qinfifo[qinpos]);
+		if (ahc_match_scb(ahc, scb, target, channel, lun, tag, role)) {
+			/*
+			 * We found an scb that needs to be acted on.
+			 */
+			found++;
+			switch (action) {
+			case SEARCH_COMPLETE:
+			{
+				cam_status ostat;
+
+				ostat = ahc_get_transaction_status(scb);
+				if (ostat == CAM_REQ_INPROG)
+					ahc_set_transaction_status(scb,
+								   status);
+				ahc_freeze_scb(scb);
+				if ((scb->flags & SCB_ACTIVE) == 0)
+					printf("Inactive SCB in qinfifo\n");
+				ahc_done(ahc, scb);
+
+				/* FALLTHROUGH */
+			case SEARCH_REMOVE:
+				break;
+			}
+			case SEARCH_COUNT:
+				ahc_qinfifo_requeue(ahc, prev_scb, scb);
+				prev_scb = scb;
+				break;
+			}
+		} else {
+			ahc_qinfifo_requeue(ahc, prev_scb, scb);
+			prev_scb = scb;
+		}
+		qinpos++;
+	}
+
+	if ((ahc->features & AHC_QUEUE_REGS) != 0) {
+		ahc_outb(ahc, HNSCB_QOFF, ahc->qinfifonext);
+	} else {
+		ahc_outb(ahc, KERNEL_QINPOS, ahc->qinfifonext);
+	}
+
+	if (action != SEARCH_COUNT
+	 && (found != 0)
+	 && (qinstart != ahc->qinfifonext)) {
+		/*
+		 * The sequencer may be in the process of dmaing
+		 * down the SCB at the beginning of the queue.
+		 * This could be problematic if either the first,
+		 * or the second SCB is removed from the queue
+		 * (the first SCB includes a pointer to the "next"
+		 * SCB to dma). If we have removed any entries, swap
+		 * the first element in the queue with the next HSCB
+		 * so the sequencer will notice that NEXT_QUEUED_SCB
+		 * has changed during its dma attempt and will retry
+		 * the DMA.
+		 */
+		scb = ahc_lookup_scb(ahc, ahc->qinfifo[qinstart]);
+
+		/*
+		 * ahc_swap_with_next_hscb forces our next pointer to
+		 * point to the reserved SCB for future commands.  Save
+		 * and restore our original next pointer to maintain
+		 * queue integrity.
+		 */
+		next = scb->hscb->next;
+		ahc->scb_data->scbindex[scb->hscb->tag] = NULL;
+		ahc_swap_with_next_hscb(ahc, scb);
+		scb->hscb->next = next;
+		ahc->qinfifo[qinstart] = scb->hscb->tag;
+
+		/* Tell the card about the new head of the qinfifo. */
+		ahc_outb(ahc, NEXT_QUEUED_SCB, scb->hscb->tag);
+
+		/* Fixup the tail "next" pointer. */
+		qintail = ahc->qinfifonext - 1;
+		scb = ahc_lookup_scb(ahc, ahc->qinfifo[qintail]);
+		scb->hscb->next = ahc->next_queued_scb->hscb->tag;
+	}
+
+	/*
+	 * Search waiting for selection list.
+	 */
+	curscbptr = ahc_inb(ahc, SCBPTR);
+	next = ahc_inb(ahc, WAITING_SCBH);  /* Start at head of list. */
+	prev = SCB_LIST_NULL;
+
+	while (next != SCB_LIST_NULL) {
+		uint8_t scb_index;
+
+		ahc_outb(ahc, SCBPTR, next);
+		scb_index = ahc_inb(ahc, SCB_TAG);
+		if (scb_index >= ahc->scb_data->numscbs) {
+			printf("Waiting List inconsistency. "
+			       "SCB index == %d, yet numscbs == %d.",
+			       scb_index, ahc->scb_data->numscbs);
+			ahc_dump_card_state(ahc);
+			panic("for safety");
+		}
+		scb = ahc_lookup_scb(ahc, scb_index);
+		if (ahc_match_scb(ahc, scb, target, channel,
+				  lun, SCB_LIST_NULL, role)) {
+			/*
+			 * We found an scb that needs to be acted on.
+			 */
+			found++;
+			switch (action) {
+			case SEARCH_COMPLETE:
+			{
+				cam_status ostat;
+
+				ostat = ahc_get_transaction_status(scb);
+				if (ostat == CAM_REQ_INPROG)
+					ahc_set_transaction_status(scb,
+								   status);
+				ahc_freeze_scb(scb);
+				if ((scb->flags & SCB_ACTIVE) == 0)
+					printf("Inactive SCB in Waiting List\n");
+				ahc_done(ahc, scb);
+				/* FALLTHROUGH */
+			}
+			case SEARCH_REMOVE:
+				next = ahc_rem_wscb(ahc, next, prev);
+				break;
+			case SEARCH_COUNT:
+				prev = next;
+				next = ahc_inb(ahc, SCB_NEXT);
+				break;
+			}
+		} else {
+			
+			prev = next;
+			next = ahc_inb(ahc, SCB_NEXT);
+		}
+	}
+	ahc_outb(ahc, SCBPTR, curscbptr);
+
+	/*
+	 * And lastly, the untagged holding queues.
+	 */
+	i = 0;
+	if ((ahc->flags & AHC_SCB_BTT) == 0) {
+
+		maxtarget = 16;
+		if (target != CAM_TARGET_WILDCARD) {
+
+			i = target;
+			if (channel == 'B')
+				i += 8;
+			maxtarget = i + 1;
+		}
+	} else {
+		maxtarget = 0;
+	}
+
+	for (; i < maxtarget; i++) {
+		struct scb_tailq *untagged_q;
+		struct scb *next_scb;
+
+		untagged_q = &(ahc->untagged_queues[i]);
+		next_scb = TAILQ_FIRST(untagged_q);
+		while (next_scb != NULL) {
+
+			scb = next_scb;
+			next_scb = TAILQ_NEXT(scb, links.tqe);
+
+			/*
+			 * The head of the list may be the currently
+			 * active untagged command for a device.
+			 * We're only searching for commands that
+			 * have not been started.  A transaction
+			 * marked active but still in the qinfifo
+			 * is removed by the qinfifo scanning code
+			 * above.
+			 */
+			if ((scb->flags & SCB_ACTIVE) != 0)
+				continue;
+
+			if (ahc_match_scb(ahc, scb, target, channel,
+					  lun, SCB_LIST_NULL, role)) {
+				/*
+				 * We found an scb that needs to be acted on.
+				 */
+				found++;
+				switch (action) {
+				case SEARCH_COMPLETE:
+				{
+					cam_status ostat;
+
+					ostat = ahc_get_transaction_status(scb);
+					if (ostat == CAM_REQ_INPROG)
+						ahc_set_transaction_status(scb,
+								   status);
+					ahc_freeze_scb(scb);
+					if ((scb->flags & SCB_ACTIVE) == 0)
+						printf("Inactive SCB in untaggedQ\n");
+					ahc_done(ahc, scb);
+					break;
+				}
+				case SEARCH_REMOVE:
+					TAILQ_REMOVE(untagged_q, scb,
+						     links.tqe);
+					break;
+				case SEARCH_COUNT:
+					break;
+				}
+			}
+		}
+	}
+
+	if (action == SEARCH_COMPLETE)
+		ahc_release_untagged_queues(ahc);
+	return (found);
+}
+
+int
+ahc_search_disc_list(struct ahc_softc *ahc, int target, char channel,
+		     int lun, u_int tag, int stop_on_first, int remove,
+		     int save_state)
+{
+	struct	scb *scbp;
+	u_int	next;
+	u_int	prev;
+	u_int	count;
+	u_int	active_scb;
+
+	count = 0;
+	next = ahc_inb(ahc, DISCONNECTED_SCBH);
+	prev = SCB_LIST_NULL;
+
+	if (save_state) {
+		/* restore this when we're done */
+		active_scb = ahc_inb(ahc, SCBPTR);
+	} else
+		/* Silence compiler */
+		active_scb = SCB_LIST_NULL;
+
+	while (next != SCB_LIST_NULL) {
+		u_int scb_index;
+
+		ahc_outb(ahc, SCBPTR, next);
+		scb_index = ahc_inb(ahc, SCB_TAG);
+		if (scb_index >= ahc->scb_data->numscbs) {
+			printf("Disconnected List inconsistency. "
+			       "SCB index == %d, yet numscbs == %d.",
+			       scb_index, ahc->scb_data->numscbs);
+			ahc_dump_card_state(ahc);
+			panic("for safety");
+		}
+
+		if (next == prev) {
+			panic("Disconnected List Loop. "
+			      "cur SCBPTR == %x, prev SCBPTR == %x.",
+			      next, prev);
+		}
+		scbp = ahc_lookup_scb(ahc, scb_index);
+		if (ahc_match_scb(ahc, scbp, target, channel, lun,
+				  tag, ROLE_INITIATOR)) {
+			count++;
+			if (remove) {
+				next =
+				    ahc_rem_scb_from_disc_list(ahc, prev, next);
+			} else {
+				prev = next;
+				next = ahc_inb(ahc, SCB_NEXT);
+			}
+			if (stop_on_first)
+				break;
+		} else {
+			prev = next;
+			next = ahc_inb(ahc, SCB_NEXT);
+		}
+	}
+	if (save_state)
+		ahc_outb(ahc, SCBPTR, active_scb);
+	return (count);
+}
+
+/*
+ * Remove an SCB from the on chip list of disconnected transactions.
+ * This is empty/unused if we are not performing SCB paging.
+ */
+static u_int
+ahc_rem_scb_from_disc_list(struct ahc_softc *ahc, u_int prev, u_int scbptr)
+{
+	u_int next;
+
+	ahc_outb(ahc, SCBPTR, scbptr);
+	next = ahc_inb(ahc, SCB_NEXT);
+
+	ahc_outb(ahc, SCB_CONTROL, 0);
+
+	ahc_add_curscb_to_free_list(ahc);
+
+	if (prev != SCB_LIST_NULL) {
+		ahc_outb(ahc, SCBPTR, prev);
+		ahc_outb(ahc, SCB_NEXT, next);
+	} else
+		ahc_outb(ahc, DISCONNECTED_SCBH, next);
+
+	return (next);
+}
+
+/*
+ * Add the SCB as selected by SCBPTR onto the on chip list of
+ * free hardware SCBs.  This list is empty/unused if we are not
+ * performing SCB paging.
+ */
+static void
+ahc_add_curscb_to_free_list(struct ahc_softc *ahc)
+{
+	/*
+	 * Invalidate the tag so that our abort
+	 * routines don't think it's active.
+	 */
+	ahc_outb(ahc, SCB_TAG, SCB_LIST_NULL);
+
+	if ((ahc->flags & AHC_PAGESCBS) != 0) {
+		ahc_outb(ahc, SCB_NEXT, ahc_inb(ahc, FREE_SCBH));
+		ahc_outb(ahc, FREE_SCBH, ahc_inb(ahc, SCBPTR));
+	}
+}
+
+/*
+ * Manipulate the waiting for selection list and return the
+ * scb that follows the one that we remove.
+ */
+static u_int
+ahc_rem_wscb(struct ahc_softc *ahc, u_int scbpos, u_int prev)
+{       
+	u_int curscb, next;
+
+	/*
+	 * Select the SCB we want to abort and
+	 * pull the next pointer out of it.
+	 */
+	curscb = ahc_inb(ahc, SCBPTR);
+	ahc_outb(ahc, SCBPTR, scbpos);
+	next = ahc_inb(ahc, SCB_NEXT);
+
+	/* Clear the necessary fields */
+	ahc_outb(ahc, SCB_CONTROL, 0);
+
+	ahc_add_curscb_to_free_list(ahc);
+
+	/* update the waiting list */
+	if (prev == SCB_LIST_NULL) {
+		/* First in the list */
+		ahc_outb(ahc, WAITING_SCBH, next); 
+
+		/*
+		 * Ensure we aren't attempting to perform
+		 * selection for this entry.
+		 */
+		ahc_outb(ahc, SCSISEQ, (ahc_inb(ahc, SCSISEQ) & ~ENSELO));
+	} else {
+		/*
+		 * Select the scb that pointed to us 
+		 * and update its next pointer.
+		 */
+		ahc_outb(ahc, SCBPTR, prev);
+		ahc_outb(ahc, SCB_NEXT, next);
+	}
+
+	/*
+	 * Point us back at the original scb position.
+	 */
+	ahc_outb(ahc, SCBPTR, curscb);
+	return next;
+}
+
+/******************************** Error Handling ******************************/
+/*
+ * Abort all SCBs that match the given description (target/channel/lun/tag),
+ * setting their status to the passed in status if the status has not already
+ * been modified from CAM_REQ_INPROG.  This routine assumes that the sequencer
+ * is paused before it is called.
+ */
+int
+ahc_abort_scbs(struct ahc_softc *ahc, int target, char channel,
+	       int lun, u_int tag, role_t role, uint32_t status)
+{
+	struct	scb *scbp;
+	struct	scb *scbp_next;
+	u_int	active_scb;
+	int	i, j;
+	int	maxtarget;
+	int	minlun;
+	int	maxlun;
+
+	int	found;
+
+	/*
+	 * Don't attempt to run any queued untagged transactions
+	 * until we are done with the abort process.
+	 */
+	ahc_freeze_untagged_queues(ahc);
+
+	/* restore this when we're done */
+	active_scb = ahc_inb(ahc, SCBPTR);
+
+	found = ahc_search_qinfifo(ahc, target, channel, lun, SCB_LIST_NULL,
+				   role, CAM_REQUEUE_REQ, SEARCH_COMPLETE);
+
+	/*
+	 * Clean out the busy target table for any untagged commands.
+	 */
+	i = 0;
+	maxtarget = 16;
+	if (target != CAM_TARGET_WILDCARD) {
+		i = target;
+		if (channel == 'B')
+			i += 8;
+		maxtarget = i + 1;
+	}
+
+	if (lun == CAM_LUN_WILDCARD) {
+
+		/*
+		 * Unless we are using an SCB based
+		 * busy targets table, there is only
+		 * one table entry for all luns of
+		 * a target.
+		 */
+		minlun = 0;
+		maxlun = 1;
+		if ((ahc->flags & AHC_SCB_BTT) != 0)
+			maxlun = AHC_NUM_LUNS;
+	} else {
+		minlun = lun;
+		maxlun = lun + 1;
+	}
+
+	for (;i < maxtarget; i++) {
+		for (j = minlun;j < maxlun; j++)
+			ahc_unbusy_tcl(ahc, BUILD_TCL(i << 4, j));
+	}
+
+	/*
+	 * Go through the disconnected list and remove any entries we
+	 * have queued for completion, 0'ing their control byte too.
+	 * We save the active SCB and restore it ourselves, so there
+	 * is no reason for this search to restore it too.
+	 */
+	ahc_search_disc_list(ahc, target, channel, lun, tag,
+			     /*stop_on_first*/FALSE, /*remove*/TRUE,
+			     /*save_state*/FALSE);
+
+	/*
+	 * Go through the hardware SCB array looking for commands that
+	 * were active but not on any list.
+	 */
+	for (i = 0; i < ahc->scb_data->maxhscbs; i++) {
+		u_int scbid;
+
+		ahc_outb(ahc, SCBPTR, i);
+		scbid = ahc_inb(ahc, SCB_TAG);
+		scbp = ahc_lookup_scb(ahc, scbid);
+		if (scbp != NULL
+		 && ahc_match_scb(ahc, scbp, target, channel, lun, tag, role))
+			ahc_add_curscb_to_free_list(ahc);
+	}
+
+	/*
+	 * Go through the pending CCB list and look for
+	 * commands for this target that are still active.
+	 * These are other tagged commands that were
+	 * disconnected when the reset occured.
+	 */
+	scbp_next = LIST_FIRST(&ahc->pending_scbs);
+	while (scbp_next != NULL) {
+		scbp = scbp_next;
+		scbp_next = LIST_NEXT(scbp, pending_links);
+		if (ahc_match_scb(ahc, scbp, target, channel, lun, tag, role)) {
+			cam_status ostat;
+
+			ostat = ahc_get_transaction_status(scbp);
+			if (ostat == CAM_REQ_INPROG)
+				ahc_set_transaction_status(scbp, status);
+			ahc_freeze_scb(scbp);
+			if ((scbp->flags & SCB_ACTIVE) == 0)
+				printf("Inactive SCB on pending list\n");
+			ahc_done(ahc, scbp);
+			found++;
+		}
+	}
+	ahc_outb(ahc, SCBPTR, active_scb);
+	ahc_platform_abort_scbs(ahc, target, channel, lun, tag, role, status);
+	ahc_release_untagged_queues(ahc);
+	return found;
+}
+
+static void
+ahc_reset_current_bus(struct ahc_softc *ahc)
+{
+	uint8_t scsiseq;
+
+	ahc_outb(ahc, SIMODE1, ahc_inb(ahc, SIMODE1) & ~ENSCSIRST);
+	scsiseq = ahc_inb(ahc, SCSISEQ);
+	ahc_outb(ahc, SCSISEQ, scsiseq | SCSIRSTO);
+	ahc_delay(AHC_BUSRESET_DELAY);
+	/* Turn off the bus reset */
+	ahc_outb(ahc, SCSISEQ, scsiseq & ~SCSIRSTO);
+
+	ahc_clear_intstat(ahc);
+
+	/* Re-enable reset interrupts */
+	ahc_outb(ahc, SIMODE1, ahc_inb(ahc, SIMODE1) | ENSCSIRST);
+}
+
+int
+ahc_reset_channel(struct ahc_softc *ahc, char channel, int initiate_reset)
+{
+	struct	ahc_devinfo devinfo;
+	u_int	initiator, target, max_scsiid;
+	u_int	sblkctl;
+	int	found;
+	int	restart_needed;
+	char	cur_channel;
+
+	ahc->pending_device = NULL;
+
+	ahc_compile_devinfo(&devinfo,
+			    CAM_TARGET_WILDCARD,
+			    CAM_TARGET_WILDCARD,
+			    CAM_LUN_WILDCARD,
+			    channel, ROLE_UNKNOWN);
+	pause_sequencer(ahc);
+
+	/* Make sure the sequencer is in a safe location. */
+	ahc_clear_critical_section(ahc);
+
+	/*
+	 * Run our command complete fifos to ensure that we perform
+	 * completion processing on any commands that 'completed'
+	 * before the reset occurred.
+	 */
+	ahc_run_qoutfifo(ahc);
+#if AHC_TARGET_MODE
+	if ((ahc->flags & AHC_TARGETROLE) != 0) {
+		ahc_run_tqinfifo(ahc, /*paused*/TRUE);
+	}
+#endif
+
+	/*
+	 * Reset the bus if we are initiating this reset
+	 */
+	sblkctl = ahc_inb(ahc, SBLKCTL);
+	cur_channel = 'A';
+	if ((ahc->features & AHC_TWIN) != 0
+	 && ((sblkctl & SELBUSB) != 0))
+	    cur_channel = 'B';
+	if (cur_channel != channel) {
+		/* Case 1: Command for another bus is active
+		 * Stealthily reset the other bus without
+		 * upsetting the current bus.
+		 */
+		ahc_outb(ahc, SBLKCTL, sblkctl ^ SELBUSB);
+		ahc_outb(ahc, SIMODE1,
+			 ahc_inb(ahc, SIMODE1) & ~(ENBUSFREE|ENSCSIRST));
+		ahc_outb(ahc, SCSISEQ,
+			 ahc_inb(ahc, SCSISEQ) & (ENSELI|ENRSELI|ENAUTOATNP));
+		if (initiate_reset)
+			ahc_reset_current_bus(ahc);
+		ahc_clear_intstat(ahc);
+		ahc_outb(ahc, SBLKCTL, sblkctl);
+		restart_needed = FALSE;
+	} else {
+		/* Case 2: A command from this bus is active or we're idle */
+		ahc_clear_msg_state(ahc);
+		ahc_outb(ahc, SIMODE1,
+			 ahc_inb(ahc, SIMODE1) & ~(ENBUSFREE|ENSCSIRST));
+		ahc_outb(ahc, SCSISEQ,
+			 ahc_inb(ahc, SCSISEQ) & (ENSELI|ENRSELI|ENAUTOATNP));
+		if (initiate_reset)
+			ahc_reset_current_bus(ahc);
+		ahc_clear_intstat(ahc);
+		restart_needed = TRUE;
+	}
+
+	/*
+	 * Clean up all the state information for the
+	 * pending transactions on this bus.
+	 */
+	found = ahc_abort_scbs(ahc, CAM_TARGET_WILDCARD, channel,
+			       CAM_LUN_WILDCARD, SCB_LIST_NULL,
+			       ROLE_UNKNOWN, CAM_SCSI_BUS_RESET);
+
+	max_scsiid = (ahc->features & AHC_WIDE) ? 15 : 7;
+
+#ifdef AHC_TARGET_MODE
+	/*
+	 * Send an immediate notify ccb to all target more peripheral
+	 * drivers affected by this action.
+	 */
+	for (target = 0; target <= max_scsiid; target++) {
+		struct tmode_tstate* tstate;
+		u_int lun;
+
+		tstate = ahc->enabled_targets[target];
+		if (tstate == NULL)
+			continue;
+		for (lun = 0; lun < AHC_NUM_LUNS; lun++) {
+			struct tmode_lstate* lstate;
+
+			lstate = tstate->enabled_luns[lun];
+			if (lstate == NULL)
+				continue;
+
+			ahc_queue_lstate_event(ahc, lstate, CAM_TARGET_WILDCARD,
+					       EVENT_TYPE_BUS_RESET, /*arg*/0);
+			ahc_send_lstate_events(ahc, lstate);
+		}
+	}
+#endif
+	/* Notify the XPT that a bus reset occurred */
+	ahc_send_async(ahc, devinfo.channel, CAM_TARGET_WILDCARD,
+		       CAM_LUN_WILDCARD, AC_BUS_RESET);
+
+	/*
+	 * Revert to async/narrow transfers until we renegotiate.
+	 */
+	for (target = 0; target <= max_scsiid; target++) {
+
+		if (ahc->enabled_targets[target] == NULL)
+			continue;
+		for (initiator = 0; initiator <= max_scsiid; initiator++) {
+			struct ahc_devinfo devinfo;
+
+			ahc_compile_devinfo(&devinfo, target, initiator,
+					    CAM_LUN_WILDCARD,
+					    channel, ROLE_UNKNOWN);
+			ahc_set_width(ahc, &devinfo, MSG_EXT_WDTR_BUS_8_BIT,
+				      AHC_TRANS_CUR, /*paused*/TRUE);
+			ahc_set_syncrate(ahc, &devinfo, /*syncrate*/NULL,
+					 /*period*/0, /*offset*/0,
+					 /*ppr_options*/0, AHC_TRANS_CUR,
+					 /*paused*/TRUE);
+		}
+	}
+
+	if (restart_needed)
+		restart_sequencer(ahc);
+	else
+		unpause_sequencer(ahc);
+	return found;
+}
+
+
+/***************************** Residual Processing ****************************/
+/*
+ * Calculate the residual for a just completed SCB.
+ */
+static void
+ahc_calc_residual(struct scb *scb)
+{
+	struct hardware_scb *hscb;
+	struct status_pkt *spkt;
+	uint32_t sgptr;
+	uint32_t resid_sgptr;
+	uint32_t resid;
+
+	/*
+	 * 5 cases.
+	 * 1) No residual.
+	 *    SG_RESID_VALID clear in sgptr.
+	 * 2) Transferless command
+	 * 3) Never performed any transfers.
+	 *    sgptr has SG_FULL_RESID set.
+	 * 4) No residual but target did not
+	 *    save data pointers after the
+	 *    last transfer, so sgptr was
+	 *    never updated.
+	 * 5) We have a partial residual.
+	 *    Use residual_sgptr to determine
+	 *    where we are.
+	 */
+
+	hscb = scb->hscb;
+	sgptr = ahc_le32toh(hscb->sgptr);
+	if ((sgptr & SG_RESID_VALID) == 0)
+		/* Case 1 */
+		return;
+	sgptr &= ~SG_RESID_VALID;
+
+	if ((sgptr & SG_LIST_NULL) != 0)
+		/* Case 2 */
+		return;
+
+	spkt = &hscb->shared_data.status;
+	resid_sgptr = ahc_le32toh(spkt->residual_sg_ptr);
+	if ((sgptr & SG_FULL_RESID) != 0) {
+		/* Case 3 */
+		resid = ahc_get_transfer_length(scb);
+	} else if ((resid_sgptr & SG_LIST_NULL) != 0) {
+		/* Case 4 */
+		return;
+	} else if ((resid_sgptr & ~SG_PTR_MASK) != 0) {
+		panic("Bogus resid sgptr value 0x%x\n", resid_sgptr);
+	} else {
+		struct ahc_dma_seg *sg;
+
+		/*
+		 * Remainder of the SG where the transfer
+		 * stopped.  
+		 */
+		resid = ahc_le32toh(spkt->residual_datacnt) & AHC_SG_LEN_MASK;
+		sg = ahc_sg_bus_to_virt(scb, resid_sgptr & SG_PTR_MASK);
+
+		/* The residual sg_ptr always points to the next sg */
+		sg--;
+
+		/*
+		 * Add up the contents of all residual
+		 * SG segments that are after the SG where
+		 * the transfer stopped.
+		 */
+		while ((ahc_le32toh(sg->len) & AHC_DMA_LAST_SEG) == 0) {
+			sg++;
+			resid += ahc_le32toh(sg->len) & AHC_SG_LEN_MASK;
+		}
+	}
+	if ((scb->flags & SCB_SENSE) == 0)
+		ahc_set_residual(scb, resid);
+	else
+		ahc_set_sense_residual(scb, resid);
+
+#ifdef AHC_DEBUG
+	if (ahc_debug & AHC_SHOWMISC) {
+		ahc_print_path(ahc, scb);
+		printf("Handled Residual of %d bytes\n", resid);
+	}
+#endif
+}
+
+/******************************* Target Mode **********************************/
+#ifdef AHC_TARGET_MODE
+/*
+ * Add a target mode event to this lun's queue
+ */
+static void
+ahc_queue_lstate_event(struct ahc_softc *ahc, struct tmode_lstate *lstate,
+		       u_int initiator_id, u_int event_type, u_int event_arg)
+{
+	struct ahc_tmode_event *event;
+	int pending;
+
+	xpt_freeze_devq(lstate->path, /*count*/1);
+	if (lstate->event_w_idx >= lstate->event_r_idx)
+		pending = lstate->event_w_idx - lstate->event_r_idx;
+	else
+		pending = AHC_TMODE_EVENT_BUFFER_SIZE + 1
+			- (lstate->event_r_idx - lstate->event_w_idx);
+
+	if (event_type == EVENT_TYPE_BUS_RESET
+	 || event_type == MSG_BUS_DEV_RESET) {
+		/*
+		 * Any earlier events are irrelevant, so reset our buffer.
+		 * This has the effect of allowing us to deal with reset
+		 * floods (an external device holding down the reset line)
+		 * without losing the event that is really interesting.
+		 */
+		lstate->event_r_idx = 0;
+		lstate->event_w_idx = 0;
+		xpt_release_devq(lstate->path, pending, /*runqueue*/FALSE);
+	}
+
+	if (pending == AHC_TMODE_EVENT_BUFFER_SIZE) {
+		xpt_print_path(lstate->path);
+		printf("immediate event %x:%x lost\n",
+		       lstate->event_buffer[lstate->event_r_idx].event_type,
+		       lstate->event_buffer[lstate->event_r_idx].event_arg);
+		lstate->event_r_idx++;
+		if (lstate->event_r_idx == AHC_TMODE_EVENT_BUFFER_SIZE)
+			lstate->event_r_idx = 0;
+		xpt_release_devq(lstate->path, /*count*/1, /*runqueue*/FALSE);
+	}
+
+	event = &lstate->event_buffer[lstate->event_w_idx];
+	event->initiator_id = initiator_id;
+	event->event_type = event_type;
+	event->event_arg = event_arg;
+	lstate->event_w_idx++;
+	if (lstate->event_w_idx == AHC_TMODE_EVENT_BUFFER_SIZE)
+		lstate->event_w_idx = 0;
+}
+
+/*
+ * Send any target mode events queued up waiting
+ * for immediate notify resources.
+ */
+void
+ahc_send_lstate_events(struct ahc_softc *ahc, struct tmode_lstate *lstate)
+{
+	struct ccb_hdr *ccbh;
+	struct ccb_immed_notify *inot;
+
+	while (lstate->event_r_idx != lstate->event_w_idx
+	    && (ccbh = SLIST_FIRST(&lstate->immed_notifies)) != NULL) {
+		struct ahc_tmode_event *event;
+
+		event = &lstate->event_buffer[lstate->event_r_idx];
+		SLIST_REMOVE_HEAD(&lstate->immed_notifies, sim_links.sle);
+		inot = (struct ccb_immed_notify *)ccbh;
+		switch (event->event_type) {
+		case EVENT_TYPE_BUS_RESET:
+			ccbh->status = CAM_SCSI_BUS_RESET|CAM_DEV_QFRZN;
+			break;
+		default:
+			ccbh->status = CAM_MESSAGE_RECV|CAM_DEV_QFRZN;
+			inot->message_args[0] = event->event_type;
+			inot->message_args[1] = event->event_arg;
+			break;
+		}
+		inot->initiator_id = event->initiator_id;
+		inot->sense_len = 0;
+		xpt_done((union ccb *)inot);
+		lstate->event_r_idx++;
+		if (lstate->event_r_idx == AHC_TMODE_EVENT_BUFFER_SIZE)
+			lstate->event_r_idx = 0;
+	}
+}
+#endif
+
+/******************** Sequencer Program Patching/Download *********************/
+
+#ifdef AHC_DUMP_SEQ
+void
+ahc_dumpseq(struct ahc_softc* ahc)
+{
+	int i;
+	int max_prog;
+
+	if ((ahc->chip & AHC_BUS_MASK) < AHC_PCI)
+		max_prog = 448;
+	else if ((ahc->features & AHC_ULTRA2) != 0)
+		max_prog = 768;
+	else
+		max_prog = 512;
+
+	ahc_outb(ahc, SEQCTL, PERRORDIS|FAILDIS|FASTMODE|LOADRAM);
+	ahc_outb(ahc, SEQADDR0, 0);
+	ahc_outb(ahc, SEQADDR1, 0);
+	for (i = 0; i < max_prog; i++) {
+		uint8_t ins_bytes[4];
+
+		ahc_insb(ahc, SEQRAM, ins_bytes, 4);
+		printf("0x%08x\n", ins_bytes[0] << 24
+				 | ins_bytes[1] << 16
+				 | ins_bytes[2] << 8
+				 | ins_bytes[3]);
+	}
+}
+#endif
+
+static void
+ahc_loadseq(struct ahc_softc *ahc)
+{
+	struct	cs cs_table[num_critical_sections];
+	u_int	begin_set[num_critical_sections];
+	u_int	end_set[num_critical_sections];
+	struct	patch *cur_patch;
+	u_int	cs_count;
+	u_int	cur_cs;
+	u_int	i;
+	int	downloaded;
+	u_int	skip_addr;
+	u_int	sg_prefetch_cnt;
+	uint8_t	download_consts[7];
+
+	/*
+	 * Start out with 0 critical sections
+	 * that apply to this firmware load.
+	 */
+	cs_count = 0;
+	cur_cs = 0;
+	memset(begin_set, 0, sizeof(begin_set));
+	memset(end_set, 0, sizeof(end_set));
+
+	/* Setup downloadable constant table */
+	download_consts[QOUTFIFO_OFFSET] = 0;
+	if (ahc->targetcmds != NULL)
+		download_consts[QOUTFIFO_OFFSET] += 32;
+	download_consts[QINFIFO_OFFSET] = download_consts[QOUTFIFO_OFFSET] + 1;
+	download_consts[CACHESIZE_MASK] = ahc->pci_cachesize - 1;
+	download_consts[INVERTED_CACHESIZE_MASK] = ~(ahc->pci_cachesize - 1);
+	sg_prefetch_cnt = ahc->pci_cachesize;
+	if (sg_prefetch_cnt < (2 * sizeof(struct ahc_dma_seg)))
+		sg_prefetch_cnt = 2 * sizeof(struct ahc_dma_seg);
+	download_consts[SG_PREFETCH_CNT] = sg_prefetch_cnt;
+	download_consts[SG_PREFETCH_ALIGN_MASK] = ~(sg_prefetch_cnt - 1);
+	download_consts[SG_PREFETCH_ADDR_MASK] = (sg_prefetch_cnt - 1);
+
+	cur_patch = patches;
+	downloaded = 0;
+	skip_addr = 0;
+	ahc_outb(ahc, SEQCTL, PERRORDIS|FAILDIS|FASTMODE|LOADRAM);
+	ahc_outb(ahc, SEQADDR0, 0);
+	ahc_outb(ahc, SEQADDR1, 0);
+
+	for (i = 0; i < sizeof(seqprog)/4; i++) {
+		if (ahc_check_patch(ahc, &cur_patch, i, &skip_addr) == 0) {
+			/*
+			 * Don't download this instruction as it
+			 * is in a patch that was removed.
+			 */
+			continue;
+		}
+		/*
+		 * Move through the CS table until we find a CS
+		 * that might apply to this instruction.
+		 */
+		for (; cur_cs < num_critical_sections; cur_cs++) {
+			if (critical_sections[cur_cs].end <= i) {
+				if (begin_set[cs_count] == TRUE
+				 && end_set[cs_count] == FALSE) {
+					cs_table[cs_count].end = downloaded;
+				 	end_set[cs_count] = TRUE;
+					cs_count++;
+				}
+				continue;
+			}
+			if (critical_sections[cur_cs].begin <= i
+			 && begin_set[cs_count] == FALSE) {
+				cs_table[cs_count].begin = downloaded;
+				begin_set[cs_count] = TRUE;
+			}
+			break;
+		}
+		ahc_download_instr(ahc, i, download_consts);
+		downloaded++;
+	}
+
+	ahc->num_critical_sections = cs_count;
+	if (cs_count != 0) {
+
+		cs_count *= sizeof(struct cs);
+		ahc->critical_sections = malloc(cs_count, M_DEVBUF, M_NOWAIT);
+		if (ahc->critical_sections == NULL)
+			panic("ahc_loadseq: Could not malloc");
+		memcpy(ahc->critical_sections, cs_table, cs_count);
+	}
+	ahc_outb(ahc, SEQCTL, PERRORDIS|FAILDIS|FASTMODE);
+	restart_sequencer(ahc);
+
+	if (bootverbose)
+		printf(" %d instructions downloaded\n", downloaded);
+}
+
+static int
+ahc_check_patch(struct ahc_softc *ahc, struct patch **start_patch,
+		u_int start_instr, u_int *skip_addr)
+{
+	struct	patch *cur_patch;
+	struct	patch *last_patch;
+	u_int	num_patches;
+
+	num_patches = sizeof(patches)/sizeof(struct patch);
+	last_patch = &patches[num_patches];
+	cur_patch = *start_patch;
+
+	while (cur_patch < last_patch && start_instr == cur_patch->begin) {
+
+		if (cur_patch->patch_func(ahc) == 0) {
+
+			/* Start rejecting code */
+			*skip_addr = start_instr + cur_patch->skip_instr;
+			cur_patch += cur_patch->skip_patch;
+		} else {
+			/* Accepted this patch.  Advance to the next
+			 * one and wait for our intruction pointer to
+			 * hit this point.
+			 */
+			cur_patch++;
+		}
+	}
+
+	*start_patch = cur_patch;
+	if (start_instr < *skip_addr)
+		/* Still skipping */
+		return (0);
+
+	return (1);
+}
+
+static void
+ahc_download_instr(struct ahc_softc *ahc, u_int instrptr, uint8_t *dconsts)
+{
+	union	ins_formats instr;
+	struct	ins_format1 *fmt1_ins;
+	struct	ins_format3 *fmt3_ins;
+	u_int	opcode;
+
+	/*
+	 * The firmware is always compiled into a little endian format.
+	 */
+	instr.integer = ahc_le32toh(*(uint32_t*)&seqprog[instrptr * 4]);
+
+	fmt1_ins = &instr.format1;
+	fmt3_ins = NULL;
+
+	/* Pull the opcode */
+	opcode = instr.format1.opcode;
+	switch (opcode) {
+	case AIC_OP_JMP:
+	case AIC_OP_JC:
+	case AIC_OP_JNC:
+	case AIC_OP_CALL:
+	case AIC_OP_JNE:
+	case AIC_OP_JNZ:
+	case AIC_OP_JE:
+	case AIC_OP_JZ:
+	{
+		struct patch *cur_patch;
+		int address_offset;
+		u_int address;
+		u_int skip_addr;
+		u_int i;
+
+		fmt3_ins = &instr.format3;
+		address_offset = 0;
+		address = fmt3_ins->address;
+		cur_patch = patches;
+		skip_addr = 0;
+
+		for (i = 0; i < address;) {
+
+			ahc_check_patch(ahc, &cur_patch, i, &skip_addr);
+
+			if (skip_addr > i) {
+				int end_addr;
+
+				end_addr = MIN(address, skip_addr);
+				address_offset += end_addr - i;
+				i = skip_addr;
+			} else {
+				i++;
+			}
+		}
+		address -= address_offset;
+		fmt3_ins->address = address;
+		/* FALLTHROUGH */
+	}
+	case AIC_OP_OR:
+	case AIC_OP_AND:
+	case AIC_OP_XOR:
+	case AIC_OP_ADD:
+	case AIC_OP_ADC:
+	case AIC_OP_BMOV:
+		if (fmt1_ins->parity != 0) {
+			fmt1_ins->immediate = dconsts[fmt1_ins->immediate];
+		}
+		fmt1_ins->parity = 0;
+		if ((ahc->features & AHC_CMD_CHAN) == 0
+		 && opcode == AIC_OP_BMOV) {
+			/*
+			 * Block move was added at the same time
+			 * as the command channel.  Verify that
+			 * this is only a move of a single element
+			 * and convert the BMOV to a MOV
+			 * (AND with an immediate of FF).
+			 */
+			if (fmt1_ins->immediate != 1)
+				panic("%s: BMOV not supported\n",
+				      ahc_name(ahc));
+			fmt1_ins->opcode = AIC_OP_AND;
+			fmt1_ins->immediate = 0xff;
+		}
+		/* FALLTHROUGH */
+	case AIC_OP_ROL:
+		if ((ahc->features & AHC_ULTRA2) != 0) {
+			int i, count;
+
+			/* Calculate odd parity for the instruction */
+			for (i = 0, count = 0; i < 31; i++) {
+				uint32_t mask;
+
+				mask = 0x01 << i;
+				if ((instr.integer & mask) != 0)
+					count++;
+			}
+			if ((count & 0x01) == 0)
+				instr.format1.parity = 1;
+		} else {
+			/* Compress the instruction for older sequencers */
+			if (fmt3_ins != NULL) {
+				instr.integer =
+					fmt3_ins->immediate
+				      | (fmt3_ins->source << 8)
+				      | (fmt3_ins->address << 16)
+				      |	(fmt3_ins->opcode << 25);
+			} else {
+				instr.integer =
+					fmt1_ins->immediate
+				      | (fmt1_ins->source << 8)
+				      | (fmt1_ins->destination << 16)
+				      |	(fmt1_ins->ret << 24)
+				      |	(fmt1_ins->opcode << 25);
+			}
+		}
+		/* The sequencer is a little endian cpu */
+		instr.integer = ahc_htole32(instr.integer);
+		ahc_outsb(ahc, SEQRAM, instr.bytes, 4);
+		break;
+	default:
+		panic("Unknown opcode encountered in seq program");
+		break;
+	}
+}
+
+void
+ahc_dump_card_state(struct ahc_softc *ahc)
+{
+	struct scb *scb;
+	struct scb_tailq *untagged_q;
+	int target;
+	int maxtarget;
+	int i;
+	uint8_t qinpos;
+	uint8_t qintail;
+	uint8_t qoutpos;
+	uint8_t scb_index;
+	uint8_t saved_scbptr;
+
+	saved_scbptr = ahc_inb(ahc, SCBPTR);
+
+	printf("%s: Dumping Card State at SEQADDR 0x%x\n",
+	       ahc_name(ahc),
+	       ahc_inb(ahc, SEQADDR0) | (ahc_inb(ahc, SEQADDR1) << 8));
+
+	printf("SCB count = %d\n", ahc->scb_data->numscbs);
+	printf("Kernel NEXTQSCB = %d\n", ahc->next_queued_scb->hscb->tag);
+	printf("Card NEXTQSCB = %d\n", ahc_inb(ahc, NEXT_QUEUED_SCB));
+	/* QINFIFO */
+	printf("QINFIFO entries: ");
+	if ((ahc->features & AHC_QUEUE_REGS) != 0) {
+		qinpos = ahc_inb(ahc, SNSCB_QOFF);
+		ahc_outb(ahc, SNSCB_QOFF, qinpos);
+	} else
+		qinpos = ahc_inb(ahc, QINPOS);
+	qintail = ahc->qinfifonext;
+	while (qinpos != qintail) {
+		printf("%d ", ahc->qinfifo[qinpos]);
+		qinpos++;
+	}
+	printf("\n");
+
+	printf("Waiting Queue entries: ");
+	scb_index = ahc_inb(ahc, WAITING_SCBH);
+	i = 0;
+	while (scb_index != SCB_LIST_NULL && i++ < 256) {
+		ahc_outb(ahc, SCBPTR, scb_index);
+		printf("%d:%d ", scb_index, ahc_inb(ahc, SCB_TAG));
+		scb_index = ahc_inb(ahc, SCB_NEXT);
+	}
+	printf("\n");
+
+	printf("Disconnected Queue entries: ");
+	scb_index = ahc_inb(ahc, DISCONNECTED_SCBH);
+	i = 0;
+	while (scb_index != SCB_LIST_NULL && i++ < 256) {
+		ahc_outb(ahc, SCBPTR, scb_index);
+		printf("%d:%d ", scb_index, ahc_inb(ahc, SCB_TAG));
+		scb_index = ahc_inb(ahc, SCB_NEXT);
+	}
+	printf("\n");
+		
+	printf("QOUTFIFO entries: ");
+	qoutpos = ahc->qoutfifonext;
+	i = 0;
+	while (ahc->qoutfifo[qoutpos] != SCB_LIST_NULL && i++ < 256) {
+		printf("%d ", ahc->qoutfifo[qoutpos]);
+		qoutpos++;
+	}
+	printf("\n");
+
+	printf("Sequencer Free SCB List: ");
+	scb_index = ahc_inb(ahc, FREE_SCBH);
+	i = 0;
+	while (scb_index != SCB_LIST_NULL && i++ < 256) {
+		ahc_outb(ahc, SCBPTR, scb_index);
+		printf("%d ", scb_index);
+		scb_index = ahc_inb(ahc, SCB_NEXT);
+	}
+	printf("\n");
+
+	printf("Pending list: ");
+	i = 0;
+	LIST_FOREACH(scb, &ahc->pending_scbs, pending_links) {
+		if (i++ > 256)
+			break;
+		printf("%d ", scb->hscb->tag);
+	}
+	printf("\n");
+
+	printf("Kernel Free SCB list: ");
+	i = 0;
+	SLIST_FOREACH(scb, &ahc->scb_data->free_scbs, links.sle) {
+		if (i++ > 256)
+			break;
+		printf("%d ", scb->hscb->tag);
+	}
+	printf("\n");
+
+	maxtarget = (ahc->features & (AHC_WIDE|AHC_TWIN)) ? 15 : 7;
+	for (target = 0; target <= maxtarget; target++) {
+		untagged_q = &ahc->untagged_queues[target];
+		if (TAILQ_FIRST(untagged_q) == NULL)
+			continue;
+		printf("Untagged Q(%d): ", target);
+		i = 0;
+		TAILQ_FOREACH(scb, untagged_q, links.tqe) {
+			if (i++ > 256)
+				break;
+			printf("%d ", scb->hscb->tag);
+		}
+		printf("\n");
+	}
+
+	ahc_platform_dump_card_state(ahc);
+	ahc_outb(ahc, SCBPTR, saved_scbptr);
+}
+
+/************************* Target Mode ****************************************/
+#ifdef AHC_TARGET_MODE
+cam_status
+ahc_find_tmode_devs(struct ahc_softc *ahc, struct cam_sim *sim, union ccb *ccb,
+		    struct tmode_tstate **tstate, struct tmode_lstate **lstate,
+		    int notfound_failure)
+{
+
+	if ((ahc->features & AHC_TARGETMODE) == 0)
+		return (CAM_REQ_INVALID);
+
+	/*
+	 * Handle the 'black hole' device that sucks up
+	 * requests to unattached luns on enabled targets.
+	 */
+	if (ccb->ccb_h.target_id == CAM_TARGET_WILDCARD
+	 && ccb->ccb_h.target_lun == CAM_LUN_WILDCARD) {
+		*tstate = NULL;
+		*lstate = ahc->black_hole;
+	} else {
+		u_int max_id;
+
+		max_id = (ahc->features & AHC_WIDE) ? 15 : 7;
+		if (ccb->ccb_h.target_id > max_id)
+			return (CAM_TID_INVALID);
+
+		if (ccb->ccb_h.target_lun >= AHC_NUM_LUNS)
+			return (CAM_LUN_INVALID);
+
+		*tstate = ahc->enabled_targets[ccb->ccb_h.target_id];
+		*lstate = NULL;
+		if (*tstate != NULL)
+			*lstate =
+			    (*tstate)->enabled_luns[ccb->ccb_h.target_lun];
+	}
+
+	if (notfound_failure != 0 && *lstate == NULL)
+		return (CAM_PATH_INVALID);
+
+	return (CAM_REQ_CMP);
+}
+
+void
+ahc_handle_en_lun(struct ahc_softc *ahc, struct cam_sim *sim, union ccb *ccb)
+{
+	struct	   tmode_tstate *tstate;
+	struct	   tmode_lstate *lstate;
+	struct	   ccb_en_lun *cel;
+	cam_status status;
+	u_int	   target;
+	u_int	   lun;
+	u_int	   target_mask;
+	u_long	   s;
+	char	   channel;
+
+	status = ahc_find_tmode_devs(ahc, sim, ccb, &tstate, &lstate,
+				     /*notfound_failure*/FALSE);
+
+	if (status != CAM_REQ_CMP) {
+		ccb->ccb_h.status = status;
+		return;
+	}
+
+	if ((ahc->features & AHC_MULTIROLE) != 0) {
+		u_int	   our_id;
+
+		if (cam_sim_bus(sim) == 0)
+			our_id = ahc->our_id;
+		else
+			our_id = ahc->our_id_b;
+
+		if (ccb->ccb_h.target_id != our_id) {
+			if ((ahc->features & AHC_MULTI_TID) != 0
+		   	 && (ahc->flags & AHC_INITIATORROLE) != 0) {
+				/*
+				 * Only allow additional targets if
+				 * the initiator role is disabled.
+				 * The hardware cannot handle a re-select-in
+				 * on the initiator id during a re-select-out
+				 * on a different target id.
+				 */
+				status = CAM_TID_INVALID;
+			} else if ((ahc->flags & AHC_INITIATORROLE) != 0
+				|| ahc->enabled_luns > 0) {
+				/*
+				 * Only allow our target id to change
+				 * if the initiator role is not configured
+				 * and there are no enabled luns which
+				 * are attached to the currently registered
+				 * scsi id.
+				 */
+				status = CAM_TID_INVALID;
+			}
+		}
+	}
+
+	if (status != CAM_REQ_CMP) {
+		ccb->ccb_h.status = status;
+		return;
+	}
+
+	/*
+	 * We now have an id that is valid.
+	 * If we aren't in target mode, switch modes.
+	 */
+	if ((ahc->flags & AHC_TARGETROLE) == 0
+	 && ccb->ccb_h.target_id != CAM_TARGET_WILDCARD) {
+		u_long	s;
+
+		printf("Configuring Target Mode\n");
+		ahc_lock(ahc, &s);
+		if (LIST_FIRST(&ahc->pending_scbs) != NULL) {
+			ccb->ccb_h.status = CAM_BUSY;
+			ahc_unlock(ahc, &s);
+			return;
+		}
+		ahc->flags |= AHC_TARGETROLE;
+		if ((ahc->features & AHC_MULTIROLE) == 0)
+			ahc->flags &= ~AHC_INITIATORROLE;
+		pause_sequencer(ahc);
+		ahc_loadseq(ahc);
+		ahc_unlock(ahc, &s);
+	}
+	cel = &ccb->cel;
+	target = ccb->ccb_h.target_id;
+	lun = ccb->ccb_h.target_lun;
+	channel = SIM_CHANNEL(ahc, sim);
+	target_mask = 0x01 << target;
+	if (channel == 'B')
+		target_mask <<= 8;
+
+	if (cel->enable != 0) {
+		u_int scsiseq;
+
+		/* Are we already enabled?? */
+		if (lstate != NULL) {
+			xpt_print_path(ccb->ccb_h.path);
+			printf("Lun already enabled\n");
+			ccb->ccb_h.status = CAM_LUN_ALRDY_ENA;
+			return;
+		}
+
+		if (cel->grp6_len != 0
+		 || cel->grp7_len != 0) {
+			/*
+			 * Don't (yet?) support vendor
+			 * specific commands.
+			 */
+			ccb->ccb_h.status = CAM_REQ_INVALID;
+			printf("Non-zero Group Codes\n");
+			return;
+		}
+
+		/*
+		 * Seems to be okay.
+		 * Setup our data structures.
+		 */
+		if (target != CAM_TARGET_WILDCARD && tstate == NULL) {
+			tstate = ahc_alloc_tstate(ahc, target, channel);
+			if (tstate == NULL) {
+				xpt_print_path(ccb->ccb_h.path);
+				printf("Couldn't allocate tstate\n");
+				ccb->ccb_h.status = CAM_RESRC_UNAVAIL;
+				return;
+			}
+		}
+		lstate = malloc(sizeof(*lstate), M_DEVBUF, M_NOWAIT);
+		if (lstate == NULL) {
+			xpt_print_path(ccb->ccb_h.path);
+			printf("Couldn't allocate lstate\n");
+			ccb->ccb_h.status = CAM_RESRC_UNAVAIL;
+			return;
+		}
+		memset(lstate, 0, sizeof(*lstate));
+		status = xpt_create_path(&lstate->path, /*periph*/NULL,
+					 xpt_path_path_id(ccb->ccb_h.path),
+					 xpt_path_target_id(ccb->ccb_h.path),
+					 xpt_path_lun_id(ccb->ccb_h.path));
+		if (status != CAM_REQ_CMP) {
+			free(lstate, M_DEVBUF);
+			xpt_print_path(ccb->ccb_h.path);
+			printf("Couldn't allocate path\n");
+			ccb->ccb_h.status = CAM_RESRC_UNAVAIL;
+			return;
+		}
+		SLIST_INIT(&lstate->accept_tios);
+		SLIST_INIT(&lstate->immed_notifies);
+		ahc_lock(ahc, &s);
+		pause_sequencer(ahc);
+		if (target != CAM_TARGET_WILDCARD) {
+			tstate->enabled_luns[lun] = lstate;
+			ahc->enabled_luns++;
+
+			if ((ahc->features & AHC_MULTI_TID) != 0) {
+				u_int targid_mask;
+
+				targid_mask = ahc_inb(ahc, TARGID)
+					    | (ahc_inb(ahc, TARGID + 1) << 8);
+
+				targid_mask |= target_mask;
+				ahc_outb(ahc, TARGID, targid_mask);
+				ahc_outb(ahc, TARGID+1, (targid_mask >> 8));
+				
+				ahc_update_scsiid(ahc, targid_mask);
+			} else {
+				u_int our_id;
+				char  channel;
+
+				channel = SIM_CHANNEL(ahc, sim);
+				our_id = SIM_SCSI_ID(ahc, sim);
+
+				/*
+				 * This can only happen if selections
+				 * are not enabled
+				 */
+				if (target != our_id) {
+					u_int sblkctl;
+					char  cur_channel;
+					int   swap;
+
+					sblkctl = ahc_inb(ahc, SBLKCTL);
+					cur_channel = (sblkctl & SELBUSB)
+						    ? 'B' : 'A';
+					if ((ahc->features & AHC_TWIN) == 0)
+						cur_channel = 'A';
+					swap = cur_channel != channel;
+					if (channel == 'A')
+						ahc->our_id = target;
+					else
+						ahc->our_id_b = target;
+
+					if (swap)
+						ahc_outb(ahc, SBLKCTL,
+							 sblkctl ^ SELBUSB);
+
+					ahc_outb(ahc, SCSIID, target);
+
+					if (swap)
+						ahc_outb(ahc, SBLKCTL, sblkctl);
+				}
+			}
+		} else
+			ahc->black_hole = lstate;
+		/* Allow select-in operations */
+		if (ahc->black_hole != NULL && ahc->enabled_luns > 0) {
+			scsiseq = ahc_inb(ahc, SCSISEQ_TEMPLATE);
+			scsiseq |= ENSELI;
+			ahc_outb(ahc, SCSISEQ_TEMPLATE, scsiseq);
+			scsiseq = ahc_inb(ahc, SCSISEQ);
+			scsiseq |= ENSELI;
+			ahc_outb(ahc, SCSISEQ, scsiseq);
+		}
+		unpause_sequencer(ahc);
+		ahc_unlock(ahc, &s);
+		ccb->ccb_h.status = CAM_REQ_CMP;
+		xpt_print_path(ccb->ccb_h.path);
+		printf("Lun now enabled for target mode\n");
+	} else {
+		struct scb *scb;
+		int i, empty;
+
+		if (lstate == NULL) {
+			ccb->ccb_h.status = CAM_LUN_INVALID;
+			return;
+		}
+
+		ahc_lock(ahc, &s);
+		
+		ccb->ccb_h.status = CAM_REQ_CMP;
+		LIST_FOREACH(scb, &ahc->pending_scbs, pending_links) {
+			struct ccb_hdr *ccbh;
+
+			ccbh = &scb->io_ctx->ccb_h;
+			if (ccbh->func_code == XPT_CONT_TARGET_IO
+			 && !xpt_path_comp(ccbh->path, ccb->ccb_h.path)){
+				printf("CTIO pending\n");
+				ccb->ccb_h.status = CAM_REQ_INVALID;
+				ahc_unlock(ahc, &s);
+				return;
+			}
+		}
+
+		if (SLIST_FIRST(&lstate->accept_tios) != NULL) {
+			printf("ATIOs pending\n");
+			ccb->ccb_h.status = CAM_REQ_INVALID;
+		}
+
+		if (SLIST_FIRST(&lstate->immed_notifies) != NULL) {
+			printf("INOTs pending\n");
+			ccb->ccb_h.status = CAM_REQ_INVALID;
+		}
+
+		if (ccb->ccb_h.status != CAM_REQ_CMP) {
+			ahc_unlock(ahc, &s);
+			return;
+		}
+
+		xpt_print_path(ccb->ccb_h.path);
+		printf("Target mode disabled\n");
+		xpt_free_path(lstate->path);
+		free(lstate, M_DEVBUF);
+
+		pause_sequencer(ahc);
+		/* Can we clean up the target too? */
+		if (target != CAM_TARGET_WILDCARD) {
+			tstate->enabled_luns[lun] = NULL;
+			ahc->enabled_luns--;
+			for (empty = 1, i = 0; i < 8; i++)
+				if (tstate->enabled_luns[i] != NULL) {
+					empty = 0;
+					break;
+				}
+
+			if (empty) {
+				ahc_free_tstate(ahc, target, channel,
+						/*force*/FALSE);
+				if (ahc->features & AHC_MULTI_TID) {
+					u_int targid_mask;
+
+					targid_mask = ahc_inb(ahc, TARGID)
+						    | (ahc_inb(ahc, TARGID + 1)
+						       << 8);
+
+					targid_mask &= ~target_mask;
+					ahc_outb(ahc, TARGID, targid_mask);
+					ahc_outb(ahc, TARGID+1,
+					 	 (targid_mask >> 8));
+					ahc_update_scsiid(ahc, targid_mask);
+				}
+			}
+		} else {
+
+			ahc->black_hole = NULL;
+
+			/*
+			 * We can't allow selections without
+			 * our black hole device.
+			 */
+			empty = TRUE;
+		}
+		if (ahc->enabled_luns == 0) {
+			/* Disallow select-in */
+			u_int scsiseq;
+
+			scsiseq = ahc_inb(ahc, SCSISEQ_TEMPLATE);
+			scsiseq &= ~ENSELI;
+			ahc_outb(ahc, SCSISEQ_TEMPLATE, scsiseq);
+			scsiseq = ahc_inb(ahc, SCSISEQ);
+			scsiseq &= ~ENSELI;
+			ahc_outb(ahc, SCSISEQ, scsiseq);
+
+			if ((ahc->features & AHC_MULTIROLE) == 0) {
+				printf("Configuring Initiator Mode\n");
+				ahc->flags &= ~AHC_TARGETROLE;
+				ahc->flags |= AHC_INITIATORROLE;
+				pause_sequencer(ahc);
+				ahc_loadseq(ahc);
+			}
+		}
+		unpause_sequencer(ahc);
+		ahc_unlock(ahc, &s);
+	}
+}
+
+static void
+ahc_update_scsiid(struct ahc_softc *ahc, u_int targid_mask)
+{
+	u_int scsiid_mask;
+	u_int scsiid;
+
+	if ((ahc->features & AHC_MULTI_TID) == 0)
+		panic("ahc_update_scsiid called on non-multitid unit\n");
+
+	/*
+	 * Since we will rely on the the TARGID mask
+	 * for selection enables, ensure that OID
+	 * in SCSIID is not set to some other ID
+	 * that we don't want to allow selections on.
+	 */
+	if ((ahc->features & AHC_ULTRA2) != 0)
+		scsiid = ahc_inb(ahc, SCSIID_ULTRA2);
+	else
+		scsiid = ahc_inb(ahc, SCSIID);
+	scsiid_mask = 0x1 << (scsiid & OID);
+	if ((targid_mask & scsiid_mask) == 0) {
+		u_int our_id;
+
+		/* ffs counts from 1 */
+		our_id = ffs(targid_mask);
+		if (our_id == 0)
+			our_id = ahc->our_id;
+		else
+			our_id--;
+		scsiid &= TID;
+		scsiid |= our_id;
+	}
+	if ((ahc->features & AHC_ULTRA2) != 0)
+		ahc_outb(ahc, SCSIID_ULTRA2, scsiid);
+	else
+		ahc_outb(ahc, SCSIID, scsiid);
+}
+
+void
+ahc_run_tqinfifo(struct ahc_softc *ahc, int paused)
+{
+	struct target_cmd *cmd;
+
+	/*
+	 * If the card supports auto-access pause,
+	 * we can access the card directly regardless
+	 * of whether it is paused or not.
+	 */
+	if ((ahc->features & AHC_AUTOPAUSE) != 0)
+		paused = TRUE;
+
+	while ((cmd = &ahc->targetcmds[ahc->tqinfifonext])->cmd_valid != 0) {
+
+		/*
+		 * Only advance through the queue if we
+		 * have the resources to process the command.
+		 */
+		if (ahc_handle_target_cmd(ahc, cmd) != 0)
+			break;
+
+		ahc->tqinfifonext++;
+		cmd->cmd_valid = 0;
+
+		/*
+		 * Lazily update our position in the target mode incomming
+		 * command queue as seen by the sequencer.
+		 */
+		if ((ahc->tqinfifonext & (HOST_TQINPOS - 1)) == 1) {
+			if ((ahc->features & AHC_HS_MAILBOX) != 0) {
+				u_int hs_mailbox;
+
+				hs_mailbox = ahc_inb(ahc, HS_MAILBOX);
+				hs_mailbox &= ~HOST_TQINPOS;
+				hs_mailbox |= ahc->tqinfifonext & HOST_TQINPOS;
+				ahc_outb(ahc, HS_MAILBOX, hs_mailbox);
+			} else {
+				if (!paused)
+					pause_sequencer(ahc);	
+				ahc_outb(ahc, KERNEL_TQINPOS,
+					 ahc->tqinfifonext & HOST_TQINPOS);
+				if (!paused)
+					unpause_sequencer(ahc);
+			}
+		}
+	}
+}
+
+static int
+ahc_handle_target_cmd(struct ahc_softc *ahc, struct target_cmd *cmd)
+{
+	struct	  tmode_tstate *tstate;
+	struct	  tmode_lstate *lstate;
+	struct	  ccb_accept_tio *atio;
+	uint8_t *byte;
+	int	  initiator;
+	int	  target;
+	int	  lun;
+
+	initiator = SCSIID_TARGET(ahc, cmd->scsiid);
+	target = SCSIID_OUR_ID(cmd->scsiid);
+	lun    = (cmd->identify & MSG_IDENTIFY_LUNMASK);
+
+	byte = cmd->bytes;
+	tstate = ahc->enabled_targets[target];
+	lstate = NULL;
+	if (tstate != NULL)
+		lstate = tstate->enabled_luns[lun];
+
+	/*
+	 * Commands for disabled luns go to the black hole driver.
+	 */
+	if (lstate == NULL)
+		lstate = ahc->black_hole;
+
+	atio = (struct ccb_accept_tio*)SLIST_FIRST(&lstate->accept_tios);
+	if (atio == NULL) {
+		ahc->flags |= AHC_TQINFIFO_BLOCKED;
+		/*
+		 * Wait for more ATIOs from the peripheral driver for this lun.
+		 */
+		return (1);
+	} else
+		ahc->flags &= ~AHC_TQINFIFO_BLOCKED;
+#if 0
+	printf("Incoming command from %d for %d:%d%s\n",
+	       initiator, target, lun,
+	       lstate == ahc->black_hole ? "(Black Holed)" : "");
+#endif
+	SLIST_REMOVE_HEAD(&lstate->accept_tios, sim_links.sle);
+
+	if (lstate == ahc->black_hole) {
+		/* Fill in the wildcards */
+		atio->ccb_h.target_id = target;
+		atio->ccb_h.target_lun = lun;
+	}
+
+	/*
+	 * Package it up and send it off to
+	 * whomever has this lun enabled.
+	 */
+	atio->sense_len = 0;
+	atio->init_id = initiator;
+	if (byte[0] != 0xFF) {
+		/* Tag was included */
+		atio->tag_action = *byte++;
+		atio->tag_id = *byte++;
+		atio->ccb_h.flags = CAM_TAG_ACTION_VALID;
+	} else {
+		atio->ccb_h.flags = 0;
+	}
+	byte++;
+
+	/* Okay.  Now determine the cdb size based on the command code */
+	switch (*byte >> CMD_GROUP_CODE_SHIFT) {
+	case 0:
+		atio->cdb_len = 6;
+		break;
+	case 1:
+	case 2:
+		atio->cdb_len = 10;
+		break;
+	case 4:
+		atio->cdb_len = 16;
+		break;
+	case 5:
+		atio->cdb_len = 12;
+		break;
+	case 3:
+	default:
+		/* Only copy the opcode. */
+		atio->cdb_len = 1;
+		printf("Reserved or VU command code type encountered\n");
+		break;
+	}
+	
+	memcpy(atio->cdb_io.cdb_bytes, byte, atio->cdb_len);
+
+	atio->ccb_h.status |= CAM_CDB_RECVD;
+
+	if ((cmd->identify & MSG_IDENTIFY_DISCFLAG) == 0) {
+		/*
+		 * We weren't allowed to disconnect.
+		 * We're hanging on the bus until a
+		 * continue target I/O comes in response
+		 * to this accept tio.
+		 */
+#if 0
+		printf("Received Immediate Command %d:%d:%d - %p\n",
+		       initiator, target, lun, ahc->pending_device);
+#endif
+		ahc->pending_device = lstate;
+		ahc_freeze_ccb((union ccb *)atio);
+		atio->ccb_h.flags |= CAM_DIS_DISCONNECT;
+	}
+	xpt_done((union ccb*)atio);
+	return (0);
+}
+
+#endif
diff -urN linux/drivers/scsi/aic7xxx/aic7xxx.h /tmp/linux/drivers/scsi/aic7xxx/aic7xxx.h
--- linux/drivers/scsi/aic7xxx/aic7xxx.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aic7xxx.h	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,1196 @@
+/*
+ * Core definitions and data structures shareable across OS platforms.
+ *
+ * Copyright (c) 1994-2001 Justin T. Gibbs.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $Id: //depot/src/aic7xxx/aic7xxx.h#18 $
+ *
+ * $FreeBSD: src/sys/dev/aic7xxx/aic7xxx.h,v 1.30 2000/11/10 20:13:40 gibbs Exp $
+ */
+
+#ifndef _AIC7XXX_H_
+#define _AIC7XXX_H_
+
+/* Register Definitions */
+#include "aic7xxx_reg.h"
+
+/************************* Forward Declarations *******************************/
+struct ahc_platform_data;
+struct scb_platform_data;
+
+/****************************** Useful Macros *********************************/
+#ifndef MAX
+#define MAX(a,b) (((a) > (b)) ? (a) : (b))
+#endif
+
+#ifndef MIN
+#define MIN(a,b) (((a) < (b)) ? (a) : (b))
+#endif
+
+#ifndef TRUE
+#define TRUE 1
+#endif
+#ifndef FALSE
+#define FALSE 0
+#endif
+
+#define NUM_ELEMENTS(array) (sizeof(array) / sizeof(*array))
+
+#define ALL_CHANNELS '\0'
+#define ALL_TARGETS_MASK 0xFFFF
+#define INITIATOR_WILDCARD	(~0)
+
+#define SCSIID_TARGET(ahc, scsiid) \
+	(((scsiid) & ((((ahc)->features & AHC_TWIN) != 0) ? TWIN_TID : TID)) \
+	>> TID_SHIFT)
+#define SCSIID_OUR_ID(scsiid) \
+	((scsiid) & OID)
+#define SCSIID_CHANNEL(ahc, scsiid) \
+	((((ahc)->features & AHC_TWIN) != 0) \
+        ? ((((scsiid) & TWIN_CHNLB) != 0) ? 'B' : 'A') \
+       : 'A')
+#define	SCB_IS_SCSIBUS_B(ahc, scb) \
+	(SCSIID_CHANNEL(ahc, (scb)->hscb->scsiid) == 'B')
+#define	SCB_GET_OUR_ID(scb) \
+	SCSIID_OUR_ID((scb)->hscb->scsiid)
+#define	SCB_GET_TARGET(ahc, scb) \
+	SCSIID_TARGET((ahc), (scb)->hscb->scsiid)
+#define	SCB_GET_CHANNEL(ahc, scb) \
+	SCSIID_CHANNEL(ahc, (scb)->hscb->scsiid)
+#define	SCB_GET_LUN(scb) \
+	((scb)->hscb->lun)
+#define SCB_GET_TARGET_OFFSET(ahc, scb)	\
+	(SCB_GET_TARGET(ahc, scb) + (SCB_IS_SCSIBUS_B(ahc, scb) ? 8 : 0))
+#define SCB_GET_TARGET_MASK(ahc, scb) \
+	(0x01 << (SCB_GET_TARGET_OFFSET(ahc, scb)))
+#define TCL_TARGET_OFFSET(tcl) \
+	((((tcl) >> 4) & TID) >> 4)
+#define TCL_LUN(tcl) \
+	(tcl & (AHC_NUM_LUNS - 1))
+#define BUILD_TCL(scsiid, lun) \
+	((lun) | (((scsiid) & TID) << 4))
+
+#ifndef	AHC_TARGET_MODE
+#undef	AHC_TMODE_ENABLE
+#define	AHC_TMODE_ENABLE 0
+#endif
+
+/**************************** Driver Constants ********************************/
+/*
+ * The maximum number of supported targets.
+ */
+#define AHC_NUM_TARGETS 16
+
+/*
+ * The maximum number of supported luns.
+ * The identify message only supports 64 luns in SPI3.
+ * You can have 2^64 luns when information unit transfers are enabled,
+ * but it is doubtful this driver will ever support IUTs.
+ */
+#define AHC_NUM_LUNS 64
+
+/*
+ * The maximum transfer per S/G segment.
+ */
+#define AHC_MAXTRANSFER_SIZE	 0x00ffffff	/* limited by 24bit counter */
+
+/*
+ * The maximum amount of SCB storage in hardware on a controller.
+ * This value represents an upper bound.  Controllers vary in the number
+ * they actually support.
+ */
+#define AHC_SCB_MAX	255
+
+/*
+ * The maximum number of concurrent transactions supported per driver instance.
+ * Sequencer Control Blocks (SCBs) store per-transaction information.  Although
+ * the space for SCBs on the host adapter varies by model, the driver will
+ * page the SCBs between host and controller memory as needed.  We are limited
+ * to 253 because:
+ * 	1) The 8bit nature of the RISC engine holds us to an 8bit value.
+ * 	2) We reserve one value, 255, to represent the invalid element.
+ *	3) Our input queue scheme requires one SCB to always be reserved
+ *	   in advance of queuing any SCBs.  This takes us down to 254.
+ *	4) To handle our output queue correctly on machines that only
+ * 	   support 32bit stores, we must clear the array 4 bytes at a
+ *	   time.  To avoid colliding with a DMA write from the sequencer,
+ *	   we must be sure that 4 slots are empty when we write to clear
+ *	   the queue.  This reduces us to 253 SCBs: 1 that just completed
+ *	   and the known three additional empty slots in the queue that
+ *	   preceed it.
+ */
+#define AHC_MAX_QUEUE	253
+
+/*
+ * Ring Buffer of incoming target commands.
+ * We allocate 256 to simplify the logic in the sequencer
+ * by using the natural wrap point of an 8bit counter.
+ */
+#define AHC_TMODE_CMDS	256
+
+/* Reset line assertion time in us */
+#define AHC_BUSRESET_DELAY	250
+
+/******************* Chip Characteristics/Operating Settings  *****************/
+/*
+ * Chip Type
+ * The chip order is from least sophisticated to most sophisticated.
+ */
+typedef enum {
+	AHC_NONE	= 0x0000,
+	AHC_CHIPID_MASK	= 0x00FF,
+	AHC_AIC7770	= 0x0001,
+	AHC_AIC7850	= 0x0002,
+	AHC_AIC7855	= 0x0003,
+	AHC_AIC7859	= 0x0004,
+	AHC_AIC7860	= 0x0005,
+	AHC_AIC7870	= 0x0006,
+	AHC_AIC7880	= 0x0007,
+	AHC_AIC7895	= 0x0008,
+	AHC_AIC7895C	= 0x0009,
+	AHC_AIC7890	= 0x000a,
+	AHC_AIC7896	= 0x000b,
+	AHC_AIC7892	= 0x000c,
+	AHC_AIC7899	= 0x000d,
+	AHC_VL		= 0x0100,	/* Bus type VL */
+	AHC_EISA	= 0x0200,	/* Bus type EISA */
+	AHC_PCI		= 0x0400,	/* Bus type PCI */
+	AHC_BUS_MASK	= 0x0F00
+} ahc_chip;
+
+/*
+ * Features available in each chip type.
+ */
+typedef enum {
+	AHC_FENONE	= 0x00000,
+	AHC_ULTRA	= 0x00001,	/* Supports 20MHz Transfers */
+	AHC_ULTRA2	= 0x00002,	/* Supports 40MHz Transfers */
+	AHC_WIDE  	= 0x00004,	/* Wide Channel */
+	AHC_TWIN	= 0x00008,	/* Twin Channel */
+	AHC_MORE_SRAM	= 0x00010,	/* 80 bytes instead of 64 */
+	AHC_CMD_CHAN	= 0x00020,	/* Has a Command DMA Channel */
+	AHC_QUEUE_REGS	= 0x00040,	/* Has Queue management registers */
+	AHC_SG_PRELOAD	= 0x00080,	/* Can perform auto-SG preload */
+	AHC_SPIOCAP	= 0x00100,	/* Has a Serial Port I/O Cap Register */
+	AHC_MULTI_TID	= 0x00200,	/* Has bitmask of TIDs for select-in */
+	AHC_HS_MAILBOX	= 0x00400,	/* Has HS_MAILBOX register */
+	AHC_DT		= 0x00800,	/* Double Transition transfers */
+	AHC_NEW_TERMCTL	= 0x01000,	/* Newer termination scheme */
+	AHC_MULTI_FUNC	= 0x02000,	/* Multi-Function Twin Channel Device */
+	AHC_LARGE_SCBS	= 0x04000,	/* 64byte SCBs */
+	AHC_AUTORATE	= 0x08000,	/* Automatic update of SCSIRATE/OFFSET*/
+	AHC_AUTOPAUSE	= 0x10000,	/* Automatic pause on register access */
+	AHC_TARGETMODE	= 0x20000,	/* Has tested target mode support */
+	AHC_MULTIROLE	= 0x40000,	/* Space for two roles at a time */
+	AHC_REMOVABLE	= 0x80000,	/* Hot-Swap supported */
+	AHC_AIC7770_FE	= AHC_FENONE,
+	AHC_AIC7850_FE	= AHC_SPIOCAP|AHC_AUTOPAUSE|AHC_TARGETMODE,
+	AHC_AIC7855_FE	= AHC_AIC7850_FE,
+	AHC_AIC7860_FE	= AHC_AIC7850_FE|AHC_ULTRA,
+	AHC_AIC7870_FE	= AHC_TARGETMODE,
+	AHC_AIC7880_FE	= AHC_AIC7870_FE|AHC_ULTRA,
+	/*
+	 * Although we have space for both the initiator and
+	 * target roles on ULTRA2 chips, we currently disable
+	 * the initiator role to allow multi-scsi-id target mode
+	 * configurations.  We can only respond on the same SCSI
+	 * ID as our initiator role if we allow initiator operation.
+	 * At some point, we should add a configuration knob to
+	 * allow both roles to be loaded.
+	 */
+	AHC_AIC7890_FE	= AHC_MORE_SRAM|AHC_CMD_CHAN|AHC_ULTRA2
+			  |AHC_QUEUE_REGS|AHC_SG_PRELOAD|AHC_MULTI_TID
+			  |AHC_HS_MAILBOX|AHC_NEW_TERMCTL|AHC_LARGE_SCBS
+			  |AHC_TARGETMODE,
+	AHC_AIC7892_FE	= AHC_AIC7890_FE|AHC_DT|AHC_AUTORATE|AHC_AUTOPAUSE,
+	AHC_AIC7895_FE	= AHC_AIC7880_FE|AHC_MORE_SRAM|AHC_AUTOPAUSE
+			  |AHC_CMD_CHAN|AHC_MULTI_FUNC|AHC_LARGE_SCBS,
+	AHC_AIC7895C_FE	= AHC_AIC7895_FE|AHC_MULTI_TID,
+	AHC_AIC7896_FE	= AHC_AIC7890_FE|AHC_MULTI_FUNC,
+	AHC_AIC7899_FE	= AHC_AIC7892_FE|AHC_MULTI_FUNC
+} ahc_feature;
+
+/*
+ * Bugs in the silicon that we work around in software.
+ */
+typedef enum {
+	AHC_BUGNONE		= 0x00,
+	/*
+	 * On all chips prior to the U2 product line,
+	 * the WIDEODD S/G segment feature does not
+	 * work during scsi->HostBus transfers.
+	 */
+	AHC_TMODE_WIDEODD_BUG	= 0x01,
+	/*
+	 * On the aic7890/91 Rev 0 chips, the autoflush
+	 * feature does not work.  A manual flush of
+	 * the DMA FIFO is required.
+	 */
+	AHC_AUTOFLUSH_BUG	= 0x02,
+	/*
+	 * On many chips, cacheline streaming does not work.
+	 */
+	AHC_CACHETHEN_BUG	= 0x04,
+	/*
+	 * On the aic7896/97 chips, cacheline
+	 * streaming must be enabled.
+	 */
+	AHC_CACHETHEN_DIS_BUG	= 0x08,
+	/*
+	 * PCI 2.1 Retry failure on non-empty data fifo.
+	 */
+	AHC_PCI_2_1_RETRY_BUG	= 0x10,
+	/*
+	 * Controller does not handle cacheline residuals
+	 * properly on S/G segments if PCI MWI instructions
+	 * are allowed.
+	 */
+	AHC_PCI_MWI_BUG		= 0x20,
+	/*
+	 * An SCB upload using the SCB channel's
+	 * auto array entry copy feature may 
+	 * corrupt data.  This appears to only
+	 * occur on 66MHz systems.
+	 */
+	AHC_SCBCHAN_UPLOAD_BUG	= 0x40
+} ahc_bug;
+
+/*
+ * Configuration specific settings.
+ * The driver determines these settings by probing the
+ * chip/controller's configuration.
+ */
+typedef enum {
+	AHC_FNONE		= 0x000,
+	AHC_PAGESCBS		= 0x001,/* Enable SCB paging */
+	AHC_CHANNEL_B_PRIMARY	= 0x002,/*
+					 * On twin channel adapters, probe
+					 * channel B first since it is the
+					 * primary bus.
+					 */
+	AHC_USEDEFAULTS		= 0x004,/*
+					 * For cards without an seeprom
+					 * or a BIOS to initialize the chip's
+					 * SRAM, we use the default target
+					 * settings.
+					 */
+	AHC_SEQUENCER_DEBUG	= 0x008,
+	AHC_SHARED_SRAM		= 0x010,
+	AHC_LARGE_SEEPROM	= 0x020,/* Uses C56_66 not C46 */
+	AHC_RESET_BUS_A		= 0x040,
+	AHC_RESET_BUS_B		= 0x080,
+	AHC_EXTENDED_TRANS_A	= 0x100,
+	AHC_EXTENDED_TRANS_B	= 0x200,
+	AHC_TERM_ENB_A		= 0x400,
+	AHC_TERM_ENB_B		= 0x800,
+	AHC_INITIATORROLE	= 0x1000,/*
+					  * Allow initiator operations on
+					  * this controller.
+					  */
+	AHC_TARGETROLE		= 0x2000,/*
+					  * Allow target operations on this
+					  * controller.
+					  */
+	AHC_NEWEEPROM_FMT	= 0x4000,
+	AHC_RESOURCE_SHORTAGE	= 0x8000,
+	AHC_TQINFIFO_BLOCKED	= 0x10000,/* Blocked waiting for ATIOs */
+	AHC_INT50_SPEEDFLEX	= 0x20000,/*
+					   * Internal 50pin connector
+					   * sits behind an aic3860
+					   */
+	AHC_SCB_BTT		= 0x40000,/*
+					   * The busy targets table is
+					   * stored in SCB space rather
+					   * than SRAM.
+					   */
+	AHC_BIOS_ENABLED	= 0x80000,
+	AHC_ALL_INTERRUPTS	= 0x100000
+} ahc_flag;
+
+/*
+ * Controller  Information composed at probe time.
+ */
+struct ahc_probe_config {
+	const char	*description;
+	char		 channel;
+	char		 channel_b;
+	ahc_chip	 chip;
+	ahc_feature	 features;
+	ahc_bug		 bugs;
+	ahc_flag	 flags;
+};
+
+/************************* Hardware  SCB Definition ***************************/
+
+/*
+ * The driver keeps up to MAX_SCB scb structures per card in memory.  The SCB
+ * consists of a "hardware SCB" mirroring the fields availible on the card
+ * and additional information the kernel stores for each transaction.
+ *
+ * To minimize space utilization, a portion of the hardware scb stores
+ * different data during different portions of a SCSI transaction.
+ * As initialized by the host driver for the initiator role, this area
+ * contains the SCSI cdb (or a pointer to the  cdb) to be executed.  After
+ * the cdb has been presented to the target, this area serves to store
+ * residual transfer information and the SCSI status byte.
+ * For the target role, the contents of this area do not change, but
+ * still serve a different purpose than for the initiator role.  See
+ * struct target_data for details.
+ */
+
+/*
+ * Status information embedded in the shared poriton of
+ * an SCB after passing the cdb to the target.  The kernel
+ * driver will only read this data for transactions that
+ * complete abnormally (non-zero status byte).
+ */
+struct status_pkt {
+	uint32_t residual_datacnt;	/* Residual in the current S/G seg */
+	uint32_t residual_sg_ptr;	/* The next S/G for this transfer */
+	uint8_t	 scsi_status;		/* Standard SCSI status byte */
+};
+
+/*
+ * Target mode version of the shared data SCB segment.
+ */
+struct target_data {
+	uint8_t	target_phases;		/* Bitmap of phases to execute */
+	uint8_t	data_phase;		/* Data-In or Data-Out */
+	uint8_t	scsi_status;		/* SCSI status to give to initiator */
+	uint8_t	initiator_tag;		/* Initiator's transaction tag */
+};
+
+struct hardware_scb {
+/*0*/	union {
+		/*
+		 * If the cdb is 12 bytes or less, we embed it directly
+		 * in the SCB.  For longer cdbs, we embed the address
+		 * of the cdb payload as seen by the chip and a DMA
+		 * is used to pull it in.
+		 */
+		uint8_t	cdb[12];
+		uint32_t	cdb_ptr;
+		struct		status_pkt status;
+		struct		target_data tdata;
+	} shared_data;
+/*
+ * A word about residuals.
+ * The scb is presented to the sequencer with the dataptr and datacnt
+ * fields initialized to the contents of the first S/G element to
+ * transfer.  The sgptr field is initialized to the bus address for
+ * the S/G element that follows the first in the in core S/G array
+ * or'ed with the SG_FULL_RESID flag.  Sgptr may point to an invalid
+ * S/G entry for this transfer (single S/G element transfer with the
+ * first elements address and length preloaded in the dataptr/datacnt
+ * fields).  If no transfer is to occur, sgptr is set to SG_LIST_NULL.
+ * The SG_FULL_RESID flag ensures that the residual will be correctly
+ * noted even if no data transfers occur.  Once the data phase is entered,
+ * the residual sgptr and datacnt are loaded from the sgptr and the
+ * datacnt fields.  After each S/G element's dataptr and length are
+ * loaded into the hardware, the residual sgptr is advanced.  After
+ * each S/G element is expired, its datacnt field is checked to see
+ * if the LAST_SEG flag is set.  If so, SG_LIST_NULL is set in the
+ * residual sg ptr and the transfer is considered complete.  If the
+ * sequencer determines that there is a residual in the tranfer, it
+ * will set the SG_RESID_VALID flag in sgptr and dma the scb back into
+ * host memory.  To sumarize:
+ *
+ * Sequencer:
+ *	o A residual has occurred if SG_FULL_RESID is set in sgptr,
+ *	  or residual_sgptr does not have SG_LIST_NULL set.
+ *
+ *	o We are transfering the last segment if residual_datacnt has
+ *	  the SG_LAST_SEG flag set.
+ *
+ * Host:
+ *	o A residual has occurred if a completed scb has the
+ *	  SG_RESID_VALID flag set.
+ *
+ *	o residual_sgptr and sgptr refer to the "next" sg entry
+ *	  and so may point beyond the last valid sg entry for the
+ *	  transfer.
+ */ 
+/*12*/	uint32_t dataptr;
+/*16*/	uint32_t datacnt;		/*
+					 * Byte 3 (numbered from 0) of
+					 * the datacnt is really the
+					 * 4th byte in that data address.
+					 */
+/*20*/	uint32_t sgptr;
+#define SG_PTR_MASK	0xFFFFFFF8
+/*24*/	uint8_t  control;	/* See SCB_CONTROL in aic7xxx.reg for details */
+/*25*/	uint8_t  scsiid;	/* what to load in the SCSIID register */
+/*26*/	uint8_t  lun;
+/*27*/	uint8_t  tag;			/*
+					 * Index into our kernel SCB array.
+					 * Also used as the tag for tagged I/O
+					 */
+/*28*/	uint8_t  cdb_len;
+/*29*/	uint8_t  scsirate;		/* Value for SCSIRATE register */
+/*30*/	uint8_t  scsioffset;		/* Value for SCSIOFFSET register */
+/*31*/	uint8_t  next;			/*
+					 * Used for threading SCBs in the
+					 * "Waiting for Selection" and
+					 * "Disconnected SCB" lists down
+					 * in the sequencer.
+					 */
+/*32*/	uint8_t  cdb32[32];		/*
+					 * CDB storage for cdbs of size
+					 * 13->32.  We store them here
+					 * because hardware scbs are
+					 * allocated from DMA safe
+					 * memory so we are guaranteed
+					 * the controller can access
+					 * this data.
+					 */
+};
+
+/************************ Kernel SCB Definitions ******************************/
+/*
+ * Some fields of the SCB are OS dependent.  Here we collect the
+ * definitions for elements that all OS platforms need to include
+ * in there SCB definition.
+ */
+
+/*
+ * Definition of a scatter/gather element as transfered to the controller.
+ * The aic7xxx chips only support a 24bit length.  We use the top byte of
+ * the length to store additional address bits and a flag to indicate
+ * that a given segment terminates the transfer.  This gives us an
+ * addressable range of 512GB on machines with 64bit PCI or with chips
+ * that can support dual address cycles on 32bit PCI busses.
+ */
+struct ahc_dma_seg {
+	uint32_t	addr;
+	uint32_t	len;
+#define	AHC_DMA_LAST_SEG	0x80000000
+#define	AHC_SG_HIGH_ADDR_MASK	0x7F000000
+#define	AHC_SG_LEN_MASK		0x00FFFFFF
+};
+
+/*
+ * The current state of this SCB.
+ */
+typedef enum {
+	SCB_FREE		= 0x0000,
+	SCB_OTHERTCL_TIMEOUT	= 0x0002,/*
+					  * Another device was active
+					  * during the first timeout for
+					  * this SCB so we gave ourselves
+					  * an additional timeout period
+					  * in case it was hogging the
+					  * bus.
+				          */
+	SCB_DEVICE_RESET	= 0x0004,
+	SCB_SENSE		= 0x0008,
+	SCB_CDB32_PTR		= 0x0010,
+	SCB_RECOVERY_SCB	= 0x0040,
+	SCB_NEGOTIATE		= 0x0080,
+	SCB_ABORT		= 0x1000,
+	SCB_UNTAGGEDQ		= 0x2000,
+	SCB_ACTIVE		= 0x4000,
+	SCB_TARGET_IMMEDIATE	= 0x8000
+} scb_flag;
+
+struct scb {
+	struct	hardware_scb	 *hscb;
+	union {
+		SLIST_ENTRY(scb)  sle;
+		TAILQ_ENTRY(scb)  tqe;
+	} links;
+	LIST_ENTRY(scb)		  pending_links;
+	ahc_io_ctx_t		  io_ctx;
+	struct ahc_softc	 *ahc_softc;
+	scb_flag		  flags;
+#ifndef __linux__
+	bus_dmamap_t		  dmamap;
+#endif
+	struct scb_platform_data *platform_data;
+	struct	ahc_dma_seg 	 *sg_list;
+	bus_addr_t		  sg_list_phys;
+	u_int			  sg_count;/* How full ahc_dma_seg is */
+};
+
+struct sg_map_node {
+	bus_dmamap_t		 sg_dmamap;
+	bus_addr_t		 sg_physaddr;
+	struct ahc_dma_seg*	 sg_vaddr;
+	SLIST_ENTRY(sg_map_node) links;
+};
+
+struct scb_data {
+	SLIST_HEAD(, scb) free_scbs;	/*
+					 * Pool of SCBs ready to be assigned
+					 * commands to execute.
+					 */
+	struct	scb *scbindex[AHC_SCB_MAX + 1];/* Mapping from tag to SCB */
+	struct	hardware_scb	*hscbs;	/* Array of hardware SCBs */
+	struct	scb *scbarray;		/* Array of kernel SCBs */
+	struct	scsi_sense_data *sense; /* Per SCB sense data */
+
+	/*
+	 * "Bus" addresses of our data structures.
+	 */
+	bus_dma_tag_t	 hscb_dmat;	/* dmat for our hardware SCB array */
+	bus_dmamap_t	 hscb_dmamap;
+	bus_addr_t	 hscb_busaddr;
+	bus_dma_tag_t	 sense_dmat;
+	bus_dmamap_t	 sense_dmamap;
+	bus_addr_t	 sense_busaddr;
+	bus_dma_tag_t	 sg_dmat;	/* dmat for our sg segments */
+	SLIST_HEAD(, sg_map_node) sg_maps;
+	uint8_t	numscbs;
+	uint8_t	maxhscbs;		/* Number of SCBs on the card */
+	uint8_t	init_level;		/*
+					 * How far we've initialized
+					 * this structure.
+					 */
+};
+
+/************************ Target Mode Definitions *****************************/
+
+/*
+ * Connection desciptor for select-in requests in target mode.
+ */
+struct target_cmd {
+	uint8_t scsiid;		/* Our ID and the initiator's ID */
+	uint8_t identify;	/* Identify message */
+	uint8_t bytes[22];	/* 
+				 * Bytes contains any additional message
+				 * bytes terminated by 0xFF.  The remainder
+				 * is the cdb to execute.
+				 */
+	uint8_t cmd_valid;	/*
+				 * When a command is complete, the firmware
+				 * will set cmd_valid to all bits set.
+				 * After the host has seen the command,
+				 * the bits are cleared.  This allows us
+				 * to just peek at host memory to determine
+				 * if more work is complete. cmd_valid is on
+				 * an 8 byte boundary to simplify setting
+				 * it on aic7880 hardware which only has
+				 * limited direct access to the DMA FIFO.
+				 */
+	uint8_t pad[7];
+};
+
+/*
+ * Number of events we can buffer up if we run out
+ * of immediate notify ccbs.
+ */
+#define AHC_TMODE_EVENT_BUFFER_SIZE 8
+struct ahc_tmode_event {
+	uint8_t initiator_id;
+	uint8_t event_type;	/* MSG type or EVENT_TYPE_BUS_RESET */
+#define	EVENT_TYPE_BUS_RESET 0xFF
+	uint8_t event_arg;
+};
+
+/*
+ * Per enabled lun target mode state.
+ * As this state is directly influenced by the host OS'es target mode
+ * environment, we let the OS module define it.  Forward declare the
+ * structure here so we can store arrays of them, etc. in OS neutral
+ * data structures.
+ */
+#ifdef AHC_TARGET_MODE 
+struct tmode_lstate {
+	struct cam_path *path;
+	struct ccb_hdr_slist accept_tios;
+	struct ccb_hdr_slist immed_notifies;
+	struct ahc_tmode_event event_buffer[AHC_TMODE_EVENT_BUFFER_SIZE];
+	uint8_t event_r_idx;
+	uint8_t event_w_idx;
+};
+#else
+struct tmode_lstate;
+#endif
+
+/******************** Transfer Negotiation Datastructures *********************/
+#define AHC_TRANS_CUR		0x01	/* Modify current neogtiation status */
+#define AHC_TRANS_ACTIVE	0x03	/* Assume this target is on the bus */
+#define AHC_TRANS_GOAL		0x04	/* Modify negotiation goal */
+#define AHC_TRANS_USER		0x08	/* Modify user negotiation settings */
+
+/*
+ * Transfer Negotiation Information.
+ */
+struct ahc_transinfo {
+	uint8_t protocol_version;	/* SCSI Revision level */
+	uint8_t transport_version;	/* SPI Revision level */
+	uint8_t width;			/* Bus width */
+	uint8_t period;			/* Sync rate factor */
+	uint8_t offset;			/* Sync offset */
+	uint8_t ppr_options;		/* Parallel Protocol Request options */
+};
+
+/*
+ * Per-initiator current, goal and user transfer negotiation information. */
+struct ahc_initiator_tinfo {
+	uint8_t scsirate;		/* Computed value for SCSIRATE reg */
+	struct ahc_transinfo current;
+	struct ahc_transinfo goal;
+	struct ahc_transinfo user;
+};
+
+/*
+ * Per enabled target ID state.
+ * Pointers to lun target state as well as sync/wide negotiation information
+ * for each initiator<->target mapping.  For the initiator role we pretend
+ * that we are the target and the targets are the initiators since the
+ * negotiation is the same regardless of role.
+ */
+struct tmode_tstate {
+	struct tmode_lstate*		enabled_luns[AHC_NUM_LUNS];
+	struct ahc_initiator_tinfo	transinfo[AHC_NUM_TARGETS];
+
+	/*
+	 * Per initiator state bitmasks.
+	 */
+	uint16_t		 ultraenb;	/* Using ultra sync rate  */
+	uint16_t	 	 discenable;	/* Disconnection allowed  */
+	uint16_t		 tagenable;	/* Tagged Queuing allowed */
+};
+
+/*
+ * Data structure for our table of allowed synchronous transfer rates.
+ */
+struct ahc_syncrate {
+	u_int sxfr_u2;	/* Value of the SXFR parameter for Ultra2+ Chips */
+	u_int sxfr;	/* Value of the SXFR parameter for <= Ultra Chips */
+#define		ULTRA_SXFR 0x100	/* Rate Requires Ultra Mode set */
+#define		ST_SXFR	   0x010	/* Rate Single Transition Only */
+#define		DT_SXFR	   0x040	/* Rate Double Transition Only */
+	uint8_t period; /* Period to send to SCSI target */
+	char *rate;
+};
+
+/*
+ * The synchronouse transfer rate table.
+ */
+extern struct ahc_syncrate ahc_syncrates[];
+
+/*
+ * Indexes into our table of syncronous transfer rates.
+ */
+#define AHC_SYNCRATE_DT		0
+#define AHC_SYNCRATE_ULTRA2	1
+#define AHC_SYNCRATE_ULTRA	3
+#define AHC_SYNCRATE_FAST	6
+
+/***************************** Lookup Tables **********************************/
+/*
+ * Textual descriptions of the different chips indexed by chip type.
+ */
+extern char *ahc_chip_names[];
+extern const u_int num_chip_names;
+
+/*
+ * Hardware error codes.
+ */
+struct hard_error_entry {
+        uint8_t errno;
+	char *errmesg;
+};
+extern struct hard_error_entry hard_error[];
+extern const u_int num_errors;
+
+/*
+ * Phase -> name and message out response
+ * to parity errors in each phase table. 
+ */
+struct phase_table_entry {
+        uint8_t phase;
+        uint8_t mesg_out; /* Message response to parity errors */
+	char *phasemsg;
+};
+extern struct phase_table_entry phase_table[];
+extern const u_int num_phases;
+
+/************************** Serial EEPROM Format ******************************/
+
+struct seeprom_config {
+/*
+ * Per SCSI ID Configuration Flags
+ */
+	uint16_t device_flags[16];	/* words 0-15 */
+#define		CFXFER		0x0007	/* synchronous transfer rate */
+#define		CFSYNCH		0x0008	/* enable synchronous transfer */
+#define		CFDISC		0x0010	/* enable disconnection */
+#define		CFWIDEB		0x0020	/* wide bus device */
+#define		CFSYNCHISULTRA	0x0040	/* CFSYNCH is an ultra offset (2940AU)*/
+#define		CFSYNCSINGLE	0x0080	/* Single-Transition signalling */
+#define		CFSTART		0x0100	/* send start unit SCSI command */
+#define		CFINCBIOS	0x0200	/* include in BIOS scan */
+#define		CFRNFOUND	0x0400	/* report even if not found */
+#define		CFMULTILUNDEV	0x0800	/* Probe multiple luns in BIOS scan */
+#define		CFWBCACHEENB	0x4000	/* Enable W-Behind Cache on disks */
+#define		CFWBCACHENOP	0xc000	/* Don't touch W-Behind Cache */
+
+/*
+ * BIOS Control Bits
+ */
+	uint16_t bios_control;		/* word 16 */
+#define		CFSUPREM	0x0001	/* support all removeable drives */
+#define		CFSUPREMB	0x0002	/* support removeable boot drives */
+#define		CFBIOSEN	0x0004	/* BIOS enabled */
+/*		UNUSED		0x0008	*/
+#define		CFSM2DRV	0x0010	/* support more than two drives */
+#define		CF284XEXTEND	0x0020	/* extended translation (284x cards) */	
+#define		CFSTPWLEVEL	0x0010	/* Termination level control */
+#define		CFEXTEND	0x0080	/* extended translation enabled */
+#define		CFSCAMEN	0x0100	/* SCAM enable */
+/*		UNUSED		0xff00	*/
+
+/*
+ * Host Adapter Control Bits
+ */
+	uint16_t adapter_control;	/* word 17 */	
+#define		CFAUTOTERM	0x0001	/* Perform Auto termination */
+#define		CFULTRAEN	0x0002	/* Ultra SCSI speed enable */
+#define		CF284XSELTO     0x0003	/* Selection timeout (284x cards) */
+#define		CF284XFIFO      0x000C	/* FIFO Threshold (284x cards) */
+#define		CFSTERM		0x0004	/* SCSI low byte termination */
+#define		CFWSTERM	0x0008	/* SCSI high byte termination */
+#define		CFSPARITY	0x0010	/* SCSI parity */
+#define		CF284XSTERM     0x0020	/* SCSI low byte term (284x cards) */	
+#define		CFMULTILUN	0x0020	/* SCSI low byte term (284x cards) */	
+#define		CFRESETB	0x0040	/* reset SCSI bus at boot */
+#define		CFCLUSTERENB	0x0080	/* Cluster Enable */
+#define		CFCHNLBPRIMARY	0x0100	/* aic7895 probe B channel first */
+#define		CFSEAUTOTERM	0x0400	/* Ultra2 Perform secondary Auto Term*/
+#define		CFSELOWTERM	0x0800	/* Ultra2 secondary low term */
+#define		CFSEHIGHTERM	0x1000	/* Ultra2 secondary high term */
+#define		CFDOMAINVAL	0x4000	/* Perform Domain Validation*/
+
+/*
+ * Bus Release Time, Host Adapter ID
+ */
+	uint16_t brtime_id;		/* word 18 */
+#define		CFSCSIID	0x000f	/* host adapter SCSI ID */
+/*		UNUSED		0x00f0	*/
+#define		CFBRTIME	0xff00	/* bus release time */
+
+/*
+ * Maximum targets
+ */
+	uint16_t max_targets;		/* word 19 */	
+#define		CFMAXTARG	0x00ff	/* maximum targets */
+#define		CFBOOTLUN	0x0f00	/* Lun to boot from */
+#define		CFBOOTID	0xf000	/* Target to boot from */
+	uint16_t res_1[10];		/* words 20-29 */
+	uint16_t signature;		/* Signature == 0x250 */
+#define		CFSIGNATURE	0x250
+	uint16_t checksum;		/* word 31 */
+};
+
+/****************************  Message Buffer *********************************/
+typedef enum {
+	MSG_TYPE_NONE			= 0x00,
+	MSG_TYPE_INITIATOR_MSGOUT	= 0x01,
+	MSG_TYPE_INITIATOR_MSGIN	= 0x02,
+	MSG_TYPE_TARGET_MSGOUT		= 0x03,
+	MSG_TYPE_TARGET_MSGIN		= 0x04
+} ahc_msg_type;
+
+typedef enum {
+	MSGLOOP_IN_PROG,
+	MSGLOOP_MSGCOMPLETE,
+	MSGLOOP_TERMINATED
+} msg_loop_stat;
+
+/*********************** Software Configuration Structure *********************/
+TAILQ_HEAD(scb_tailq, scb);
+
+struct ahc_suspend_channel_state {
+	uint8_t	scsiseq;
+	uint8_t	sxfrctl0;
+	uint8_t	sxfrctl1;
+	uint8_t	simode0;
+	uint8_t	simode1;
+	uint8_t	seltimer;
+	uint8_t	seqctl;
+};
+
+struct ahc_suspend_state {
+	struct	ahc_suspend_channel_state channel[2];
+	uint8_t	optionmode;
+	uint8_t	dscommand0;
+	uint8_t	dspcistatus;
+	/* hsmailbox */
+	uint8_t	crccontrol1;
+	uint8_t	scbbaddr;
+	/* Host and sequencer SCB counts */
+	uint8_t	dff_thrsh;
+	uint8_t	*scratch_ram;
+	uint8_t	*btt;
+};
+
+struct ahc_softc {
+	bus_space_tag_t           tag;
+	bus_space_handle_t        bsh;
+#ifndef __linux__
+	bus_dma_tag_t		  buffer_dmat;   /* dmat for buffer I/O */
+#endif
+	struct scb_data		 *scb_data;
+
+	struct scb		 *next_queued_scb;
+
+	/*
+	 * SCBs that have been sent to the controller
+	 */
+	LIST_HEAD(, scb)	 pending_scbs;
+
+	/*
+	 * Counting lock for deferring the release of additional
+	 * untagged transactions from the untagged_queues.  When
+	 * the lock is decremented to 0, all queues in the
+	 * untagged_queues array are run.
+	 */
+	u_int			  untagged_queue_lock;
+
+	/*
+	 * Per-target queue of untagged-transactions.  The
+	 * transaction at the head of the queue is the
+	 * currently pending untagged transaction for the
+	 * target.  The driver only allows a single untagged
+	 * transaction per target.
+	 */
+	struct scb_tailq	  untagged_queues[AHC_NUM_TARGETS];
+
+	/*
+	 * Platform specific data.
+	 */
+	struct ahc_platform_data *platform_data;
+
+	/*
+	 * Platform specific device information.
+	 */
+	ahc_dev_softc_t		  dev_softc;
+
+	/*
+	 * Target mode related state kept on a per enabled lun basis.
+	 * Targets that are not enabled will have null entries.
+	 * As an initiator, we keep one target entry for our initiator
+	 * ID to store our sync/wide transfer settings.
+	 */
+	struct tmode_tstate*	  enabled_targets[AHC_NUM_TARGETS];
+
+	/*
+	 * The black hole device responsible for handling requests for
+	 * disabled luns on enabled targets.
+	 */
+	struct tmode_lstate*	  black_hole;
+
+	/*
+	 * Device instance currently on the bus awaiting a continue TIO
+	 * for a command that was not given the disconnect priveledge.
+	 */
+	struct tmode_lstate*	  pending_device;
+
+	/*
+	 * Card characteristics
+	 */
+	ahc_chip		  chip;
+	ahc_feature		  features;
+	ahc_bug			  bugs;
+	ahc_flag		  flags;
+
+	/* Values to store in the SEQCTL register for pause and unpause */
+	uint8_t			  unpause;
+	uint8_t			  pause;
+
+	/* Command Queues */
+	uint8_t			  qoutfifonext;
+	uint8_t			  qinfifonext;
+	uint8_t			 *qoutfifo;
+	uint8_t			 *qinfifo;
+
+	/* Critical Section Data */
+	struct cs		 *critical_sections;
+	u_int			  num_critical_sections;
+
+	/* Links for chaining softcs */
+	TAILQ_ENTRY(ahc_softc)	  links;
+
+	/* Channel Names ('A', 'B', etc.) */
+	char			  channel;
+	char			  channel_b;
+
+	/* Initiator Bus ID */
+	uint8_t			  our_id;
+	uint8_t			  our_id_b;
+
+	/* Targets that need negotiation messages */
+	uint16_t		  targ_msg_req;
+
+	/*
+	 * PCI error detection.
+	 */
+	int			  unsolicited_ints;
+
+	/*
+	 * Target incoming command FIFO.
+	 */
+	struct target_cmd	 *targetcmds;
+	uint8_t			  tqinfifonext;
+
+	/*
+	 * Incoming and outgoing message handling.
+	 */
+	uint8_t			  send_msg_perror;
+	ahc_msg_type		  msg_type;
+	uint8_t			  msgout_buf[12];/* Message we are sending */
+	uint8_t			  msgin_buf[12];/* Message we are receiving */
+	u_int			  msgout_len;	/* Length of message to send */
+	u_int			  msgout_index;	/* Current index in msgout */
+	u_int			  msgin_index;	/* Current index in msgin */
+
+	/*
+	 * Mapping information for data structures shared
+	 * between the sequencer and kernel.
+	 */
+	bus_dma_tag_t		  parent_dmat;
+	bus_dma_tag_t		  shared_data_dmat;
+	bus_dmamap_t		  shared_data_dmamap;
+	bus_addr_t		  shared_data_busaddr;
+
+	/*
+	 * Bus address of the one byte buffer used to
+	 * work-around a DMA bug for chips <= aic7880
+	 * in target mode.
+	 */
+	bus_addr_t		  dma_bug_buf;
+
+	/* Information saved through suspend/resume cycles */
+	struct ahc_suspend_state  suspend_state;
+
+	/* Number of enabled target mode device on this card */
+	u_int			  enabled_luns;
+
+	/* Initialization level of this data structure */
+	u_int			  init_level;
+
+	/* PCI cacheline size. */
+	u_int			  pci_cachesize;
+
+	/* Per-Unit descriptive information */
+	const char		 *description;
+	char			 *name;
+	int			  unit;
+
+	/* Selection Timer settings */
+	int			  seltime;
+	int			  seltime_b;
+
+	uint16_t	 	  user_discenable;/* Disconnection allowed  */
+	uint16_t		  user_tagenable;/* Tagged Queuing allowed */
+};
+
+TAILQ_HEAD(ahc_softc_tailq, ahc_softc);
+extern struct ahc_softc_tailq ahc_tailq;
+
+/************************ Active Device Information ***************************/
+typedef enum {
+	ROLE_UNKNOWN,
+	ROLE_INITIATOR,
+	ROLE_TARGET
+} role_t;
+
+struct ahc_devinfo {
+	int	 our_scsiid;
+	int	 target_offset;
+	uint16_t target_mask;
+	u_int	 target;
+	u_int	 lun;
+	char	 channel;
+	role_t	 role;		/*
+				 * Only guaranteed to be correct if not
+				 * in the busfree state.
+				 */
+};
+
+/****************************** PCI Structures ********************************/
+typedef int (ahc_device_setup_t)(ahc_dev_softc_t,
+				 struct ahc_probe_config *);
+
+struct ahc_pci_identity {
+	uint64_t		 full_id;
+	uint64_t		 id_mask;
+	char			*name;
+	ahc_device_setup_t	*setup;
+};
+extern struct ahc_pci_identity ahc_pci_ident_table [];
+extern const u_int ahc_num_pci_devs;
+
+/***************************** VL/EISA Declarations ***************************/
+struct aic7770_identity {
+	uint32_t		 full_id;
+	uint32_t		 id_mask;
+	char			*name;
+	ahc_device_setup_t	*setup;
+};
+extern struct aic7770_identity aic7770_ident_table [];
+extern const int ahc_num_aic7770_devs;
+
+#define AHC_EISA_SLOT_OFFSET	0xc00
+#define AHC_EISA_IOSIZE		0x100
+
+/*************************** Function Declarations ****************************/
+/******************************************************************************/
+u_int			ahc_index_busy_tcl(struct ahc_softc *ahc, u_int tcl);
+void			ahc_unbusy_tcl(struct ahc_softc *ahc, u_int tcl);
+void			ahc_busy_tcl(struct ahc_softc *ahc,
+				     u_int tcl, u_int busyid);
+
+/***************************** PCI Front End *********************************/
+struct ahc_pci_identity	*ahc_find_pci_device(ahc_dev_softc_t);
+int			 ahc_pci_config(struct ahc_softc *,
+					struct ahc_pci_identity *);
+
+/*************************** EISA/VL Front End ********************************/
+struct aic7770_identity *aic7770_find_device(uint32_t);
+int			 aic7770_config(struct ahc_softc *ahc,
+					struct aic7770_identity *);
+
+/************************** SCB and SCB queue management **********************/
+int		ahc_probe_scbs(struct ahc_softc *);
+void		ahc_run_untagged_queues(struct ahc_softc *ahc);
+void		ahc_run_untagged_queue(struct ahc_softc *ahc,
+				       struct scb_tailq *queue);
+void		ahc_qinfifo_requeue_tail(struct ahc_softc *ahc,
+					 struct scb *scb);
+int		ahc_match_scb(struct ahc_softc *ahc, struct scb *scb,
+			      int target, char channel, int lun,
+			      u_int tag, role_t role);
+
+/****************************** Initialization ********************************/
+void			 ahc_init_probe_config(struct ahc_probe_config *);
+struct ahc_softc	*ahc_alloc(void *platform_arg, char *name);
+int			 ahc_softc_init(struct ahc_softc *,
+					struct ahc_probe_config*);
+void			 ahc_controller_info(struct ahc_softc *ahc, char *buf);
+int			 ahc_init(struct ahc_softc *ahc);
+void			 ahc_pause_and_flushwork(struct ahc_softc *ahc);
+int			 ahc_suspend(struct ahc_softc *ahc); 
+int			 ahc_resume(struct ahc_softc *ahc);
+void			 ahc_softc_insert(struct ahc_softc *);
+void			 ahc_set_unit(struct ahc_softc *, int);
+void			 ahc_set_name(struct ahc_softc *, char *);
+void			 ahc_alloc_scbs(struct ahc_softc *ahc);
+void			 ahc_free(struct ahc_softc *ahc);
+int			 ahc_reset(struct ahc_softc *ahc);
+void			 ahc_shutdown(void *arg);
+
+/*************************** Interrupt Services *******************************/
+void			ahc_pci_intr(struct ahc_softc *ahc);
+void			ahc_clear_intstat(struct ahc_softc *ahc);
+void			ahc_run_qoutfifo(struct ahc_softc *ahc);
+#ifdef AHC_TARGET_MODE
+void			ahc_run_tqinfifo(struct ahc_softc *ahc, int paused);
+#endif
+void			ahc_handle_brkadrint(struct ahc_softc *ahc);
+void			ahc_handle_seqint(struct ahc_softc *ahc, u_int intstat);
+void			ahc_handle_scsiint(struct ahc_softc *ahc,
+					   u_int intstat);
+void			ahc_clear_critical_section(struct ahc_softc *ahc);
+
+/***************************** Error Recovery *********************************/
+typedef enum {
+	SEARCH_COMPLETE,
+	SEARCH_COUNT,
+	SEARCH_REMOVE
+} ahc_search_action;
+int			ahc_search_qinfifo(struct ahc_softc *ahc, int target,
+					   char channel, int lun, u_int tag,
+					   role_t role, uint32_t status,
+					   ahc_search_action action);
+int			ahc_search_disc_list(struct ahc_softc *ahc, int target,
+					     char channel, int lun, u_int tag,
+					     int stop_on_first, int remove,
+					     int save_state);
+void			ahc_freeze_devq(struct ahc_softc *ahc, struct scb *scb);
+int			ahc_reset_channel(struct ahc_softc *ahc, char channel,
+					  int initiate_reset);
+void			restart_sequencer(struct ahc_softc *ahc);
+/*************************** Utility Functions ********************************/
+void			ahc_compile_devinfo(struct ahc_devinfo *devinfo,
+					    u_int our_id, u_int target,
+					    u_int lun, char channel,
+					    role_t role);
+/************************** Transfer Negotiation ******************************/
+struct ahc_syncrate*	ahc_find_syncrate(struct ahc_softc *ahc, u_int *period,
+					  u_int *ppr_options, u_int maxsync);
+u_int			ahc_find_period(struct ahc_softc *ahc,
+					u_int scsirate, u_int maxsync);
+void			ahc_validate_offset(struct ahc_softc *ahc,
+					    struct ahc_initiator_tinfo *tinfo,
+					    struct ahc_syncrate *syncrate,
+					    u_int *offset, int wide,
+					    role_t role);
+void			ahc_validate_width(struct ahc_softc *ahc,
+					   struct ahc_initiator_tinfo *tinfo,
+					   u_int *bus_width,
+					   role_t role);
+void			ahc_update_target_msg_request(struct ahc_softc *ahc,
+					struct ahc_devinfo *dinfo,
+					struct ahc_initiator_tinfo *tinfo,
+					int force, int paused);
+void			ahc_set_width(struct ahc_softc *ahc,
+				      struct ahc_devinfo *devinfo,
+				      u_int width, u_int type, int paused);
+void			ahc_set_syncrate(struct ahc_softc *ahc,
+					 struct ahc_devinfo *devinfo,
+					 struct ahc_syncrate *syncrate,
+					 u_int period, u_int offset,
+					 u_int ppr_options,
+					 u_int type, int paused);
+void			ahc_set_tags(struct ahc_softc *ahc,
+				     struct ahc_devinfo *devinfo, int enable);
+
+/**************************** Target Mode *************************************/
+#ifdef AHC_TARGET_MODE
+void		ahc_send_lstate_events(struct ahc_softc *,
+				       struct tmode_lstate *);
+void		ahc_handle_en_lun(struct ahc_softc *ahc,
+				  struct cam_sim *sim, union ccb *ccb);
+cam_status	ahc_find_tmode_devs(struct ahc_softc *ahc,
+				    struct cam_sim *sim, union ccb *ccb,
+				    struct tmode_tstate **tstate,
+				    struct tmode_lstate **lstate,
+				    int notfound_failure);
+void		ahc_setup_target_msgin(struct ahc_softc *ahc,
+				       struct ahc_devinfo *devinfo);
+#ifndef AHC_TMODE_ENABLE
+#define AHC_TMODE_ENABLE 0
+#endif
+#endif
+/******************************* Debug ***************************************/
+void			ahc_print_scb(struct scb *scb);
+void			ahc_dump_card_state(struct ahc_softc *ahc);
+#endif /* _AIC7XXX_H_ */
diff -urN linux/drivers/scsi/aic7xxx/aic7xxx.reg /tmp/linux/drivers/scsi/aic7xxx/aic7xxx.reg
--- linux/drivers/scsi/aic7xxx/aic7xxx.reg	Wed May  3 18:16:44 2000
+++ /tmp/linux/drivers/scsi/aic7xxx/aic7xxx.reg	Fri Feb  2 19:06:14 2001
@@ -1,7 +1,7 @@
 /*
  * Aic7xxx register and scratch ram definitions.
  *
- * Copyright (c) 1994-1998 Justin Gibbs.
+ * Copyright (c) 1994-2001 Justin Gibbs.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -9,16 +9,12 @@
  * are met:
  * 1. Redistributions of source code must retain the above copyright
  *    notice, this list of conditions, and the following disclaimer,
- *    without modification, immediately at the beginning of the file.
+ *    without modification.
  * 2. The name of the author may not be used to endorse or promote products
  *    derived from this software without specific prior written permission.
  *
- * Where this Software is combined with software released under the terms of 
- * the GNU Public License ("GPL") and the terms of the GPL would require the 
- * combined work to also be released under the terms of the GPL, the terms
- * and conditions of this License will apply in addition to those of the
- * GPL with the exception of any terms or conditions of this License that
- * conflict with, or are expressly prohibited by, the GPL.
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
  *
  * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
  * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
@@ -32,7 +28,9 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- *	$Id: aic7xxx.reg,v 1.4 1997/06/27 19:38:39 gibbs Exp $
+ * $Id: //depot/src/aic7xxx/aic7xxx.reg#11 $
+ *
+ * $FreeBSD: src/sys/dev/aic7xxx/aic7xxx.reg,v 1.31 2000/11/10 20:13:40 gibbs Exp $
  */
 
 /*
@@ -114,6 +112,8 @@
 	mask	PHASE_MASK	CDI|IOI|MSGI
 	mask	P_DATAOUT	0x00
 	mask	P_DATAIN	IOI
+	mask	P_DATAOUT_DT	P_DATAOUT|MSGI
+	mask	P_DATAIN_DT	P_DATAIN|MSGI
 	mask	P_COMMAND	CDI
 	mask	P_MESGOUT	CDI|MSGI
 	mask	P_STATUS	CDI|IOI
@@ -160,8 +160,10 @@
 	address			0x004
 	access_mode RW
 	bit	WIDEXFER	0x80		/* Wide transfer control */
+	bit	ENABLE_CRC	0x40		/* CRC for D-Phases */
+	bit	SINGLE_EDGE	0x10		/* Disable DT Transfers */
 	mask	SXFR		0x70		/* Sync transfer rate */
-	mask	SXFR_ULTRA2	0x7f		/* Sync transfer rate */
+	mask	SXFR_ULTRA2	0x0f		/* Sync transfer rate */
 	mask	SOFS		0x0f		/* Sync offset */
 }
 
@@ -174,6 +176,8 @@
 	address			0x005
 	access_mode RW
 	mask	TID		0xf0		/* Target ID mask */
+	mask	TWIN_TID	0x70
+	bit	TWIN_CHNLB	0x80
 	mask	OID		0x0f		/* Our ID mask */
 	/*
 	 * SCSI Maximum Offset (p. 4-61 aic7890/91 Data Book)
@@ -213,24 +217,27 @@
 	access_mode RW
 }
 
-/*
- * Option Mode Register (Alternate Mode) (p. 5-198)
- * This register is used to set certain options on Ultra3 based chips.
- * The chip must be in alternate mode (bit ALT_MODE in SFUNCT must be set)
- */
+/* ALT_MODE register on Ultra160 chips */
 register OPTIONMODE {
 	address			0x008
 	access_mode RW
-	bit	AUTORATEEN	0x80
-	bit	AUTOACKEN	0x40
-	bit	ATNMGMNTEN	0x20
-	bit	BUSFREEREV	0x10
-	bit	EXPPHASEDIS	0x08
-	bit	SCSIDATL_IMGEN	0x04
-	bit	AUTO_MSGOUT_DE	0x02
+	bit	AUTORATEEN		0x80
+	bit	AUTOACKEN		0x40
+	bit	ATNMGMNTEN		0x20
+	bit	BUSFREEREV		0x10
+	bit	EXPPHASEDIS		0x08
+	bit	SCSIDATL_IMGEN		0x04
+	bit	AUTO_MSGOUT_DE		0x02
 	bit	DIS_MSGIN_DUALEDGE	0x01
+	mask	OPTIONMODE_DEFAULTS	AUTO_MSGOUT_DE|DIS_MSGIN_DUALEDGE
 }
 
+/* ALT_MODE register on Ultra160 chips */
+register TARGCRCCNT {
+	address			0x00a
+	size	2
+	access_mode RW
+}
 
 /*
  * Clear SCSI Interrupt 0 (p. 3-20)
@@ -243,6 +250,7 @@
 	bit	CLRSELDI	0x20
 	bit	CLRSELINGO	0x10
 	bit	CLRSWRAP	0x08
+	bit	CLRIOERR	0x08	/* Ultra2 Only */
 	bit	CLRSPIORDY	0x02
 }
 
@@ -304,13 +312,12 @@
 	address			0x00d
 	access_mode RO
 	bit	OVERRUN		0x80
-	bit	SHVALID		0x40
-	bit	WIDE_RES	0x20
+	bit	SHVALID		0x40	/* Shaddow Layer non-zero */
 	bit	EXP_ACTIVE	0x10	/* SCSI Expander Active */
-	bit	CRCVALERR	0x08	/* CRC Value Error */
-	bit	CRCENDERR	0x04	/* CRC End Error */
-	bit	CRCREQERR	0x02	/* CRC REQ Error */
-	bit	DUAL_EDGE_ERROR	0x01	/* Invalid pins for Dual Edge phase */
+	bit	CRCVALERR	0x08	/* CRC doesn't match (U3 only) */
+	bit	CRCENDERR	0x04	/* No terminal CRC packet (U3 only) */
+	bit	CRCREQERR	0x02	/* Illegal CRC packet req (U3 only) */
+	bit	DUAL_EDGE_ERR	0x01	/* Incorrect data phase (U3 only) */
 	mask	SFCNT		0x1f
 }
 
@@ -376,12 +383,12 @@
  */
 register SCSIBUSL {
 	address			0x012
-	access_mode RO
+	access_mode RW
 }
 
 register SCSIBUSH {
 	address			0x013
-	access_mode RO
+	access_mode RW
 }
 
 /*
@@ -410,6 +417,7 @@
 	bit	STAGE3		0x04
 	bit	STAGE2		0x02
 	bit	STAGE1		0x01
+	alias	TARGIDIN
 }
 
 /*
@@ -424,6 +432,25 @@
 	bit	ONEBIT		0x08
 }
 
+register SCAMCTL {
+	address			0x01a
+	access_mode RW
+	bit	ENSCAMSELO	0x80
+	bit	CLRSCAMSELID	0x40
+	bit	ALTSTIM		0x20
+	bit	DFLTTID		0x10
+	mask	SCAMLVL		0x03
+}
+
+/*
+ * Target Mode Selecting in ID bitmask (aic7890/91/96/97)
+ */
+register TARGID {
+	address			0x01b
+	size			2
+	access_mode RW
+}
+
 /*
  * Serial Port I/O Cabability register (p. 4-95 aic7860 Data Book)
  * Indicates if external logic has been attached to the chip to
@@ -445,6 +472,59 @@
 	bit	SSPIOCPS	0x01	/* Termination and cable detection */
 }
 
+register BRDCTL	{
+	address			0x01d
+	bit	BRDDAT7		0x80
+	bit	BRDDAT6		0x40
+	bit	BRDDAT5		0x20
+	bit	BRDSTB		0x10
+	bit	BRDCS		0x08
+	bit	BRDRW		0x04
+	bit	BRDCTL1		0x02
+	bit	BRDCTL0		0x01
+	/* 7890 Definitions */
+	bit	BRDDAT4		0x10
+	bit	BRDDAT3		0x08
+	bit	BRDDAT2		0x04
+	bit	BRDRW_ULTRA2	0x02
+	bit	BRDSTB_ULTRA2	0x01
+}
+
+/*
+ * Serial EEPROM Control (p. 4-92 in 7870 Databook)
+ * Controls the reading and writing of an external serial 1-bit
+ * EEPROM Device.  In order to access the serial EEPROM, you must
+ * first set the SEEMS bit that generates a request to the memory
+ * port for access to the serial EEPROM device.  When the memory
+ * port is not busy servicing another request, it reconfigures
+ * to allow access to the serial EEPROM.  When this happens, SEERDY
+ * gets set high to verify that the memory port access has been
+ * granted.  
+ *
+ * After successful arbitration for the memory port, the SEECS bit of 
+ * the SEECTL register is connected to the chip select.  The SEECK, 
+ * SEEDO, and SEEDI are connected to the clock, data out, and data in 
+ * lines respectively.  The SEERDY bit of SEECTL is useful in that it 
+ * gives us an 800 nsec timer.  After a write to the SEECTL register, 
+ * the SEERDY goes high 800 nsec later.  The one exception to this is 
+ * when we first request access to the memory port.  The SEERDY goes 
+ * high to signify that access has been granted and, for this case, has 
+ * no implied timing.
+ *
+ * See 93cx6.c for detailed information on the protocol necessary to 
+ * read the serial EEPROM.
+ */
+register SEECTL {
+	address			0x01e
+	bit	EXTARBACK	0x80
+	bit	EXTARBREQ	0x40
+	bit	SEEMS		0x20
+	bit	SEERDY		0x10
+	bit	SEECS		0x08
+	bit	SEECK		0x04
+	bit	SEEDO		0x02
+	bit	SEEDI		0x01
+}
 /*
  * SCSI Block Control (p. 3-32)
  * Controls Bus type and channel selection.  In a twin channel configuration
@@ -585,30 +665,22 @@
 	bit	ENABLE		0x01
 }
 
-register DSCOMMAND0 {
-	address			0x084
-	access_mode RW
-	bit	CACHETHEN	0x80
-	bit	DPARCKEN	0x40
-	bit	MPARCKEN	0x20
-	bit	EXTREQLCK	0x10
-	bit	INTSCBRAMSEL	0x08
-	bit	RAMPS		0x04
-	bit	USCBSIZE32	0x02
-	bit	CIOPARCKEN	0x01
-}
-
 /*
  * On the aic78X0 chips, Board Control is replaced by the DSCommand
  * register (p. 4-64)
  */
-register DSCOMMAND {
+register DSCOMMAND0 {
 	address			0x084
 	access_mode RW
 	bit	CACHETHEN	0x80	/* Cache Threshold enable */
 	bit	DPARCKEN	0x40	/* Data Parity Check Enable */
 	bit	MPARCKEN	0x20	/* Memory Parity Check Enable */
 	bit	EXTREQLCK	0x10	/* External Request Lock */
+	/* aic7890/91/96/97 only */
+	bit	INTSCBRAMSEL	0x08	/* Internal SCB RAM Select */
+	bit	RAMPS		0x04	/* External SCB RAM Present */
+	bit	USCBSIZE32	0x02	/* Use 32byte SCB Page Size */
+	bit	CIOPARCKEN	0x01	/* Internal bus parity error enable */
 }
 
 /*
@@ -622,7 +694,7 @@
 }
 
 /*
- * Bus Speed (p. 3-45)
+ * Bus Speed (p. 3-45) aic7770 only
  */
 register BUSSPD {
 	address			0x086
@@ -631,8 +703,26 @@
 	mask	STBOFF		0x38
 	mask	STBON		0x07
 	mask	DFTHRSH_100	0xc0
+	mask	DFTHRSH_75	0x80
 }
 
+/* aic7850/55/60/70/80/95 only */
+register DSPCISTATUS {
+	address			0x086
+	mask	DFTHRSH_100	0xc0
+}
+
+/* aic7890/91/96/97 only */
+register HS_MAILBOX {
+	address			0x086
+	mask	HOST_MAILBOX	0xF0
+	mask	SEQ_MAILBOX	0x0F
+	mask	HOST_TQINPOS	0x80	/* Boundary at either 0 or 128 */
+}
+
+const	HOST_MAILBOX_SHIFT	4
+const	SEQ_MAILBOX_SHIFT	0
+
 /*
  * Host Control (p. 3-47) R/W
  * Overall host control of the device.
@@ -668,7 +758,7 @@
 
 /*
  * SCB Pointer (p. 3-49)
- * Gate one of the four SCBs into the SCBARRAY window.
+ * Gate one of the SCBs into the SCBARRAY window.
  */
 register SCBPTR {
 	address			0x090
@@ -690,31 +780,48 @@
 	mask	SEND_REJECT	0x10|SEQINT	/* sending a message reject */
 	mask	NO_IDENT	0x20|SEQINT	/* no IDENTIFY after reconnect*/
 	mask	NO_MATCH	0x30|SEQINT	/* no cmd match for reconnect */
-	mask	EXTENDED_MSG	0x40|SEQINT	/* Extended message received */
-	mask	WIDE_RESIDUE	0x50|SEQINT	/* need kernel to back up */
-						/* the SG array for us */
-	mask	REJECT_MSG	0x60|SEQINT	/* Reject message received */
-	mask	BAD_STATUS	0x70|SEQINT	/* Bad status from target */
-	mask	RESIDUAL	0x80|SEQINT	/* Residual byte count != 0 */
-	mask	AWAITING_MSG	0xa0|SEQINT	/*
-						 * Kernel requested to specify
-						 * a message to this target
-						 * (command was null), so tell
-						 * it that it can fill the
-						 * message buffer.
+	mask	IGN_WIDE_RES	0x40|SEQINT	/* Complex IGN Wide Res Msg */
+	mask	RESIDUAL	0x50|SEQINT	/* Residual byte count != 0 */
+	mask	HOST_MSG_LOOP	0x60|SEQINT	/*
+						 * The bus is ready for the
+						 * host to perform another
+						 * message transaction.  This
+						 * mechanism is used for things
+						 * like sync/wide negotiation
+						 * that require a kernel based
+						 * message state engine.
 						 */
-	mask	TRACEPOINT	0xb0|SEQINT
-	mask	TRACEPOINT2	0xc0|SEQINT
-	mask	MSGIN_PHASEMIS	0xd0|SEQINT	/*
-						 * Target changed phase on us
-						 * when we were expecting
-						 * another msgin byte.
+	mask	BAD_STATUS	0x70|SEQINT	/* Bad status from target */
+	mask	PERR_DETECTED	0x80|SEQINT	/*
+						 * Either the phase_lock
+						 * or inb_next routine has
+						 * noticed a parity error.
 						 */
-	mask	DATA_OVERRUN	0xe0|SEQINT	/*
+	mask	DATA_OVERRUN	0x90|SEQINT	/*
 						 * Target attempted to write
 						 * beyond the bounds of its
 						 * command.
 						 */
+	mask	MKMSG_FAILED	0xa0|SEQINT	/*
+						 * Target completed command
+						 * without honoring our ATN
+						 * request to issue a message. 
+						 */
+	mask	MISSED_BUSFREE	0xb0|SEQINT	/*
+						 * The sequencer never saw
+						 * the bus go free after
+						 * either a command complete
+						 * or disconnect message.
+						 */
+	mask	SCB_MISMATCH	0xc0|SEQINT	/*
+						 * Downloaded SCB's tag does
+						 * not match the entry we
+						 * intended to download.
+						 */
+	mask	NO_FREE_SCB	0xd0|SEQINT	/*
+						 * get_free_or_disc_scb failed.
+						 */
+	mask	OUT_OF_RANGE	0xe0|SEQINT
 
 	mask	SEQINT_MASK	0xf0|SEQINT	/* SEQINT Status Codes */
 	mask	INT_PEND  (BRKADRINT|SEQINT|SCSIINT|CMDCMPLT)
@@ -735,7 +842,6 @@
 	bit	SQPARERR	0x08
 	bit	ILLOPCODE	0x04
 	bit	ILLSADDR	0x02
-	bit	DSCTMOUT	0x02	/* Ultra3 only */
 	bit	ILLHADDR	0x01
 }
 
@@ -779,6 +885,16 @@
 	bit	FIFOEMP		0x01
 }
 
+register DFWADDR {
+	address			0x95
+	access_mode RW
+}
+
+register DFRADDR {
+	address			0x97
+	access_mode RW
+}
+
 register DFDAT {
 	address			0x099
 	access_mode RW
@@ -815,17 +931,6 @@
 }
 
 /*
- * SCSIDATL IMAGE Register (p. 5-104)
- * Write to this register also go to SCSIDATL but this register will preserve
- * the data for later reading as long as the SCSIDATL_IMGEN bit in the
- * OPTIONMODE register is set.
- */
-register SCSIDATL_IMG {
-	address			0x09c
-	access_mode RW
-}
-
-/*
  * Queue Out FIFO (p. 3-61)
  * Queue of SCBs that have completed and await the host
  */
@@ -834,21 +939,18 @@
 	access_mode WO
 }
 
-/*
- * CRC Control 1 Register (p. 5-105)
- * Control bits for the Ultra 160/m CRC facilities
- */
 register CRCCONTROL1 {
 	address			0x09d
 	access_mode RW
-	bit	CRCONSEEN	0x80 /* CRC ON Single Edge ENable */
-	bit	CRCVALCHKEN	0x40 /* CRC Value Check Enable */
-	bit	CRCENDCHKEN	0x20 /* CRC End Check Enable */
-	bit	CRCREQCHKEN	0x10
-	bit	TARGCRCENDEN	0x08 /* Enable End CRC transfer when target */
-	bit	TARGCRCCNTEN	0x04 /* Enable CRC transfer when target */
+	bit	CRCONSEEN		0x80
+	bit	CRCVALCHKEN		0x40
+	bit	CRCENDCHKEN		0x20
+	bit	CRCREQCHKEN		0x10
+	bit	TARGCRCENDEN		0x08
+	bit	TARGCRCCNTEN		0x04
 }
 
+
 /*
  * Queue Out Count (p. 3-61)
  * Number of queued SCBs in the Out FIFO
@@ -858,19 +960,15 @@
 	access_mode RO
 }
 
-/*
- * SCSI Phase Register (p. 5-106)
- * Current bus phase
- */
 register SCSIPHASE {
 	address			0x09e
 	access_mode RO
-	bit	SP_STATUS		0x20
-	bit	SP_COMMAND		0x10
-	bit	SP_MSG_IN		0x08
-	bit	SP_MSG_OUT		0x04
-	bit	SP_DATA_IN		0x02
-	bit	SP_DATA_OUT	0x01
+	bit	STATUS_PHASE	0x20
+	bit	COMMAND_PHASE	0x10
+	bit	MSG_IN_PHASE	0x08
+	bit	MSG_OUT_PHASE	0x04
+	bit	DATA_IN_PHASE	0x02
+	bit	DATA_OUT_PHASE	0x01
 }
 
 /*
@@ -887,33 +985,19 @@
  */
 scb {
 	address			0x0a0
-	SCB_CONTROL {
-		size	1
-		bit	MK_MESSAGE      0x80
-		bit	DISCENB         0x40
-		bit	TAG_ENB		0x20
-		bit	DISCONNECTED	0x04
-		mask	SCB_TAG_TYPE	0x03
-	}
-	SCB_TCL {
-		size	1
-		bit	SELBUSB		0x08
-		mask	TID		0xf0
-		mask	LID		0x07
-	}
-	SCB_TARGET_STATUS {
-		size	1
-	}
-	SCB_SGCOUNT {
-		size	1
+	SCB_CDB_PTR {
+		size	4
+		alias	SCB_RESIDUAL_DATACNT
+		alias	SCB_CDB_STORE
+		alias	SCB_TARGET_INFO
 	}
-	SCB_SGPTR {
+	SCB_RESIDUAL_SGPTR {
 		size	4
 	}
-	SCB_RESID_SGCNT {
+	SCB_SCSI_STATUS {
 		size	1
 	}
-	SCB_RESID_DCNT	{
+	SCB_CDB_STORE_PAD {
 		size	3
 	}
 	SCB_DATAPTR {
@@ -921,31 +1005,67 @@
 	}
 	SCB_DATACNT {
 		/*
-		 * Really only 3 bytes, but padded to make
-		 * the kernel's job easier.
+		 * The last byte is really the high address bits for
+		 * the data address.
 		 */
 		size	4
+		bit	SG_LAST_SEG		0x80	/* In the fourth byte */
+		mask	SG_HIGH_ADDR_BITS	0x7F	/* In the fourth byte */
 	}
-	SCB_CMDPTR {
+	SCB_SGPTR {
 		size	4
+		bit	SG_RESID_VALID	0x04	/* In the first byte */
+		bit	SG_FULL_RESID	0x02	/* In the first byte */
+		bit	SG_LIST_NULL	0x01	/* In the first byte */
 	}
-	SCB_CMDLEN {
+	SCB_CONTROL {
+		size	1
+		bit	TARGET_SCB			0x80
+		bit	DISCENB				0x40
+		bit	TAG_ENB				0x20
+		bit	MK_MESSAGE			0x10
+		bit	ULTRAENB			0x08
+		bit	DISCONNECTED			0x04
+		mask	SCB_TAG_TYPE			0x03
+	}
+	SCB_SCSIID {
+		size	1
+		bit	TWIN_CHNLB			0x80
+		mask	TWIN_TID			0x70
+		mask	TID				0xf0
+		mask	OID				0x0f
+	}
+	SCB_LUN {
+		mask	LID				0xff
 		size	1
 	}
 	SCB_TAG {
 		size	1
 	}
-	SCB_NEXT {
+	SCB_CDB_LEN {
 		size	1
 	}
-	SCB_PREV {
+	SCB_SCSIRATE {
 		size	1
 	}
-	SCB_BUSYTARGETS {
-		size	4
+	SCB_SCSIOFFSET {
+		size	1
+	}
+	SCB_NEXT {
+		size	1
+	}
+	SCB_64_SPARE {
+		size	16
+	}
+	SCB_64_BTT {
+		size	16
 	}
 }
 
+const	SCB_UPLOAD_SIZE		32
+const	SCB_DOWNLOAD_SIZE	32
+const	SCB_DOWNLOAD_SIZE_64	48
+
 const	SG_SIZEOF	0x08		/* sizeof(struct ahc_dma) */
 
 /* --------------------- AHA-2840-only definitions -------------------- */
@@ -969,11 +1089,6 @@
 
 /* --------------------- AIC-7870-only definitions -------------------- */
 
-register DSPCISTATUS {
-	address			0x086
-	mask	DFTHRSH_100	0xc0
-}
-
 register CCHADDR {
 	address			0x0E0
 	size 8
@@ -995,7 +1110,7 @@
 	address			0x0EB
 	bit	CCSGDONE	0x80
 	bit	CCSGEN		0x08
-	bit	FLAG		0x02
+	bit	SG_FETCH_NEEDED 0x02	/* Bit used for software state */
 	bit	CCSGRESET	0x01
 }
 
@@ -1021,6 +1136,14 @@
 	address			0xEC
 }
 
+/*
+ * SCB bank address (7895/7896/97 only)
+ */
+register SCBBADDR {
+	address			0x0F0
+	access_mode RW
+}
+
 register CCSCBPTR {
 	address			0x0F1
 }
@@ -1029,29 +1152,19 @@
 	address			0x0F4
 }
 
-register HESCB_QOFF {
-	address			0x0F5
-}
-
 register SNSCB_QOFF {
 	address			0x0F6
 }
 
-register SESCB_QOFF {
-	address			0x0F7
-}
-
 register SDSCB_QOFF {
 	address			0x0F8
 }
 
 register QOFF_CTLSTA {
 	address			0x0FA
-	bit	ESTABLISH_SCB_AVAIL	0x80
 	bit	SCB_AVAIL	0x40
 	bit	SNSCB_ROLLOVER	0x20
 	bit	SDSCB_ROLLOVER	0x10
-	bit	SESCB_ROLLOVER	0x08
 	mask	SCB_QSIZE	0x07
 	mask	SCB_QSIZE_256	0x06
 }
@@ -1078,66 +1191,22 @@
 	mask	WR_DFTHRSH_MAX	0x70
 }
 
-register SG_CACHEPTR {
-	access_mode RW
+register SG_CACHE_PRE {
+	access_mode WO
 	address			0x0fc
-	mask	SG_USER_DATA	0xfc
+	mask	SG_ADDR_MASK	0xf8
+	bit	ODD_SEG		0x04
 	bit	LAST_SEG	0x02
 	bit	LAST_SEG_DONE	0x01
 }
 
-register BRDCTL	{
-	address			0x01d
-	bit	BRDDAT7		0x80
-	bit	BRDDAT6		0x40
-	bit	BRDDAT5		0x20
-	bit	BRDSTB		0x10
-	bit	BRDCS		0x08
-	bit	BRDRW		0x04
-	bit	BRDCTL1		0x02
-	bit	BRDCTL0		0x01
-	/* 7890 Definitions */
-	bit	BRDDAT4		0x10
-	bit	BRDDAT3		0x08
-	bit	BRDDAT2		0x04
-	bit	BRDRW_ULTRA2	0x02
-	bit	BRDSTB_ULTRA2	0x01
-}
-
-/*
- * Serial EEPROM Control (p. 4-92 in 7870 Databook)
- * Controls the reading and writing of an external serial 1-bit
- * EEPROM Device.  In order to access the serial EEPROM, you must
- * first set the SEEMS bit that generates a request to the memory
- * port for access to the serial EEPROM device.  When the memory
- * port is not busy servicing another request, it reconfigures
- * to allow access to the serial EEPROM.  When this happens, SEERDY
- * gets set high to verify that the memory port access has been
- * granted.  
- *
- * After successful arbitration for the memory port, the SEECS bit of 
- * the SEECTL register is connected to the chip select.  The SEECK, 
- * SEEDO, and SEEDI are connected to the clock, data out, and data in 
- * lines respectively.  The SEERDY bit of SEECTL is useful in that it 
- * gives us an 800 nsec timer.  After a write to the SEECTL register, 
- * the SEERDY goes high 800 nsec later.  The one exception to this is 
- * when we first request access to the memory port.  The SEERDY goes 
- * high to signify that access has been granted and, for this case, has 
- * no implied timing.
- *
- * See 93cx6.c for detailed information on the protocol necessary to 
- * read the serial EEPROM.
- */
-register SEECTL {
-	address			0x01e
-	bit	EXTARBACK	0x80
-	bit	EXTARBREQ	0x40
-	bit	SEEMS		0x20
-	bit	SEERDY		0x10
-	bit	SEECS		0x08
-	bit	SEECK		0x04
-	bit	SEEDO		0x02
-	bit	SEEDI		0x01
+register SG_CACHE_SHADOW {
+	access_mode RO
+	address			0x0fc
+	mask	SG_ADDR_MASK	0xf8
+	bit	ODD_SEG		0x04
+	bit	LAST_SEG	0x02
+	bit	LAST_SEG_DONE	0x01
 }
 /* ---------------------- Scratch RAM Offsets ------------------------- */
 /* These offsets are either to values that are initialized by the board's
@@ -1160,21 +1229,46 @@
 	/*
 	 * 1 byte per target starting at this address for configuration values
 	 */
-	TARG_SCSIRATE {
+	BUSY_TARGETS {
+		alias		TARG_SCSIRATE
 		size		16
 	}
 	/*
-	 * Bit vector of targets that have ULTRA enabled.
+	 * Bit vector of targets that have ULTRA enabled as set by
+	 * the BIOS.  The Sequencer relies on a per-SCB field to
+	 * control whether to enable Ultra transfers or not.  During
+	 * initialization, we read this field and reuse it for 2
+	 * entries in the busy target table.
 	 */
 	ULTRA_ENB {
+		alias		CMDSIZE_TABLE
 		size		2
 	}
 	/*
-	 * Bit vector of targets that have disconnection disabled.
+	 * Bit vector of targets that have disconnection disabled as set by
+	 * the BIOS.  The Sequencer relies in a per-SCB field to control the
+	 * disconnect priveldge.  During initialization, we read this field
+	 * and reuse it for 2 entries in the busy target table.
 	 */
 	DISC_DSB {
 		size		2
 	}
+	CMDSIZE_TABLE_TAIL {
+		size		4
+	}
+	/*
+	 * Partial transfer past cacheline end to be
+	 * transferred using an extra S/G.
+	 */
+	MWI_RESIDUAL {
+		size		1
+	}
+	/*
+	 * SCBID of the next SCB to be started by the controller.
+	 */
+	NEXT_QUEUED_SCB {
+		size		1
+	}
 	/*
 	 * Single byte buffer used to designate the type or message
 	 * to send to a target.
@@ -1198,29 +1292,27 @@
 	}
 	SEQ_FLAGS {
 		size		1
-		bit	IDENTIFY_SEEN	0x80
-		bit	SCBPTR_VALID	0x20
-		bit	DPHASE		0x10
-		bit	AMTARGET	0x08
-		bit	WIDE_BUS	0x02
-		bit	TWIN_BUS	0x01
+		bit	IDENTIFY_SEEN		0x80
+		bit	TARGET_CMD_IS_TAGGED	0x40
+		bit	DPHASE			0x20
+		/* Target flags */
+		bit	TARG_CMD_PENDING	0x10
+		bit	CMDPHASE_PENDING	0x08
+		bit	DPHASE_PENDING		0x04
+		bit	SPHASE_PENDING		0x02
+		bit	NO_DISCONNECT		0x01
 	}
 	/*
 	 * Temporary storage for the
 	 * target/channel/lun of a
 	 * reconnecting target
 	 */
-	SAVED_TCL {
+	SAVED_SCSIID {
 		size		1
 	}
-	/* Working value of the number of SG segments left */
-	SG_COUNT {
+	SAVED_LUN {
 		size		1
 	}
-	/* Working value of SG pointer */
-	SG_NEXT	{
-		size		4
-	}
 	/*
 	 * The last bus phase as seen by the sequencer. 
 	 */
@@ -1267,17 +1359,11 @@
 		size		4
 	}
 	/*
-	 * Address of the 256 byte array storing the SCBID of outstanding
-	 * untagged SCBs indexed by TCL.
-	 */
-	SCBID_ADDR {
-		size		4
-	}
-	/*
-	 * Address of the array of command descriptors used to store
-	 * information about incoming selections.
+	 * Base address of our shared data with the kernel driver in host
+	 * memory.  This includes the qoutfifo and target mode
+	 * incoming command queue.
 	 */
-	TMODE_CMDADDR {
+	SHARED_DATA_ADDR {
 		size		4
 	}
 	KERNEL_QINPOS {
@@ -1290,18 +1376,25 @@
 		size		1
 	}
 	/*
-	 * Offset into the command descriptor array for the next
-	 * available desciptor to use.
+	 * Kernel and sequencer offsets into the queue of
+	 * incoming target mode command descriptors.  The
+	 * queue is full when the KERNEL_TQINPOS == TQINPOS.
 	 */
-	TMODE_CMDADDR_NEXT {
+	KERNEL_TQINPOS {
+		size		1
+	}
+	TQINPOS {                
 		size		1
 	}
 	ARG_1 {
 		size		1
-		mask	SEND_MSG	0x80
-		mask	SEND_SENSE	0x40
-		mask	SEND_REJ	0x20
-		mask	MSGOUT_PHASEMIS	0x10
+		mask	SEND_MSG		0x80
+		mask	SEND_SENSE		0x40
+		mask	SEND_REJ		0x20
+		mask	MSGOUT_PHASEMIS		0x10
+		mask	EXIT_MSG_LOOP		0x08
+		mask	CONT_MSG_LOOP		0x04
+		mask	CONT_TARG_SESSION	0x02
 		alias	RETURN_1
 	}
 	ARG_2 {
@@ -1317,13 +1410,43 @@
 	}
 
 	/*
-	 * Number of times we have filled the CCSGRAM with prefetched
-	 * SG elements.
+	 * Interrupt kernel for a message to this target on
+	 * the next transaction.  This is usually used for
+	 * negotiation requests.
+	 */
+	TARGET_MSG_REQUEST {
+		size		2
+	}
+
+	/*
+	 * Sequences the kernel driver has okayed for us.  This allows
+	 * the driver to do things like prevent initiator or target
+	 * operations.
+	 */
+	SCSISEQ_TEMPLATE {
+		size		1
+		bit	ENSELO		0x40
+		bit	ENSELI		0x20
+		bit	ENRSELI		0x10
+		bit	ENAUTOATNO	0x08
+		bit	ENAUTOATNI	0x04
+		bit	ENAUTOATNP	0x02
+	}
+
+	/*
+	 * Track whether the transfer byte count for
+	 * the current data phase is odd.
 	 */
-	PREFETCH_CNT {
+	DATA_COUNT_ODD {
 		size		1
 	}
 
+	/*
+	 * The initiator specified tag for this target mode transaction.
+	 */
+	INITIATOR_TAG {
+		size		1
+	}
 
 	/*
 	 * These are reserved registers in the card's scratch ram.  Some of
@@ -1335,9 +1458,16 @@
 		size		1
 		bit	TERM_ENB	0x80
 		bit	RESET_SCSI	0x40
+		bit	ENSPCHK		0x20
 		mask	HSCSIID		0x07	/* our SCSI ID */
 		mask	HWSCSIID	0x0f	/* our SCSI ID if Wide Bus */
 	}
+	INTDEF {
+		address		0x05c
+		size		1
+		bit	EDGE_TRIG	0x80
+		mask	VECTOR		0x0f
+	}
 	HOSTCONF {
 		address		0x05d
 		size		1
@@ -1358,16 +1488,13 @@
 	}
 }
 
+const TID_SHIFT		4
 const SCB_LIST_NULL	0xff
+const TARGET_CMD_CMPLT	0xfe
 
 const CCSGADDR_MAX	0x80
 const CCSGRAM_MAXSEGS	16
 
-/* Offsets into the SCBID array where different data is stored */
-const UNTAGGEDSCB_OFFSET	0
-const QOUTFIFO_OFFSET		1
-const QINFIFO_OFFSET		2
-
 /* WDTR Message values */
 const BUS_8_BIT			0x00
 const BUS_16_BIT		0x01
@@ -1381,16 +1508,23 @@
 
 /* Target mode command processing constants */
 const CMD_GROUP_CODE_SHIFT	0x05
-const CMD_GROUP0_BYTE_DELTA	-4
-const CMD_GROUP2_BYTE_DELTA	-6
-const CMD_GROUP4_BYTE_DELTA	4
-const CMD_GROUP5_BYTE_DELTA	11
 
-/*
- * Downloaded (kernel inserted) constants
- */
+const STATUS_BUSY		0x08
+const STATUS_QUEUE_FULL	0x28
+const SCB_TARGET_PHASES		0
+const SCB_TARGET_DATA_DIR	1
+const SCB_TARGET_STATUS		2
+const SCB_INITIATOR_TAG		3
+const TARGET_DATA_IN		1
 
 /*
- * Number of command descriptors in the command descriptor array.
+ * Downloaded (kernel inserted) constants
  */
-const TMODE_NUMCMDS	download
+/* Offsets into the SCBID array where different data is stored */
+const QOUTFIFO_OFFSET download
+const QINFIFO_OFFSET download
+const CACHESIZE_MASK download
+const INVERTED_CACHESIZE_MASK download
+const SG_PREFETCH_CNT download
+const SG_PREFETCH_ALIGN_MASK download
+const SG_PREFETCH_ADDR_MASK download
diff -urN linux/drivers/scsi/aic7xxx/aic7xxx.seq /tmp/linux/drivers/scsi/aic7xxx/aic7xxx.seq
--- linux/drivers/scsi/aic7xxx/aic7xxx.seq	Mon Sep  4 11:39:20 2000
+++ /tmp/linux/drivers/scsi/aic7xxx/aic7xxx.seq	Fri Feb  2 19:06:14 2001
@@ -1,7 +1,7 @@
 /*
  * Adaptec 274x/284x/294x device driver firmware for Linux and FreeBSD.
  *
- * Copyright (c) 1994-1999 Justin Gibbs.
+ * Copyright (c) 1994-2001 Justin Gibbs.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -9,16 +9,12 @@
  * are met:
  * 1. Redistributions of source code must retain the above copyright
  *    notice, this list of conditions, and the following disclaimer,
- *    without modification, immediately at the beginning of the file.
+ *    without modification.
  * 2. The name of the author may not be used to endorse or promote products
  *    derived from this software without specific prior written permission.
  *
- * Where this Software is combined with software released under the terms of 
- * the GNU Public License (GPL) and the terms of the GPL would require the 
- * combined work to also be released under the terms of the GPL, the terms
- * and conditions of this License will apply in addition to those of the
- * GPL with the exception of any terms or conditions of this License that
- * conflict with, or are expressly prohibited by, the GPL.
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
  *
  * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
  * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
@@ -32,7 +28,9 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- *	$Id: aic7xxx.seq,v 1.77 1998/06/28 02:58:57 gibbs Exp $
+ * $Id: //depot/src/aic7xxx/aic7xxx.seq#17 $
+ *
+ * $FreeBSD: src/sys/dev/aic7xxx/aic7xxx.seq,v 1.106 2000/11/12 05:19:46 gibbs Exp $
  */
 
 #include "aic7xxx.reg"
@@ -56,31 +54,23 @@
  * automatically consume the entries.
  */
 
-reset:
-	clr	SCSISIGO;		/* De-assert BSY */
-	and	SXFRCTL1, ~BITBUCKET;
-	/* Always allow reselection */
-	mvi	SCSISEQ, ENRSELI|ENAUTOATNP;
-
-	if ((p->features & AHC_CMD_CHAN) != 0) {
-		/* Ensure that no DMA operations are in progress */
-		clr	CCSGCTL;
-		clr	CCSCBCTL;
-	}
-
-	call	clear_target_state;
+bus_free_sel:
+	/*
+	 * Turn off the selection hardware.  We need to reset the
+	 * selection request in order to perform a new selection.
+	 */
+	and	SCSISEQ, TEMODE|ENSELI|ENRSELI|ENAUTOATNP, SCSISEQ;
+	and	SIMODE1, ~ENBUSFREE;
 poll_for_work:
+	call	clear_target_state;
 	and	SXFRCTL0, ~SPIOEN;
-	if ((p->features & AHC_QUEUE_REGS) == 0) {
-		mov	A, QINPOS;
+	if ((ahc->features & AHC_ULTRA2) != 0) {
+		clr	SCSIBUSL;
 	}
 poll_for_work_loop:
-	if ((p->features & AHC_QUEUE_REGS) == 0) {
-		and	SEQCTL, ~PAUSEDIS;
-	}
 	test	SSTAT0, SELDO|SELDI	jnz selection;
-	test	SCSISEQ, ENSELO	jnz poll_for_work;
-	if ((p->features & AHC_TWIN) != 0) {
+	test	SCSISEQ, ENSELO	jnz poll_for_work_loop;
+	if ((ahc->features & AHC_TWIN) != 0) {
 		/*
 		 * Twin channel devices cannot handle things like SELTO
 		 * interrupts on the "background" channel.  So, if we
@@ -89,167 +79,353 @@
 		 */
 		xor	SBLKCTL,SELBUSB;	/* Toggle to the other bus */
 		test	SSTAT0, SELDO|SELDI	jnz selection;
-		test	SCSISEQ, ENSELO	jnz poll_for_work;
 		xor	SBLKCTL,SELBUSB;	/* Toggle back */
 	}
 	cmp	WAITING_SCBH,SCB_LIST_NULL jne start_waiting;
 test_queue:
 	/* Has the driver posted any work for us? */
-	if ((p->features & AHC_QUEUE_REGS) != 0) {
+BEGIN_CRITICAL
+	if ((ahc->features & AHC_QUEUE_REGS) != 0) {
 		test	QOFF_CTLSTA, SCB_AVAIL jz poll_for_work_loop;
-		mov	NONE, SNSCB_QOFF;
-		inc	QINPOS;
 	} else {
-		or	SEQCTL, PAUSEDIS;
+		mov	A, QINPOS;
 		cmp	KERNEL_QINPOS, A je poll_for_work_loop;
-		inc	QINPOS;
-		and	SEQCTL, ~PAUSEDIS;
 	}
+	mov	ARG_1, NEXT_QUEUED_SCB;
+END_CRITICAL
 
-/*
- * We have at least one queued SCB now and we don't have any 
- * SCBs in the list of SCBs awaiting selection.  If we have
- * any SCBs available for use, pull the tag from the QINFIFO
- * and get to work on it.
- */
-	if ((p->flags & AHC_PAGESCBS) != 0) {
+	/*
+	 * We have at least one queued SCB now and we don't have any 
+	 * SCBs in the list of SCBs awaiting selection.  Allocate a
+	 * card SCB for the host's SCB and get to work on it.
+	 */
+	if ((ahc->flags & AHC_PAGESCBS) != 0) {
 		mov	ALLZEROS	call	get_free_or_disc_scb;
-	}
-
-dequeue_scb:
-	add	A, -1, QINPOS;
-	mvi	QINFIFO_OFFSET call fetch_byte;
-
-	if ((p->flags & AHC_PAGESCBS) == 0) {
+	} else {
 		/* In the non-paging case, the SCBID == hardware SCB index */
-		mov	SCBPTR, RETURN_2;
+		mov	SCBPTR, ARG_1;
 	}
 dma_queued_scb:
-/*
- * DMA the SCB from host ram into the current SCB location.
- */
+	/*
+	 * DMA the SCB from host ram into the current SCB location.
+	 */
 	mvi	DMAPARAMS, HDMAEN|DIRECTION|FIFORESET;
-	mov	RETURN_2	 call dma_scb;
-
-/*
- * Preset the residual fields in case we never go through a data phase.
- * This isn't done by the host so we can avoid a DMA to clear these
- * fields for the normal case of I/O that completes without underrun
- * or overrun conditions.
- */
-	if ((p->features & AHC_CMD_CHAN) != 0) {
-		bmov    SCB_RESID_DCNT, SCB_DATACNT, 3;
-	} else {
-		mov     SCB_RESID_DCNT[0],SCB_DATACNT[0];
-		mov     SCB_RESID_DCNT[1],SCB_DATACNT[1];
-		mov     SCB_RESID_DCNT[2],SCB_DATACNT[2];
-	}
-	mov     SCB_RESID_SGCNT, SCB_SGCOUNT;
-
-start_scb:
+	mov	ARG_1	call dma_scb;
 	/*
-	 * Place us on the waiting list in case our selection
-	 * doesn't win during bus arbitration.
+	 * Check one last time to see if this SCB was canceled
+	 * before we completed the DMA operation.  If it was,
+	 * the QINFIFO next pointer will not match our saved
+	 * value.
 	 */
+	mov	A, ARG_1;
+BEGIN_CRITICAL
+	cmp	NEXT_QUEUED_SCB, A jne abort_qinscb;
+	if ((ahc->flags & AHC_SEQUENCER_DEBUG) != 0) {
+		cmp	SCB_TAG, A je . + 2;
+		mvi	SCB_MISMATCH call set_seqint;
+	}
+	mov	NEXT_QUEUED_SCB, SCB_NEXT;
 	mov	SCB_NEXT,WAITING_SCBH;
 	mov	WAITING_SCBH, SCBPTR;
+	if ((ahc->features & AHC_QUEUE_REGS) != 0) {
+		mov	NONE, SNSCB_QOFF;
+	} else {
+		inc	QINPOS;
+	}
+END_CRITICAL
 start_waiting:
 	/*
-	 * Pull the first entry off of the waiting SCB list.
+	 * Start the first entry on the waiting SCB list.
 	 */
 	mov	SCBPTR, WAITING_SCBH;
 	call	start_selection;
-	jmp	poll_for_work;
+	jmp	poll_for_work_loop;
+
+abort_qinscb:
+	call	add_scb_to_free_list;
+	jmp	poll_for_work_loop;
 
 start_selection:
-	if ((p->features & AHC_TWIN) != 0) {
+	/*
+	 * If bus reset interrupts have been disabled (from a previous
+	 * reset), re-enable them now.  Resets are only of interest
+	 * when we have outstanding transactions, so we can safely
+	 * defer re-enabling the interrupt until, as an initiator,
+	 * we start sending out transactions again.
+	 */
+	test	SIMODE1, ENSCSIRST	jnz . + 3;
+	mvi	CLRSINT1, CLRSCSIRSTI;
+	or	SIMODE1, ENSCSIRST;
+	if ((ahc->features & AHC_TWIN) != 0) {
 		and	SINDEX,~SELBUSB,SBLKCTL;/* Clear channel select bit */
-		and	A,SELBUSB,SCB_TCL;	/* Get new channel bit */
-		or	SINDEX,A;
+		test	SCB_SCSIID, TWIN_CHNLB jz . + 2;
+		or	SINDEX, SELBUSB;
 		mov	SBLKCTL,SINDEX;		/* select channel */
 	}
 initialize_scsiid:
-	if ((p->features & AHC_ULTRA2) != 0) {
-		and	A, TID, SCB_TCL;	/* Get target ID */
-		and	SCSIID_ULTRA2, OID;	/* Clear old target */
-		or	SCSIID_ULTRA2, A;
-	} else {
-		and	A, TID, SCB_TCL;	/* Get target ID */
-		and	SCSIID, OID;		/* Clear old target */
-		or	SCSIID, A;
-	}
-	mov	SCSIDATL, ALLZEROS;		/* clear out the latched */
-						/* data register, this */
-						/* fixes a bug on some */
-						/* controllers where the */
-						/* last byte written to */
-						/* this register can leak */
-						/* onto the data bus at */
-						/* bad times, such as during */
-						/* selection timeouts */
-	mvi	SCSISEQ, ENSELO|ENAUTOATNO|ENRSELI|ENAUTOATNP ret;
+	if ((ahc->features & AHC_ULTRA2) != 0) {
+		mov	SCSIID_ULTRA2, SCB_SCSIID;
+	} else if ((ahc->features & AHC_TWIN) != 0) {
+		and	SCSIID, TWIN_TID|OID, SCB_SCSIID;
+	} else {
+		mov	SCSIID, SCB_SCSIID;
+	}
+	if ((ahc->flags & AHC_TARGETROLE) != 0) {
+		mov	SINDEX, SCSISEQ_TEMPLATE;
+		test	SCB_CONTROL, TARGET_SCB jz . + 2;
+		or	SINDEX, TEMODE;
+		mov	SCSISEQ, SINDEX ret;
+	} else {
+		mov	SCSISEQ, SCSISEQ_TEMPLATE ret;
+	}
 
 /*
- * Initialize Ultra mode setting and clear the SCSI channel.
+ * Initialize transfer settings and clear the SCSI channel.
  * SINDEX should contain any additional bit's the client wants
- * set in SXFRCTL0.
+ * set in SXFRCTL0.  We also assume that the current SCB is
+ * a valid SCB for the target we wish to talk to.
  */
 initialize_channel:
-	or	SXFRCTL0, CLRSTCNT|CLRCHN, SINDEX;
-	if ((p->features & AHC_ULTRA) != 0) {
-ultra:
-		mvi	SINDEX, ULTRA_ENB+1;
-		test	SAVED_TCL, 0x80		jnz ultra_2; /* Target ID > 7 */
-		dec	SINDEX;
-ultra_2:
-		mov     FUNCTION1,SAVED_TCL;
-		mov     A,FUNCTION1;
-		test	SINDIR, A	jz ndx_dtr;
+	or	SXFRCTL0, SPIOEN|CLRSTCNT|CLRCHN;
+set_transfer_settings:
+	if ((ahc->features & AHC_ULTRA) != 0) {
+		test	SCB_CONTROL, ULTRAENB jz . + 2;
 		or	SXFRCTL0, FAST20;
 	} 
-/*
- * Initialize SCSIRATE with the appropriate value for this target.
- * The SCSIRATE settings for each target are stored in an array
- * based at TARG_SCSIRATE.
- */
-ndx_dtr:
-	shr	A,4,SAVED_TCL;
-	if ((p->features & AHC_TWIN) != 0) {
-		test	SBLKCTL,SELBUSB	jz ndx_dtr_2;
-		or	SAVED_TCL, SELBUSB; 
-		or	A,0x08;			/* Channel B entries add 8 */
-ndx_dtr_2:
-	}
-
-	if ((p->features & AHC_ULTRA2) != 0) {
-		add	SINDEX, TARG_OFFSET, A;
-		mov	SCSIOFFSET, SINDIR;
+	/*
+	 * Initialize SCSIRATE with the appropriate value for this target.
+	 */
+	if ((ahc->features & AHC_ULTRA2) != 0) {
+		bmov	SCSIRATE, SCB_SCSIRATE, 2 ret;
+	} else {
+		mov	SCSIRATE, SCB_SCSIRATE ret;
 	}
 
-	add	SINDEX,TARG_SCSIRATE,A;
-	mov	SCSIRATE,SINDIR ret;
-
-
 selection:
+	/*
+	 * We aren't expecting a bus free, so interrupt
+	 * the kernel driver if it happens.
+	 */
+	mvi	CLRSINT1,CLRBUSFREE;
+	or	SIMODE1, ENBUSFREE;
+
+	/*
+	 * Guard against a bus free after (re)selection
+	 * but prior to enabling the busfree interrupt.  SELDI
+	 * and SELDO will be cleared in that case.
+	 */
+	test	SSTAT0, SELDI|SELDO	jz bus_free_sel;
 	test	SSTAT0,SELDO	jnz select_out;
+select_in:
+	if ((ahc->flags & AHC_TARGETROLE) != 0) {
+		if ((ahc->flags & AHC_INITIATORROLE) != 0) {
+			test	SSTAT0, TARGET	jz initiator_reselect;
+		}
+		mvi	CLRSINT0, CLRSELDI;
+
+		/*
+		 * We've just been selected.  Assert BSY and
+		 * setup the phase for receiving messages
+		 * from the target.
+		 *
+		 * If bus reset interrupts have been disabled (from a
+		 * previous reset), re-enable them now.  Resets are only
+		 * of interest when we have outstanding transactions, so
+		 * we can safely defer re-enabling the interrupt until,
+		 * as a target, we start receiving transactions again.
+		 */
+		test	SIMODE1, ENSCSIRST	jnz . + 3;
+		mvi	CLRSINT1, CLRSCSIRSTI;
+		or	SIMODE1, ENSCSIRST;
+		mvi	SCSISIGO, P_MESGOUT|BSYO;
+
+		/*
+		 * Setup the DMA for sending the identify and
+		 * command information.
+		 */
+		or	SEQ_FLAGS, CMDPHASE_PENDING;
+
+		mov     A, TQINPOS;
+		if ((ahc->features & AHC_CMD_CHAN) != 0) {
+			mvi	DINDEX, CCHADDR;
+			mvi	SHARED_DATA_ADDR call set_32byte_addr;
+			mvi	CCSCBCTL, CCSCBRESET;
+		} else {
+			mvi	DINDEX, HADDR;
+			mvi	SHARED_DATA_ADDR call set_32byte_addr;
+			mvi	DFCNTRL, FIFORESET;
+		}
+
+		/* Initiator that selected us */
+		and	SAVED_SCSIID, SELID_MASK, SELID;
+		/* The Target ID we were selected at */
+		if ((ahc->features & AHC_MULTI_TID) != 0) {
+			and	A, OID, TARGIDIN;
+		} else if ((ahc->features & AHC_ULTRA2) != 0) {
+			and	A, OID, SCSIID_ULTRA2;
+		} else {
+			and	A, OID, SCSIID;
+		}
+		or	SAVED_SCSIID, A;
+		if ((ahc->features & AHC_TWIN) != 0) {
+			test 	SBLKCTL, SELBUSB jz . + 2;
+			or	SAVED_SCSIID, TWIN_CHNLB;
+		}
+		if ((ahc->features & AHC_CMD_CHAN) != 0) {
+			mov	CCSCBRAM, SAVED_SCSIID;
+		} else {
+			mov	DFDAT, SAVED_SCSIID;
+		}
+
+		/*
+		 * If ATN isn't asserted, the target isn't interested
+		 * in talking to us.  Go directly to bus free.
+		 * XXX SCSI-1 may require us to assume lun 0 if
+		 * ATN is false.
+		 */
+		test	SCSISIGI, ATNI	jz	target_busfree;
+
+		/*
+		 * Watch ATN closely now as we pull in messages from the
+		 * initiator.  We follow the guidlines from section 6.5
+		 * of the SCSI-2 spec for what messages are allowed when.
+		 */
+		call	target_inb;
+
+		/*
+		 * Our first message must be one of IDENTIFY, ABORT, or
+		 * BUS_DEVICE_RESET.
+		 */
+		test	DINDEX, MSG_IDENTIFYFLAG jz host_target_message_loop;
+		/* Store for host */
+		if ((ahc->features & AHC_CMD_CHAN) != 0) {
+			mov	CCSCBRAM, DINDEX;
+		} else {
+			mov	DFDAT, DINDEX;
+		}
+
+		/* Remember for disconnection decision */
+		test	DINDEX, MSG_IDENTIFY_DISCFLAG jnz . + 2;
+		/* XXX Honor per target settings too */
+		or	SEQ_FLAGS, NO_DISCONNECT;
+
+		test	SCSISIGI, ATNI	jz	ident_messages_done;
+		call	target_inb;
+		/*
+		 * If this is a tagged request, the tagged message must
+		 * immediately follow the identify.  We test for a valid
+		 * tag message by seeing if it is >= MSG_SIMPLE_Q_TAG and
+		 * < MSG_IGN_WIDE_RESIDUE.
+		 */
+		add	A, -MSG_SIMPLE_Q_TAG, DINDEX;
+		jnc	ident_messages_done;
+		add	A, -MSG_IGN_WIDE_RESIDUE, DINDEX;
+		jc	ident_messages_done;
+		/* Store for host */
+		if ((ahc->features & AHC_CMD_CHAN) != 0) {
+			mov	CCSCBRAM, DINDEX;
+		} else {
+			mov	DFDAT, DINDEX;
+		}
+		
+		/*
+		 * If the initiator doesn't feel like providing a tag number,
+		 * we've got a failed selection and must transition to bus
+		 * free.
+		 */
+		test	SCSISIGI, ATNI	jz	target_busfree;
+
+		/*
+		 * Store the tag for the host.
+		 */
+		call	target_inb;
+		if ((ahc->features & AHC_CMD_CHAN) != 0) {
+			mov	CCSCBRAM, DINDEX;
+		} else {
+			mov	DFDAT, DINDEX;
+		}
+		mov	INITIATOR_TAG, DINDEX;
+		or	SEQ_FLAGS, TARGET_CMD_IS_TAGGED;
+		test	SCSISIGI, ATNI	jz . + 2;
+		/* Initiator still wants to give us messages */
+		call	target_inb;
+		jmp	ident_messages_done;
+
+		/*
+		 * Pushed message loop to allow the kernel to
+		 * run it's own target mode message state engine.
+		 */
+host_target_message_loop:
+		mvi	HOST_MSG_LOOP call set_seqint;
+		cmp	RETURN_1, EXIT_MSG_LOOP	je target_ITloop;
+		test	SSTAT0, SPIORDY jz .;
+		jmp	host_target_message_loop;
+
+ident_messages_done:
+		/* If ring buffer is full, return busy or queue full */
+		if ((ahc->features & AHC_HS_MAILBOX) != 0) {
+			and	A, HOST_TQINPOS, HS_MAILBOX;
+		} else {
+			mov	A, KERNEL_TQINPOS;
+		}
+		cmp	TQINPOS, A jne tqinfifo_has_space;
+		mvi	P_STATUS|BSYO call change_phase;
+		test	SEQ_FLAGS, TARGET_CMD_IS_TAGGED jz . + 3;
+		mvi	STATUS_QUEUE_FULL call target_outb;
+		jmp	target_busfree_wait;
+		mvi	STATUS_BUSY call target_outb;
+		jmp	target_busfree_wait;
+tqinfifo_has_space:	
+		/* Terminate the ident list */
+		if ((ahc->features & AHC_CMD_CHAN) != 0) {
+			mvi	CCSCBRAM, SCB_LIST_NULL;
+		} else {
+			mvi	DFDAT, SCB_LIST_NULL;
+		}
+		or	SEQ_FLAGS, TARG_CMD_PENDING|IDENTIFY_SEEN;
+		test	SCSISIGI, ATNI	jnz target_mesgout_pending;
+		jmp	target_ITloop;
+		
+/*
+ * We carefully toggle SPIOEN to allow us to return the 
+ * message byte we receive so it can be checked prior to
+ * driving REQ on the bus for the next byte.
+ */
+target_inb:
+		/*
+		 * Drive REQ on the bus by enabling SCSI PIO.
+		 */
+		or	SXFRCTL0, SPIOEN;
+		/* Wait for the byte */
+		test	SSTAT0, SPIORDY jz .;
+		/* Prevent our read from triggering another REQ */
+		and	SXFRCTL0, ~SPIOEN;
+		/* Save latched contents */
+		mov	DINDEX, SCSIDATL ret;
+	}
+
+if ((ahc->flags & AHC_INITIATORROLE) != 0) {
 /*
  * Reselection has been initiated by a target. Make a note that we've been
  * reselected, but haven't seen an IDENTIFY message from the target yet.
  */
 initiator_reselect:
-	mvi	CLRSINT0, CLRSELDI;
 	/* XXX test for and handle ONE BIT condition */
-	and	SAVED_TCL, SELID_MASK, SELID;
-	mvi	CLRSINT1,CLRBUSFREE;
-	or	SIMODE1, ENBUSFREE;		/*
-						 * We aren't expecting a
-						 * bus free, so interrupt
-						 * the kernel driver if it
-						 * happens.
-						 */
-	mvi	SPIOEN call	initialize_channel;
-	mvi	MSG_OUT, MSG_NOOP;		/* No message to send */
+	or	SXFRCTL0, SPIOEN|CLRSTCNT|CLRCHN;
+	and	SAVED_SCSIID, SELID_MASK, SELID;
+	if ((ahc->features & AHC_ULTRA2) != 0) {
+		and	A, OID, SCSIID_ULTRA2;
+	} else {
+		and	A, OID, SCSIID;
+	}
+	or	SAVED_SCSIID, A;
+	if ((ahc->features & AHC_TWIN) != 0) {
+		test	SBLKCTL, SELBUSB	jz . + 2;
+		or	SAVED_SCSIID, TWIN_CHNLB;
+	}
+	mvi	CLRSINT0, CLRSELDI;
 	jmp	ITloop;
+}
 
 /*
  * After the selection, remove this SCB from the "waiting SCB"
@@ -259,33 +435,198 @@
  */
 select_out:
 	/* Turn off the selection hardware */
-	mvi	SCSISEQ, ENRSELI|ENAUTOATNP;	/*
-						 * ATN on parity errors
-						 * for "in" phases
-						 */
+	and	SCSISEQ, TEMODE|ENSELI|ENRSELI|ENAUTOATNP, SCSISEQ;
 	mvi	CLRSINT0, CLRSELDO;
 	mov	SCBPTR, WAITING_SCBH;
 	mov	WAITING_SCBH,SCB_NEXT;
-	mov	SAVED_TCL, SCB_TCL;
-	mvi	CLRSINT1,CLRBUSFREE;
-	or	SIMODE1, ENBUSFREE;		/*
-						 * We aren't expecting a
-						 * bus free, so interrupt
-						 * the kernel driver if it
-						 * happens.
-						 */
-	mvi	SPIOEN call	initialize_channel;
-/*
- * As soon as we get a successful selection, the target should go
- * into the message out phase since we have ATN asserted.
- */
+	mov	SAVED_SCSIID, SCB_SCSIID;
+	mov	SAVED_LUN, SCB_LUN;
+	call	initialize_channel;
+	if ((ahc->flags & AHC_TARGETROLE) != 0) {
+		test	SSTAT0, TARGET	jz initiator_select;
+
+		/*
+		 * We've just re-selected an initiator.
+		 * Assert BSY and setup the phase for
+		 * sending our identify messages.
+		 */
+		mvi	P_MESGIN|BSYO call change_phase;
+
+		/*
+		 * Start out with a simple identify message.
+		 */
+		or	SCB_LUN, MSG_IDENTIFYFLAG call target_outb;
+
+		/*
+		 * If we are the result of a tagged command, send
+		 * a simple Q tag and the tag id.
+		 */
+		test	SCB_CONTROL, TAG_ENB	jz . + 3;
+		mvi	MSG_SIMPLE_Q_TAG call target_outb;
+		mov	SCB_TARGET_INFO[SCB_INITIATOR_TAG] call target_outb;
+target_synccmd:
+		/*
+		 * Now determine what phases the host wants us
+		 * to go through.
+		 */
+		mov	SEQ_FLAGS, SCB_TARGET_INFO[SCB_TARGET_PHASES];
+		
+target_ITloop:
+		/*
+		 * Start honoring ATN signals now that
+		 * we properly identified ourselves.
+		 */
+		test	SCSISIGI, ATNI			jnz target_mesgout;
+		test	SEQ_FLAGS, CMDPHASE_PENDING	jnz target_cmdphase;
+		test	SEQ_FLAGS, DPHASE_PENDING	jnz target_dphase;
+		test	SEQ_FLAGS, SPHASE_PENDING	jnz target_sphase;
+
+		/*
+		 * No more work to do.  Either disconnect or not depending
+		 * on the state of NO_DISCONNECT.
+		 */
+		test	SEQ_FLAGS, NO_DISCONNECT jz target_disconnect; 
+		if ((ahc->flags & AHC_PAGESCBS) != 0) {
+			mov	ALLZEROS	call	get_free_or_disc_scb;
+		}
+		mov	RETURN_1, ALLZEROS;
+		call	complete_target_cmd;
+		cmp	RETURN_1, CONT_MSG_LOOP jne .;
+		mvi	DMAPARAMS, HDMAEN|DIRECTION|FIFORESET;
+		mov	SCB_TAG	 call dma_scb;
+		jmp	target_synccmd;
+
+target_mesgout:
+		mvi	SCSISIGO, P_MESGOUT|BSYO;
+target_mesgout_continue:
+		call	target_inb;
+target_mesgout_pending:
+		/* Local Processing goes here... */
+		jmp	host_target_message_loop;
+		
+target_disconnect:
+		mvi	P_MESGIN|BSYO call change_phase;
+		test	SEQ_FLAGS, DPHASE	jz . + 2;
+		mvi	MSG_SAVEDATAPOINTER call target_outb;
+		mvi	MSG_DISCONNECT call target_outb;
+
+target_busfree_wait:
+		/* Wait for preceeding I/O session to complete. */
+		test	SCSISIGI, ACKI jnz .;
+target_busfree:
+		and	SIMODE1, ~ENBUSFREE;
+		if ((ahc->features & AHC_ULTRA2) != 0) {
+			clr	SCSIBUSL;
+		}
+		clr	SCSISIGO;
+		mvi	LASTPHASE, P_BUSFREE;
+		call	complete_target_cmd;
+		jmp	poll_for_work;
+
+target_cmdphase:
+		mvi	P_COMMAND|BSYO call change_phase;
+		call	target_inb;
+		mov	A, DINDEX;
+		/* Store for host */
+		if ((ahc->features & AHC_CMD_CHAN) != 0) {
+			mov	CCSCBRAM, A;
+		} else {
+			mov	DFDAT, A;
+		}
+
+		/*
+		 * Determine the number of bytes to read
+		 * based on the command group code via table lookup.
+		 * We reuse the first 8 bytes of the TARG_SCSIRATE
+		 * BIOS array for this table. Count is one less than
+		 * the total for the command since we've already fetched
+		 * the first byte.
+		 */
+		shr	A, CMD_GROUP_CODE_SHIFT;
+		add	SINDEX, CMDSIZE_TABLE, A;
+		mov	A, SINDIR;
+
+		test	A, 0xFF jz command_phase_done;
+		or	SXFRCTL0, SPIOEN;
+command_loop:
+		test	SSTAT0, SPIORDY jz .;
+		cmp	A, 1 jne . + 2;
+		and	SXFRCTL0, ~SPIOEN;	/* Last Byte */
+		if ((ahc->features & AHC_CMD_CHAN) != 0) {
+			mov	CCSCBRAM, SCSIDATL;
+		} else {
+			mov	DFDAT, SCSIDATL;
+		}
+		dec	A;
+		test	A, 0xFF jnz command_loop;
+
+command_phase_done:
+		and	SEQ_FLAGS, ~CMDPHASE_PENDING;
+		jmp	target_ITloop;
+
+target_dphase:
+		/*
+		 * Data phases on the bus are from the
+		 * perspective of the initiator.  The dma
+		 * code looks at LASTPHASE to determine the
+		 * data direction of the DMA.  Toggle it for
+		 * target transfers.
+		 */
+		xor	LASTPHASE, IOI, SCB_TARGET_INFO[SCB_TARGET_DATA_DIR];
+		or	SCB_TARGET_INFO[SCB_TARGET_DATA_DIR], BSYO
+			call change_phase;
+		jmp	p_data;
+
+target_sphase:
+		mvi	P_STATUS|BSYO call change_phase;
+		mvi	LASTPHASE, P_STATUS;
+		mov	SCB_TARGET_INFO[SCB_TARGET_STATUS] call target_outb;
+		/* XXX Watch for ATN or parity errors??? */
+		mvi	SCSISIGO, P_MESGIN|BSYO;
+		/* MSG_CMDCMPLT is 0, but we can't do an immediate of 0 */
+		mov	ALLZEROS call target_outb;
+		jmp	target_busfree_wait;
+	
+complete_target_cmd:
+		test	SEQ_FLAGS, TARG_CMD_PENDING	jnz . + 2;
+		mov	SCB_TAG jmp complete_post;
+		if ((ahc->features & AHC_CMD_CHAN) != 0) {
+			/* Set the valid byte */
+			mvi	CCSCBADDR, 24;
+			mov	CCSCBRAM, ALLONES;
+			mvi	CCHCNT, 28;
+			or	CCSCBCTL, CCSCBEN|CCSCBRESET;
+			test	CCSCBCTL, CCSCBDONE jz .;
+			clr	CCSCBCTL;
+		} else {
+			/* Set the valid byte */
+			or	DFCNTRL, FIFORESET;
+			mvi	DFWADDR, 3; /* Third 64bit word or byte 24 */
+			mov	DFDAT, ALLONES;
+			mvi	28	call set_hcnt;
+			or	DFCNTRL, HDMAEN|FIFOFLUSH;
+			call	dma_finish;
+		}
+		inc	TQINPOS;
+		mvi	INTSTAT,CMDCMPLT ret;
+	}
+
+if ((ahc->flags & AHC_INITIATORROLE) != 0) {
+initiator_select:
+	/*
+	 * As soon as we get a successful selection, the target
+	 * should go into the message out phase since we have ATN
+	 * asserted.
+	 */
 	mvi	MSG_OUT, MSG_IDENTIFYFLAG;
 	or	SEQ_FLAGS, IDENTIFY_SEEN;
 
-/*
- * Main loop for information transfer phases.  Wait for the target
- * to assert REQ before checking MSG, C/D and I/O for the bus phase.
- */
+	/*
+	 * Main loop for information transfer phases.  Wait for the
+	 * target to assert REQ before checking MSG, C/D and I/O for
+	 * the bus phase.
+	 */
+mesgin_phasemis:
 ITloop:
 	call	phase_lock;
 
@@ -297,17 +638,20 @@
 	cmp	A,P_STATUS	je p_status;
 	cmp	A,P_MESGIN	je p_mesgin;
 
-	mvi	INTSTAT,BAD_PHASE;	/* unknown phase - signal driver */
+	mvi	BAD_PHASE call set_seqint;
 	jmp	ITloop;			/* Try reading the bus again. */
 
 await_busfree:
 	and	SIMODE1, ~ENBUSFREE;
-	call	clear_target_state;
 	mov	NONE, SCSIDATL;		/* Ack the last byte */
+	if ((ahc->features & AHC_ULTRA2) != 0) {
+		clr	SCSIBUSL;	/* Prevent bit leakage durint SELTO */
+	}
 	and	SXFRCTL0, ~SPIOEN;
 	test	SSTAT1,REQINIT|BUSFREE	jz .;
 	test	SSTAT1, BUSFREE jnz poll_for_work;
-	mvi	INTSTAT, BAD_PHASE;
+	mvi	MISSED_BUSFREE call set_seqint;
+}
 	
 clear_target_state:
 	/*
@@ -316,42 +660,154 @@
 	 * clear DFCNTRL too.
 	 */
 	clr	DFCNTRL;
+	or	SXFRCTL0, CLRSTCNT|CLRCHN;
 
 	/*
 	 * We don't know the target we will connect to,
 	 * so default to narrow transfers to avoid
 	 * parity problems.
 	 */
-	if ((p->features & AHC_ULTRA2) != 0) {
-		bmov    SCSIRATE, ALLZEROS, 2;
+	if ((ahc->features & AHC_ULTRA2) != 0) {
+		bmov	SCSIRATE, ALLZEROS, 2;
 	} else {
-		clr     SCSIRATE;
-		and     SXFRCTL0, ~(FAST20);
+		clr	SCSIRATE;
+		if ((ahc->features & AHC_ULTRA) != 0) {
+			and	SXFRCTL0, ~(FAST20);
+		}
 	}
 	mvi	LASTPHASE, P_BUSFREE;
 	/* clear target specific flags */
 	clr	SEQ_FLAGS ret;
 
+sg_advance:
+	clr	A;			/* add sizeof(struct scatter) */
+	add	SCB_RESIDUAL_SGPTR[0],SG_SIZEOF;
+	adc	SCB_RESIDUAL_SGPTR[1],A;
+	adc	SCB_RESIDUAL_SGPTR[2],A;
+	adc	SCB_RESIDUAL_SGPTR[3],A ret;
+
+idle_loop:
+	if ((ahc->features & AHC_CMD_CHAN) != 0) {
+		/* Did we just finish fetching segs? */
+		cmp	CCSGCTL, CCSGEN|CCSGDONE je idle_sgfetch_complete;
+
+		/* Are we actively fetching segments? */
+		test	CCSGCTL, CCSGEN jnz return;
+
+		/*
+		 * Do we need any more segments?
+		 */
+		test	SCB_RESIDUAL_DATACNT[3], SG_LAST_SEG jnz return;
+
+		/*
+		 * Do we have any prefetch left???
+		 */
+		cmp	CCSGADDR, SG_PREFETCH_CNT jne idle_sg_avail;
+
+		/*
+		 * Need to fetch segments, but we can only do that
+		 * if the command channel is completely idle.  Make
+		 * sure we don't have an SCB prefetch going on.
+		 */
+		test	CCSCBCTL, CCSCBEN jnz return;
+
+		/*
+		 * We fetch a "cacheline aligned" and sized amount of data
+		 * so we don't end up referencing a non-existant page.
+		 * Cacheline aligned is in quotes because the kernel will
+		 * set the prefetch amount to a reasonable level if the
+		 * cacheline size is unknown.
+		 */
+		mvi	CCHCNT, SG_PREFETCH_CNT;
+		and	CCHADDR[0], SG_PREFETCH_ALIGN_MASK, SCB_RESIDUAL_SGPTR;
+		bmov	CCHADDR[1], SCB_RESIDUAL_SGPTR[1], 3;
+		mvi	CCSGCTL, CCSGEN|CCSGRESET ret;
+idle_sgfetch_complete:
+		clr	CCSGCTL;
+		test	CCSGCTL, CCSGEN jnz .;
+		and	CCSGADDR, SG_PREFETCH_ADDR_MASK, SCB_RESIDUAL_SGPTR;
+idle_sg_avail:
+		if ((ahc->features & AHC_ULTRA2) != 0) {
+			/* Does the hardware have space for another SG entry? */
+			test	DFSTATUS, PRELOAD_AVAIL jz return;
+			bmov 	HADDR, CCSGRAM, 4;
+			bmov	SINDEX, CCSGRAM, 1;
+			test	SINDEX, 0x1 jz . + 2;
+			xor	DATA_COUNT_ODD, 0x1;
+			bmov	HCNT[0], SINDEX, 1;
+			bmov	HCNT[1], CCSGRAM, 2;
+			bmov	SCB_RESIDUAL_DATACNT[3], CCSGRAM, 1;
+			call	sg_advance;
+			mov	SINDEX, SCB_RESIDUAL_SGPTR[0];
+			test	DATA_COUNT_ODD, 0x1 jz . + 2;
+			or	SINDEX, ODD_SEG;
+			test	SCB_RESIDUAL_DATACNT[3], SG_LAST_SEG jz . + 2;
+			or	SINDEX, LAST_SEG;
+			mov	SG_CACHE_PRE, SINDEX;
+			/* Load the segment by writing DFCNTRL again */
+			mov	DFCNTRL, DMAPARAMS;
+		}
+		ret;
+	}
+
+if ((ahc->bugs & AHC_PCI_MWI_BUG) != 0 && ahc->pci_cachesize != 0) {
+/*
+ * Calculate the trailing portion of this S/G segment that cannot
+ * be transferred using memory write and invalidate PCI transactions.  
+ * XXX Can we optimize this for PCI writes only???
+ */
+calc_mwi_residual:
+	/*
+	 * If the ending address is on a cacheline boundary,
+	 * there is no need for an extra segment.
+	 */
+	mov	A, HCNT[0];
+	add	A, A, HADDR[0];
+	and	A, CACHESIZE_MASK;
+	test	A, 0xFF jz return;
+
+	/*
+	 * If the transfer is less than a cachline,
+	 * there is no need for an extra segment.
+	 */
+	test	HCNT[1], 0xFF	jnz calc_mwi_residual_final;
+	test	HCNT[2], 0xFF	jnz calc_mwi_residual_final;
+	add	NONE, INVERTED_CACHESIZE_MASK, HCNT[0];
+	jnc	return;
+
+calc_mwi_residual_final:
+	mov	MWI_RESIDUAL, A;
+	not	A;
+	inc	A;
+	add	HCNT[0], A;
+	adc	HCNT[1], -1;
+	adc	HCNT[2], -1 ret;
+}
+
 /*
  * If we re-enter the data phase after going through another phase, the
  * STCNT may have been cleared, so restore it from the residual field.
  */
 data_phase_reinit:
-	if ((p->features & AHC_ULTRA2) != 0) {
+	if ((ahc->features & AHC_ULTRA2) != 0) {
+		/*
+		 * The preload circuitry requires us to
+		 * reload the address too, so pull it from
+		 * the shaddow address.
+		 */
 		bmov	HADDR, SHADDR, 4;
-		bmov    HCNT, SCB_RESID_DCNT, 3;
-	}
-	if ((p->chip & AHC_CHIPID_MASK) == AHC_AIC7895) {
-		bmov    STCNT, SCB_RESID_DCNT, 3;
-	}
-	if ((p->features & AHC_CMD_CHAN) == 0) {
+		bmov	HCNT, SCB_RESIDUAL_DATACNT, 3;
+	} else if ((ahc->features & AHC_CMD_CHAN) != 0) {
+		bmov	STCNT, SCB_RESIDUAL_DATACNT, 3;
+	} else {
 		mvi	DINDEX, STCNT;
-		mvi	SCB_RESID_DCNT	call bcopy_3;
+		mvi	SCB_RESIDUAL_DATACNT call bcopy_3;
 	}
+	and	DATA_COUNT_ODD, 0x1, SCB_RESIDUAL_DATACNT[0];
 	jmp	data_phase_loop;
 
 p_data:
-	if ((p->features & AHC_ULTRA2) != 0) {
+	if ((ahc->features & AHC_ULTRA2) != 0) {
 		mvi	DMAPARAMS, PRELOADEN|SCSIEN|HDMAEN;
 	} else {
 		mvi	DMAPARAMS, WIDEODD|SCSIEN|SDMAEN|HDMAEN|FIFORESET;
@@ -362,8 +818,9 @@
 					 * Ensure entering a data
 					 * phase is okay - seen identify, etc.
 					 */
-	if ((p->features & AHC_CMD_CHAN) != 0) {
-		mvi	CCSGADDR, CCSGADDR_MAX;
+	if ((ahc->features & AHC_CMD_CHAN) != 0) {
+		/* We don't have any valid S/G elements */
+		mvi	CCSGADDR, SG_PREFETCH_CNT;
 	}
 	test	SEQ_FLAGS, DPHASE	jnz data_phase_reinit;
 
@@ -372,256 +829,479 @@
 
 	/*
 	 * Initialize the DMA address and counter from the SCB.
-	 * Also set SG_COUNT and SG_NEXT in memory since we cannot
-	 * modify the values in the SCB itself until we see a
-	 * save data pointers message.
+	 * Also set SCB_RESIDUAL_SGPTR, including the LAST_SEG
+	 * flag in the highest byte of the data count.  We cannot
+	 * modify the saved values in the SCB until we see a save
+	 * data pointers message.
 	 */
-	if ((p->features & AHC_CMD_CHAN) != 0) {
+	if ((ahc->features & AHC_CMD_CHAN) != 0) {
 		bmov	HADDR, SCB_DATAPTR, 7;
-		bmov    STCNT, HCNT, 3;
-		bmov    SG_COUNT, SCB_SGCOUNT, 5;
+		bmov	SCB_RESIDUAL_DATACNT[3], SCB_DATACNT[3], 5;
 	} else {
 		mvi	DINDEX, HADDR;
 		mvi	SCB_DATAPTR	call bcopy_7;
-		call	set_stcnt_from_hcnt;
-		mvi	DINDEX, SG_COUNT;
-		mvi	SCB_SGCOUNT	call bcopy_5;
+		mvi	DINDEX, SCB_RESIDUAL_DATACNT + 3;
+		mvi	SCB_DATACNT + 3 call bcopy_5;
+	}
+	if ((ahc->bugs & AHC_PCI_MWI_BUG) != 0 && ahc->pci_cachesize != 0) {
+		call	calc_mwi_residual;
+	}
+	and	SCB_RESIDUAL_SGPTR[0], ~SG_FULL_RESID;
+	and	DATA_COUNT_ODD, 0x1, HCNT[0];
+
+	if ((ahc->features & AHC_ULTRA2) == 0) {
+		if ((ahc->features & AHC_CMD_CHAN) != 0) {
+			bmov	STCNT, HCNT, 3;
+		} else {
+			call	set_stcnt_from_hcnt;
+		}
 	}
 
 data_phase_loop:
-/* Guard against overruns */
-	test	SG_COUNT, 0xff jnz data_phase_inbounds;
-/*
- * Turn on 'Bit Bucket' mode, set the transfer count to
- * 16meg and let the target run until it changes phase.
- * When the transfer completes, notify the host that we
- * had an overrun.
- */
+	/* Guard against overruns */
+	test	SCB_RESIDUAL_SGPTR[0], SG_LIST_NULL jz data_phase_inbounds;
+
+	/*
+	 * Turn on `Bit Bucket' mode, wait until the target takes
+	 * us to another phase, and then notify the host.
+	 */
+	and	DMAPARAMS, DIRECTION;
+	mov	DFCNTRL, DMAPARAMS;
 	or	SXFRCTL1,BITBUCKET;
-	and	DMAPARAMS, ~(HDMAEN|SDMAEN);
-	if ((p->features & AHC_CMD_CHAN) != 0) {
-		if ((p->features & AHC_ULTRA2) != 0) {
-			bmov	HCNT, ALLONES, 3;
-		}
-		bmov	STCNT, ALLONES, 3;
-	} else {
-		mvi	STCNT[0], 0xFF;
-		mvi	STCNT[1], 0xFF;
-		mvi	STCNT[2], 0xFF;
-	}
+	test	SSTAT1,PHASEMIS	jz .;
+	and	SXFRCTL1, ~BITBUCKET;
+	mvi	DATA_OVERRUN call set_seqint;
+	jmp	ITloop;
+
 data_phase_inbounds:
-/* If we are the last SG block, tell the hardware. */
-	cmp	SG_COUNT,0x01 jne data_phase_wideodd;
-	if ((p->features & AHC_ULTRA2) == 0) {
-		and	DMAPARAMS, ~WIDEODD;
-	} else {
-		mvi	SG_CACHEPTR, LAST_SEG;
-	}
-data_phase_wideodd:
-	if ((p->features & AHC_ULTRA2) != 0) {
-		mov	SINDEX, ALLONES;
+	if ((ahc->features & AHC_ULTRA2) != 0) {
+		mov	SINDEX, SCB_RESIDUAL_SGPTR[0];
+		test	SCB_RESIDUAL_DATACNT[3], SG_LAST_SEG jz . + 2;
+		or	SINDEX, LAST_SEG;
+		test	DATA_COUNT_ODD, 0x1 jz . + 2;
+		or	SINDEX, ODD_SEG;
+		mov	SG_CACHE_PRE, SINDEX;
 		mov	DFCNTRL, DMAPARAMS;
-		test	SSTAT0, SDONE jnz .;
-data_phase_dma_loop:
-		test	SSTAT0, SDONE jnz data_phase_dma_done;
-		test	SSTAT1,PHASEMIS	jz data_phase_dma_loop;	/* ie. underrun */
-data_phase_dma_phasemis:
-		test	SSTAT0,SDONE	jnz data_phase_dma_done;
-		clr	SINDEX;			/* Remember the phasemiss */
-	} else {
-		mov	DMAPARAMS  call dma;
-	}
+ultra2_dma_loop:
+		call	idle_loop;
+		/*
+		 * The transfer is complete if either the last segment
+		 * completes or the target changes phase.
+		 */
+		test	SG_CACHE_SHADOW, LAST_SEG_DONE jnz ultra2_dmafinish;
+		if ((ahc->flags & AHC_TARGETROLE) != 0) {
+			 /*
+			  * As a target, we control the phases,
+			  * so ignore PHASEMIS.
+			  */
+			test	SSTAT0, TARGET jnz ultra2_dma_loop;
+		}
+		if ((ahc->flags & AHC_INITIATORROLE) != 0) {
+			test	SSTAT1,PHASEMIS	jz ultra2_dma_loop;
+		}
 
-data_phase_dma_done:
-/* Go tell the host about any overruns */
-	test	SXFRCTL1,BITBUCKET jnz data_phase_overrun;
+ultra2_dmafinish:
+		test	DFCNTRL, DIRECTION jnz ultra2_dmafifoempty;
+		and	DFCNTRL, ~SCSIEN;
+		test	DFCNTRL, SCSIEN jnz .;
+		if ((ahc->bugs & AHC_AUTOFLUSH_BUG) != 0) {
+			test	DFSTATUS, FIFOEMP jnz ultra2_dmafifoempty;
+		}
+ultra2_dmafifoflush:
+		if ((ahc->bugs & AHC_AUTOFLUSH_BUG) != 0) {
+			/*
+			 * On Rev A of the aic7890, the autoflush
+			 * features doesn't function correctly.
+			 * Perform an explicit manual flush.  During
+			 * a manual flush, the FIFOEMP bit becomes
+			 * true every time the PCI FIFO empties
+			 * regardless of the state of the SCSI FIFO.
+			 * It can take up to 4 clock cycles for the
+			 * SCSI FIFO to get data into the PCI FIFO
+			 * and for FIFOEMP to de-assert.  Here we
+			 * guard against this condition by making
+			 * sure the FIFOEMP bit stays on for 5 full
+			 * clock cycles.
+			 */
+			or	DFCNTRL, FIFOFLUSH;
+			test	DFSTATUS, FIFOEMP jz ultra2_dmafifoflush;
+			test	DFSTATUS, FIFOEMP jz ultra2_dmafifoflush;
+			test	DFSTATUS, FIFOEMP jz ultra2_dmafifoflush;
+			test	DFSTATUS, FIFOEMP jz ultra2_dmafifoflush;
+		}
+		test	DFSTATUS, FIFOEMP jz ultra2_dmafifoflush;
+ultra2_dmafifoempty:
+		/* Don't clobber an inprogress host data transfer */
+		test	DFSTATUS, MREQPEND	jnz ultra2_dmafifoempty;
+ultra2_dmahalt:
+		and     DFCNTRL, ~(SCSIEN|HDMAEN);
+		test	DFCNTRL, HDMAEN jnz .;
 
-/* Exit if we had an underrun.  dma clears SINDEX in this case. */
-	test	SINDEX,0xff	jz data_phase_finish;
+		/*
+		 * If, by chance, we stopped before being able
+		 * to fetch additional segments for this transfer,
+		 * yet the last S/G was completely exhausted,
+		 * call our idle loop until it is able to load
+		 * another segment.  This will allow us to immediately
+		 * pickup on the next segment on the next data phase.
+		 *
+		 * If we happened to stop on the last segment, then
+		 * our residual information is still correct from
+		 * the idle loop and there is no need to perform
+		 * any fixups.  Just jump to data_phase_finish.
+		 */
+ultra2_ensure_sg:
+		test	SG_CACHE_SHADOW, LAST_SEG jz ultra2_shvalid;
+		/* Record if we've consumed all S/G entries */
+		test	SG_CACHE_SHADOW, LAST_SEG_DONE jz data_phase_finish;
+		or	SCB_RESIDUAL_SGPTR[0], SG_LIST_NULL;
+		jmp	data_phase_finish;
+
+ultra2_shvalid:
+                test    SSTAT2, SHVALID	jnz sgptr_fixup;
+		call	idle_loop;
+		jmp	ultra2_ensure_sg;
 
-/*
- * Advance the scatter-gather pointers if needed 
- */
-sg_advance:
-	dec	SG_COUNT;	/* one less segment to go */
+sgptr_fixup:
+		/*
+		 * Fixup the residual next S/G pointer.  The S/G preload
+		 * feature of the chip allows us to load two elements
+		 * in addition to the currently active element.  We
+		 * store the bottom byte of the next S/G pointer in
+		 * the SG_CACEPTR register so we can restore the
+		 * correct value when the DMA completes.  If the next
+		 * sg ptr value has advanced to the point where higher
+		 * bytes in the address have been affected, fix them
+		 * too.
+		 */
+		test	SG_CACHE_SHADOW, 0x80 jz sgptr_fixup_done;
+		test	SCB_RESIDUAL_SGPTR[0], 0x80 jnz sgptr_fixup_done;
+		add	SCB_RESIDUAL_SGPTR[1], -1;
+		adc	SCB_RESIDUAL_SGPTR[2], -1; 
+		adc	SCB_RESIDUAL_SGPTR[3], -1;
+sgptr_fixup_done:
+		and	SCB_RESIDUAL_SGPTR[0], SG_ADDR_MASK, SG_CACHE_SHADOW;
+		clr	DATA_COUNT_ODD;
+		test	SG_CACHE_SHADOW, ODD_SEG jz . + 2;
+		or	DATA_COUNT_ODD, 0x1;
+		clr	SCB_RESIDUAL_DATACNT[3]; /* We are not the last seg */
+	} else {
+		/* If we are the last SG block, tell the hardware. */
+		if ((ahc->bugs & AHC_PCI_MWI_BUG) != 0
+		  && ahc->pci_cachesize != 0) {
+			test	MWI_RESIDUAL, 0xFF jnz dma_mid_sg;
+		}
+		test	SCB_RESIDUAL_DATACNT[3], SG_LAST_SEG jz dma_mid_sg;
+		if ((ahc->flags & AHC_TARGETROLE) != 0) {
+			test	SSTAT0, TARGET jz dma_last_sg;
+			if ((ahc->flags & AHC_TMODE_WIDEODD_BUG) != 0) {
+				test	DMAPARAMS, DIRECTION jz dma_mid_sg;
+			}
+		}
+dma_last_sg:
+		and	DMAPARAMS, ~WIDEODD;
+dma_mid_sg:
+		/* Start DMA data transfer. */
+		mov	DFCNTRL, DMAPARAMS;
+dma_loop:
+		if ((ahc->features & AHC_CMD_CHAN) != 0) {
+			call	idle_loop;
+		}
+		test	SSTAT0,DMADONE	jnz dma_dmadone;
+		test	SSTAT1,PHASEMIS	jz dma_loop;	/* ie. underrun */
+dma_phasemis:
+		/*
+		 * We will be "done" DMAing when the transfer count goes to
+		 * zero, or the target changes the phase (in light of this,
+		 * it makes sense that the DMA circuitry doesn't ACK when
+		 * PHASEMIS is active).  If we are doing a SCSI->Host transfer,
+		 * the data FIFO should be flushed auto-magically on STCNT=0
+		 * or a phase change, so just wait for FIFO empty status.
+		 */
+dma_checkfifo:
+		test	DFCNTRL,DIRECTION	jnz dma_fifoempty;
+dma_fifoflush:
+		test	DFSTATUS,FIFOEMP	jz dma_fifoflush;
+dma_fifoempty:
+		/* Don't clobber an inprogress host data transfer */
+		test	DFSTATUS, MREQPEND	jnz dma_fifoempty;
 
-	test	SG_COUNT, 0xff	jz data_phase_finish; /* Are we done? */
-/*
- * Load a struct scatter and set up the data address and length.
- * If the working value of the SG count is nonzero, then
- * we need to load a new set of values.
- *
- * This, like all DMA's, assumes little-endian host data storage.
- */
-sg_load:
-	if ((p->features & AHC_CMD_CHAN) != 0) {
 		/*
-		 * Do we have any prefetch left???
+		 * Now shut off the DMA and make sure that the DMA
+		 * hardware has actually stopped.  Touching the DMA
+		 * counters, etc. while a DMA is active will result
+		 * in an ILLSADDR exception.
+		 */
+dma_dmadone:
+		and	DFCNTRL, ~(SCSIEN|SDMAEN|HDMAEN);
+dma_halt:
+		/*
+		 * Some revisions of the aic78XX have a problem where, if the
+		 * data fifo is full, but the PCI input latch is not empty, 
+		 * HDMAEN cannot be cleared.  The fix used here is to drain
+		 * the prefetched but unused data from the data fifo until
+		 * there is space for the input latch to drain.
 		 */
-		cmp	CCSGADDR, CCSGADDR_MAX jne prefetched_segs_avail;
+		if ((ahc->bugs & AHC_PCI_2_1_RETRY_BUG) != 0) {
+			mov	NONE, DFDAT;
+		}
+		test	DFCNTRL, (SCSIEN|SDMAEN|HDMAEN) jnz dma_halt;
+
+		/* See if we have completed this last segment */
+		test	STCNT[0], 0xff	jnz data_phase_finish;
+		test	STCNT[1], 0xff	jnz data_phase_finish;
+		test	STCNT[2], 0xff	jnz data_phase_finish;
 
 		/*
-		 * Fetch MIN(CCSGADDR_MAX, (SG_COUNT * 8)) bytes.
+		 * Advance the scatter-gather pointers if needed 
 		 */
-		add	A, -(CCSGRAM_MAXSEGS + 1), SG_COUNT;
-		mvi	A, CCSGADDR_MAX;
-		jc	. + 2;
-		shl	A, 3, SG_COUNT;
-		mov	CCHCNT, A;
-		bmov	CCHADDR, SG_NEXT, 4;
-		mvi	CCSGCTL, CCSGEN|CCSGRESET;
-		test	CCSGCTL, CCSGDONE jz .;
-		and	CCSGCTL, ~CCSGEN;
-		test	CCSGCTL, CCSGEN jnz .;
-		mvi	CCSGCTL, CCSGRESET;
-prefetched_segs_avail:
-		bmov 	HADDR, CCSGRAM, 8;
-		if ((p->features & AHC_ULTRA2) == 0) {
-			bmov    STCNT, HCNT, 3;
+		if ((ahc->bugs & AHC_PCI_MWI_BUG) != 0
+		  && ahc->pci_cachesize != 0) {
+			test	MWI_RESIDUAL, 0xFF jz no_mwi_resid;
+			/*
+			 * Reload HADDR from SHADDR and setup the
+			 * count to be the size of our residual.
+			 */
+			if ((ahc->features & AHC_CMD_CHAN) != 0) {
+				bmov	HADDR, SHADDR, 4;
+				mov	HCNT, MWI_RESIDUAL;
+				bmov	HCNT[1], ALLZEROS, 2;
+			} else {
+				mvi	DINDEX, HADDR;
+				mvi	SHADDR call bcopy_4;
+				mov	MWI_RESIDUAL call set_hcnt;
+			}
+			clr	MWI_RESIDUAL;
+			jmp	sg_load_done;
+no_mwi_resid:
 		}
-	} else {
-		mvi	DINDEX, HADDR;
-		mvi	SG_NEXT	call bcopy_4;
+		test	SCB_RESIDUAL_DATACNT[3], SG_LAST_SEG jz sg_load;
+		or	SCB_RESIDUAL_SGPTR[0], SG_LIST_NULL;
+		jmp	data_phase_finish;
+sg_load:
+		/*
+		 * Load the next SG element's data address and length
+		 * into the DMA engine.  If we don't have hardware
+		 * to perform a prefetch, we'll have to fetch the
+		 * segment from host memory first.
+		 */
+		if ((ahc->features & AHC_CMD_CHAN) != 0) {
+			/* Wait for the idle loop to complete */
+			test	CCSGCTL, CCSGEN jz . + 3;
+			call	idle_loop;
+			test	CCSGCTL, CCSGEN jnz . - 1;
+			bmov 	HADDR, CCSGRAM, 7;
+			test	CCSGRAM, SG_LAST_SEG jz . + 2;
+			or	SCB_RESIDUAL_DATACNT[3], SG_LAST_SEG;
+		} else {
+			mvi	DINDEX, HADDR;
+			mvi	SCB_RESIDUAL_SGPTR	call bcopy_4;
 
-		mvi	HCNT[0],SG_SIZEOF;
-		clr	HCNT[1];
-		clr	HCNT[2];
+			mvi	SG_SIZEOF	call set_hcnt;
 
-		or	DFCNTRL, HDMAEN|DIRECTION|FIFORESET;
+			or	DFCNTRL, HDMAEN|DIRECTION|FIFORESET;
 
-		call	dma_finish;
+			call	dma_finish;
 
-		/*
-		 * Copy data from FIFO into SCB data pointer and data count.
-		 * This assumes that the SG segments are of the form:
-		 * struct ahc_dma_seg {
-		 *	u_int32_t	addr;	four bytes, little-endian order
-		 *	u_int32_t	len;	four bytes, little endian order
-		 * };
-		 */
-		mvi	HADDR	call dfdat_in_7;
-		call	set_stcnt_from_hcnt;
-	}
+			mvi	DINDEX, HADDR;
+			call	dfdat_in_7;
+			mov	SCB_RESIDUAL_DATACNT[3], DFDAT;
+		}
 
-/* Advance the SG pointer */
-	clr	A;			/* add sizeof(struct scatter) */
-	add	SG_NEXT[0],SG_SIZEOF;
-	adc	SG_NEXT[1],A;
+		if ((ahc->bugs & AHC_PCI_MWI_BUG) != 0
+		  && ahc->pci_cachesize != 0) {
+			call calc_mwi_residual;
+		}
 
-	test    SSTAT1, REQINIT jz .;
-	test	SSTAT1,PHASEMIS	jz data_phase_loop;
+		/* Point to the new next sg in memory */
+		call	sg_advance;
 
-/* This drops the last SG segment down to the shadow layer for us */
-	if ((p->features & AHC_ULTRA2) != 0) {
-		mov	DFCNTRL, DMAPARAMS;
-		test	SSTAT0, SDONE	jnz .;
-	}
+sg_load_done:
+		if ((ahc->features & AHC_CMD_CHAN) != 0) {
+			bmov	STCNT, HCNT, 3;
+		} else {
+			call	set_stcnt_from_hcnt;
+		}
+		/* Track odd'ness */
+		test	HCNT[0], 0x1 jz . + 2;
+		xor	DATA_COUNT_ODD, 0x1;
 
+		if ((ahc->flags & AHC_TARGETROLE) != 0) {
+			test	SSTAT0, TARGET jnz data_phase_loop;
+		}
+	}
 data_phase_finish:
-/*
- * After a DMA finishes, save the SG and STCNT residuals back into the SCB
- * We use STCNT instead of HCNT, since it's a reflection of how many bytes 
- * were transferred on the SCSI (as opposed to the host) bus.
- */
-	if ((p->features & AHC_ULTRA2) != 0) {
-		call	ultra2_dmafinish;
-	}
-	if ((p->features & AHC_ULTRA2) == 0) {
-		if ((p->features & AHC_CMD_CHAN) != 0) {
-			bmov    SCB_RESID_DCNT, STCNT, 3;
-			mov	SCB_RESID_SGCNT, SG_COUNT;
-		} else {
-			mov	SCB_RESID_DCNT[0],STCNT[0];
-			mov	SCB_RESID_DCNT[1],STCNT[1];
-			mov	SCB_RESID_DCNT[2],STCNT[2];
-			mov	SCB_RESID_SGCNT, SG_COUNT;
+	/*
+	 * If the target has left us in data phase, loop through
+	 * the dma code again.  In the case of ULTRA2 adapters,
+	 * we should only loop if there is a data overrun.  For
+	 * all other adapters, we'll loop after each S/G element
+	 * is loaded as well as if there is an overrun.
+	 */
+	if ((ahc->flags & AHC_TARGETROLE) != 0) {
+		test	SSTAT0, TARGET jnz data_phase_done;
+	}
+	if ((ahc->flags & AHC_INITIATORROLE) != 0) {
+		test	SSTAT1, REQINIT jz .;
+		test	SSTAT1,PHASEMIS	jz data_phase_loop;
+	
+		if ((ahc->features & AHC_CMD_CHAN) != 0) {
+			/* Kill off any pending prefetch */
+			clr	CCSGCTL;
+			test	CCSGCTL, CCSGEN jnz .;
 		}
 	}
 
-	jmp	ITloop;
+data_phase_done:
+	/*
+	 * After a DMA finishes, save the SG and STCNT residuals back into
+	 * the SCB.  We use STCNT instead of HCNT, since it's a reflection
+	 * of how many bytes were transferred on the SCSI (as opposed to the
+	 * host) bus.
+	 */
+	if ((ahc->features & AHC_CMD_CHAN) != 0) {
+		/* Kill off any pending prefetch */
+		clr	CCSGCTL;
+		test	CCSGCTL, CCSGEN jnz .;
+	}
 
-data_phase_overrun:
-	if ((p->features & AHC_ULTRA2) != 0) {
-		call	ultra2_dmafinish;
+	if ((ahc->bugs & AHC_PCI_MWI_BUG) != 0
+	  && ahc->pci_cachesize != 0) {
+		if ((ahc->features & AHC_CMD_CHAN) != 0) {
+			test	MWI_RESIDUAL, 0xFF jz bmov_resid;
+		}
+		mov	A, MWI_RESIDUAL;
+		add	SCB_RESIDUAL_DATACNT[0], A, STCNT[0];
+		clr	A;
+		adc	SCB_RESIDUAL_DATACNT[1], A, STCNT[1];
+		adc	SCB_RESIDUAL_DATACNT[2], A, STCNT[2];
+		clr	MWI_RESIDUAL;
+		if ((ahc->features & AHC_CMD_CHAN) != 0) {
+			jmp	. + 2;
+bmov_resid:
+			bmov	SCB_RESIDUAL_DATACNT, STCNT, 3;
+		}
+	} else if ((ahc->features & AHC_CMD_CHAN) != 0) {
+		bmov	SCB_RESIDUAL_DATACNT, STCNT, 3;
+	} else {
+		mov	SCB_RESIDUAL_DATACNT[0], STCNT[0];
+		mov	SCB_RESIDUAL_DATACNT[1], STCNT[1];
+		mov	SCB_RESIDUAL_DATACNT[2], STCNT[2];
 	}
-/*
- * Turn off BITBUCKET mode and notify the host
- */
-	and	SXFRCTL1, ~BITBUCKET;
-	mvi	INTSTAT,DATA_OVERRUN;
-	jmp	ITloop;
 
-ultra2_dmafinish:
-	if ((p->features & AHC_ULTRA2) != 0) {
-		test	DFCNTRL, DIRECTION jnz ultra2_dmahalt;
-		and	DFCNTRL, ~SCSIEN;
-		test	DFCNTRL, SCSIEN jnz .;
-ultra2_dmafifoflush:
-		or	DFCNTRL, FIFOFLUSH;
-		test	DFSTATUS, FIFOEMP jz . - 1;
+	/*
+	 * Since we've been through a data phase, the SCB_RESID* fields
+	 * are now initialized.  Clear the full residual flag.
+	 */
+	and	SCB_SGPTR[0], ~SG_FULL_RESID;
+
+	if ((ahc->features & AHC_ULTRA2) != 0) {
+		/* Clear the channel in case we return to data phase later */
+		or	SXFRCTL0, CLRSTCNT|CLRCHN;
+		or	SXFRCTL0, CLRSTCNT|CLRCHN;
+	}
+
+	if ((ahc->flags & AHC_TARGETROLE) != 0) {
+		test	SEQ_FLAGS, DPHASE_PENDING jz ITloop;
+		and	SEQ_FLAGS, ~DPHASE_PENDING;
 		/*
-		 * hardware bug alert!  This needless set of jumps is to
-		 * protect against a FIFOEMP status bit glitch in the
-		 * silicon.
+		 * For data-in phases, wait for any pending acks from the
+		 * initiator before changing phase.
 		 */
-		test	DFSTATUS, FIFOEMP jz ultra2_dmafifoflush;
-		test	DFSTATUS, FIFOEMP jz ultra2_dmafifoflush;
-		test	DFSTATUS, FIFOEMP jz ultra2_dmafifoflush;
-		test	DFSTATUS, FIFOEMP jz ultra2_dmafifoflush;
-		test	DFSTATUS, FIFOEMP jz ultra2_dmafifoflush;
-		test	DFSTATUS, MREQPEND	jnz .;
-ultra2_dmahalt:
-		test	SCSIOFFSET, 0x7f	jnz ultra2_shutdown;
-ultra2_await_nreq:
-		test	SCSISIGI, REQI	jz ultra2_shutdown;
-		test	SSTAT1, (PHASEMIS|REQINIT)	jz ultra2_await_nreq;
-ultra2_shutdown:
-		and     DFCNTRL, ~(HDMAEN|SCSIEN);
-		test	DFCNTRL, (HDMAEN|SCSIEN) jnz .;
-		bmov	SCB_RESID_DCNT, STCNT, 3;
-		mov	SCB_RESID_SGCNT, SG_COUNT;
-		or	SXFRCTL0, CLRSTCNT|CLRCHN;
-		ret;
+		test	DFCNTRL, DIRECTION jz target_ITloop;
+		test	SSTAT1, REQINIT	jnz .;
+		jmp	target_ITloop;
+	} else {
+		jmp	ITloop;
 	}
 
+if ((ahc->flags & AHC_INITIATORROLE) != 0) {
 /*
  * Command phase.  Set up the DMA registers and let 'er rip.
  */
 p_command:
 	call	assert;
 
-/*
- * Load HADDR and HCNT.
- */
-	if ((p->features & AHC_CMD_CHAN) != 0) {
-		bmov	HADDR, SCB_CMDPTR, 5;
+	if ((ahc->features & AHC_ULTRA2) != 0) {
+		bmov	HCNT[0], SCB_CDB_LEN,  1;
 		bmov	HCNT[1], ALLZEROS, 2;
-		if ((p->features & AHC_ULTRA2) == 0) {
-			bmov	STCNT, HCNT, 3;
-		}
+		mvi	SG_CACHE_PRE, LAST_SEG;
+	} else if ((ahc->features & AHC_CMD_CHAN) != 0) {
+		bmov	STCNT[0], SCB_CDB_LEN, 1;
+		bmov	STCNT[1], ALLZEROS, 2;
 	} else {
-		mvi	DINDEX, HADDR;
-		mvi	SCB_CMDPTR	call bcopy_5;
-		clr	HCNT[1];
-		clr	HCNT[2];
-		call	set_stcnt_from_hcnt;
+		mov	STCNT[0], SCB_CDB_LEN;
+		clr	STCNT[1];
+		clr	STCNT[2];
+	}
+	add	NONE, -13, SCB_CDB_LEN;
+	mvi	SCB_CDB_STORE jnc p_command_embedded;
+p_command_from_host:
+	if ((ahc->features & AHC_ULTRA2) != 0) {
+		bmov	HADDR[0], SCB_CDB_PTR, 4;
+		mvi	DFCNTRL, (PRELOADEN|SCSIEN|HDMAEN|DIRECTION);
+	} else {
+		if ((ahc->features & AHC_CMD_CHAN) != 0) {
+			bmov	HADDR[0], SCB_CDB_PTR, 4;
+			bmov	HCNT, STCNT, 3;
+		} else {
+			mvi	DINDEX, HADDR;
+			mvi	SCB_CDB_PTR call bcopy_4;
+			mov	SCB_CDB_LEN call set_hcnt;
+		}
+		mvi	DFCNTRL, (SCSIEN|SDMAEN|HDMAEN|DIRECTION|FIFORESET);
 	}
-
-	if ((p->features & AHC_ULTRA2) == 0) {
-		mvi	(SCSIEN|SDMAEN|HDMAEN|DIRECTION|FIFORESET) call dma;
+	jmp	p_command_loop;
+p_command_embedded:
+	/*
+	 * The data fifo seems to require 4 byte alligned
+	 * transfers from the sequencer.  Force this to
+	 * be the case by clearing HADDR[0] even though
+	 * we aren't going to touch host memeory.
+	 */
+	clr	HADDR[0];
+	if ((ahc->features & AHC_ULTRA2) != 0) {
+		mvi	DFCNTRL, (PRELOADEN|SCSIEN|DIRECTION);
+		bmov	DFDAT, SCB_CDB_STORE, 12; 
+	} else if ((ahc->features & AHC_CMD_CHAN) != 0) {
+		if ((ahc->flags & AHC_SCB_BTT) != 0) {
+			/*
+			 * On the 7895 the data FIFO will
+			 * get corrupted if you try to dump
+			 * data from external SCB memory into
+			 * the FIFO while it is enabled.  So,
+			 * fill the fifo and then enable SCSI
+			 * transfers.
+			 */
+			mvi	DFCNTRL, (DIRECTION|FIFORESET);
+		} else {
+			mvi	DFCNTRL, (SCSIEN|SDMAEN|DIRECTION|FIFORESET);
+		}
+		bmov	DFDAT, SCB_CDB_STORE, 12; 
+		if ((ahc->flags & AHC_SCB_BTT) != 0) {
+			mvi	DFCNTRL, (SCSIEN|SDMAEN|DIRECTION|FIFOFLUSH);
+		} else {
+			or	DFCNTRL, FIFOFLUSH;
+		}
 	} else {
-		mvi	DFCNTRL, (PRELOADEN|SCSIEN|HDMAEN|DIRECTION);
-		test	SSTAT0, SDONE jnz .;
-p_command_dma_loop:
-		test	SSTAT0, SDONE jnz p_command_ultra2_dma_done;
-		test	SSTAT1,PHASEMIS	jz p_command_dma_loop;	/* ie. underrun */
-p_command_ultra2_dma_done:
-		test	SCSISIGI, REQI	jz p_command_ultra2_shutdown;
-		test	SSTAT1, (PHASEMIS|REQINIT)	jz p_command_ultra2_dma_done;
-p_command_ultra2_shutdown:
-		and     DFCNTRL, ~(HDMAEN|SCSIEN);
-		test	DFCNTRL, (HDMAEN|SCSIEN) jnz .;
-		or	SXFRCTL0, CLRSTCNT|CLRCHN;
+		mvi	DFCNTRL, (SCSIEN|SDMAEN|DIRECTION|FIFORESET);
+		call	copy_to_fifo_6;
+		call	copy_to_fifo_6;
+		or	DFCNTRL, FIFOFLUSH;
+	}
+p_command_loop:
+	test	SSTAT0, SDONE jnz . + 2;
+	test    SSTAT1, PHASEMIS jz p_command_loop;
+	/*
+	 * Wait for our ACK to go-away on it's own
+	 * instead of being killed by SCSIEN getting cleared.
+	 */
+	test	SCSISIGI, ACKI jnz .;
+	and	DFCNTRL, ~(SCSIEN|SDMAEN|HDMAEN);
+	test	DFCNTRL, (SCSIEN|SDMAEN|HDMAEN) jnz .;
+	if ((ahc->features & AHC_ULTRA2) != 0) {
+		/* Drop any residual from the S/G Preload queue */
+		or	SXFRCTL0, CLRSTCNT;
 	}
 	jmp	ITloop;
 
@@ -632,21 +1312,26 @@
 p_status:
 	call	assert;
 
-	mov	SCB_TARGET_STATUS, SCSIDATL;
+	mov	SCB_SCSI_STATUS, SCSIDATL;
 	jmp	ITloop;
 
 /*
- * Message out phase.  If MSG_OUT is 0x80, build I full indentify message
- * sequence and send it to the target.  In addition, if the MK_MESSAGE bit
- * is set in the SCB_CONTROL byte, interrupt the host and allow it to send
- * it's own message.
+ * Message out phase.  If MSG_OUT is MSG_IDENTIFYFLAG, build a full
+ * indentify message sequence and send it to the target.  The host may
+ * override this behavior by setting the MK_MESSAGE bit in the SCB
+ * control byte.  This will cause us to interrupt the host and allow
+ * it to handle the message phase completely on its own.  If the bit
+ * associated with this target is set, we will also interrupt the host,
+ * thereby allowing it to send a message on the next selection regardless
+ * of the transaction being sent.
  * 
  * If MSG_OUT is == HOST_MSG, also interrupt the host and take a message.
- * This is done to allow the hsot to send messages outside of an identify
+ * This is done to allow the host to send messages outside of an identify
  * sequence while protecting the seqencer from testing the MK_MESSAGE bit
  * on an SCB that might not be for the current nexus. (For example, a
  * BDR message in responce to a bad reselection would leave us pointed to
  * an SCB that doesn't have anything to do with the current target).
+ *
  * Otherwise, treat MSG_OUT as a 1 byte message to send (abort, abort tag,
  * bus device reset).
  *
@@ -655,23 +1340,27 @@
  * reason.
  */
 p_mesgout_retry:
-	or      SCSISIGO,ATNO,LASTPHASE;/* turn on ATN for the retry */
+	or	SCSISIGO,ATNO,LASTPHASE;/* turn on ATN for the retry */
 p_mesgout:
 	mov	SINDEX, MSG_OUT;
 	cmp	SINDEX, MSG_IDENTIFYFLAG jne p_mesgout_from_host;
-p_mesgout_identify:
-	if ((p->features & AHC_WIDE) != 0) {
-		and	SINDEX,0xf,SCB_TCL;	/* lun */
-	} else {
-		and	SINDEX,0x7,SCB_TCL;	/* lun */
+	test	SCB_CONTROL,MK_MESSAGE	jnz host_message_loop;
+	mov	FUNCTION1, SCB_SCSIID;
+	mov	A, FUNCTION1;
+	mov	SINDEX, TARGET_MSG_REQUEST[0];
+	if ((ahc->features & AHC_TWIN) != 0) {
+		/* Second Channel uses high byte bits */
+		test	SCB_SCSIID, TWIN_CHNLB jz . + 2;
+		mov	SINDEX, TARGET_MSG_REQUEST[1];
+	} else if ((ahc->features & AHC_WIDE) != 0) {
+		test	SCB_SCSIID, 0x80	jz . + 2; /* target > 7 */
+		mov	SINDEX, TARGET_MSG_REQUEST[1];
 	}
-	and	A,DISCENB,SCB_CONTROL;	/* mask off disconnect privledge */
-	or	SINDEX,A;		/* or in disconnect privledge */
-	or	SINDEX,MSG_IDENTIFYFLAG;
-p_mesgout_mk_message:
-	test	SCB_CONTROL,MK_MESSAGE  jz p_mesgout_tag;
-	mov	SCSIDATL, SINDEX;	/* Send the last byte */
-	jmp	p_mesgout_from_host + 1;/* Skip HOST_MSG test */
+	test	SINDEX, A	jnz host_message_loop;
+p_mesgout_identify:
+	or	SINDEX, MSG_IDENTIFYFLAG|DISCENB, SCB_LUN;
+	test	SCB_CONTROL, DISCENB jnz . + 2;
+	and	SINDEX, ~DISCENB;
 /*
  * Send a tag message if TAG_ENB is set in the SCB control block.
  * Use SCB_TAG (the position in the kernel's SCB array) as the tag value.
@@ -686,34 +1375,27 @@
 	cmp	LASTPHASE, P_MESGOUT	jne p_mesgout_done;
 	mov	SCB_TAG	jmp p_mesgout_onebyte;
 /*
- * Interrupt the driver, and allow it to send a message
- * if it asks.
+ * Interrupt the driver, and allow it to handle this message
+ * phase and any required retries.
  */
 p_mesgout_from_host:
 	cmp	SINDEX, HOST_MSG	jne p_mesgout_onebyte;
-	mvi     INTSTAT,AWAITING_MSG;
-	nop;
-	/*
-	 * Did the host detect a phase change?
-	 */
-	cmp	RETURN_1, MSGOUT_PHASEMIS je p_mesgout_done;
+	jmp	host_message_loop;
 
 p_mesgout_onebyte:
 	mvi	CLRSINT1, CLRATNO;
 	mov	SCSIDATL, SINDEX;
 
 /*
- * If the next bus phase after ATN drops is a message out, it means
+ * If the next bus phase after ATN drops is message out, it means
  * that the target is requesting that the last message(s) be resent.
  */
 	call	phase_lock;
-	cmp     LASTPHASE, P_MESGOUT    je p_mesgout_retry;
+	cmp	LASTPHASE, P_MESGOUT	je p_mesgout_retry;
 
 p_mesgout_done:
 	mvi	CLRSINT1,CLRATNO;	/* Be sure to turn ATNO off */
 	mov	LAST_MSG, MSG_OUT;
-	cmp	MSG_OUT, MSG_IDENTIFYFLAG jne . + 2;
-	and	SCB_CONTROL, ~MK_MESSAGE;
 	mvi	MSG_OUT, MSG_NOOP;	/* No message left */
 	jmp	ITloop;
 
@@ -728,113 +1410,136 @@
 	cmp	A,MSG_SAVEDATAPOINTER	je mesgin_sdptrs;
 	cmp	ALLZEROS,A		je mesgin_complete;
 	cmp	A,MSG_RESTOREPOINTERS	je mesgin_rdptrs;
-	cmp	A,MSG_EXTENDED		je mesgin_extended;
-	cmp	A,MSG_MESSAGE_REJECT	je mesgin_reject;
+	cmp	A,MSG_IGN_WIDE_RESIDUE	je mesgin_ign_wide_residue;
 	cmp	A,MSG_NOOP		je mesgin_done;
-	cmp	A,MSG_IGN_WIDE_RESIDUE	je mesgin_wide_residue;
 
-rej_mesgin:
 /*
- * We have no idea what this message in is, so we issue a message reject
- * and hope for the best.  In any case, rejection should be a rare
- * occurrence - signal the driver when it happens.
+ * Pushed message loop to allow the kernel to
+ * run it's own message state engine.  To avoid an
+ * extra nop instruction after signaling the kernel,
+ * we perform the phase_lock before checking to see
+ * if we should exit the loop and skip the phase_lock
+ * in the ITloop.  Performing back to back phase_locks
+ * shouldn't hurt, but why do it twice...
  */
-	mvi	INTSTAT,SEND_REJECT;		/* let driver know */
+host_message_loop:
+	mvi	HOST_MSG_LOOP call set_seqint;
+	call	phase_lock;
+	cmp	RETURN_1, EXIT_MSG_LOOP	je ITloop + 1;
+	jmp	host_message_loop;
 
-	mvi	MSG_MESSAGE_REJECT	call mk_mesg;
+mesgin_ign_wide_residue:
+if ((ahc->features & AHC_WIDE) != 0) {
+	test	SCSIRATE, WIDEXFER jz mesgin_reject;
+	/* Pull the residue byte */
+	mvi	ARG_1	call inb_next;
+	cmp	ARG_1, 0x01 jne mesgin_reject;
+	test	SCB_RESIDUAL_SGPTR[0], SG_LIST_NULL jz . + 2;
+	test	DATA_COUNT_ODD, 0x1	jz mesgin_done;
+	mvi	IGN_WIDE_RES call set_seqint;
+	jmp	mesgin_done;
+}
 
+mesgin_reject:
+	mvi	MSG_MESSAGE_REJECT	call mk_mesg;
 mesgin_done:
 	mov	NONE,SCSIDATL;		/*dummy read from latch to ACK*/
 	jmp	ITloop;
 
-
 mesgin_complete:
 /*
- * We got a "command complete" message, so put the SCB_TAG into the QOUTFIFO,
+ * We received a "command complete" message.  Put the SCB_TAG into the QOUTFIFO,
  * and trigger a completion interrupt.  Before doing so, check to see if there
  * is a residual or the status byte is something other than STATUS_GOOD (0).
  * In either of these conditions, we upload the SCB back to the host so it can
  * process this information.  In the case of a non zero status byte, we 
  * additionally interrupt the kernel driver synchronously, allowing it to
  * decide if sense should be retrieved.  If the kernel driver wishes to request
- * sense, it will fill the kernel SCB with a request sense command and set
- * RETURN_1 to SEND_SENSE.  If RETURN_1 is set to SEND_SENSE we redownload
- * the SCB, and process it as the next command by adding it to the waiting list.
- * If the kernel driver does not wish to request sense, it need only clear
- * RETURN_1, and the command is allowed to complete normally.  We don't bother
- * to post to the QOUTFIFO in the error cases since it would require extra
- * work in the kernel driver to ensure that the entry was removed before the
- * command complete code tried processing it.
+ * sense, it will fill the kernel SCB with a request sense command, requeue
+ * it to the QINFIFO and tell us not to post to the QOUTFIFO by setting 
+ * RETURN_1 to SEND_SENSE.
+ */
+
+/*
+ * If ATN is raised, we still want to give the target a message.
+ * Perhaps there was a parity error on this last message byte.
+ * Either way, the target should take us to message out phase
+ * and then attempt to complete the command again.
+ * XXX - Need a critical section to do this corrctly.  Wait until
+ *       we queue completions.
+	test	SCSISIGI, ATNI jnz mesgin_done;
+ */
+
+/*
+ * See if we attempted to deliver a message but the target ingnored us.
  */
+	test	SCB_CONTROL, MK_MESSAGE jz . + 2;
+	mvi	MKMSG_FAILED call set_seqint;
 
 /*
- * First check for residuals
+ * Check for residuals
  */
-	test	SCB_RESID_SGCNT,0xff	jnz upload_scb;
-	test	SCB_TARGET_STATUS,0xff	jz complete;	/* Good Status? */
+	test	SCB_SGPTR, SG_LIST_NULL jnz check_status;/* No xfer */
+	test	SCB_SGPTR, SG_FULL_RESID jnz upload_scb;/* Never xfered */
+	test	SCB_RESIDUAL_SGPTR, SG_LIST_NULL jz upload_scb;
+check_status:
+	test	SCB_SCSI_STATUS,0xff	jz complete;	/* Good Status? */
 upload_scb:
+	or	SCB_SGPTR, SG_RESID_VALID;
 	mvi	DMAPARAMS, FIFORESET;
 	mov	SCB_TAG		call dma_scb;
-check_status:
-	test	SCB_TARGET_STATUS,0xff	jz complete;	/* Just a residual? */
-	mvi	INTSTAT,BAD_STATUS;			/* let driver know */
-	nop;
+	test	SCB_SCSI_STATUS, 0xff	jz complete;	/* Just a residual? */
+	mvi	BAD_STATUS call set_seqint;		/* let driver know */
 	cmp	RETURN_1, SEND_SENSE	jne complete;
-	/* This SCB becomes the next to execute as it will retrieve sense */
-	mvi	DMAPARAMS, HDMAEN|DIRECTION|FIFORESET;
-	mov	SCB_TAG		call dma_scb;
-add_to_waiting_list:
-	mov	SCB_NEXT,WAITING_SCBH;
-	mov	WAITING_SCBH, SCBPTR;
-	/*
-	 * Prepare our selection hardware before the busfree so we have a
-	 * high probability of winning arbitration.
-	 */
-	call	start_selection;
+	call	add_scb_to_free_list;
 	jmp	await_busfree;
-
 complete:
-	/* If we are untagged, clear our address up in host ram */
-	test	SCB_CONTROL, TAG_ENB jnz complete_post;
-	mov	A, SAVED_TCL;
-	mvi	UNTAGGEDSCB_OFFSET call post_byte_setup;
-	mvi	SCB_LIST_NULL call post_byte;
+	mov	SCB_TAG call complete_post;
+	jmp	await_busfree;
+}
 
 complete_post:
-	/* Post the SCB and issue an interrupt */
-	if ((p->features & AHC_QUEUE_REGS) != 0) {
+	/* Post the SCBID in SINDEX and issue an interrupt */
+	call	add_scb_to_free_list;
+	mov	ARG_1, SINDEX;
+	if ((ahc->features & AHC_QUEUE_REGS) != 0) {
 		mov	A, SDSCB_QOFF;
 	} else {
 		mov	A, QOUTPOS;
 	}
 	mvi	QOUTFIFO_OFFSET call post_byte_setup;
-	mov	SCB_TAG call post_byte;
-	if ((p->features & AHC_QUEUE_REGS) == 0) {
+	mov	ARG_1 call post_byte;
+	if ((ahc->features & AHC_QUEUE_REGS) == 0) {
 		inc 	QOUTPOS;
 	}
-	mvi	INTSTAT,CMDCMPLT;
-
-add_to_free_list:
-	call	add_scb_to_free_list;
-	jmp	await_busfree;
-
-/*
- * Is it an extended message?  Copy the message to our message buffer and
- * notify the host.  The host will tell us whether to reject this message,
- * respond to it with the message that the host placed in our message buffer,
- * or simply to do nothing.
- */
-mesgin_extended:
-	mvi	INTSTAT,EXTENDED_MSG;		/* let driver know */
-	jmp	ITloop;
+	mvi	INTSTAT,CMDCMPLT ret;
 
+if ((ahc->flags & AHC_INITIATORROLE) != 0) {
 /*
  * Is it a disconnect message?  Set a flag in the SCB to remind us
- * and await the bus going free.
+ * and await the bus going free.  If this is an untagged transaction
+ * store the SCB id for it in our untagged target table for lookup on
+ * a reselction.
  */
 mesgin_disconnect:
+	/*
+	 * If ATN is raised, we still want to give the target a message.
+	 * Perhaps there was a parity error on this last message byte
+	 * or we want to abort this command.  Either way, the target
+	 * should take us to message out phase and then attempt to
+	 * disconnect again.
+	 * XXX - Wait for more testing.
+	test	SCSISIGI, ATNI jnz mesgin_done;
+	 */
+
 	or	SCB_CONTROL,DISCONNECTED;
-	call	add_scb_to_disc_list;
+	if ((ahc->flags & AHC_PAGESCBS) != 0) {
+		call	add_scb_to_disc_list;
+	}
+	test	SCB_CONTROL, TAG_ENB jnz await_busfree;
+	mov	ARG_1, SCB_TAG;
+	mov	SAVED_LUN, SCB_LUN;
+	mov	SCB_SCSIID	call set_busy_target;
 	jmp	await_busfree;
 
 /*
@@ -846,22 +1551,20 @@
  */
 mesgin_sdptrs:
 	test	SEQ_FLAGS, DPHASE	jz mesgin_done;
+
 	/*
-	 * The SCB SGPTR becomes the next one we'll download,
-	 * and the SCB DATAPTR becomes the current SHADDR.
+	 * The SCB_SGPTR becomes the next one we'll download,
+	 * and the SCB_DATAPTR becomes the current SHADDR.
 	 * Use the residual number since STCNT is corrupted by
 	 * any message transfer.
 	 */
-	if ((p->features & AHC_CMD_CHAN) != 0) {
-		bmov    SCB_SGCOUNT, SG_COUNT, 5;
-		bmov    SCB_DATAPTR, SHADDR, 4;
-		bmov    SCB_DATACNT, SCB_RESID_DCNT, 3;
+	if ((ahc->features & AHC_CMD_CHAN) != 0) {
+		bmov	SCB_DATAPTR, SHADDR, 4;
+		bmov	SCB_DATACNT, SCB_RESIDUAL_DATACNT, 8;
 	} else {
-		mvi	DINDEX, SCB_SGCOUNT;
-		mvi	SG_COUNT	call bcopy_5;
 		mvi	DINDEX, SCB_DATAPTR;
-		mvi	SHADDR		call bcopy_4;
-		mvi	SCB_RESID_DCNT	call	bcopy_3;
+		mvi	SHADDR call bcopy_4;
+		mvi	SCB_RESIDUAL_DATACNT call bcopy_8;
 	}
 	jmp	mesgin_done;
 
@@ -880,109 +1583,147 @@
 	jmp	mesgin_done;
 
 /*
+ * Index into our Busy Target table.  SINDEX and DINDEX are modified
+ * upon return.  SCBPTR may be modified by this action.
+ */
+set_busy_target:
+	shr	DINDEX, 4, SINDEX;
+	if ((ahc->flags & AHC_SCB_BTT) != 0) {
+		mov	SCBPTR, SAVED_LUN;
+		add	DINDEX, SCB_64_BTT;
+	} else {
+		add	DINDEX, BUSY_TARGETS;
+	}
+	mov	DINDIR, ARG_1 ret;
+
+/*
  * Identify message?  For a reconnecting target, this tells us the lun
  * that the reconnection is for - find the correct SCB and switch to it,
  * clearing the "disconnected" bit so we don't "find" it by accident later.
  */
 mesgin_identify:
-	
-	if ((p->features & AHC_WIDE) != 0) {
-		and	A,0x0f;		/* lun in lower four bits */
+	/*
+	 * Determine whether a target is using tagged or non-tagged
+	 * transactions by first looking at the transaction stored in
+	 * the busy target array.  If there is no untagged transaction
+	 * for this target or the transaction is for a different lun, then
+	 * this must be an untagged transaction.
+	 */
+	shr	SINDEX, 4, SELID;
+	and	SAVED_LUN, MSG_IDENTIFY_LUNMASK, A;
+	if ((ahc->flags & AHC_SCB_BTT) != 0) {
+		add	SINDEX, SCB_64_BTT;
+		mov	SCBPTR, SAVED_LUN;
+		if ((ahc->flags & AHC_SEQUENCER_DEBUG) != 0) {
+			add	NONE, -SCB_64_BTT, SINDEX;
+			jc	. + 2;
+			mvi	INTSTAT, OUT_OF_RANGE;
+			nop;
+			add	NONE, -(SCB_64_BTT + 16), SINDEX;
+			jnc	. + 2;
+			mvi	INTSTAT, OUT_OF_RANGE;
+			nop;
+		}
 	} else {
-		and	A,0x07;		/* lun in lower three bits */
+		add	SINDEX, BUSY_TARGETS;
+		if ((ahc->flags & AHC_SEQUENCER_DEBUG) != 0) {
+			add	NONE, -BUSY_TARGETS, SINDEX;
+			jc	. + 2;
+			mvi	INTSTAT, OUT_OF_RANGE;
+			nop;
+			add	NONE, -(BUSY_TARGETS + 16), SINDEX;
+			jnc	. + 2;
+			mvi	INTSTAT, OUT_OF_RANGE;
+			nop;
+		}
 	}
-	or      SAVED_TCL,A;		/* SAVED_TCL should be complete now */
-
-	mvi     ARG_2, SCB_LIST_NULL;   /* SCBID of prev SCB in disc List */
-	call	get_untagged_SCBID;
+	mov	ARG_1, SINDIR;
 	cmp	ARG_1, SCB_LIST_NULL	je snoop_tag;
-	if ((p->flags & AHC_PAGESCBS) != 0) {
-		test	SEQ_FLAGS, SCBPTR_VALID	jz use_retrieveSCB;
+	if ((ahc->flags & AHC_PAGESCBS) != 0) {
+		mov	ARG_1 call findSCB;
+	} else {
+		mov	SCBPTR, RETURN_1;
 	}
-	/*
-	 * If the SCB was found in the disconnected list (as is
-	 * always the case in non-paging scenarios), SCBPTR is already
-	 * set to the correct SCB.  So, simply setup the SCB and get
-	 * on with things.
-	 */
-	mov	SCBPTR	call rem_scb_from_disc_list;
-	jmp	setup_SCB;
+	if ((ahc->flags & AHC_SCB_BTT) != 0) {
+		jmp setup_SCB_id_lun_okay;
+	} else {
+		jmp	setup_SCB_id_okay;
+	}
+
 /*
  * Here we "snoop" the bus looking for a SIMPLE QUEUE TAG message.
  * If we get one, we use the tag returned to find the proper
- * SCB.  With SCB paging, this requires using search for both tagged
- * and non-tagged transactions since the SCB may exist in any slot.
- * If we're not using SCB paging, we can use the tag as the direct
- * index to the SCB.
+ * SCB.  With SCB paging, we must search for non-tagged
+ * transactions since the SCB may exist in any slot.  If we're not
+ * using SCB paging, we can use the tag as the direct index to the
+ * SCB.
  */
 snoop_tag:
+	if ((ahc->flags & AHC_SEQUENCER_DEBUG) != 0) {
+		or	SEQ_FLAGS, 0x80;
+	}
 	mov	NONE,SCSIDATL;		/* ACK Identify MSG */
-snoop_tag_loop:
 	call	phase_lock;
+	if ((ahc->flags & AHC_SEQUENCER_DEBUG) != 0) {
+		or	SEQ_FLAGS, 0x1;
+	}
 	cmp	LASTPHASE, P_MESGIN	jne not_found;
+	if ((ahc->flags & AHC_SEQUENCER_DEBUG) != 0) {
+		or	SEQ_FLAGS, 0x2;
+	}
 	cmp	SCSIBUSL,MSG_SIMPLE_Q_TAG jne not_found;
 get_tag:
-	mvi	ARG_1	call inb_next;	/* tag value */
+	if ((ahc->flags & AHC_PAGESCBS) != 0) {
+		mvi	ARG_1	call inb_next;	/* tag value */
+		mov	ARG_1	call findSCB;
+	} else {
+		mvi	ARG_1	call inb_next;	/* tag value */
+		mov	SCBPTR, ARG_1;
+	}
 
-use_retrieveSCB:
-	call	retrieveSCB;
+/*
+ * Ensure that the SCB the tag points to is for
+ * an SCB transaction to the reconnecting target.
+ */
 setup_SCB:
-	mov	A, SAVED_TCL;
-	cmp	SCB_TCL, A	jne not_found_cleanup_scb;
+	if ((ahc->flags & AHC_SEQUENCER_DEBUG) != 0) {
+		or	SEQ_FLAGS, 0x4;
+	}
+	mov	A, SCB_SCSIID;
+	cmp	SAVED_SCSIID, A	jne not_found_cleanup_scb;
+	if ((ahc->flags & AHC_SEQUENCER_DEBUG) != 0) {
+		or	SEQ_FLAGS, 0x8;
+	}
+setup_SCB_id_okay:
+	mov	A, SCB_LUN;
+	cmp	SAVED_LUN, A	jne not_found_cleanup_scb;
+setup_SCB_id_lun_okay:
+	if ((ahc->flags & AHC_SEQUENCER_DEBUG) != 0) {
+		or	SEQ_FLAGS, 0x10;
+	}
 	test	SCB_CONTROL,DISCONNECTED jz not_found_cleanup_scb;
 	and	SCB_CONTROL,~DISCONNECTED;
-	or	SEQ_FLAGS,IDENTIFY_SEEN;	  /* make note of IDENTIFY */
+	test	SCB_CONTROL, TAG_ENB	jnz setup_SCB_tagged;
+	mov	A, SCBPTR;
+	mvi	ARG_1, SCB_LIST_NULL;
+	mov	SAVED_SCSIID	call	set_busy_target;
+	mov	SCBPTR, A;
+setup_SCB_tagged:
+	mvi	SEQ_FLAGS,IDENTIFY_SEEN;	/* make note of IDENTIFY */
+	call	set_transfer_settings;
 	/* See if the host wants to send a message upon reconnection */
 	test	SCB_CONTROL, MK_MESSAGE jz mesgin_done;
-	and	SCB_CONTROL, ~MK_MESSAGE;
 	mvi	HOST_MSG	call mk_mesg;
 	jmp	mesgin_done;
 
 not_found_cleanup_scb:
-	test	SCB_CONTROL, DISCONNECTED jz . + 3;
-	call	add_scb_to_disc_list;
-	jmp	not_found;
-	call	add_scb_to_free_list;
+	if ((ahc->flags & AHC_PAGESCBS) != 0) {
+		call	add_scb_to_free_list;
+	}
 not_found:
-	mvi	INTSTAT, NO_MATCH;
-	mvi	MSG_BUS_DEV_RESET	call mk_mesg;
-	jmp	mesgin_done;
-
-/*
- * Message reject?  Let the kernel driver handle this.  If we have an 
- * outstanding WDTR or SDTR negotiation, assume that it's a response from 
- * the target selecting 8bit or asynchronous transfer, otherwise just ignore 
- * it since we have no clue what it pertains to.
- */
-mesgin_reject:
-	mvi	INTSTAT, REJECT_MSG;
-	jmp	mesgin_done;
-
-/*
- * Wide Residue.  We handle the simple cases, but pass of the one hard case
- * to the kernel (when the residue byte happened to cause us to advance our
- * sg element array, so we know have to back that advance out).
- */
-mesgin_wide_residue:
-	mvi	ARG_1	call inb_next; /* ACK the wide_residue and get */
-				       /* the size byte */
-/*
- * In order for this to be reliable, we have to do all sorts of horrible
- * magic in terms of resetting the datafifo and reloading the shadow layer
- * with the correct new values (so that a subsequent save data pointers
- * message will do the right thing).  We let the kernel do that work.
- */
- 	mvi	INTSTAT, WIDE_RESIDUE;
+	mvi	NO_MATCH call set_seqint;
 	jmp	mesgin_done;
-	
-/*
- * [ ADD MORE MESSAGE HANDLING HERE ]
- */
 
-/*
- * Locking the driver out, build a one-byte message passed in SINDEX
- * if there is no active message already.  SINDEX is returned intact.
- */
 mk_mesg:
 	or	SCSISIGO,ATNO,LASTPHASE;/* turn on ATNO */
 	mov	MSG_OUT,SINDEX ret;
@@ -1002,7 +1743,9 @@
  * and that REQ is already set when inb_first is called.  inb_{first,next}
  * use the same calling convention as inb.
  */
-
+inb_next_wait_perr:
+	mvi	PERR_DETECTED call set_seqint;
+	jmp	inb_next_wait;
 inb_next:
 	mov	NONE,SCSIDATL;		/*dummy read from latch to ACK*/
 inb_next_wait:
@@ -1012,7 +1755,8 @@
 	 * before continuing.
 	 */
 	test	SSTAT1, REQINIT	jz inb_next_wait;
-	test	SSTAT1, SCSIPERR jnz .;
+	test	SSTAT1, SCSIPERR jnz inb_next_wait_perr;
+inb_next_check_phase:
 	and	LASTPHASE, PHASE_MASK, SCSISIGI;
 	cmp	LASTPHASE, P_MESGIN jne mesgin_phasemis;
 inb_first:
@@ -1020,71 +1764,48 @@
 	mov	DINDIR,SCSIBUSL	ret;		/*read byte directly from bus*/
 inb_last:
 	mov	NONE,SCSIDATL ret;		/*dummy read from latch to ACK*/
+}
 
-mesgin_phasemis:
+if ((ahc->flags & AHC_TARGETROLE) != 0) {
 /*
- * We expected to receive another byte, but the target changed phase
+ * Change to a new phase.  If we are changing the state of the I/O signal,
+ * from out to in, wait an additional data release delay before continuing.
  */
-	mvi	INTSTAT, MSGIN_PHASEMIS;
-	jmp	ITloop;
-
-/*
- * DMA data transfer.  HADDR and HCNT must be loaded first, and
- * SINDEX should contain the value to load DFCNTRL with - 0x3d for
- * host->scsi, or 0x39 for scsi->host.  The SCSI channel is cleared
- * during initialization.
- */
-if ((p->features & AHC_ULTRA2) == 0) {
-dma:
-	mov	DFCNTRL,SINDEX;
-dma_loop:
-	test	SSTAT0,DMADONE	jnz dma_dmadone;
-	test	SSTAT1,PHASEMIS	jz dma_loop;	/* ie. underrun */
-dma_phasemis:
-	test	SSTAT0,SDONE	jnz dma_checkfifo;
-	mov	SINDEX,ALLZEROS;		/* Notify caller of phasemiss */
+change_phase:
+	/* Wait for preceeding I/O session to complete. */
+	test	SCSISIGI, ACKI jnz .;
+
+	/* Change the phase */
+	and	DINDEX, IOI, SCSISIGI;
+	mov	SCSISIGO, SINDEX;
+	and	A, IOI, SINDEX;
 
-/*
- * We will be "done" DMAing when the transfer count goes to zero, or
- * the target changes the phase (in light of this, it makes sense that
- * the DMA circuitry doesn't ACK when PHASEMIS is active).  If we are
- * doing a SCSI->Host transfer, the data FIFO should be flushed auto-
- * magically on STCNT=0 or a phase change, so just wait for FIFO empty
- * status.
- */
-dma_checkfifo:
-	test	DFCNTRL,DIRECTION	jnz dma_fifoempty;
-dma_fifoflush:
-	test	DFSTATUS,FIFOEMP	jz dma_fifoflush;
+	/*
+	 * If the data direction has changed, from
+	 * out (initiator driving) to in (target driving),
+	 * we must wait at least a data release delay plus
+	 * the normal bus settle delay. [SCSI III SPI 10.11.0]
+	 */
+	cmp 	DINDEX, A je change_phase_wait;
+	test	SINDEX, IOI jz change_phase_wait;
+	call	change_phase_wait;
+change_phase_wait:
+	nop;
+	nop;
+	nop;
+	nop ret;
 
-dma_fifoempty:
-	/* Don't clobber an inprogress host data transfer */
-	test	DFSTATUS, MREQPEND	jnz dma_fifoempty;
 /*
- * Now shut the DMA enables off and make sure that the DMA enables are 
- * actually off first lest we get an ILLSADDR.
+ * Send a byte to an initiator in Automatic PIO mode.
  */
-dma_dmadone:
-	cmp	LASTPHASE, P_COMMAND	je dma_await_nreq;
-	test	SCSIRATE, 0x0f	jnz dma_shutdown;
-dma_await_nreq:
-	test	SCSISIGI, REQI	jz dma_shutdown;
-	test	SSTAT1, (PHASEMIS|REQINIT)	jz dma_await_nreq;
-dma_shutdown:
-	and	DFCNTRL, ~(SCSIEN|SDMAEN|HDMAEN);
-dma_halt:
-	/*
-	 * Some revisions of the aic7880 have a problem where, if the
-	 * data fifo is full, but the PCI input latch is not empty, 
-	 * HDMAEN cannot be cleared.  The fix used here is to attempt
-	 * to drain the data fifo until there is space for the input
-	 * latch to drain and HDMAEN de-asserts.
-	 */
-	mov	NONE, DFDAT;
-	test	DFCNTRL, (SCSIEN|SDMAEN|HDMAEN) jnz dma_halt;
+target_outb:
+	or	SXFRCTL0, SPIOEN;
+	test	SSTAT0, SPIORDY	jz .;
+	mov	SCSIDATL, SINDEX;
+	test	SSTAT0, SPIORDY	jz .;
+	and	SXFRCTL0, ~SPIOEN ret;
 }
-return:
-	ret;
+	
 
 /*
  * Assert that if we've been reselected, then we've seen an IDENTIFY
@@ -1093,138 +1814,69 @@
 assert:
 	test	SEQ_FLAGS,IDENTIFY_SEEN	jnz return;	/* seen IDENTIFY? */
 
-	mvi	INTSTAT,NO_IDENT 	ret;	/* no - tell the kernel */
+	mvi	NO_IDENT jmp set_seqint;	/* no - tell the kernel */
 
 /*
- * Locate a disconnected SCB either by SAVED_TCL (ARG_1 is SCB_LIST_NULL)
- * or by the SCBID ARG_1.  The search begins at the SCB index passed in
- * via SINDEX which is an SCB that must be on the disconnected list.  If
- * the SCB cannot be found, SINDEX will be SCB_LIST_NULL, otherwise, SCBPTR
- * is set to the proper SCB.
+ * Locate a disconnected SCB by SCBID.  Upon return, SCBPTR and SINDEX will
+ * be set to the position of the SCB.  If the SCB cannot be found locally,
+ * it will be paged in from host memory.  RETURN_2 stores the address of the
+ * preceding SCB in the disconnected list which can be used to speed up
+ * removal of the found SCB from the disconnected list.
  */
+if ((ahc->flags & AHC_PAGESCBS) != 0) {
+BEGIN_CRITICAL
 findSCB:
-	mov	SCBPTR,SINDEX;			/* Initialize SCBPTR */
-	cmp	ARG_1, SCB_LIST_NULL	jne findSCB_by_SCBID;
-	mov	A, SAVED_TCL;
-	mvi	SCB_TCL	jmp findSCB_loop;	/* &SCB_TCL -> SINDEX */
-findSCB_by_SCBID:
-	mov	A, ARG_1;			/* Tag passed in ARG_1 */
-	mvi	SCB_TAG	jmp findSCB_loop;	/* &SCB_TAG -> SINDEX */
+	mov	A, SINDEX;			/* Tag passed in SINDEX */
+	cmp	DISCONNECTED_SCBH, SCB_LIST_NULL je findSCB_notFound;
+	mov	SCBPTR, DISCONNECTED_SCBH;	/* Initialize SCBPTR */
+	mvi	ARG_2, SCB_LIST_NULL;		/* Head of list */
+	jmp	findSCB_loop;
 findSCB_next:
-	mov     ARG_2, SCBPTR;
-	cmp	SCB_NEXT, SCB_LIST_NULL je notFound;
+	cmp	SCB_NEXT, SCB_LIST_NULL je findSCB_notFound;
+	mov	ARG_2, SCBPTR;
 	mov	SCBPTR,SCB_NEXT;
-	dec	SINDEX;		/* Last comparison moved us too far */
 findSCB_loop:
-	cmp	SINDIR, A	jne findSCB_next;
-	mov	SINDEX, SCBPTR 	ret;
-notFound:
-	mvi	SINDEX, SCB_LIST_NULL	ret;
-
-/*
- * Retrieve an SCB by SCBID first searching the disconnected list falling
- * back to DMA'ing the SCB down from the host.  This routine assumes that
- * ARG_1 is the SCBID of interrest and that SINDEX is the position in the
- * disconnected list to start the search from.  If SINDEX is SCB_LIST_NULL,
- * we go directly to the host for the SCB.
- */
-retrieveSCB:
-	test	SEQ_FLAGS, SCBPTR_VALID	jz retrieve_from_host;
-	mov	SCBPTR	call findSCB;	/* Continue the search */
-	cmp	SINDEX, SCB_LIST_NULL	je retrieve_from_host;
-
-/*
- * This routine expects SINDEX to contain the index of the SCB to be
- * removed, SCBPTR to be pointing to that SCB, and ARG_2 to be the
- * SCBID of the SCB just previous to this one in the list or SCB_LIST_NULL
- * if it is at the head.
- */
+	cmp	SCB_TAG, A	jne findSCB_next;
 rem_scb_from_disc_list:
-/* Remove this SCB from the disconnection list */
-	cmp     ARG_2, SCB_LIST_NULL    je rHead;
+	cmp	ARG_2, SCB_LIST_NULL	je rHead;
 	mov	DINDEX, SCB_NEXT;
+	mov	SINDEX, SCBPTR;
 	mov	SCBPTR, ARG_2;
 	mov	SCB_NEXT, DINDEX;
 	mov	SCBPTR, SINDEX ret;
 rHead:
 	mov	DISCONNECTED_SCBH,SCB_NEXT ret;
-
-retrieve_from_host:
-/*
- * We didn't find it.  Pull an SCB and DMA down the one we want.
- * We should never get here in the non-paging case.
- */
-	mov	ALLZEROS	call	get_free_or_disc_scb;
+END_CRITICAL
+findSCB_notFound:
+	/*
+	 * We didn't find it.  Page in the SCB.
+	 */
+	mov	ARG_1, A; /* Save tag */
+	mov	ALLZEROS call get_free_or_disc_scb;
 	mvi	DMAPARAMS, HDMAEN|DIRECTION|FIFORESET;
-	/* Jump instead of call as we want to return anyway */
 	mov	ARG_1	jmp dma_scb;
-
-/*
- * Determine whether a target is using tagged or non-tagged transactions
- * by first looking for a matching transaction based on the TCL and if
- * that fails, looking up this device in the host's untagged SCB array.
- * The TCL to search for is assumed to be in SAVED_TCL.  The value is
- * returned in ARG_1 (SCB_LIST_NULL for tagged, SCBID for non-tagged).
- * The SCBPTR_VALID bit is set in SEQ_FLAGS if we found the information
- * in an SCB instead of having to go to the host.
- */
-get_untagged_SCBID:
-	cmp	DISCONNECTED_SCBH, SCB_LIST_NULL je get_SCBID_from_host;
-	mvi	ARG_1, SCB_LIST_NULL;
-	mov	DISCONNECTED_SCBH call findSCB;
-	cmp	SINDEX, SCB_LIST_NULL	je get_SCBID_from_host;
-	or	SEQ_FLAGS, SCBPTR_VALID;/* Was in disconnected list */
-	test	SCB_CONTROL, TAG_ENB	jnz . + 2;
-	mov	ARG_1, SCB_TAG	ret;
-	mvi	ARG_1, SCB_LIST_NULL ret;
-
-/*
- * Fetch a byte from host memory given an index of (A + (256 * SINDEX))
- * and a base address of SCBID_ADDR.  The byte is returned in RETURN_2.
- */
-fetch_byte:
-	mov	ARG_2, SINDEX;
-	if ((p->features & AHC_CMD_CHAN) != 0) {
-		mvi	DINDEX, CCHADDR;
-		mvi	SCBID_ADDR call set_1byte_addr;
-		mvi	CCHCNT, 1;
-		mvi	CCSGCTL, CCSGEN|CCSGRESET;
-		test	CCSGCTL, CCSGDONE jz .;
-		mvi	CCSGCTL, CCSGRESET;
-		bmov	RETURN_2, CCSGRAM, 1 ret;
-	} else {
-		mvi	DINDEX, HADDR;
-		mvi	SCBID_ADDR call set_1byte_addr;
-		mvi	HCNT[0], 1;
-		clr	HCNT[1];
-		clr	HCNT[2];
-		mvi	DFCNTRL, HDMAEN|DIRECTION|FIFORESET;
-		call	dma_finish;
-		mov	RETURN_2, DFDAT ret;
-	}
+}
 
 /*
  * Prepare the hardware to post a byte to host memory given an
- * index of (A + (256 * SINDEX)) and a base address of SCBID_ADDR.
+ * index of (A + (256 * SINDEX)) and a base address of SHARED_DATA_ADDR.
  */
 post_byte_setup:
 	mov	ARG_2, SINDEX;
-	if ((p->features & AHC_CMD_CHAN) != 0) {
+	if ((ahc->features & AHC_CMD_CHAN) != 0) {
 		mvi	DINDEX, CCHADDR;
-		mvi	SCBID_ADDR call	set_1byte_addr;
+		mvi	SHARED_DATA_ADDR call	set_1byte_addr;
 		mvi	CCHCNT, 1;
 		mvi	CCSCBCTL, CCSCBRESET ret;
 	} else {
 		mvi	DINDEX, HADDR;
-		mvi	SCBID_ADDR call	set_1byte_addr;
-		mvi	HCNT[0], 1;
-		clr	HCNT[1];
-		clr	HCNT[2];
+		mvi	SHARED_DATA_ADDR call	set_1byte_addr;
+		mvi	1	call set_hcnt;
 		mvi	DFCNTRL, FIFORESET ret;
 	}
 
 post_byte:
-	if ((p->features & AHC_CMD_CHAN) != 0) {
+	if ((ahc->features & AHC_CMD_CHAN) != 0) {
 		bmov	CCSCBRAM, SINDEX, 1;
 		or	CCSCBCTL, CCSCBEN|CCSCBRESET;
 		test	CCSCBCTL, CCSCBDONE jz .;
@@ -1235,23 +1887,34 @@
 		jmp	dma_finish;
 	}
 
-get_SCBID_from_host:
-	mov	A, SAVED_TCL;
-	mvi	UNTAGGEDSCB_OFFSET call fetch_byte;
-	mov	RETURN_1,  RETURN_2 ret;
-
+phase_lock_perr:
+	mvi	PERR_DETECTED call set_seqint;
 phase_lock:     
+	/*
+	 * If there is a parity error, wait for the kernel to
+	 * see the interrupt and prepare our message response
+	 * before continuing.
+	 */
 	test	SSTAT1, REQINIT jz phase_lock;
-	test	SSTAT1, SCSIPERR jnz phase_lock;
+	test	SSTAT1, SCSIPERR jnz phase_lock_perr;
+phase_lock_latch_phase:
 	and	SCSISIGO, PHASE_MASK, SCSISIGI;
 	and	LASTPHASE, PHASE_MASK, SCSISIGI ret;
 
-if ((p->features & AHC_CMD_CHAN) == 0) {
+if ((ahc->features & AHC_CMD_CHAN) == 0) {
+set_hcnt:
+	mov	HCNT[0], SINDEX;
+clear_hcnt:
+	clr	HCNT[1];
+	clr	HCNT[2] ret;
+
 set_stcnt_from_hcnt:
 	mov	STCNT[0], HCNT[0];
 	mov	STCNT[1], HCNT[1];
 	mov	STCNT[2], HCNT[2] ret;
 
+bcopy_8:
+	mov	DINDIR, SINDIR;
 bcopy_7:
 	mov	DINDIR, SINDIR;
 	mov	DINDIR, SINDIR;
@@ -1265,6 +1928,7 @@
 	mov	DINDIR, SINDIR ret;
 }
 
+if ((ahc->flags & AHC_TARGETROLE) != 0) {
 /*
  * Setup addr assuming that A is an index into
  * an array of 32byte objects, SINDEX contains
@@ -1275,16 +1939,30 @@
 set_32byte_addr:
 	shr	ARG_2, 3, A;
 	shl	A, 5;
+	jmp	set_1byte_addr;
+}
+
+/*
+ * Setup addr assuming that A is an index into
+ * an array of 64byte objects, SINDEX contains
+ * the base address of that array, and DINDEX
+ * contains the base address of the location
+ * to store the indexed address.
+ */
+set_64byte_addr:
+	shr	ARG_2, 2, A;
+	shl	A, 6;
+
 /*
- * Setup addr assuming that A + (ARG_1 * 256) is an
+ * Setup addr assuming that A + (ARG_2 * 256) is an
  * index into an array of 1byte objects, SINDEX contains
  * the base address of that array, and DINDEX contains
  * the base address of the location to store the computed
  * address.
  */
 set_1byte_addr:
-	add	DINDIR, A, SINDIR;
-	mov	A, ARG_2;
+	add     DINDIR, A, SINDIR;
+	mov     A, ARG_2;
 	adc	DINDIR, A, SINDIR;
 	clr	A;
 	adc	DINDIR, A, SINDIR;
@@ -1296,21 +1974,32 @@
  */
 dma_scb:
 	mov	A, SINDEX;
-	if ((p->features & AHC_CMD_CHAN) != 0) {
+	if ((ahc->features & AHC_CMD_CHAN) != 0) {
 		mvi	DINDEX, CCHADDR;
-		mvi	HSCB_ADDR call set_32byte_addr;
+		mvi	HSCB_ADDR call set_64byte_addr;
 		mov	CCSCBPTR, SCBPTR;
-		mvi	CCHCNT, 32;
 		test	DMAPARAMS, DIRECTION jz dma_scb_tohost;
+		if ((ahc->flags & AHC_SCB_BTT) != 0) {
+			mvi	CCHCNT, SCB_DOWNLOAD_SIZE_64;
+		} else {
+			mvi	CCHCNT, SCB_DOWNLOAD_SIZE;
+		}
 		mvi	CCSCBCTL, CCARREN|CCSCBEN|CCSCBDIR|CCSCBRESET;
 		cmp	CCSCBCTL, CCSCBDONE|ARRDONE|CCARREN|CCSCBEN|CCSCBDIR jne .;
 		jmp	dma_scb_finish;
 dma_scb_tohost:
-		if ((p->chip & AHC_CHIPID_MASK) == AHC_AIC7895) {
+		mvi	CCHCNT, SCB_UPLOAD_SIZE;
+		if ((ahc->features & AHC_ULTRA2) == 0) {
 			mvi	CCSCBCTL, CCSCBRESET;
-			bmov	CCSCBRAM, SCB_CONTROL, 32;
+			bmov	CCSCBRAM, SCB_BASE, SCB_UPLOAD_SIZE;
 			or	CCSCBCTL, CCSCBEN|CCSCBRESET;
 			test	CCSCBCTL, CCSCBDONE jz .;
+		} else if ((ahc->bugs & AHC_SCBCHAN_UPLOAD_BUG) != 0) {
+			mvi	CCSCBCTL, CCARREN|CCSCBRESET;
+			cmp	CCSCBCTL, ARRDONE|CCARREN jne .;
+			mvi	CCHCNT, SCB_UPLOAD_SIZE;
+			mvi	CCSCBCTL, CCSCBEN|CCSCBRESET;
+			cmp	CCSCBCTL, CCSCBDONE|CCSCBEN jne .;
 		} else {
 			mvi	CCSCBCTL, CCARREN|CCSCBEN|CCSCBRESET;
 			cmp	CCSCBCTL, CCSCBDONE|ARRDONE|CCARREN|CCSCBEN jne .;
@@ -1321,85 +2010,142 @@
 		ret;
 	} else {
 		mvi	DINDEX, HADDR;
-		mvi	HSCB_ADDR call set_32byte_addr;
-		mvi	HCNT[0], 32;
-		clr	HCNT[1];
-		clr	HCNT[2];
+		mvi	HSCB_ADDR call set_64byte_addr;
+		mvi	SCB_DOWNLOAD_SIZE call set_hcnt;
 		mov	DFCNTRL, DMAPARAMS;
 		test	DMAPARAMS, DIRECTION	jnz dma_scb_fromhost;
 		/* Fill it with the SCB data */
 copy_scb_tofifo:
-		mvi	SINDEX, SCB_CONTROL;
-		add	A, 32, SINDEX;
+		mvi	SINDEX, SCB_BASE;
+		add	A, SCB_DOWNLOAD_SIZE, SINDEX;
 copy_scb_tofifo_loop:
-		mov	DFDAT,SINDIR;
-		mov	DFDAT,SINDIR;
-		mov	DFDAT,SINDIR;
-		mov	DFDAT,SINDIR;
-		mov	DFDAT,SINDIR;
-		mov	DFDAT,SINDIR;
-		mov	DFDAT,SINDIR;
-		mov	DFDAT,SINDIR;
+		call	copy_to_fifo_8;
 		cmp	SINDEX, A jne copy_scb_tofifo_loop;
 		or	DFCNTRL, HDMAEN|FIFOFLUSH;
+		jmp	dma_finish;
 dma_scb_fromhost:
-		call	dma_finish;
-		/* If we were putting the SCB, we are done */
-		test	DMAPARAMS, DIRECTION	jz	return;
-		mvi	SCB_CONTROL  call dfdat_in_7;
-		call	dfdat_in_7_continued;
-		call	dfdat_in_7_continued;
-		jmp	dfdat_in_7_continued;
+		mvi	DINDEX, SCB_BASE;
+		if ((ahc->bugs & AHC_PCI_2_1_RETRY_BUG) != 0) {
+			/*
+			 * The PCI module will only issue a PCI
+			 * retry if the data FIFO is empty.  If the
+			 * host disconnects in the middle of a
+			 * transfer, we must empty the fifo of all
+			 * available data to force the chip to
+			 * continue the transfer.  This does not
+			 * happen for SCSI transfers as the SCSI module
+			 * will drain the FIFO as data is made available.
+			 * When the hang occurs, we know that at least
+			 * 8 bytes are in the FIFO because the PCI
+			 * module has an 8 byte input latch that only
+			 * dumps to the FIFO when HCNT == 0 or the
+			 * latch is full.
+			 */
+			mvi	A, -24;
+			/* Wait for some data to arrive. */
+dma_scb_hang_fifo:
+			test	DFSTATUS, FIFOEMP jnz dma_scb_hang_fifo;
+dma_scb_hang_wait:
+			test	DFSTATUS, MREQPEND jnz dma_scb_hang_wait;
+			test	DFSTATUS, HDONE	jnz dma_scb_hang_dma_done;
+			test	DFSTATUS, HDONE	jnz dma_scb_hang_dma_done;
+			test	DFSTATUS, HDONE	jnz dma_scb_hang_dma_done;
+			/*
+			 * The PCI no longer intends to perform a PCI
+			 * transaction and HDONE has not come true.
+			 * We are hung.  Drain the fifo.
+			 */
+dma_scb_hang_empty_fifo:
+			call	dfdat_in_8;
+			add	A, 8;
+			add	SINDEX, A, HCNT; 
+			/*
+			 * The result will be <= 0 (carry set) if at
+			 * least 8 bytes of data have been placed
+			 * into the fifo.
+			 */
+			jc	dma_scb_hang_empty_fifo;
+			jmp	dma_scb_hang_fifo;
+dma_scb_hang_dma_done:
+			and	DFCNTRL, ~HDMAEN;
+			test	DFCNTRL, HDMAEN jnz .;
+			call	dfdat_in_8;
+			add	A, 8;
+			cmp	A, 8 jne . - 2;
+		} else {
+			call	dma_finish;
+			/* If we were putting the SCB, we are done */
+			call	dfdat_in_8;
+			call	dfdat_in_8;
+			call	dfdat_in_8;
+		}
+dfdat_in_8:
+		mov	DINDIR,DFDAT;
 dfdat_in_7:
-		mov     DINDEX,SINDEX;
-dfdat_in_7_continued:
 		mov	DINDIR,DFDAT;
 		mov	DINDIR,DFDAT;
 		mov	DINDIR,DFDAT;
 		mov	DINDIR,DFDAT;
 		mov	DINDIR,DFDAT;
+dfdat_in_2:
 		mov	DINDIR,DFDAT;
 		mov	DINDIR,DFDAT ret;
 	}
 
+copy_to_fifo_8:
+	mov	DFDAT,SINDIR;
+	mov	DFDAT,SINDIR;
+copy_to_fifo_6:
+	mov	DFDAT,SINDIR;
+copy_to_fifo_5:
+	mov	DFDAT,SINDIR;
+copy_to_fifo_4:
+	mov	DFDAT,SINDIR;
+	mov	DFDAT,SINDIR;
+	mov	DFDAT,SINDIR;
+	mov	DFDAT,SINDIR ret;
 
 /*
  * Wait for DMA from host memory to data FIFO to complete, then disable
  * DMA and wait for it to acknowledge that it's off.
  */
-if ((p->features & AHC_CMD_CHAN) == 0) {
 dma_finish:
 	test	DFSTATUS,HDONE	jz dma_finish;
 	/* Turn off DMA */
 	and	DFCNTRL, ~HDMAEN;
 	test	DFCNTRL, HDMAEN jnz .;
 	ret;
-}
 
 add_scb_to_free_list:
-	if ((p->flags & AHC_PAGESCBS) != 0) {
+	if ((ahc->flags & AHC_PAGESCBS) != 0) {
+BEGIN_CRITICAL
 		mov	SCB_NEXT, FREE_SCBH;
-		mov	FREE_SCBH, SCBPTR;
+		mvi	SCB_TAG, SCB_LIST_NULL;
+		mov	FREE_SCBH, SCBPTR ret;
+END_CRITICAL
+	} else {
+		mvi	SCB_TAG, SCB_LIST_NULL ret;
 	}
-	mvi	SCB_TAG, SCB_LIST_NULL ret;
 
-if ((p->flags & AHC_PAGESCBS) != 0) {
+if ((ahc->flags & AHC_PAGESCBS) != 0) {
 get_free_or_disc_scb:
+BEGIN_CRITICAL
 	cmp	FREE_SCBH, SCB_LIST_NULL jne dequeue_free_scb;
 	cmp	DISCONNECTED_SCBH, SCB_LIST_NULL jne dequeue_disc_scb;
 return_error:
+	mvi	NO_FREE_SCB call set_seqint;
 	mvi	SINDEX, SCB_LIST_NULL	ret;
 dequeue_disc_scb:
 	mov	SCBPTR, DISCONNECTED_SCBH;
-dma_up_scb:
+	mov	DISCONNECTED_SCBH, SCB_NEXT;
+END_CRITICAL
 	mvi	DMAPARAMS, FIFORESET;
-	mov	SCB_TAG		call dma_scb;
-unlink_disc_scb:
-	mov	DISCONNECTED_SCBH, SCB_NEXT ret;
+	mov	SCB_TAG	jmp dma_scb;
+BEGIN_CRITICAL
 dequeue_free_scb:
 	mov	SCBPTR, FREE_SCBH;
 	mov	FREE_SCBH, SCB_NEXT ret;
-}
+END_CRITICAL
 
 add_scb_to_disc_list:
 /*
@@ -1407,5 +2153,13 @@
  * candidates for paging out an SCB if one is needed for a new command.
  * Modifying the disconnected list is a critical(pause dissabled) section.
  */
+BEGIN_CRITICAL
 	mov	SCB_NEXT, DISCONNECTED_SCBH;
 	mov	DISCONNECTED_SCBH, SCBPTR ret;
+END_CRITICAL
+}
+set_seqint:
+	mov	INTSTAT, SINDEX;
+	nop;
+return:
+	ret;
diff -urN linux/drivers/scsi/aic7xxx/aic7xxx_93cx6.c /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_93cx6.c
--- linux/drivers/scsi/aic7xxx/aic7xxx_93cx6.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_93cx6.c	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,205 @@
+/*
+ * Interface for the 93C66/56/46/26/06 serial eeprom parts.
+ *
+ * Copyright (c) 1995, 1996 Daniel M. Eischen
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $Id: //depot/src/aic7xxx/aic7xxx_93cx6.c#7 $
+ *
+ * $FreeBSD: src/sys/dev/aic7xxx/aic7xxx_93cx6.c,v 1.9 2000/11/10 20:13:41 gibbs Exp $
+ */
+
+/*
+ *   The instruction set of the 93C66/56/46/26/06 chips are as follows:
+ *
+ *               Start  OP	    *
+ *     Function   Bit  Code  Address**  Data     Description
+ *     -------------------------------------------------------------------
+ *     READ        1    10   A5 - A0             Reads data stored in memory,
+ *                                               starting at specified address
+ *     EWEN        1    00   11XXXX              Write enable must preceed
+ *                                               all programming modes
+ *     ERASE       1    11   A5 - A0             Erase register A5A4A3A2A1A0
+ *     WRITE       1    01   A5 - A0   D15 - D0  Writes register
+ *     ERAL        1    00   10XXXX              Erase all registers
+ *     WRAL        1    00   01XXXX    D15 - D0  Writes to all registers
+ *     EWDS        1    00   00XXXX              Disables all programming
+ *                                               instructions
+ *     *Note: A value of X for address is a don't care condition.
+ *    **Note: There are 8 address bits for the 93C56/66 chips unlike
+ *	      the 93C46/26/06 chips which have 6 address bits.
+ *
+ *   The 93C46 has a four wire interface: clock, chip select, data in, and
+ *   data out.  In order to perform one of the above functions, you need
+ *   to enable the chip select for a clock period (typically a minimum of
+ *   1 usec, with the clock high and low a minimum of 750 and 250 nsec
+ *   respectively).  While the chip select remains high, you can clock in
+ *   the instructions (above) starting with the start bit, followed by the
+ *   OP code, Address, and Data (if needed).  For the READ instruction, the
+ *   requested 16-bit register contents is read from the data out line but
+ *   is preceded by an initial zero (leading 0, followed by 16-bits, MSB
+ *   first).  The clock cycling from low to high initiates the next data
+ *   bit to be sent from the chip.
+ *
+ */
+
+#include "aic7xxx_osm.h"
+#include "aic7xxx_inline.h"
+#include "aic7xxx_93cx6.h"
+
+/*
+ * Right now, we only have to read the SEEPROM.  But we make it easier to
+ * add other 93Cx6 functions.
+ */
+static struct seeprom_cmd {
+  	uint8_t len;
+ 	uint8_t bits[3];
+} seeprom_read = {3, {1, 1, 0}};
+
+/*
+ * Wait for the SEERDY to go high; about 800 ns.
+ */
+#define CLOCK_PULSE(sd, rdy)				\
+	while ((SEEPROM_STATUS_INB(sd) & rdy) == 0) {	\
+		;  /* Do nothing */			\
+	}						\
+	(void)SEEPROM_INB(sd);	/* Clear clock */
+
+/*
+ * Read the serial EEPROM and returns 1 if successful and 0 if
+ * not successful.
+ */
+int
+read_seeprom(sd, buf, start_addr, count)
+	struct seeprom_descriptor *sd;
+	uint16_t *buf;
+	u_int start_addr;
+	u_int count;
+{
+	int i = 0;
+	u_int k = 0;
+	uint16_t v;
+	uint8_t temp;
+
+	/*
+	 * Read the requested registers of the seeprom.  The loop
+	 * will range from 0 to count-1.
+	 */
+	for (k = start_addr; k < count + start_addr; k++) {
+		/* Send chip select for one clock cycle. */
+		temp = sd->sd_MS ^ sd->sd_CS;
+		SEEPROM_OUTB(sd, temp ^ sd->sd_CK);
+		CLOCK_PULSE(sd, sd->sd_RDY);
+
+		/*
+		 * Now we're ready to send the read command followed by the
+		 * address of the 16-bit register we want to read.
+		 */
+		for (i = 0; i < seeprom_read.len; i++) {
+			if (seeprom_read.bits[i] != 0)
+				temp ^= sd->sd_DO;
+			SEEPROM_OUTB(sd, temp);
+			CLOCK_PULSE(sd, sd->sd_RDY);
+			SEEPROM_OUTB(sd, temp ^ sd->sd_CK);
+			CLOCK_PULSE(sd, sd->sd_RDY);
+			if (seeprom_read.bits[i] != 0)
+				temp ^= sd->sd_DO;
+		}
+		/* Send the 6 or 8 bit address (MSB first, LSB last). */
+		for (i = (sd->sd_chip - 1); i >= 0; i--) {
+			if ((k & (1 << i)) != 0)
+				temp ^= sd->sd_DO;
+			SEEPROM_OUTB(sd, temp);
+			CLOCK_PULSE(sd, sd->sd_RDY);
+			SEEPROM_OUTB(sd, temp ^ sd->sd_CK);
+			CLOCK_PULSE(sd, sd->sd_RDY);
+			if ((k & (1 << i)) != 0)
+				temp ^= sd->sd_DO;
+		}
+
+		/*
+		 * Now read the 16 bit register.  An initial 0 precedes the
+		 * register contents which begins with bit 15 (MSB) and ends
+		 * with bit 0 (LSB).  The initial 0 will be shifted off the
+		 * top of our word as we let the loop run from 0 to 16.
+		 */
+		v = 0;
+		for (i = 16; i >= 0; i--) {
+			SEEPROM_OUTB(sd, temp);
+			CLOCK_PULSE(sd, sd->sd_RDY);
+			v <<= 1;
+			if (SEEPROM_DATA_INB(sd) & sd->sd_DI)
+				v |= 1;
+			SEEPROM_OUTB(sd, temp ^ sd->sd_CK);
+			CLOCK_PULSE(sd, sd->sd_RDY);
+		}
+
+		buf[k - start_addr] = v;
+
+		/* Reset the chip select for the next command cycle. */
+		temp = sd->sd_MS;
+		SEEPROM_OUTB(sd, temp);
+		CLOCK_PULSE(sd, sd->sd_RDY);
+		SEEPROM_OUTB(sd, temp ^ sd->sd_CK);
+		CLOCK_PULSE(sd, sd->sd_RDY);
+		SEEPROM_OUTB(sd, temp);
+		CLOCK_PULSE(sd, sd->sd_RDY);
+	}
+#ifdef AHC_DUMP_EEPROM
+	printf("\nSerial EEPROM:\n\t");
+	for (k = 0; k < count; k = k + 1) {
+		if (((k % 8) == 0) && (k != 0)) {
+			printf ("\n\t");
+		}
+		printf (" 0x%x", buf[k]);
+	}
+	printf ("\n");
+#endif
+	return (1);
+}
+
+int
+verify_cksum(struct seeprom_config *sc)
+{
+	int i;
+	int maxaddr;
+	uint32_t checksum;
+	uint16_t *scarray;
+
+	maxaddr = (sizeof(*sc)/2) - 1;
+	checksum = 0;
+	scarray = (uint16_t *)sc;
+
+	for (i = 0; i < maxaddr; i++)
+		checksum = checksum + scarray[i];
+	if (checksum == 0
+	 || (checksum & 0xFFFF) != sc->checksum) {
+		return (0);
+	} else {
+		return(1);
+	}
+}
diff -urN linux/drivers/scsi/aic7xxx/aic7xxx_93cx6.h /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_93cx6.h
--- linux/drivers/scsi/aic7xxx/aic7xxx_93cx6.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_93cx6.h	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,92 @@
+/*
+ * Interface to the 93C46/56 serial EEPROM that is used to store BIOS
+ * settings for the aic7xxx based adaptec SCSI controllers.  It can
+ * also be used for 93C26 and 93C06 serial EEPROMS.
+ *
+ * Copyright (c) 1994, 1995, 2000 Justin T. Gibbs.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $Id: //depot/src/aic7xxx/aic7xxx_93cx6.h#5 $
+ *
+ * $FreeBSD: src/sys/dev/aic7xxx/aic7xxx_93cx6.h,v 1.8 2000/11/10 20:13:41 gibbs Exp $
+ */
+#ifndef _AIC7XXX_93CX6_H_
+#define _AIC7XXX_93CX6_H_
+
+typedef enum {
+	C46 = 6,
+	C56_66 = 8
+} seeprom_chip_t;
+
+struct seeprom_descriptor {
+	struct ahc_softc *sd_ahc;
+	u_int sd_control_offset;
+	u_int sd_status_offset;
+	u_int sd_dataout_offset;
+	seeprom_chip_t sd_chip;
+	uint16_t sd_MS;
+	uint16_t sd_RDY;
+	uint16_t sd_CS;
+	uint16_t sd_CK;
+	uint16_t sd_DO;
+	uint16_t sd_DI;
+};
+
+/*
+ * This function will read count 16-bit words from the serial EEPROM and
+ * return their value in buf.  The port address of the aic7xxx serial EEPROM
+ * control register is passed in as offset.  The following parameters are
+ * also passed in:
+ *
+ *   CS  - Chip select
+ *   CK  - Clock
+ *   DO  - Data out
+ *   DI  - Data in
+ *   RDY - SEEPROM ready
+ *   MS  - Memory port mode select
+ *
+ *  A failed read attempt returns 0, and a successful read returns 1.
+ */
+
+#define	SEEPROM_INB(sd) \
+	ahc_inb(sd->sd_ahc, sd->sd_control_offset)
+#define	SEEPROM_OUTB(sd, value)					\
+do {								\
+	ahc_outb(sd->sd_ahc, sd->sd_control_offset, value);	\
+	ahc_flush_device_writes(sd->sd_ahc);			\
+} while(0)
+
+#define	SEEPROM_STATUS_INB(sd) \
+	ahc_inb(sd->sd_ahc, sd->sd_status_offset)
+#define	SEEPROM_DATA_INB(sd) \
+	ahc_inb(sd->sd_ahc, sd->sd_dataout_offset)
+
+int read_seeprom(struct seeprom_descriptor *sd, uint16_t *buf,
+		 u_int start_addr, u_int count);
+int verify_cksum(struct seeprom_config *sc);
+
+#endif /* _AIC7XXX_93CX6_H_ */
diff -urN linux/drivers/scsi/aic7xxx/aic7xxx_inline.h /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_inline.h
--- linux/drivers/scsi/aic7xxx/aic7xxx_inline.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_inline.h	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,478 @@
+/*
+ * Inline routines shareable across OS platforms.
+ *
+ * Copyright (c) 1994-2001 Justin T. Gibbs.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $Id: //depot/src/aic7xxx/aic7xxx_inline.h#15 $
+ *
+ * $FreeBSD: src/sys/dev/aic7xxx/aic7xxx_inline.h,v 1.8 2000/11/12 05:19:46 gibbs Exp $
+ */
+
+#ifndef _AIC7XXX_INLINE_H_
+#define _AIC7XXX_INLINE_H_
+
+/************************* Sequencer Execution Control ************************/
+static __inline int  sequencer_paused(struct ahc_softc *ahc);
+static __inline void ahc_pause_bug_fix(struct ahc_softc *ahc);
+static __inline void pause_sequencer(struct ahc_softc *ahc);
+static __inline void unpause_sequencer(struct ahc_softc *ahc);
+
+/*
+ * Work around any chip bugs related to halting sequencer execution.
+ * On Ultra2 controllers, we must clear the CIOBUS stretch signal by
+ * reading a register that will set this signal and deassert it.
+ * Without this workaround, if the chip is paused, by an interrupt or
+ * manual pause while accessing scb ram, accesses to certain registers
+ * will hang the system (infinite pci retries).
+ */
+static __inline void
+ahc_pause_bug_fix(struct ahc_softc *ahc)
+{
+	if ((ahc->features & AHC_ULTRA2) != 0)
+		(void)ahc_inb(ahc, CCSCBCTL);
+}
+
+/*
+ * Determine whether the sequencer has halted code execution.
+ * Returns non-zero status if the sequencer is stopped.
+ */
+static __inline int
+sequencer_paused(struct ahc_softc *ahc)
+{
+	return ((ahc_inb(ahc, HCNTRL) & PAUSE) != 0);
+}
+
+/*
+ * Request that the sequencer stop and wait, indefinitely, for it
+ * to stop.  The sequencer will only acknowledge that it is paused
+ * once it has reached an instruction boundary and PAUSEDIS is
+ * cleared in the SEQCTL register.  The sequencer may use PAUSEDIS
+ * for critical sections.
+ */
+static __inline void
+pause_sequencer(struct ahc_softc *ahc)
+{
+	ahc_outb(ahc, HCNTRL, ahc->pause);
+
+	/*
+	 * Since the sequencer can disable pausing in a critical section, we
+	 * must loop until it actually stops.
+	 */
+	while (sequencer_paused(ahc) == 0)
+		;
+
+	ahc_pause_bug_fix(ahc);
+}
+
+/*
+ * Allow the sequencer to continue program execution.
+ * We check here to ensure that no additional interrupt
+ * sources that would cause the sequencer to halt have been
+ * asserted.  If, for example, a SCSI bus reset is detected
+ * while we are fielding a different, pausing, interrupt type,
+ * we don't want to release the sequencer before going back
+ * into our interrupt handler and dealing with this new
+ * condition.
+ */
+static __inline void
+unpause_sequencer(struct ahc_softc *ahc)
+{
+	if ((ahc_inb(ahc, INTSTAT) & (SCSIINT | SEQINT | BRKADRINT)) == 0)
+		ahc_outb(ahc, HCNTRL, ahc->unpause);
+}
+
+/*********************** Untagged Transaction Routines ************************/
+static __inline void	ahc_freeze_untagged_queues(struct ahc_softc *ahc);
+static __inline void	ahc_release_untagged_queues(struct ahc_softc *ahc);
+
+/*
+ * Block our completion routine from starting the next untagged
+ * transaction for this target or target lun.
+ */
+static __inline void
+ahc_freeze_untagged_queues(struct ahc_softc *ahc)
+{
+	if ((ahc->flags & AHC_SCB_BTT) == 0)
+		ahc->untagged_queue_lock++;
+}
+
+/*
+ * Allow the next untagged transaction for this target or target lun
+ * to be executed.  We use a counting semaphore to allow the lock
+ * to be acquired recursively.  Once the count drops to zero, the
+ * transaction queues will be run.
+ */
+static __inline void
+ahc_release_untagged_queues(struct ahc_softc *ahc)
+{
+	if ((ahc->flags & AHC_SCB_BTT) == 0) {
+		ahc->untagged_queue_lock--;
+		if (ahc->untagged_queue_lock == 0)
+			ahc_run_untagged_queues(ahc);
+	}
+}
+
+/************************** Memory mapping routines ***************************/
+static __inline struct ahc_dma_seg *
+			ahc_sg_bus_to_virt(struct scb *scb,
+					   uint32_t sg_busaddr);
+static __inline uint32_t
+			ahc_sg_virt_to_bus(struct scb *scb,
+					   struct ahc_dma_seg *sg);
+static __inline uint32_t
+			ahc_hscb_busaddr(struct ahc_softc *ahc, u_int index);
+
+static __inline struct ahc_dma_seg *
+ahc_sg_bus_to_virt(struct scb *scb, uint32_t sg_busaddr)
+{
+	int sg_index;
+
+	sg_index = (sg_busaddr - scb->sg_list_phys)/sizeof(struct ahc_dma_seg);
+	/* sg_list_phys points to entry 1, not 0 */
+	sg_index++;
+
+	return (&scb->sg_list[sg_index]);
+}
+
+static __inline uint32_t
+ahc_sg_virt_to_bus(struct scb *scb, struct ahc_dma_seg *sg)
+{
+	int sg_index;
+
+	/* sg_list_phys points to entry 1, not 0 */
+	sg_index = sg - &scb->sg_list[1];
+
+	return (scb->sg_list_phys + (sg_index * sizeof(*scb->sg_list)));
+}
+
+static __inline uint32_t
+ahc_hscb_busaddr(struct ahc_softc *ahc, u_int index)
+{
+	return (ahc->scb_data->hscb_busaddr
+		+ (sizeof(struct hardware_scb) * index));
+}
+
+/******************************** Debugging ***********************************/
+static __inline char *ahc_name(struct ahc_softc *ahc);
+
+static __inline char *
+ahc_name(struct ahc_softc *ahc)
+{
+	return (ahc->name);
+}
+
+/*********************** Miscelaneous Support Functions ***********************/
+
+static __inline int	ahc_check_residual(struct scb *scb);
+static __inline struct ahc_initiator_tinfo *
+			ahc_fetch_transinfo(struct ahc_softc *ahc,
+					    char channel, u_int our_id,
+					    u_int remote_id,
+					    struct tmode_tstate **tstate);
+static __inline struct scb*
+			ahc_get_scb(struct ahc_softc *ahc);
+static __inline void	ahc_free_scb(struct ahc_softc *ahc, struct scb *scb);
+static __inline void	ahc_swap_with_next_hscb(struct ahc_softc *ahc,
+						struct scb *scb);
+static __inline void	ahc_queue_scb(struct ahc_softc *ahc, struct scb *scb);
+static __inline struct scsi_sense_data *
+			ahc_get_sense_buf(struct ahc_softc *ahc,
+					  struct scb *scb);
+static __inline uint32_t
+			ahc_get_sense_bufaddr(struct ahc_softc *ahc,
+					      struct scb *scb);
+
+/*
+ * Determine whether the sequencer reported a residual
+ * for this SCB/transaction.
+ */
+static __inline int
+ahc_check_residual(struct scb *scb)
+{
+	struct status_pkt *sp;
+
+	sp = &scb->hscb->shared_data.status;
+	if ((scb->hscb->sgptr & SG_RESID_VALID) != 0)
+		return (1);
+	return (0);
+}
+
+/*
+ * Return pointers to the transfer negotiation information
+ * for the specified our_id/remote_id pair.
+ */
+static __inline struct ahc_initiator_tinfo *
+ahc_fetch_transinfo(struct ahc_softc *ahc, char channel, u_int our_id,
+		    u_int remote_id, struct tmode_tstate **tstate)
+{
+	/*
+	 * Transfer data structures are stored from the perspective
+	 * of the target role.  Since the parameters for a connection
+	 * in the initiator role to a given target are the same as
+	 * when the roles are reversed, we pretend we are the target.
+	 */
+	if (channel == 'B')
+		our_id += 8;
+	*tstate = ahc->enabled_targets[our_id];
+	return (&(*tstate)->transinfo[remote_id]);
+}
+
+/*
+ * Get a free scb. If there are none, see if we can allocate a new SCB.
+ */
+static __inline struct scb *
+ahc_get_scb(struct ahc_softc *ahc)
+{
+	struct scb *scb;
+
+	if ((scb = SLIST_FIRST(&ahc->scb_data->free_scbs)) == NULL) {
+		ahc_alloc_scbs(ahc);
+		scb = SLIST_FIRST(&ahc->scb_data->free_scbs);
+		if (scb == NULL)
+			return (NULL);
+	}
+	SLIST_REMOVE_HEAD(&ahc->scb_data->free_scbs, links.sle);
+	return (scb);
+}
+
+/*
+ * Return an SCB resource to the free list.
+ */
+static __inline void
+ahc_free_scb(struct ahc_softc *ahc, struct scb *scb)
+{       
+	struct hardware_scb *hscb;
+
+	hscb = scb->hscb;
+	/* Clean up for the next user */
+	ahc->scb_data->scbindex[hscb->tag] = NULL;
+	scb->flags = SCB_FREE;
+	hscb->control = 0;
+
+	SLIST_INSERT_HEAD(&ahc->scb_data->free_scbs, scb, links.sle);
+
+	/* Notify the OSM that a resource is now available. */
+	ahc_platform_scb_free(ahc, scb);
+}
+
+static __inline struct scb *
+ahc_lookup_scb(struct ahc_softc *ahc, u_int tag)
+{
+	return (ahc->scb_data->scbindex[tag]);
+
+}
+
+static __inline void
+ahc_swap_with_next_hscb(struct ahc_softc *ahc, struct scb *scb)
+{
+	struct hardware_scb *q_hscb;
+	u_int  saved_tag;
+
+	/*
+	 * Our queuing method is a bit tricky.  The card
+	 * knows in advance which HSCB to download, and we
+	 * can't disappoint it.  To achieve this, the next
+	 * SCB to download is saved off in ahc->next_queued_scb.
+	 * When we are called to queue "an arbitrary scb",
+	 * we copy the contents of the incoming HSCB to the one
+	 * the sequencer knows about, swap HSCB pointers and
+	 * finally assign the SCB to the tag indexed location
+	 * in the scb_array.  This makes sure that we can still
+	 * locate the correct SCB by SCB_TAG.
+	 */
+	q_hscb = ahc->next_queued_scb->hscb;
+	saved_tag = q_hscb->tag;
+	memcpy(q_hscb, scb->hscb, sizeof(*scb->hscb));
+	if ((scb->flags & SCB_CDB32_PTR) != 0) {
+		q_hscb->shared_data.cdb_ptr =
+		    ahc_hscb_busaddr(ahc, q_hscb->tag)
+		  + offsetof(struct hardware_scb, cdb32);	
+	}
+	q_hscb->tag = saved_tag;
+	q_hscb->next = scb->hscb->tag;
+
+	/* Now swap HSCB pointers. */
+	ahc->next_queued_scb->hscb = scb->hscb;
+	scb->hscb = q_hscb;
+
+	/* Now define the mapping from tag to SCB in the scbindex */
+	ahc->scb_data->scbindex[scb->hscb->tag] = scb;
+}
+
+/*
+ * Tell the sequencer about a new transaction to execute.
+ */
+static __inline void
+ahc_queue_scb(struct ahc_softc *ahc, struct scb *scb)
+{
+	ahc_swap_with_next_hscb(ahc, scb);
+
+	if (scb->hscb->tag == SCB_LIST_NULL
+	 || scb->hscb->next == SCB_LIST_NULL)
+		panic("Attempt to queue invalid SCB tag %x:%x\n",
+		      scb->hscb->tag, scb->hscb->next);
+
+	/*
+	 * Keep a history of SCBs we've downloaded in the qinfifo.
+	 */
+	ahc->qinfifo[ahc->qinfifonext++] = scb->hscb->tag;
+	if ((ahc->features & AHC_QUEUE_REGS) != 0) {
+		ahc_outb(ahc, HNSCB_QOFF, ahc->qinfifonext);
+	} else {
+		if ((ahc->features & AHC_AUTOPAUSE) == 0)
+			pause_sequencer(ahc);
+		ahc_outb(ahc, KERNEL_QINPOS, ahc->qinfifonext);
+		if ((ahc->features & AHC_AUTOPAUSE) == 0)
+			unpause_sequencer(ahc);
+	}
+}
+
+static __inline struct scsi_sense_data *
+ahc_get_sense_buf(struct ahc_softc *ahc, struct scb *scb)
+{
+	int offset;
+
+	offset = scb - ahc->scb_data->scbarray;
+	return (&ahc->scb_data->sense[offset]);
+}
+
+static __inline uint32_t
+ahc_get_sense_bufaddr(struct ahc_softc *ahc, struct scb *scb)
+{
+	int offset;
+
+	offset = scb - ahc->scb_data->scbarray;
+	return (ahc->scb_data->sense_busaddr
+	      + (offset * sizeof(struct scsi_sense_data)));
+}
+
+/************************** Interrupt Processing ******************************/
+static __inline u_int ahc_check_cmdcmpltqueues(struct ahc_softc *ahc);
+static __inline void ahc_intr(struct ahc_softc *ahc);
+
+/*
+ * See if the firmware has posted any completed commands
+ * into our in-core command complete fifos.
+ */
+#define AHC_RUN_QOUTFIFO 0x1
+#define AHC_RUN_TQINFIFO 0x2
+static __inline u_int
+ahc_check_cmdcmpltqueues(struct ahc_softc *ahc)
+{
+	u_int retval;
+
+	retval = 0;
+	if (ahc->qoutfifo[ahc->qoutfifonext] != SCB_LIST_NULL)
+		retval |= AHC_RUN_QOUTFIFO;
+#ifdef AHC_TARGET_MODE
+	if ((ahc->flags & AHC_TARGETROLE) != 0
+	 && ahc->targetcmds[ahc->tqinfifonext].cmd_valid != 0)
+		retval |= AHC_RUN_TQINFIFO;
+#endif
+	return (retval);
+}
+
+/*
+ * Catch an interrupt from the adapter
+ */
+static __inline void
+ahc_intr(struct ahc_softc *ahc)
+{
+	u_int	intstat;
+	u_int 	queuestat;
+
+	/*
+	 * Instead of directly reading the interrupt status register,
+	 * infer the cause of the interrupt by checking our in-core
+	 * completion queues.  This avoids a costly PCI bus read in
+	 * most cases.
+	 */
+	intstat = 0;
+	if ((queuestat = ahc_check_cmdcmpltqueues(ahc)) != 0)
+		intstat = CMDCMPLT;
+
+	if ((intstat & INT_PEND) == 0
+	 || (ahc->flags & AHC_ALL_INTERRUPTS) != 0) {
+
+		intstat = ahc_inb(ahc, INTSTAT);
+#if AHC_PCI_CONFIG > 0
+		if (ahc->unsolicited_ints > 500
+		 && (ahc->chip & AHC_PCI) != 0
+		 && (ahc_inb(ahc, ERROR) & PCIERRSTAT) != 0)
+			ahc_pci_intr(ahc);
+#endif
+	}
+
+	if (intstat == 0xFF && (ahc->features & AHC_REMOVABLE) != 0)
+		/* Hot eject */
+		return;
+
+	if ((intstat & INT_PEND) == 0) {
+		ahc->unsolicited_ints++;
+		return;
+	}
+	ahc->unsolicited_ints = 0;
+
+	if (intstat & CMDCMPLT) {
+		ahc_outb(ahc, CLRINT, CLRCMDINT);
+
+		/*
+		 * Ensure that the chip sees that we've cleared
+		 * this interrupt before we walk the output fifo.
+		 * Otherwise, we may, due to posted bus writes,
+		 * clear the interrupt after we finish the scan,
+		 * and after the sequencer has added new entries
+		 * and asserted the interrupt again.
+		 */
+		ahc_flush_device_writes(ahc);
+#ifdef AHC_TARGET_MODE
+		if ((queuestat & AHC_RUN_QOUTFIFO) != 0)
+#endif
+			ahc_run_qoutfifo(ahc);
+#ifdef AHC_TARGET_MODE
+		if ((queuestat & AHC_RUN_TQINFIFO) != 0)
+			ahc_run_tqinfifo(ahc, /*paused*/FALSE);
+#endif
+	}
+	if (intstat & BRKADRINT) {
+		ahc_handle_brkadrint(ahc);
+		/* Fatal error, no more interrupts to handle. */
+		return;
+	}
+
+	if ((intstat & (SEQINT|SCSIINT)) != 0)
+		ahc_pause_bug_fix(ahc);
+
+	if ((intstat & SEQINT) != 0)
+		ahc_handle_seqint(ahc, intstat);
+
+	if ((intstat & SCSIINT) != 0)
+		ahc_handle_scsiint(ahc, intstat);
+}
+
+#endif  /* _AIC7XXX_INLINE_H_ */
diff -urN linux/drivers/scsi/aic7xxx/aic7xxx_linux.c /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_linux.c
--- linux/drivers/scsi/aic7xxx/aic7xxx_linux.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_linux.c	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,2577 @@
+/*
+ * Adaptec AIC7xxx device driver for Linux.
+ *
+ * $Id: //depot/src/linux/drivers/scsi/aic7xxx/aic7xxx_linux.c#50 $
+ *
+ * Copyright (c) 1994 John Aycock
+ *   The University of Calgary Department of Computer Science.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2, or (at your option)
+ * any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; see the file COPYING.  If not, write to
+ * the Free Software Foundation, 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ * Sources include the Adaptec 1740 driver (aha1740.c), the Ultrastor 24F
+ * driver (ultrastor.c), various Linux kernel source, the Adaptec EISA
+ * config file (!adp7771.cfg), the Adaptec AHA-2740A Series User's Guide,
+ * the Linux Kernel Hacker's Guide, Writing a SCSI Device Driver for Linux,
+ * the Adaptec 1542 driver (aha1542.c), the Adaptec EISA overlay file
+ * (adp7770.ovl), the Adaptec AHA-2740 Series Technical Reference Manual,
+ * the Adaptec AIC-7770 Data Book, the ANSI SCSI specification, the
+ * ANSI SCSI-2 specification (draft 10c), ...
+ *
+ * --------------------------------------------------------------------------
+ *
+ *  Modifications by Daniel M. Eischen (deischen@iworks.InterWorks.org):
+ *
+ *  Substantially modified to include support for wide and twin bus
+ *  adapters, DMAing of SCBs, tagged queueing, IRQ sharing, bug fixes,
+ *  SCB paging, and other rework of the code.
+ *
+ * --------------------------------------------------------------------------
+ * Copyright (c) 1994, 1995, 1996, 1997, 1998, 1999, 2000 Justin T. Gibbs.
+ * Copyright (c) 2000 Adaptec Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ *---------------------------------------------------------------------------
+ *
+ *  Thanks also go to (in alphabetical order) the following:
+ *
+ *    Rory Bolt     - Sequencer bug fixes
+ *    Jay Estabrook - Initial DEC Alpha support
+ *    Doug Ledford  - Much needed abort/reset bug fixes
+ *    Kai Makisara  - DMAing of SCBs
+ *
+ *  A Boot time option was also added for not resetting the scsi bus.
+ *
+ *    Form:  aic7xxx=extended
+ *           aic7xxx=no_reset
+ *           aic7xxx=ultra
+ *           aic7xxx=irq_trigger:[0,1]  # 0 edge, 1 level
+ *           aic7xxx=verbose
+ *
+ *  Daniel M. Eischen, deischen@iworks.InterWorks.org, 1/23/97
+ *
+ *  Id: aic7xxx.c,v 4.1 1997/06/12 08:23:42 deang Exp
+ */
+
+/*
+ * Further driver modifications made by Doug Ledford <dledford@redhat.com>
+ *
+ * Copyright (c) 1997-1999 Doug Ledford
+ *
+ * These changes are released under the same licensing terms as the FreeBSD
+ * driver written by Justin Gibbs.  Please see his Copyright notice above
+ * for the exact terms and conditions covering my changes as well as the
+ * warranty statement.
+ *
+ * Modifications made to the aic7xxx.c,v 4.1 driver from Dan Eischen include
+ * but are not limited to:
+ *
+ *  1: Import of the latest FreeBSD sequencer code for this driver
+ *  2: Modification of kernel code to accomodate different sequencer semantics
+ *  3: Extensive changes throughout kernel portion of driver to improve
+ *     abort/reset processing and error hanndling
+ *  4: Other work contributed by various people on the Internet
+ *  5: Changes to printk information and verbosity selection code
+ *  6: General reliability related changes, especially in IRQ management
+ *  7: Modifications to the default probe/attach order for supported cards
+ *  8: SMP friendliness has been improved
+ *
+ */
+
+/*
+ * The next three defines are user configurable.  These should be the only
+ * defines a user might need to get in here and change.  There are other
+ * defines buried deeper in the code, but those really shouldn't need touched
+ * under normal conditions.
+ */
+
+#if defined(MODULE) || defined(PCMCIA)
+#include <linux/module.h>
+#endif
+
+#if defined(PCMCIA)
+#undef MODULE
+#endif
+
+#include "aic7xxx_osm.h"
+#include "aic7xxx_inline.h"
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,3,0)
+#include <linux/init.h>		/* __setup */
+#endif
+
+#include "../sd.h"		/* For geometry detection */
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,3,0)
+DECLARE_MUTEX_LOCKED(eh_sem);
+#else
+struct semaphore eh_sem = MUTEX_LOCKED;
+#endif
+
+/*
+ * To generate the correct addresses for the controller to issue
+ * on the bus.  Originally added for DEC Alpha support.
+ */
+#define VIRT_TO_BUS(a) (uint32_t)virt_to_bus((void *)(a))
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,3,0)
+struct proc_dir_entry proc_scsi_aic7xxx = {
+	PROC_SCSI_AIC7XXX, 7, "aic7xxx",
+	S_IFDIR | S_IRUGO | S_IXUGO, 2,
+	0, 0, 0, NULL, NULL, NULL, NULL, NULL, NULL, NULL
+};
+#endif
+
+/*
+ * Set this to the delay in seconds after SCSI bus reset.
+ * Note, we honor this only for the initial bus reset.
+ * The scsi error recovery code performs its own bus settle
+ * delay handling for error recovery actions.
+ */
+#ifdef CONFIG_AIC7XXX_RESET_DELAY
+#define AIC7XXX_RESET_DELAY CONFIG_AIC7XXX_RESET_DELAY
+#else
+#define AIC7XXX_RESET_DELAY 5000
+#endif
+
+/*
+ * Control collection of SCSI transfer statistics for the /proc filesystem.
+ *
+ * NOTE: Do NOT enable this when running on kernels version 1.2.x and below.
+ * NOTE: This does affect performance since it has to maintain statistics.
+ */
+#ifdef CONFIG_AIC7XXX_PROC_STATS
+#define AIC7XXX_PROC_STATS
+#endif
+
+/*
+ * To change the default number of tagged transactions allowed per-device,
+ * add a line to the lilo.conf file like:
+ * append="aic7xxx=verbose,tag_info:{{32,32,32,32},{32,32,32,32}}"
+ * which will result in the first four devices on the first two
+ * controllers being set to a tagged queue depth of 32.
+ *
+ * The tag_commands is an array of 16 to allow for wide and twin adapters.
+ * Twin adapters will use indexes 0-7 for channel 0, and indexes 8-15
+ * for channel 1.
+ */
+typedef struct {
+	uint8_t tag_commands[16];	/* Allow for wide/twin adapters. */
+} adapter_tag_info_t;
+
+/*
+ * Modify this as you see fit for your system.
+ *
+ * 0			tagged queuing disabled
+ * 1 <= n <= 253	n == max tags ever dispatched.
+ *
+ * The driver will throttle the number of commands dispatched to a
+ * device if it returns queue full.  For devices with a fixed maximum
+ * queue depth, the driver will eventually determine this depth and
+ * lock it in (a console message is printed to indicate that a lock
+ * has occurred).  On some devices, queue full is returned for a temporary
+ * resource shortage.  These devices will return queue full at varying
+ * depths.  The driver will throttle back when the queue fulls occur and
+ * attempt to slowly increase the depth over time as the device recovers
+ * from the resource shortage.
+ *
+ * In this example, the first line will disable tagged queueing for all
+ * the devices on the first probed aic7xxx adapter.
+ *
+ * The second line enables tagged queueing with 4 commands/LUN for IDs
+ * (0, 2-11, 13-15), disables tagged queueing for ID 12, and tells the
+ * driver to attempt to use up to 64 tags for ID 1.
+ *
+ * The third line is the same as the first line.
+ *
+ * The fourth line disables tagged queueing for devices 0 and 3.  It
+ * enables tagged queueing for the other IDs, with 16 commands/LUN
+ * for IDs 1 and 4, 127 commands/LUN for ID 8, and 4 commands/LUN for
+ * IDs 2, 5-7, and 9-15.
+ */
+
+/*
+ * NOTE: The below structure is for reference only, the actual structure
+ *       to modify in order to change things is just below this comment block.
+adapter_tag_info_t aic7xxx_tag_info[] =
+{
+	{{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}},
+	{{4, 64, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4}},
+	{{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}},
+	{{0, 16, 4, 0, 16, 4, 4, 4, 127, 4, 4, 4, 4, 4, 4, 4}}
+};
+*/
+
+#ifdef CONFIG_AIC7XXX_CMDS_PER_DEVICE
+#define AIC7XXX_CMDS_PER_DEVICE CONFIG_AIC7XXX_CMDS_PER_DEVICE
+#else
+#define AIC7XXX_CMDS_PER_DEVICE AHC_MAX_QUEUE
+#endif
+
+#define AIC7XXX_CONFIGED_TAG_COMMANDS {					\
+	AIC7XXX_CMDS_PER_DEVICE, AIC7XXX_CMDS_PER_DEVICE,		\
+	AIC7XXX_CMDS_PER_DEVICE, AIC7XXX_CMDS_PER_DEVICE,		\
+	AIC7XXX_CMDS_PER_DEVICE, AIC7XXX_CMDS_PER_DEVICE,		\
+	AIC7XXX_CMDS_PER_DEVICE, AIC7XXX_CMDS_PER_DEVICE,		\
+	AIC7XXX_CMDS_PER_DEVICE, AIC7XXX_CMDS_PER_DEVICE,		\
+	AIC7XXX_CMDS_PER_DEVICE, AIC7XXX_CMDS_PER_DEVICE,		\
+	AIC7XXX_CMDS_PER_DEVICE, AIC7XXX_CMDS_PER_DEVICE,		\
+	AIC7XXX_CMDS_PER_DEVICE, AIC7XXX_CMDS_PER_DEVICE		\
+}
+
+/*
+ * By default, use the number of commands specified by
+ * the users kernel configuration.
+ */
+static adapter_tag_info_t aic7xxx_tag_info[] =
+{
+	{AIC7XXX_CONFIGED_TAG_COMMANDS},
+	{AIC7XXX_CONFIGED_TAG_COMMANDS},
+	{AIC7XXX_CONFIGED_TAG_COMMANDS},
+	{AIC7XXX_CONFIGED_TAG_COMMANDS},
+	{AIC7XXX_CONFIGED_TAG_COMMANDS},
+	{AIC7XXX_CONFIGED_TAG_COMMANDS},
+	{AIC7XXX_CONFIGED_TAG_COMMANDS},
+	{AIC7XXX_CONFIGED_TAG_COMMANDS},
+	{AIC7XXX_CONFIGED_TAG_COMMANDS},
+	{AIC7XXX_CONFIGED_TAG_COMMANDS},
+	{AIC7XXX_CONFIGED_TAG_COMMANDS},
+	{AIC7XXX_CONFIGED_TAG_COMMANDS},
+	{AIC7XXX_CONFIGED_TAG_COMMANDS},
+	{AIC7XXX_CONFIGED_TAG_COMMANDS},
+	{AIC7XXX_CONFIGED_TAG_COMMANDS},
+	{AIC7XXX_CONFIGED_TAG_COMMANDS}
+};
+
+/*
+ * There should be a specific return value for this in scsi.h, but
+ * it seems that most drivers ignore it.
+ */
+#define DID_UNDERFLOW   DID_ERROR
+
+void
+ahc_print_path(struct ahc_softc *ahc, struct scb *scb)
+{
+	printk("(scsi%d:%c:%d:%d): ",
+	       ahc->platform_data->host_no,
+	       scb != NULL ? SCB_GET_CHANNEL(ahc, scb) : 'X',
+	       scb != NULL ? SCB_GET_TARGET(ahc, scb) : -1,
+	       scb != NULL ? SCB_GET_LUN(scb) : -1);
+}
+
+/*
+ * XXX - these options apply unilaterally to _all_ 274x/284x/294x
+ *       cards in the system.  This should be fixed.  Exceptions to this
+ *       rule are noted in the comments.
+ */
+
+/*
+ * Skip the scsi bus reset.  Non 0 make us skip the reset at startup.  This
+ * has no effect on any later resets that might occur due to things like
+ * SCSI bus timeouts.
+ */
+static uint32_t aic7xxx_no_reset;
+
+/*
+ * Certain PCI motherboards will scan PCI devices from highest to lowest,
+ * others scan from lowest to highest, and they tend to do all kinds of
+ * strange things when they come into contact with PCI bridge chips.  The
+ * net result of all this is that the PCI card that is actually used to boot
+ * the machine is very hard to detect.  Most motherboards go from lowest
+ * PCI slot number to highest, and the first SCSI controller found is the
+ * one you boot from.  The only exceptions to this are when a controller
+ * has its BIOS disabled.  So, we by default sort all of our SCSI controllers
+ * from lowest PCI slot number to highest PCI slot number.  We also force
+ * all controllers with their BIOS disabled to the end of the list.  This
+ * works on *almost* all computers.  Where it doesn't work, we have this
+ * option.  Setting this option to non-0 will reverse the order of the sort
+ * to highest first, then lowest, but will still leave cards with their BIOS
+ * disabled at the very end.  That should fix everyone up unless there are
+ * really strange cirumstances.
+ */
+static int aic7xxx_reverse_scan = 0;
+
+/*
+ * Should we force EXTENDED translation on a controller.
+ *     0 == Use whatever is in the SEEPROM or default to off
+ *     1 == Use whatever is in the SEEPROM or default to on
+ */
+static uint32_t aic7xxx_extended = 0;
+
+/*
+ * The IRQ trigger method used on EISA controllers. Does not effect PCI cards.
+ *   -1 = Use detected settings.
+ *    0 = Force Edge triggered mode.
+ *    1 = Force Level triggered mode.
+ */
+static int aic7xxx_irq_trigger = -1;
+
+/*
+ * This variable is used to override the termination settings on a controller.
+ * This should not be used under normal conditions.  However, in the case
+ * that a controller does not have a readable SEEPROM (so that we can't
+ * read the SEEPROM settings directly) and that a controller has a buggered
+ * version of the cable detection logic, this can be used to force the
+ * correct termination.  It is preferable to use the manual termination
+ * settings in the BIOS if possible, but some motherboard controllers store
+ * those settings in a format we can't read.  In other cases, auto term
+ * should also work, but the chipset was put together with no auto term
+ * logic (common on motherboard controllers).  In those cases, we have
+ * 32 bits here to work with.  That's good for 8 controllers/channels.  The
+ * bits are organized as 4 bits per channel, with scsi0 getting the lowest
+ * 4 bits in the int.  A 1 in a bit position indicates the termination setting
+ * that corresponds to that bit should be enabled, a 0 is disabled.
+ * It looks something like this:
+ *
+ *    0x0f =  1111-Single Ended Low Byte Termination on/off
+ *            ||\-Single Ended High Byte Termination on/off
+ *            |\-LVD Low Byte Termination on/off
+ *            \-LVD High Byte Termination on/off
+ *
+ * For non-Ultra2 controllers, the upper 2 bits are not important.  So, to
+ * enable both high byte and low byte termination on scsi0, I would need to
+ * make sure that the override_term variable was set to 0x03 (bits 0011).
+ * To make sure that all termination is enabled on an Ultra2 controller at
+ * scsi2 and only high byte termination on scsi1 and high and low byte
+ * termination on scsi0, I would set override_term=0xf23 (bits 1111 0010 0011)
+ *
+ * For the most part, users should never have to use this, that's why I
+ * left it fairly cryptic instead of easy to understand.  If you need it,
+ * most likely someone will be telling you what your's needs to be set to.
+ */
+static int aic7xxx_override_term = -1;
+
+/*
+ * Certain motherboard chipset controllers tend to screw
+ * up the polarity of the term enable output pin.  Use this variable
+ * to force the correct polarity for your system.  This is a bitfield variable
+ * similar to the previous one, but this one has one bit per channel instead
+ * of four.
+ *    0 = Force the setting to active low.
+ *    1 = Force setting to active high.
+ * Most Adaptec cards are active high, several motherboards are active low.
+ * To force a 2940 card at SCSI 0 to active high and a motherboard 7895
+ * controller at scsi1 and scsi2 to active low, and a 2910 card at scsi3
+ * to active high, you would need to set stpwlev=0x9 (bits 1001).
+ *
+ * People shouldn't need to use this, but if you are experiencing lots of
+ * SCSI timeout problems, this may help.  There is one sure way to test what
+ * this option needs to be.  Using a boot floppy to boot the system, configure
+ * your system to enable all SCSI termination (in the Adaptec SCSI BIOS) and
+ * if needed then also pass a value to override_term to make sure that the
+ * driver is enabling SCSI termination, then set this variable to either 0
+ * or 1.  When the driver boots, make sure there are *NO* SCSI cables
+ * connected to your controller.  If it finds and inits the controller
+ * without problem, then the setting you passed to stpwlev was correct.  If
+ * the driver goes into a reset loop and hangs the system, then you need the
+ * other setting for this variable.  If neither setting lets the machine
+ * boot then you have definite termination problems that may not be fixable.
+ */
+static int aic7xxx_stpwlev = -1;
+
+/*
+ * Set this to non-0 in order to force the driver to panic the kernel
+ * and print out debugging info on a SCSI abort or reset cycle.
+ */
+static int aic7xxx_panic_on_abort = 0;
+
+/*
+ * PCI bus parity checking of the Adaptec controllers.  This is somewhat
+ * dubious at best.  To my knowledge, this option has never actually
+ * solved a PCI parity problem, but on certain machines with broken PCI
+ * chipset configurations, it can generate tons of false error messages.
+ * It's included in the driver for completeness.
+ *   0 = Shut off PCI parity check
+ *  -1 = Normal polarity pci parity checking
+ *   1 = reverse polarity pci parity checking
+ *
+ * NOTE: you can't actually pass -1 on the lilo prompt.  So, to set this
+ * variable to -1 you would actually want to simply pass the variable
+ * name without a number.  That will invert the 0 which will result in
+ * -1.
+ */
+static int aic7xxx_pci_parity = 0;
+
+/*
+ * Set this to a non-0 value to make us dump out the 32 bit instruction
+ * registers on the card after completing the sequencer download.  This
+ * allows the actual sequencer download to be verified.  It is possible
+ * to use this option and still boot up and run your system.  This is
+ * only intended for debugging purposes.
+ */
+static int aic7xxx_dump_sequencer = 0;
+
+/*
+ * Certain newer motherboards have put new PCI based devices into the
+ * IO spaces that used to typically be occupied by VLB or EISA cards.
+ * This overlap can cause these newer motherboards to lock up when scanned
+ * for older EISA and VLB devices.  Setting this option to non-0 will
+ * cause the driver to skip scanning for any VLB or EISA controllers and
+ * only support the PCI controllers.  NOTE: this means that if the kernel
+ * os compiled with PCI support disabled, then setting this to non-0
+ * would result in never finding any devices :)
+ */
+int aic7xxx_no_probe;
+
+/*
+ * aic7xxx_detect() has been run, so register all device arrivals
+ * immediately with the system rather than deferring to the sorted
+ * attachment performed by aic7xxx_detect().
+ */
+int aic7xxx_detect_complete;
+
+/*
+ * So that we can set how long each device is given as a selection timeout.
+ * The table of values goes like this:
+ *   0 - 256ms
+ *   1 - 128ms
+ *   2 - 64ms
+ *   3 - 32ms
+ * We default to 256ms because some older devices need a longer time
+ * to respond to initial selection.
+ */
+static int aic7xxx_seltime = 0x00;
+
+/*
+ * So that insmod can find the variable and make it point to something
+ */
+#ifdef MODULE
+static char *aic7xxx = NULL;
+
+MODULE_PARM(aic7xxx, "s");
+
+/*
+ * Just in case someone uses commas to separate items on the insmod
+ * command line, we define a dummy buffer here to avoid having insmod
+ * write wild stuff into our code segment
+ */
+static char dummy_buffer[60] = "Please don't trounce on me insmod!!\n";
+
+#endif
+
+static void ahc_handle_scsi_status(struct ahc_softc *,
+				   struct ahc_linux_device *, struct scb *);
+static void ahc_filter_command(struct ahc_softc *ahc, Scsi_Cmnd *cmd);
+static int aic7xxx_queue_recovery_cmd(Scsi_Cmnd *cmd, scb_flag flag);
+static void aic7xxx_initialize_scsi_bus(struct ahc_softc *ahc);
+static void aic7xxx_select_queue_depth(struct Scsi_Host *host,
+				       Scsi_Device *scsi_devs);
+static void aic7xxx_device_queue_depth(struct ahc_softc *ahc,
+				       Scsi_Device *device);
+static struct ahc_linux_target*	ahc_alloc_target(struct ahc_softc *,
+						 u_int, u_int);
+static void			ahc_free_target(struct ahc_softc *ahc,
+						struct ahc_linux_target *targ);
+static struct ahc_linux_device*	ahc_alloc_device(struct ahc_softc *,
+						 struct ahc_linux_target *,
+						 u_int);
+static void			ahc_free_device(struct ahc_softc *ahc,
+						struct ahc_linux_device *dev);
+static void ahc_run_device_queue(struct ahc_softc *ahc,
+				 struct ahc_linux_device *dev);
+static void ahc_setup_tag_info(char *p, char *end);
+static int ahc_next_unit(void);
+
+static __inline struct ahc_linux_device*
+		     ahc_get_device(struct ahc_softc *ahc, u_int channel,
+				    u_int target, u_int lun, int /*alloc*/);
+static __inline void ahc_queue_cmd_complete(struct ahc_softc *ahc,
+					    Scsi_Cmnd *cmd);
+static __inline void ahc_run_complete_queue(struct ahc_softc *ahc,
+					    struct ahc_cmd *acmd);
+static __inline void ahc_check_device_queue(struct ahc_softc *ahc,
+					  struct ahc_linux_device *dev);
+static __inline void ahc_sniff_command(struct ahc_softc *ahc, Scsi_Cmnd *cmd);
+static __inline void ahc_unmap_scb(struct ahc_softc *ahc, struct scb *scb);
+
+static __inline struct ahc_linux_device*
+ahc_get_device(struct ahc_softc *ahc, u_int channel, u_int target,
+	       u_int lun, int alloc)
+{
+	struct ahc_linux_target *targ;
+	struct ahc_linux_device *dev;
+	u_int target_offset;
+
+	target_offset = target;
+	if (channel != 0)
+		target_offset += 8;
+	targ = ahc->platform_data->targets[target_offset];
+	if (targ == NULL) {
+		if (alloc != 0) {
+			targ = ahc_alloc_target(ahc, channel, target);
+			if (targ == NULL)
+				return (NULL);
+		} else
+			return (NULL);
+	}
+	dev = targ->devices[lun];
+	if (dev == NULL && alloc != 0)
+		dev = ahc_alloc_device(ahc, targ, lun);
+	return (dev);
+}
+
+static __inline void
+ahc_queue_cmd_complete(struct ahc_softc *ahc, Scsi_Cmnd *cmd)
+{
+	/*
+	 * Typically, the complete queue has very few entries
+	 * queued to it before the queue is emptied by
+	 * ahc_run_complete_queue, so sorting the entries
+	 * by generation number should be inexpensive.
+	 * We perform the sort so that commands that complete
+	 * with an error are retuned in the order origionally
+	 * queued to the controller so that any subsequent retries
+	 * are performed in order.  The underlying ahc routines do
+	 * not guarantee the order that aborted commands will be
+	 * returned to us.
+	 */
+	struct ahc_completeq *completeq;
+	struct ahc_cmd *list_cmd;
+	struct ahc_cmd *acmd;
+
+	/*
+	 * If we want the request requeued, make sure there
+	 * are sufficent retries.  In the old scsi error code,
+	 * we used to be able to specify a result code that
+	 * bypassed the retry count.  Now we must use this
+	 * hack.
+	 */
+	if (cmd->result == (CAM_REQUEUE_REQ << 16))
+		cmd->retries--;
+	completeq = &ahc->platform_data->completeq;
+	list_cmd = TAILQ_FIRST(completeq);
+	acmd = (struct ahc_cmd *)cmd;
+	while (list_cmd != NULL
+	    && acmd_scsi_cmd(list_cmd).serial_number
+	     < acmd_scsi_cmd(acmd).serial_number)
+		list_cmd = TAILQ_NEXT(list_cmd, acmd_links.tqe);
+	if (list_cmd != NULL)
+		TAILQ_INSERT_BEFORE(list_cmd, acmd, acmd_links.tqe);
+	else
+		TAILQ_INSERT_TAIL(completeq, acmd, acmd_links.tqe);
+}
+
+static __inline void
+ahc_run_complete_queue(struct ahc_softc *ahc, struct ahc_cmd *acmd)
+{	
+	u_long done_flags;
+
+	ahc_done_lock(ahc, &done_flags);
+	while (acmd != NULL) {
+		Scsi_Cmnd *cmd;
+
+		cmd = &acmd_scsi_cmd(acmd);
+		acmd = TAILQ_NEXT(acmd, acmd_links.tqe);
+		cmd->host_scribble = NULL;
+		cmd->scsi_done(cmd);
+	}
+	ahc_done_unlock(ahc, &done_flags);
+}
+
+static __inline void
+ahc_check_device_queue(struct ahc_softc *ahc, struct ahc_linux_device *dev)
+{
+	if ((dev->flags & AHC_DEV_FREEZE_TIL_EMPTY) != 0
+	 && dev->active == 0) {
+		dev->flags &= ~AHC_DEV_FREEZE_TIL_EMPTY;
+		dev->qfrozen--;
+	}
+
+	if (TAILQ_FIRST(&dev->busyq) == NULL
+	 || dev->openings == 0 || dev->qfrozen != 0)
+		return;
+
+	ahc_run_device_queue(ahc, dev);
+}
+
+static __inline void
+ahc_run_device_queues(struct ahc_softc *ahc)
+{
+	struct ahc_linux_device *dev;
+
+	while ((ahc->flags & AHC_RESOURCE_SHORTAGE) == 0
+	    && (dev = LIST_FIRST(&ahc->platform_data->device_runq)) != NULL) {
+		LIST_REMOVE(dev, links);
+		dev->flags &= ~AHC_DEV_ON_RUN_LIST;
+		ahc_check_device_queue(ahc, dev);
+	}
+}
+
+static __inline void
+ahc_sniff_command(struct ahc_softc *ahc, Scsi_Cmnd *cmd)
+{
+	/*
+	 * Determine whether we care to filter
+	 * information out of this command.  If so,
+	 * pass it on to ahc_filter_command() for more
+	 * heavy weight processing.
+	 */
+	if (cmd->cmnd[0] == INQUIRY)
+		ahc_filter_command(ahc, cmd);
+}
+
+static __inline void
+ahc_unmap_scb(struct ahc_softc *ahc, struct scb *scb)
+{
+	Scsi_Cmnd *cmd;
+
+	cmd = scb->io_ctx;
+	if (cmd->use_sg != 0) {
+		struct scatterlist *sg;
+
+		sg = (struct scatterlist *)cmd->request_buffer;
+		pci_unmap_sg(ahc->dev_softc, sg, cmd->use_sg,
+			     scsi_to_pci_dma_dir(cmd->sc_data_direction));
+	} else if (cmd->request_bufflen != 0)
+		pci_unmap_single(ahc->dev_softc,
+				 ahc_le32toh(scb->sg_list[0].addr),
+				 cmd->request_bufflen,
+				 scsi_to_pci_dma_dir(cmd->sc_data_direction));
+}
+
+/******************************** Macros **************************************/
+#define BUILD_SCSIID(ahc, cmd)						\
+	((((cmd)->target << TID_SHIFT) & TID)				\
+	| (((cmd)->channel == 0) ? (ahc)->our_id : (ahc)->our_id_b)	\
+	| (((cmd)->channel == 0) ? 0 : TWIN_CHNLB))
+
+/******************************** Bus DMA *************************************/
+int
+ahc_dma_tag_create(struct ahc_softc *ahc, bus_dma_tag_t parent,
+		   bus_size_t alignment, bus_size_t boundary,
+		   bus_addr_t lowaddr, bus_addr_t highaddr,
+		   bus_dma_filter_t *filter, void *filterarg,
+		   bus_size_t maxsize, int nsegments,
+		   bus_size_t maxsegsz, int flags, bus_dma_tag_t *ret_tag)
+{
+	bus_dma_tag_t dmat;
+
+	dmat = malloc(sizeof(*dmat), M_DEVBUF, M_NOWAIT);
+	if (dmat == NULL)
+		return (ENOMEM);
+
+	/*
+	 * Linux is very simplistic about DMA memory.  For now don't
+	 * maintain all specification information.  Once Linux supplies
+	 * better facilities for doing these operations, or the
+	 * needs of this particular driver change, we might need to do
+	 * more here.
+	 */
+	dmat->alignment = alignment;
+	dmat->boundary = boundary;
+	dmat->maxsize = maxsize;
+	*ret_tag = dmat;
+	return (0);
+}
+
+void
+ahc_dma_tag_destroy(struct ahc_softc *ahc, bus_dma_tag_t dmat)
+{
+	free(dmat, M_DEVBUF);
+}
+
+int
+ahc_dmamem_alloc(struct ahc_softc *ahc, bus_dma_tag_t dmat, void** vaddr,
+		 int flags, bus_dmamap_t *mapp)
+{
+	bus_dmamap_t map;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,3,0)
+	map = malloc(sizeof(*map), M_DEVBUF, M_NOWAIT);
+	if (map == NULL)
+		return (ENOMEM);
+	*vaddr = pci_alloc_consistent(ahc->dev_softc,
+				      dmat->maxsize, &map->bus_addr);
+#else
+	/*
+	 * At least in 2.2.14, malloc is a slab allocator so all
+	 * allocations are aligned.  We assume, for these kernel versions
+	 * that all allocations will be bellow 4Gig, physically contiguous,
+	 * and accessable via DMA by the controller.
+	 */
+	map = NULL; /* No additional information to store */
+	*vaddr = malloc(dmat->maxsize, M_DEVBUF, M_NOWAIT);
+#endif
+	if (*vaddr == NULL)
+		return (ENOMEM);
+	*mapp = map;
+	return(0);
+}
+
+void
+ahc_dmamem_free(struct ahc_softc *ahc, bus_dma_tag_t dmat,
+		void* vaddr, bus_dmamap_t map)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,3,0)
+	pci_free_consistent(ahc->dev_softc, dmat->maxsize,
+			    vaddr, map->bus_addr);
+#else
+	free(vaddr, M_DEVBUF);
+#endif
+}
+
+int
+ahc_dmamap_load(struct ahc_softc *ahc, bus_dma_tag_t dmat, bus_dmamap_t map,
+		void *buf, bus_size_t buflen, bus_dmamap_callback_t *cb,
+		void *cb_arg, int flags)
+{
+	/*
+	 * Assume for now that this will only be used during
+	 * initialization and not for per-transaction buffer mapping.
+	 */
+	bus_dma_segment_t stack_sg;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,3,0)
+	stack_sg.ds_addr = map->bus_addr;
+#else
+	stack_sg.ds_addr = VIRT_TO_BUS(buf);
+#endif
+	stack_sg.ds_len = dmat->maxsize;
+	cb(cb_arg, &stack_sg, /*nseg*/1, /*error*/0);
+	return (0);
+}
+
+void
+ahc_dmamap_destroy(struct ahc_softc *ahc, bus_dma_tag_t dmat, bus_dmamap_t map)
+{
+	/*
+	 * The map may is NULL in our < 2.3.X implementation.
+	 */
+	if (map != NULL)
+		free(map, M_DEVBUF);
+}
+
+int
+ahc_dmamap_unload(struct ahc_softc *ahc, bus_dma_tag_t dmat, bus_dmamap_t map)
+{
+	/* Nothing to do */
+	return (0);
+}
+
+/********************* Platform Dependent Functions ***************************/
+int
+ahc_softc_comp(struct ahc_softc *lahc, struct ahc_softc *rahc)
+{
+	int	value;
+	int	rvalue;
+	int	lvalue;
+
+	/*
+	 * Under Linux, cards are ordered as follows:
+	 *	1) VLB/EISA BIOS enabled devices sorted by BIOS address.
+	 *	2) PCI devices with BIOS enabled sorted by bus/slot/func.
+	 *	3) All remaining VLB/EISA devices sorted by ioport.
+	 *	4) All remaining PCI devices sorted by bus/slot/func.
+	 */
+	value = (lahc->flags & AHC_BIOS_ENABLED)
+	      - (rahc->flags & AHC_BIOS_ENABLED);
+	if (value != 0)
+		/* Controllers with BIOS enabled have a *higher* priority */
+		return (-value);
+
+	/*
+	 * Same BIOS setting, now sort based on bus type.
+	 * EISA and VL controllers sort together.  EISA/VL
+	 * have higher priority than PCI.
+	 */
+	rvalue = (rahc->chip & AHC_BUS_MASK);
+ 	if (rvalue == AHC_VL)
+		rvalue = AHC_EISA;
+	lvalue = (lahc->chip & AHC_BUS_MASK);
+ 	if (lvalue == AHC_VL)
+		lvalue = AHC_EISA;
+	value = lvalue - rvalue;
+	if (value != 0)
+		return (value);
+
+	/* Still equal.  Sort by BIOS address, ioport, or bus/slot/func. */
+	switch (rvalue) {
+	case AHC_PCI:
+		value = ahc_get_pci_bus(lahc->dev_softc)
+		      - ahc_get_pci_bus(rahc->dev_softc);
+		if (value != 0)
+			break;
+		value = ahc_get_pci_slot(lahc->dev_softc)
+		      - ahc_get_pci_slot(rahc->dev_softc);
+		if (value != 0)
+			break;
+		value = ahc_get_pci_function(lahc->dev_softc)
+		      - ahc_get_pci_function(rahc->dev_softc);
+		/*
+		 * On multi-function devices, the user can choose
+		 * to have function 1 probed before function 0.
+		 * Function 0 is the only one that will have
+		 * CHANNEL_B_PRIMARY set.
+		 */
+		if (value < 0
+		 && (lahc->flags & AHC_CHANNEL_B_PRIMARY) != 0)
+			/* Swap the two */
+			value = -value;
+		break;
+	case AHC_EISA:
+		if ((rahc->flags & AHC_BIOS_ENABLED) != 0) {
+			value = lahc->platform_data->bios_address
+			      - rahc->platform_data->bios_address; 
+		} else {
+			value = lahc->bsh.ioport
+			      - rahc->bsh.ioport; 
+		}
+		break;
+	default:
+		panic("ahc_softc_sort: invalid bus type");
+	}
+	return (value);
+}
+
+static void
+ahc_setup_tag_info(char *p, char *end)
+{
+	char	*base;
+	char	*tok;
+	char	*tok_end;
+	char	*tok_end2;
+	int      i;
+	int      instance;
+	int	 device;
+	int	 done;
+	char	 tok_list[] = {'.', ',', '{', '}', '\0'};
+
+	if (*p != ':')
+		return;
+
+	instance = -1;
+	device = -1;
+	done = FALSE;
+	base = p;
+	/* Forward us just past the ':' */
+	tok = base + 1;
+	tok_end = strchr(tok, '\0');
+	if (tok_end < end)
+		*tok_end = ',';
+	while (!done) {
+		switch (*tok) {
+		case '{':
+			if (instance == -1)
+				instance = 0;
+			else if (device == -1)
+				device = 0;
+			tok++;
+			break;
+		case '}':
+			if (device != -1)
+				device = -1;
+			else if (instance != -1)
+				instance = -1;
+			tok++;
+			break;
+		case ',':
+		case '.':
+			if (instance == -1)
+				done = TRUE;
+			else if (device >= 0)
+				device++;
+			else if (instance >= 0)
+				instance++;
+			if ((device >= AHC_NUM_TARGETS) ||
+			    (instance >= NUM_ELEMENTS(aic7xxx_tag_info)))
+				done = TRUE;
+			tok++;
+			if (!done) {
+				base = tok;
+			}
+			break;
+		case '\0':
+			done = TRUE;
+			break;
+		default:
+			done = TRUE;
+			tok_end = strchr(tok, '\0');
+			for (i = 0; tok_list[i]; i++) {
+				tok_end2 = strchr(tok, tok_list[i]);
+				if ((tok_end2) && (tok_end2 < tok_end)) {
+					tok_end = tok_end2;
+					done = FALSE;
+				}
+			}
+			if ((instance >= 0) && (device >= 0) &&
+			    (instance < NUM_ELEMENTS(aic7xxx_tag_info)) &&
+			    (device < AHC_NUM_TARGETS))
+				aic7xxx_tag_info[instance].tag_commands[device] =
+				    simple_strtoul(tok, NULL, 0) & 0xff;
+			tok = tok_end;
+			break;
+		}
+	}
+	while ((p != base) && (p != NULL))
+		p = strtok(NULL, ",.");
+}
+
+/*
+ * Handle Linux boot parameters. This routine allows for assigning a value
+ * to a parameter with a ':' between the parameter and the value.
+ * ie. aic7xxx=unpause:0x0A,extended
+ */
+int
+aic7xxx_setup(char *s)
+{
+	int	i, n;
+	char   *p;
+	char   *end;
+
+	static struct {
+		const char *name;
+		uint32_t *flag;
+	} options[] = {
+		{ "extended", &aic7xxx_extended },
+		{ "no_reset", &aic7xxx_no_reset },
+		{ "irq_trigger", &aic7xxx_irq_trigger },
+		{ "verbose", &aic7xxx_verbose },
+		{ "reverse_scan", &aic7xxx_reverse_scan },
+		{ "override_term", &aic7xxx_override_term },
+		{ "stpwlev", &aic7xxx_stpwlev },
+		{ "no_probe", &aic7xxx_no_probe },
+		{ "panic_on_abort", &aic7xxx_panic_on_abort },
+		{ "pci_parity", &aic7xxx_pci_parity },
+		{ "dump_sequencer", &aic7xxx_dump_sequencer },
+		{ "seltime", &aic7xxx_seltime },
+		{ "tag_info", NULL }
+	};
+
+	end = strchr(s, '\0');
+
+	for (p = strtok(s, ",."); p; p = strtok(NULL, ",.")) {
+		for (i = 0; i < NUM_ELEMENTS(options); i++) {
+			n = strlen(options[i].name);
+
+			if (strncmp(options[i].name, p, n) != 0)
+				continue;
+
+			if (strncmp(p, "tag_info", n) == 0) {
+				ahc_setup_tag_info(p + n, end);
+			} else if (p[n] == ':') {
+				*(options[i].flag) =
+				    simple_strtoul(p + n + 1, NULL, 0);
+			} else if (!strncmp(p, "verbose", n)) {
+				*(options[i].flag) = 1;
+			} else {
+				*(options[i].flag) = ~(*(options[i].flag));
+			}
+		}
+	}
+	return 1;
+}
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,3,0)
+__setup("aic7xxx=", aic7xxx_setup);
+#endif
+
+int aic7xxx_verbose;
+
+/*
+ * Try to detect an Adaptec 7XXX controller.
+ */
+int
+aic7xxx_detect(Scsi_Host_Template *template)
+{
+	struct	ahc_softc *ahc;
+	int     found;
+
+	/*
+	 * Sanity checking of Linux SCSI data structures so
+	 * that some of our hacks^H^H^H^H^Hassumptions aren't
+	 * violated.
+	 */
+	if (offsetof(struct ahc_cmd_internal, end)
+	  > offsetof(struct scsi_cmnd, host_scribble)) {
+		printf("aic7xxx_detect: SCSI data structures changed.\n");
+		printf("aic7xxx_detect: Unable to attach\n");
+		return (0);
+	}
+#ifdef MODULE
+	/*
+	 * If we've been passed any parameters, process them now.
+	 */
+	if (aic7xxx)
+		aic7xxx_setup(aic7xxx);
+	if (dummy_buffer[0] != 'P')
+		printk(KERN_WARNING
+"aic7xxx: Please read the file /usr/src/linux/drivers/scsi/README.aic7xxx\n"
+"aic7xxx: to see the proper way to specify options to the aic7xxx module\n"
+"aic7xxx: Specifically, don't use any commas when passing arguments to\n"
+"aic7xxx: insmod or else it might trash certain memory areas.\n");
+#endif
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,3,0)
+	template->proc_name = "aic7xxx";
+#else
+	template->proc_dir = &proc_scsi_aic7xxx;
+#endif
+	template->sg_tablesize = AHC_NSEG;
+
+#ifdef CONFIG_PCI
+	ahc_linux_pci_probe(template);
+#endif
+
+	aic7770_linux_probe(template);
+
+	/*
+	 * Register with the SCSI layer all
+	 * controllers we've found.
+	 */
+	found = 0;
+	TAILQ_FOREACH(ahc, &ahc_tailq, links) {
+
+		if (aic7xxx_register_host(ahc, template) == 0)
+			found++;
+	}
+	aic7xxx_detect_complete++;
+	return (found);
+}
+
+int
+aic7xxx_register_host(struct ahc_softc *ahc, Scsi_Host_Template *template)
+{
+	char  buf[80];
+	struct Scsi_Host *host;
+	char *new_name;
+	u_long s;
+
+
+	template->name = ahc->description;
+	host = scsi_register(template, sizeof(struct ahc_softc *));
+	if (host == NULL)
+		return (ENOMEM);
+
+	ahc_lock(ahc, &s);
+	*((struct ahc_softc **)host->hostdata) = ahc;
+	ahc->platform_data->host_no = host->host_no;
+	ahc->platform_data->host = host;
+	host->can_queue = AHC_MAX_QUEUE;
+	host->cmd_per_lun = 2;
+	host->sg_tablesize = AHC_NSEG;
+	host->select_queue_depths = aic7xxx_select_queue_depth;
+	host->this_id = ahc->our_id;
+	host->irq = ahc->platform_data->irq;
+	host->max_id = (ahc->features & AHC_WIDE) ? 16 : 8;
+	host->max_channel = (ahc->features & AHC_TWIN) ? 1 : 0;
+	ahc_set_unit(ahc, ahc_next_unit());
+	sprintf(buf, "scsi%d", host->host_no);
+	new_name = malloc(strlen(buf) + 1, M_DEVBUF, M_NOWAIT);
+	if (new_name != NULL) {
+		strcpy(new_name, buf);
+		ahc_set_name(ahc, new_name);
+	}
+	host->unique_id = ahc->unit;
+	aic7xxx_initialize_scsi_bus(ahc);
+	ahc_unlock(ahc, &s);
+	return (0);
+}
+
+/*
+ * Find the smallest available unit number to use
+ * for a new device.  We don't just use a static
+ * count to handle the "repeated hot-(un)plug"
+ * scenario.
+ */
+static int
+ahc_next_unit()
+{
+	struct ahc_softc *ahc;
+	int unit;
+
+	unit = 0;
+retry:
+	TAILQ_FOREACH(ahc, &ahc_tailq, links) {
+		if (ahc->unit == unit) {
+			unit++;
+			goto retry;
+		}
+	}
+	return (unit);
+}
+
+/*
+ * Place the SCSI bus into a known state by either resetting it,
+ * or forcing transfer negotiations on the next command to any
+ * target.
+ */
+void
+aic7xxx_initialize_scsi_bus(struct ahc_softc *ahc)
+{
+	int i;
+	int numtarg;
+
+	i = 0;
+	numtarg = 0;
+
+	if (aic7xxx_no_reset != 0)
+		ahc->flags &= ~(AHC_RESET_BUS_A|AHC_RESET_BUS_B);
+
+	if ((ahc->flags & AHC_RESET_BUS_A) != 0)
+		ahc_reset_channel(ahc, 'A', /*initiate_reset*/TRUE);
+	else
+		numtarg = (ahc->features & AHC_WIDE) ? 16 : 8;
+
+	if ((ahc->features & AHC_TWIN) != 0) {
+
+		if ((ahc->flags & AHC_RESET_BUS_B) != 0) {
+			ahc_reset_channel(ahc, 'B', /*initiate_reset*/TRUE);
+		} else {
+			if (numtarg == 0)
+				i = 8;
+			numtarg += 8;
+		}
+	}
+
+	for (; i < numtarg; i++) {
+		struct ahc_devinfo devinfo;
+		struct ahc_initiator_tinfo *tinfo;
+		struct tmode_tstate *tstate;
+		u_int our_id;
+		u_int target_id;
+		char channel;
+
+		channel = 'A';
+		our_id = ahc->our_id;
+		target_id = i;
+		if (i > 7 && (ahc->features & AHC_TWIN) != 0) {
+			channel = 'B';
+			our_id = ahc->our_id_b;
+			target_id = i % 8;
+		}
+		tinfo = ahc_fetch_transinfo(ahc, channel, our_id,
+					    target_id, &tstate);
+		tinfo->goal = tinfo->user;
+		ahc_compile_devinfo(&devinfo, our_id, target_id,
+				   CAM_LUN_WILDCARD, channel, ROLE_INITIATOR);
+		ahc_update_target_msg_request(ahc, &devinfo, tinfo,
+					     /*force*/FALSE, /*paused*/FALSE);
+	}
+
+	/* Give the bus some time to recover */
+	if ((ahc->flags & (AHC_RESET_BUS_A|AHC_RESET_BUS_B)) != 0)
+		ahc_delay(AIC7XXX_RESET_DELAY * 1000);
+}
+
+int
+ahc_platform_alloc(struct ahc_softc *ahc, void *platform_arg)
+{
+	ahc->platform_data =
+	    malloc(sizeof(struct ahc_platform_data), M_DEVBUF, M_NOWAIT);
+	if (ahc->platform_data == NULL)
+		return (ENOMEM);
+	memset(ahc->platform_data, 0, sizeof(struct ahc_platform_data));
+	TAILQ_INIT(&ahc->platform_data->completeq);
+	LIST_INIT(&ahc->platform_data->device_runq);
+	ahc_lockinit(ahc);
+	ahc_done_lockinit(ahc);
+	ahc->seltime = (aic7xxx_seltime << 4) & 0x3;
+	ahc->seltime_b = (aic7xxx_seltime << 4) & 0x3;
+	return (0);
+}
+
+void
+ahc_platform_free(struct ahc_softc *ahc)
+{
+	if (ahc->platform_data != NULL) {
+		if (ahc->platform_data->host != NULL)
+			scsi_unregister(ahc->platform_data->host);
+		if (ahc->platform_data->irq)
+			free_irq(ahc->platform_data->irq, ahc);
+		if (ahc->tag == BUS_SPACE_PIO
+		 && ahc->bsh.ioport != 0)
+			release_region(ahc->bsh.ioport, 256);
+		if (ahc->tag == BUS_SPACE_MEMIO
+		 && ahc->bsh.maddr != NULL) {
+			u_long base_addr;
+
+			base_addr = (u_long)ahc->bsh.maddr;
+			base_addr &= PAGE_MASK;
+			iounmap((void *)base_addr);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,4,0)
+			release_mem_region(ahc->platform_data->mem_busaddr,
+					   0x1000);
+#endif
+		}
+		free(ahc->platform_data, M_DEVBUF);
+	}
+}
+
+void
+ahc_platform_freeze_devq(struct ahc_softc *ahc, struct scb *scb)
+{
+	ahc_platform_abort_scbs(ahc, SCB_GET_TARGET(ahc, scb),
+				SCB_GET_CHANNEL(ahc, scb),
+				SCB_GET_LUN(scb), SCB_LIST_NULL,
+				ROLE_UNKNOWN, CAM_REQUEUE_REQ);
+}
+
+void
+ahc_platform_set_tags(struct ahc_softc *ahc,
+		      struct ahc_devinfo *devinfo, int enable)
+{
+	struct ahc_linux_device *dev;
+
+	dev = ahc_get_device(ahc, devinfo->channel - 'A',
+			     devinfo->target,
+			     devinfo->lun, /*alloc*/FALSE);
+	if ((dev->flags & AHC_DEV_FREEZE_TIL_EMPTY) == 0
+	 && (dev->active != 0)) {
+		dev->flags |= AHC_DEV_FREEZE_TIL_EMPTY;
+		dev->qfrozen++;
+	}
+
+	if (enable) {
+		/*
+		 * Start out agressively and allow our
+		 * dynamic queue depth algorithm take
+		 * care of the rest.
+		 */
+		dev->maxtags = AHC_MAX_QUEUE;
+		dev->openings = dev->maxtags - dev->active;
+	} else {
+		/* We can only have one opening */
+		dev->maxtags = 0;
+		dev->openings =  1 - dev->active;
+	}
+}
+
+int
+ahc_platform_abort_scbs(struct ahc_softc *ahc, int target, char channel,
+			int lun, u_int tag, role_t role, uint32_t status)
+{
+	int chan;
+	int maxchan;
+	int targ;
+	int maxtarg;
+	int clun;
+	int maxlun;
+	int count;
+
+	if (tag != SCB_LIST_NULL)
+		return (0);
+
+	chan = 0;
+	if (channel != ALL_CHANNELS) {
+		chan = channel - 'A';
+		maxchan = chan + 1;
+	} else {
+		maxchan = (ahc->features & AHC_TWIN) ? 2 : 1;
+	}
+	targ = 0;
+	if (target != CAM_TARGET_WILDCARD) {
+		targ = target;
+		maxtarg = targ + 1;
+	} else {
+		maxtarg = (ahc->features & AHC_WIDE) ? 16 : 8;
+	}
+	clun = 0;
+	if (lun != CAM_LUN_WILDCARD) {
+		clun = lun;
+		maxlun = clun + 1;
+	} else {
+		maxlun = 16;
+	}
+
+	count = 0;
+	for (; chan < maxchan; chan++) {
+		for (; targ < maxtarg; targ++) {
+			for (; clun < maxlun; clun++) {
+				struct ahc_linux_device *dev;
+				struct ahc_busyq *busyq;
+				struct ahc_cmd *acmd;
+
+				dev = ahc_get_device(ahc, chan, targ,
+						     clun, /*alloc*/FALSE);
+
+				if (dev == NULL)
+					continue;
+
+				busyq = &dev->busyq;
+				while ((acmd = TAILQ_FIRST(busyq)) != NULL) {
+					Scsi_Cmnd *cmd;
+
+					cmd = &acmd_scsi_cmd(acmd);
+					TAILQ_REMOVE(busyq, acmd,
+						     acmd_links.tqe);
+					count++;
+					cmd->result = status << 16;
+					ahc_queue_cmd_complete(ahc, cmd);
+				}
+			}
+		}
+	}
+
+	return (count);
+}
+
+/*
+ * Sets the queue depth for each SCSI device hanging
+ * off the input host adapter.
+ */
+static void
+aic7xxx_select_queue_depth(struct Scsi_Host * host,
+			   Scsi_Device * scsi_devs)
+{
+	Scsi_Device *device;
+	struct	ahc_softc *ahc;
+	u_long	flags;
+	int	scbnum;
+
+	ahc = *((struct ahc_softc **)host->hostdata);
+	ahc_lock(ahc, &flags);
+	scbnum = 0;
+	for (device = scsi_devs; device != NULL; device = device->next) {
+		if (device->host == host) {
+			aic7xxx_device_queue_depth(ahc, device);
+			scbnum += device->queue_depth;
+		}
+	}
+	ahc_unlock(ahc, &flags);
+}
+
+/*
+ * Determines the queue depth for a given device.
+ */
+static void
+aic7xxx_device_queue_depth(struct ahc_softc *ahc, Scsi_Device * device)
+{
+	struct	ahc_devinfo devinfo;
+	struct	ahc_initiator_tinfo *targ_info;
+	struct	tmode_tstate *tstate;
+	uint8_t tags;
+
+	ahc_compile_devinfo(&devinfo,
+			    device->channel == 0 ? ahc->our_id : ahc->our_id_b,
+			    device->id, device->lun,
+			    device->channel == 0 ? 'A' : 'B',
+			    ROLE_INITIATOR);
+	targ_info = ahc_fetch_transinfo(ahc, devinfo.channel,
+					devinfo.our_scsiid,
+					devinfo.target, &tstate);
+
+	tags = 0;
+	if (device->tagged_supported != 0
+	 && (tstate->discenable & devinfo.target_mask) != 0) {
+		if (ahc->unit >= NUM_ELEMENTS(aic7xxx_tag_info)) {
+
+			printf("aic7xxx: WARNING, insufficient "
+			       "tag_info instances for installed "
+			       "controllers. Using defaults\n");
+			printf("aic7xxx: Please update the "
+			       "aic7xxx_tag_info array in the "
+			       "aic7xxx.c source file.\n");
+			tags = AHC_MAX_QUEUE;
+		} else {
+			adapter_tag_info_t *tag_info;
+
+			tag_info = &aic7xxx_tag_info[ahc->unit];
+			tags = tag_info->tag_commands[devinfo.target_offset];
+			if (tags > AHC_MAX_QUEUE)
+				tags = AHC_MAX_QUEUE;
+		}
+	}
+	if (tags != 0) {
+		device->queue_depth = tags;
+		ahc_set_tags(ahc, &devinfo, TRUE);
+		printf("scsi%d:%d:%d:%d: Tagged Queuing enabled.  Depth %d\n",
+		       ahc->unit, device->channel, device->id, device->lun,
+		       tags);
+	} else {
+		/*
+		 * We allow the OS to queue 2 untagged transactions to
+		 * us at any time even though we can only execute them
+		 * serially on the controller/device.  This should remove
+		 * some latency.
+		 */
+		device->queue_depth = 2;
+	}
+}
+
+/*
+ * Queue an SCB to the controller.
+ */
+int
+aic7xxx_queue(Scsi_Cmnd * cmd, void (*scsi_done) (Scsi_Cmnd *))
+{
+	struct	 ahc_softc *ahc;
+	struct	 ahc_linux_device *dev;
+	struct	 ahc_cmd *acmd;
+	u_long	 flags;
+
+	ahc = *(struct ahc_softc **)cmd->host->hostdata;
+
+	/*
+	 * Save the callback on completion function.
+	 */
+	cmd->scsi_done = scsi_done;
+
+	ahc_lock(ahc, &flags);
+	dev = ahc_get_device(ahc, cmd->channel, cmd->target,
+			     cmd->lun, /*alloc*/TRUE);
+	if (dev == NULL) {
+		ahc_unlock(ahc, &flags);
+		printf("aic7xxx_queue: Unable to allocate device!\n");
+		return (-ENOMEM);
+	}
+	cmd->result = CAM_REQ_INPROG << 16;
+	TAILQ_INSERT_TAIL(&dev->busyq, (struct ahc_cmd *)cmd, acmd_links.tqe);
+	ahc_run_device_queue(ahc, dev);
+	acmd = TAILQ_FIRST(&ahc->platform_data->completeq);
+	ahc_unlock(ahc, &flags);
+	if (acmd != NULL)
+		ahc_run_complete_queue(ahc, acmd);
+	return (0);
+}
+
+static void
+ahc_run_device_queue(struct ahc_softc *ahc, struct ahc_linux_device *dev)
+{
+	struct	 ahc_cmd *acmd;
+	struct	 scsi_cmnd *cmd;
+	struct	 scb *scb;
+	struct	 hardware_scb *hscb;
+	struct	 ahc_initiator_tinfo *tinfo;
+	struct	 tmode_tstate *tstate;
+	uint16_t mask;
+
+	while ((acmd = TAILQ_FIRST(&dev->busyq)) != NULL
+	    && dev->openings > 0 && dev->qfrozen == 0) {
+
+		/*
+		 * Get an scb to use.
+		 */
+		if ((scb = ahc_get_scb(ahc)) == NULL) {
+			if ((dev->flags & AHC_DEV_ON_RUN_LIST) != 0)
+				panic("running device on run list");
+			LIST_INSERT_HEAD(&ahc->platform_data->device_runq,
+					 dev, links);
+			dev->flags |= AHC_DEV_ON_RUN_LIST;
+			ahc->flags |= AHC_RESOURCE_SHORTAGE;
+			printf("%s: Temporary Resource Shortage\n",
+			       ahc_name(ahc));
+			return;
+		}
+		TAILQ_REMOVE(&dev->busyq, acmd, acmd_links.tqe);
+		cmd = &acmd_scsi_cmd(acmd);
+		scb->io_ctx = cmd;
+		scb->platform_data->dev = dev;
+		hscb = scb->hscb;
+		cmd->host_scribble = (char *)scb;
+
+		/*
+		 * Fill out basics of the HSCB.
+		 */
+		hscb->control = 0;
+		hscb->scsiid = BUILD_SCSIID(ahc, cmd);
+		hscb->lun = cmd->lun;
+		mask = SCB_GET_TARGET_MASK(ahc, scb);
+		tinfo = ahc_fetch_transinfo(ahc, SCB_GET_CHANNEL(ahc, scb),
+					    SCB_GET_OUR_ID(scb),
+					    SCB_GET_TARGET(ahc, scb), &tstate);
+		hscb->scsirate = tinfo->scsirate;
+		hscb->scsioffset = tinfo->current.offset;
+		if ((tstate->ultraenb & mask) != 0)
+			hscb->control |= ULTRAENB;
+
+		if ((tstate->discenable & mask) != 0)
+			hscb->control |= DISCENB;
+
+		if ((tstate->tagenable & mask) != 0) {
+			/* XXX What about devices that dislike ordered tags? */
+			if ((dev->num_commands % 256) == 0)
+				hscb->control |= MSG_ORDERED_Q_TAG;
+			else
+				hscb->control |= MSG_SIMPLE_Q_TAG;
+		}
+
+		hscb->cdb_len = cmd->cmd_len;
+		if (hscb->cdb_len <= 12) {
+			memcpy(hscb->shared_data.cdb, cmd->cmnd, hscb->cdb_len);
+		} else {
+			memcpy(hscb->cdb32, cmd->cmnd, hscb->cdb_len);
+			scb->flags |= SCB_CDB32_PTR;
+		}
+
+		scb->platform_data->xfer_len = 0;
+		if (cmd->use_sg != 0) {
+			struct	ahc_dma_seg *sg;
+			struct	scatterlist *cur_seg;
+			struct	scatterlist *end_seg;
+			int	nseg;
+
+			cur_seg = (struct scatterlist *)cmd->request_buffer;
+			nseg = pci_map_sg(ahc->dev_softc, cur_seg, cmd->use_sg,
+				 scsi_to_pci_dma_dir(cmd ->sc_data_direction));
+			end_seg = cur_seg + nseg;
+			/* Copy the segments into the SG list. */
+			sg = scb->sg_list;
+			while(cur_seg < end_seg) {
+				sg->addr = ahc_htole32(sg_dma_address(cur_seg));
+/* XXX Add in the 5th byte of the address later.*/
+				sg->len = ahc_htole32(sg_dma_len(cur_seg));
+				scb->platform_data->xfer_len +=
+				    sg_dma_len(cur_seg);
+				sg++;
+				cur_seg++;
+			}
+			sg--;
+			sg->len |= ahc_htole32(AHC_DMA_LAST_SEG);
+
+			/*
+			 * Reset the sg list pointer.
+			 */
+			scb->hscb->sgptr =
+			    ahc_htole32(scb->sg_list_phys | SG_FULL_RESID);
+
+			/*
+			 * Copy the first SG into the "current"
+			 * data pointer area.
+			 */
+			scb->hscb->dataptr = scb->sg_list->addr;
+			scb->hscb->datacnt = scb->sg_list->len;
+
+			/*
+			 * Remember the number of segments for later
+			 * residual calculations.
+			 */
+			scb->sg_count = nseg;
+		} else if (cmd->request_bufflen != 0) {
+			struct	 ahc_dma_seg *sg;
+			uint32_t baddr;
+
+			sg = scb->sg_list;
+			baddr = pci_map_single(ahc->dev_softc,
+			       cmd->request_buffer,
+			       cmd->request_bufflen,
+			       scsi_to_pci_dma_dir(cmd->sc_data_direction));
+			sg->addr = ahc_htole32(baddr);
+			sg->len = ahc_htole32(cmd->request_bufflen
+					      | AHC_DMA_LAST_SEG);
+
+			/*
+			 * Reset the sg list pointer.
+			 */
+			scb->hscb->sgptr =
+			    ahc_htole32(scb->sg_list_phys | SG_FULL_RESID);
+
+			/*
+			 * Copy the first SG into the "current"
+			 * data pointer area.
+			 */
+			scb->hscb->dataptr = sg->addr;
+			scb->hscb->datacnt = sg->len;
+			scb->platform_data->xfer_len = cmd->request_bufflen;
+
+			/*
+			 * Remember the number of segments for later
+			 * residual calculations.
+			 */
+			scb->sg_count = 1;
+		} else {
+			scb->hscb->sgptr = ahc_htole32(SG_LIST_NULL);
+			scb->hscb->dataptr = 0;
+			scb->hscb->datacnt = 0;
+			scb->sg_count = 0;
+		}
+
+		LIST_INSERT_HEAD(&ahc->pending_scbs, scb, pending_links);
+		dev->openings--;
+		dev->active++;
+		dev->num_commands++;
+
+		/*
+		 * We only allow one untagged transaction
+		 * per target in the initiator role unless
+		 * we are storing a full busy target *lun*
+		 * table in SCB space.
+		 */
+		if ((scb->hscb->control & (TARGET_SCB|TAG_ENB)) == 0
+		 && (ahc->features & AHC_SCB_BTT) == 0) {
+			struct scb_tailq *untagged_q;
+
+			untagged_q = &(ahc->untagged_queues[cmd->target]);
+			TAILQ_INSERT_TAIL(untagged_q, scb, links.tqe);
+			scb->flags |= SCB_UNTAGGEDQ;
+			if (TAILQ_FIRST(untagged_q) != scb)
+				continue;
+		}
+		scb->flags |= SCB_ACTIVE;
+		ahc_queue_scb(ahc, scb);
+	}
+}
+
+/*
+ * SCSI controller interrupt handler.
+ */
+void
+aic7xxx_isr(int irq, void *dev_id, struct pt_regs * regs)
+{
+	struct ahc_softc *ahc;
+	struct ahc_cmd *acmd;
+	u_long flags;
+
+	ahc = (struct ahc_softc *) dev_id;
+	ahc_lock(ahc, &flags); 
+	ahc_intr(ahc);
+	/*
+	 * It would be nice to run the device queues from a
+	 * bottom half handler, but as there is no way to
+	 * dynamically register one, we'll have to postpone
+	 * that until we get integrated into the kernel.
+	 */
+	ahc_run_device_queues(ahc);
+	acmd = TAILQ_FIRST(&ahc->platform_data->completeq);
+	TAILQ_INIT(&ahc->platform_data->completeq);
+	ahc_unlock(ahc, &flags);
+	if (acmd != NULL)
+		ahc_run_complete_queue(ahc, acmd);
+}
+
+void
+ahc_platform_flushwork(struct ahc_softc *ahc)
+{
+	struct ahc_cmd *acmd;
+
+	acmd = TAILQ_FIRST(&ahc->platform_data->completeq);
+	TAILQ_INIT(&ahc->platform_data->completeq);
+	if (acmd != NULL)
+		ahc_run_complete_queue(ahc, acmd);
+}
+
+static struct ahc_linux_target*
+ahc_alloc_target(struct ahc_softc *ahc, u_int channel, u_int target)
+{
+	struct ahc_linux_target *targ;
+	u_int target_offset;
+
+	targ = malloc(sizeof(*targ), M_DEVBUG, M_NOWAIT);
+	if (targ == NULL)
+		return (NULL);
+	memset(targ, 0, sizeof(*targ));
+	targ->channel = channel;
+	targ->target = target;
+	target_offset = target;
+	if (channel != 0)
+		target_offset += 8;
+	ahc->platform_data->targets[target_offset] = targ;
+	return (targ);
+}
+
+static void
+ahc_free_target(struct ahc_softc *ahc, struct ahc_linux_target *targ)
+{
+	u_int target_offset;
+
+	target_offset = targ->target;
+	if (targ->channel != 0)
+		target_offset += 8;
+	ahc->platform_data->targets[target_offset] = NULL;
+	free(targ, M_DEVBUF);
+}
+
+static struct ahc_linux_device*
+ahc_alloc_device(struct ahc_softc *ahc,
+		 struct ahc_linux_target *targ, u_int lun)
+{
+	struct ahc_linux_device *dev;
+
+	dev = malloc(sizeof(*dev), M_DEVBUG, M_NOWAIT);
+	if (dev == NULL)
+		return (NULL);
+	memset(dev, 0, sizeof(*dev));
+	TAILQ_INIT(&dev->busyq);
+	dev->flags = AHC_DEV_UNCONFIGURED;
+	dev->lun = lun;
+	dev->target = targ;
+
+	/*
+	 * We start out life using untagged
+	 * transactions of which we allow one.
+	 */
+	dev->openings = 1;
+
+	/*
+	 * Set maxtags to 0.  This will be changed if we
+	 * later determine that we are dealing with
+	 * a tagged queuing capable device.
+	 */
+	dev->maxtags = 0;
+	
+	targ->refcount++;
+	targ->devices[lun] = dev;
+	return (dev);
+}
+
+static void
+ahc_free_device(struct ahc_softc *ahc, struct ahc_linux_device *dev)
+{
+	struct ahc_linux_target *targ;
+
+	targ = dev->target;
+	targ->devices[dev->lun] = NULL;
+	free(dev, M_DEVBUF);
+	targ->refcount--;
+	if (targ->refcount == 0)
+		ahc_free_target(ahc, targ);
+}
+
+/*
+ * Return a string describing the driver.
+ */
+const char *
+aic7xxx_info(struct Scsi_Host *host)
+{
+	static char buffer[512];
+	char	ahc_info[256];
+	char   *bp;
+	struct ahc_softc *ahc;
+
+	bp = &buffer[0];
+	ahc = *(struct ahc_softc **)host->hostdata;
+	memset(bp, 0, sizeof(buffer));
+	strcpy(bp, "Adaptec AIC7XXX EISA/VLB/PCI SCSI HBA DRIVER, Rev ");
+	strcat(bp, AIC7XXX_DRIVER_VERSION);
+	strcat(bp, "\n");
+	strcat(bp, "        <");
+	strcat(bp, ahc->description);
+	strcat(bp, ">\n");
+	strcat(bp, "        ");
+	ahc_controller_info(ahc, ahc_info);
+	strcat(bp, ahc_info);
+	strcat(bp, "\n");
+
+	return (bp);
+}
+
+void
+ahc_send_async(struct ahc_softc *ahc, char channel,
+	       u_int target, u_int lun, ac_code code)
+{
+	switch (code) {
+	case AC_TRANSFER_NEG:
+	{
+		struct	ahc_initiator_tinfo *targ_info;
+		struct	tmode_tstate *tstate;
+		struct	ahc_transinfo *tinfo; 
+		struct	ahc_syncrate *rate;
+		u_int	period;
+		u_int	ppr_options;
+
+		targ_info = ahc_fetch_transinfo(ahc, channel,
+						channel == 'A' ? ahc->our_id
+							       : ahc->our_id_b,
+						target, &tstate);
+		tinfo = &targ_info->current;
+		period = tinfo->period;
+		ppr_options = tinfo->ppr_options;
+		rate = ahc_find_syncrate(ahc, &period, &ppr_options,
+					 /*maxsync*/AHC_SYNCRATE_DT);
+		if (tinfo->offset == 0)
+			rate = NULL;
+
+		printf("(%s:%c:", ahc_name(ahc), channel);
+		if (target == CAM_TARGET_WILDCARD)
+			printf("*): ");
+		else
+			printf("%d): ", target);
+		if (rate != NULL)
+			printf("synchronous at %sMHz%s, offset 0x%x, ",
+			       rate->rate,
+			       (ppr_options & MSG_EXT_PPR_DT_REQ)
+			       ? " DT" : "", tinfo->offset);
+		else
+			printf("async, ");
+		printf("%dbit\n", (0x1 << tinfo->width) * 8);
+		break;
+	}
+        case AC_SENT_BDR:
+		break;
+        case AC_BUS_RESET:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,3,0)
+		scsi_report_bus_reset(ahc->platform_data->host, channel - 'A');
+#endif
+                break;
+        default:
+                panic("ahc_send_async: Unexpected async event");
+        }
+}
+
+/*
+ * Calls the higher level scsi done function and frees the scb.
+ */
+void
+ahc_done(struct ahc_softc *ahc, struct scb * scb)
+{
+	Scsi_Cmnd *cmd;
+	struct ahc_linux_device *dev;
+
+	LIST_REMOVE(scb, pending_links);
+	if ((scb->flags & SCB_UNTAGGEDQ) != 0) {
+		struct scb_tailq *untagged_q;
+
+		untagged_q = &ahc->untagged_queues[SCB_GET_TARGET(ahc, scb)];
+		TAILQ_REMOVE(untagged_q, scb, links.tqe);
+		ahc_run_untagged_queue(ahc, untagged_q);
+	}
+
+	if ((scb->flags & SCB_ACTIVE) == 0) {
+		printf("SCB %d done'd twice\n", scb->hscb->tag);
+		ahc_dump_card_state(ahc);
+		panic("Stopping for safety");
+	}
+	cmd = scb->io_ctx;
+	dev = scb->platform_data->dev;
+	dev->active--;
+	dev->openings++;
+	ahc_unmap_scb(ahc, scb);
+	if (scb->flags & SCB_SENSE) {
+		memcpy(cmd->sense_buffer, ahc_get_sense_buf(ahc, scb),
+		       MIN(sizeof(struct scsi_sense_data),
+			   sizeof(cmd->sense_buffer)));
+	}
+	if (ahc_get_transaction_status(scb) == CAM_REQ_INPROG) {
+		uint32_t amount_xferred;
+
+		amount_xferred =
+		    ahc_get_transfer_length(scb) - ahc_get_residual(scb);
+		if (amount_xferred < scb->io_ctx->underflow) {
+			printf("Saw underflow (%ld of %ld bytes). "
+			       "Treated as error\n",
+				ahc_get_residual(scb),
+				ahc_get_transfer_length(scb));
+			ahc_set_transaction_status(scb, CAM_DATA_RUN_ERR);
+		} else {
+			ahc_set_transaction_status(scb, CAM_REQ_CMP);
+			ahc_sniff_command(ahc, cmd);
+		}
+	} else if (ahc_get_transaction_status(scb) == DID_OK) {
+		ahc_handle_scsi_status(ahc, dev, scb);
+	}
+
+	if (dev->openings == 1
+	 && ahc_get_transaction_status(scb) == CAM_REQ_CMP
+	 && ahc_get_scsi_status(scb) != SCSI_STATUS_QUEUE_FULL)
+		dev->tag_success_count++;
+	/*
+	 * Some devices deal with temporary internal resource
+	 * shortages by returning queue full.  When the queue
+	 * full occurrs, we throttle back.  Slowly try to get
+	 * back to our previous queue depth.
+	 */
+	if ((dev->openings + dev->active) < dev->maxtags
+	 && dev->tag_success_count > AHC_TAG_SUCCESS_INTERVAL) {
+		dev->tag_success_count = 0;
+		dev->openings++;
+	}
+
+	if (dev->active == 0
+	 && (dev->flags & AHC_DEV_UNCONFIGURED) != 0)
+		ahc_free_device(ahc, dev);
+	else if ((dev->flags & AHC_DEV_ON_RUN_LIST) == 0) {
+		LIST_INSERT_HEAD(&ahc->platform_data->device_runq, dev, links);
+		dev->flags |= AHC_DEV_ON_RUN_LIST;
+	}
+
+	if ((scb->flags & SCB_RECOVERY_SCB) != 0) {
+		printf("Recovery SCB completes\n");
+		up(&eh_sem);
+	}
+
+	ahc_free_scb(ahc, scb);
+	ahc_queue_cmd_complete(ahc, cmd);
+}
+
+static void
+ahc_handle_scsi_status(struct ahc_softc *ahc,
+		       struct ahc_linux_device *dev, struct scb *scb)
+{
+	/*
+	 * We don't currently trust the mid-layer to
+	 * properly deal with queue full or busy.  So,
+	 * when one occurs, we tell the mid-layer to
+	 * unconditionally requeue the command to us
+	 * so that we can retry it ourselves.  We also
+	 * implement our own throttling mechanism so
+	 * we don't clobber the device with too many
+	 * commands.
+	 */
+	switch (ahc_get_scsi_status(scb)) {
+	default:
+		break;
+	case SCSI_STATUS_QUEUE_FULL:
+	{
+		/*
+		 * By the time the core driver has returned this
+		 * command, all other commands that were queued
+		 * to us but not the device have been returned.
+		 * This ensures that dev->active is equal to
+		 * the number of commands actually queued to
+		 * the device.
+		 */
+		dev->tag_success_count = 0;
+		if (dev->active != 0) {
+			/*
+			 * Drop our opening count to the number
+			 * of commands currently outstanding.
+			 */
+			dev->openings = 0;
+/*
+			ahc_print_path(ahc, scb);
+			printf("Dropping tag count to %d\n", dev->active);
+ */
+			if (dev->active == dev->tags_on_last_queuefull) {
+
+				dev->last_queuefull_same_count++;
+				/*
+				 * If we repeatedly see a queue full
+				 * at the same queue depth, this
+				 * device has a fixed number of tag
+				 * slots.  Lock in this tag depth
+				 * so we stop seeing queue fulls from
+				 * this device.
+				 */
+				if (dev->last_queuefull_same_count
+				 == AHC_LOCK_TAGS_COUNT) {
+					dev->maxtags = dev->active;
+					ahc_print_path(ahc, scb);
+					printf("Locking max tag count at %d\n",
+					       dev->active);
+				}
+			} else {
+				dev->tags_on_last_queuefull = dev->active;
+				dev->last_queuefull_same_count = 0;
+			}
+			ahc_set_transaction_status(scb, CAM_REQUEUE_REQ);
+			ahc_set_scsi_status(scb, SCSI_STATUS_OK);
+			break;
+		}
+		/*
+		 * Drop down to a single opening, and treat this
+		 * as if the target return BUSY SCSI status.
+		 */
+		dev->openings = 1;
+		/* FALLTHROUGH */
+	}
+	case SCSI_STATUS_BUSY:
+		/*
+		 * XXX Set a timer and handle ourselves????
+		 * For now we pray that the mid-layer does something
+		 * sane for devices that are busy.
+		 */
+		ahc_set_scsi_status(scb, SCSI_STATUS_BUSY);
+		break;
+	}
+}
+
+static void
+ahc_filter_command(struct ahc_softc *ahc, Scsi_Cmnd *cmd)
+{
+	switch (cmd->cmnd[0]) {
+	case INQUIRY:
+	{
+		struct	ahc_devinfo devinfo;
+		struct	scsi_inquiry_data *sid;
+		struct	ahc_initiator_tinfo *targ_info;
+		struct	tmode_tstate *tstate;
+		struct	ahc_syncrate *syncrate;
+		u_int	scsiid;
+		u_int	maxsync;
+		int	minlen;
+		u_int	width;
+		u_int	period;
+		u_int	offset;
+		u_int	ppr_options;
+
+		if (cmd->use_sg != 0) {
+			printf("%s: SG Inquiry response ignored\n",
+			       ahc_name(ahc));
+			break;
+		}
+		sid = (struct scsi_inquiry_data *)cmd->request_buffer;
+
+		/*
+		 * Determine if this lun actually exists.  If so,
+		 * hold on to its corresponding device structure.
+		 */
+		if (cmd->request_bufflen >= 1
+		 && SID_QUAL(sid) == SID_QUAL_LU_CONNECTED) {
+			struct ahc_linux_device *dev;
+
+			dev = ahc_get_device(ahc, cmd->channel,
+					     cmd->target, cmd->lun,
+					     /*alloc*/FALSE);
+			dev->flags &= ~AHC_DEV_UNCONFIGURED;
+		}
+
+		/*
+		 * Update our notion of this device's transfer
+		 * negotiation capabilities.
+		 */
+		scsiid = BUILD_SCSIID(ahc, cmd);
+		ahc_compile_devinfo(&devinfo, SCSIID_OUR_ID(scsiid),
+				    cmd->target, cmd->lun,
+				    SCSIID_CHANNEL(ahc, scsiid),
+				    ROLE_INITIATOR);
+		targ_info = ahc_fetch_transinfo(ahc, devinfo.channel,
+						devinfo.our_scsiid,
+						devinfo.target, &tstate);
+		/* Structure copy */
+		width = targ_info->user.width;
+		period = targ_info->user.period;
+		offset = targ_info->user.offset;
+		ppr_options = targ_info->user.ppr_options;
+		minlen = offsetof(struct scsi_inquiry_data, version) + 1;
+		if (cmd->request_bufflen >= minlen) {
+			targ_info->current.protocol_version = SID_ANSI_REV(sid);
+
+			/*
+			 * Only attempt SPI3 once we've verified that
+			 * the device claims to support SPI3 features.
+			 */
+			if (targ_info->current.protocol_version < SCSI_REV_2)
+				targ_info->current.transport_version =
+				    SID_ANSI_REV(sid);
+			else
+				targ_info->current.transport_version =
+				     SCSI_REV_2;
+		}
+
+		minlen = offsetof(struct scsi_inquiry_data, flags) + 1;
+		if (cmd->request_bufflen >= minlen
+		 && (sid->additional_length + 4) >= minlen) {
+			if ((sid->flags & SID_WBus16) == 0)
+				width = MSG_EXT_WDTR_BUS_8_BIT;
+			if ((sid->flags & SID_Sync) == 0) {
+				period = 0;
+				offset = 0;
+				ppr_options = 0;
+			}
+		} else {
+			/* Keep current settings */
+			break;
+		}
+		minlen = offsetof(struct scsi_inquiry_data, spi3data) + 1;
+		if (cmd->request_bufflen >= minlen
+		 && (sid->additional_length + 4) >= minlen) {
+			if ((sid->spi3data & SID_SPI_CLOCK_DT) == 0)
+				ppr_options = 0;
+
+			if ((sid->spi3data & SID_SPI_MASK) != 0
+			 && targ_info->current.protocol_version > SCSI_REV_2)
+				targ_info->current.transport_version = 3;
+		} else {
+			ppr_options = 0;
+		}
+
+		ahc_validate_width(ahc, /*tinfo limit*/NULL, &width,
+				   ROLE_UNKNOWN);
+		if ((ahc->features & AHC_ULTRA2) != 0)
+			maxsync = AHC_SYNCRATE_DT;
+		else if ((ahc->features & AHC_ULTRA) != 0)
+			maxsync = AHC_SYNCRATE_ULTRA;
+		else
+			maxsync = AHC_SYNCRATE_FAST;
+
+		syncrate = ahc_find_syncrate(ahc, &period,
+					     &ppr_options, maxsync);
+		ahc_validate_offset(ahc, /*tinfo limit*/NULL, syncrate,
+				    &offset, width, ROLE_UNKNOWN);
+		/* Apply our filtered user settings. */
+		ahc_set_width(ahc, &devinfo, width,
+			      AHC_TRANS_GOAL, /*paused*/FALSE);
+		ahc_set_syncrate(ahc, &devinfo, syncrate, period,
+				 offset, ppr_options, AHC_TRANS_GOAL,
+				 /*paused*/FALSE);
+		break;
+	}
+	default:
+		panic("ahc_filter_command: Unexpected Command type  %x\n",
+		      cmd->cmnd[0]);
+		break;
+	}
+}
+
+static void
+ahc_eh_cmd_done(u_long arg)
+{
+	up(&eh_sem);
+}
+
+static int
+aic7xxx_queue_recovery_cmd(Scsi_Cmnd *cmd, scb_flag flag)
+{
+	struct ahc_softc *ahc;
+	struct ahc_cmd *acmd;
+	struct ahc_cmd *list_acmd;
+	struct ahc_linux_device *dev;
+	struct scb *pending_scb;
+	u_long s;
+	u_int  saved_scbptr;
+	u_int  active_scb_index;
+	u_int  last_phase;
+	int    retval;
+	int    paused;
+	int    wait;
+	int    disconnected;
+
+	paused = FALSE;
+	wait = FALSE;
+	ahc = *(struct ahc_softc **)cmd->host->hostdata;
+	acmd = (struct ahc_cmd *)cmd;
+
+	printf("%s:%d:%d:%d: Attempting to queue a%s message\n",
+	       ahc_name(ahc), cmd->channel, cmd->target, cmd->lun,
+	       flag == SCB_ABORT ? "n ABORT" : "TARGET RESET");
+
+	/*
+	 * It is a bug that the upper layer takes
+	 * this lock just prior to calling us.
+	 */
+	spin_unlock_irq(&io_request_lock);
+
+	ahc_lock(ahc, &s);
+
+	/*
+	 * First determine if we currently own this command.
+	 * Start by searching the device queue.  If not found
+	 * there, check the pending_scb list.  If not found
+	 * at all, and the system wanted us to just abort the
+	 * command return success.
+	 */
+	dev = ahc_get_device(ahc, cmd->channel, cmd->target,
+			     cmd->lun, /*alloc*/FALSE);
+
+	if (dev == NULL) {
+		/*
+		 * No target device for this command exists,
+		 * so we must not still own the command.
+		 */
+		printf("%s:%d:%d:%d: Is not an active device\n",
+		       ahc_name(ahc), cmd->channel, cmd->target, cmd->lun);
+		goto no_cmd;
+	}
+
+	TAILQ_FOREACH(list_acmd, &dev->busyq, acmd_links.tqe) {
+		if (list_acmd == acmd)
+			break;
+	}
+
+	if (list_acmd != NULL) {
+		printf("%s:%d:%d:%d: Command found on device queue\n",
+		       ahc_name(ahc), cmd->channel, cmd->target, cmd->lun);
+		if (flag == SCB_ABORT) {
+			TAILQ_REMOVE(&dev->busyq, list_acmd, acmd_links.tqe);
+			cmd->result = DID_ABORT << 16;
+			ahc_queue_cmd_complete(ahc, cmd);
+			retval = SUCCESS;
+			goto done;
+		}
+	}
+
+	/*
+	 * See if we can find a matching cmd in the pending list.
+	 */
+	LIST_FOREACH(pending_scb, &ahc->pending_scbs, pending_links) {
+		if (pending_scb->io_ctx == cmd)
+			break;
+	}
+
+	if (pending_scb == NULL && flag == SCB_DEVICE_RESET) {
+
+		/* Any SCB for this device will do for a target reset */
+		LIST_FOREACH(pending_scb, &ahc->pending_scbs, pending_links) {
+		  	if (ahc_match_scb(ahc, pending_scb, cmd->target,
+					  cmd->channel, CAM_LUN_WILDCARD,
+					  SCB_LIST_NULL, ROLE_INITIATOR) == 0)
+				break;
+		}
+	}
+
+	if (pending_scb == NULL) {
+		printf("%s:%d:%d:%d: Command not found\n",
+		       ahc_name(ahc), cmd->channel, cmd->target, cmd->lun);
+		goto no_cmd;
+	}
+
+	if ((pending_scb->flags & SCB_RECOVERY_SCB) != 0) {
+		/*
+		 * We can't queue two recovery actions using the same SCB
+		 */
+		retval = FAILED;
+		goto  done;
+	}
+
+	/*
+	 * Ensure that the card doesn't do anything
+	 * behind our back.  Also make sure that we
+	 * didn't "just" miss an interrupt that would
+	 * affect this cmd.
+	 */
+	ahc->flags |= AHC_ALL_INTERRUPTS;
+	do {
+		ahc_intr(ahc);
+		pause_sequencer(ahc);
+		ahc_clear_critical_section(ahc);
+	} while (ahc_inb(ahc, INTSTAT) & INT_PEND);
+	ahc->flags &= ~AHC_ALL_INTERRUPTS;
+	paused = TRUE;
+
+	if ((pending_scb->flags & SCB_ACTIVE) == 0) {
+		printf("%s:%d:%d:%d: Command already completed\n",
+		       ahc_name(ahc), cmd->channel, cmd->target, cmd->lun);
+		goto no_cmd;
+	}
+
+	disconnected = TRUE;
+	if (flag == SCB_ABORT) {
+		if (ahc_search_qinfifo(ahc, cmd->target, cmd->channel + 'A',
+				       cmd->lun, pending_scb->hscb->tag,
+				       ROLE_INITIATOR, CAM_REQ_ABORTED,
+				       SEARCH_COMPLETE) > 0) {
+			printf("%s:%d:%d:%d: Cmd aborted from QINFIFO\n",
+			       ahc_name(ahc), cmd->channel, cmd->target,
+					cmd->lun);
+			retval = SUCCESS;
+			goto done;
+		}
+	} else if (ahc_search_qinfifo(ahc, cmd->target, cmd->channel + 'A',
+				      cmd->lun, pending_scb->hscb->tag,
+				      ROLE_INITIATOR, /*status*/0,
+				      SEARCH_COUNT) > 0) {
+		disconnected = FALSE;
+	}
+
+	/*
+	 * At this point, pending_scb is the scb associated with the
+	 * passed in command.  That command is currently active on the
+	 * bus, is in the disconnected state, or we're hoping to find
+	 * a command for the same target active on the bus to abuse to
+	 * send a BDR.  Queue the appropriate message based on which of
+	 * these states we are in.
+	 */
+	last_phase = ahc_inb(ahc, LASTPHASE);
+	saved_scbptr = ahc_inb(ahc, SCBPTR);
+	active_scb_index = ahc_inb(ahc, SCB_TAG);
+	if (last_phase != P_BUSFREE
+	 && (pending_scb->hscb->tag == active_scb_index
+	  || (flag == SCB_DEVICE_RESET
+	   && SCSIID_TARGET(ahc, ahc_inb(ahc, SAVED_SCSIID)) == cmd->target))) {
+
+		/*
+		 * We're active on the bus, so assert ATN
+		 * and hope that the target responds.
+		 */
+		pending_scb = ahc_lookup_scb(ahc, active_scb_index);
+		pending_scb->flags |= SCB_RECOVERY_SCB|flag;
+		ahc_outb(ahc, MSG_OUT, HOST_MSG);
+		ahc_outb(ahc, SCSISIGO, last_phase|ATNO);
+		printf("%s:%d:%d:%d: Device is active, asserting ATN\n",
+		       ahc_name(ahc), cmd->channel, cmd->target, cmd->lun);
+		wait = TRUE;
+	} else if (disconnected) {
+
+		/*
+		 * Actually re-queue this SCB in an attempt
+		 * to select the device before it reconnects.
+		 * In either case (selection or reselection),
+		 * we will now issue a the approprate message
+		 * to the timed-out device.
+		 *
+		 * Set the MK_MESSAGE control bit indicating
+		 * that we desire to send a message.  We
+		 * also set the disconnected flag since
+		 * in the paging case there is no guarantee
+		 * that our SCB control byte matches the
+		 * version on the card.  We don't want the
+		 * sequencer to abort the command thinking
+		 * an unsolicited reselection occurred.
+		 */
+		pending_scb->hscb->control |= MK_MESSAGE|DISCONNECTED;
+		pending_scb->flags |= SCB_RECOVERY_SCB|flag;
+
+		/*
+		 * Remove any cached copy of this SCB in the
+		 * disconnected list in preparation for the
+		 * queuing of our abort SCB.  We use the
+		 * same element in the SCB, SCB_NEXT, for
+		 * both the qinfifo and the disconnected list.
+		 */
+		ahc_search_disc_list(ahc, cmd->target, cmd->channel + 'A',
+				     cmd->lun, pending_scb->hscb->tag,
+				     /*stop_on_first*/TRUE,
+				     /*remove*/TRUE,
+				     /*save_state*/FALSE);
+
+		/*
+		 * In the non-paging case, the sequencer will
+		 * never re-reference the in-core SCB.
+		 * To make sure we are notified during
+		 * reslection, set the MK_MESSAGE flag in
+		 * the card's copy of the SCB.
+		 */
+		if ((ahc->flags & AHC_PAGESCBS) != 0) {
+			ahc_outb(ahc, SCBPTR, pending_scb->hscb->tag);
+			ahc_outb(ahc, SCB_CONTROL,
+				 ahc_inb(ahc, SCB_CONTROL)|MK_MESSAGE);
+		}
+
+		/*
+		 * Clear out any entries in the QINFIFO first
+		 * so we are the next SCB for this target
+		 * to run.
+		 */
+		ahc_search_qinfifo(ahc, cmd->target, cmd->channel + 'A',
+				   cmd->lun, SCB_LIST_NULL, ROLE_INITIATOR,
+				   CAM_REQUEUE_REQ, SEARCH_COMPLETE);
+		ahc_print_path(ahc, pending_scb);
+		printf("Queuing a recovery SCB\n");
+		ahc_qinfifo_requeue_tail(ahc, pending_scb);
+		ahc_outb(ahc, SCBPTR, saved_scbptr);
+		printf("%s:%d:%d:%d: Device is disconnected, re-queuing SCB\n",
+		       ahc_name(ahc), cmd->channel, cmd->target, cmd->lun);
+		wait = TRUE;
+	} else {
+		printf("%s:%d:%d:%d: Unable to deliver message\n",
+		       ahc_name(ahc), cmd->channel, cmd->target, cmd->lun);
+		retval = FAILED;
+		goto done;
+	}
+
+no_cmd:
+	/*
+	 * Our assumption is that if we don't have the command, no
+	 * recovery action was required, so we return success.  Again,
+	 * the semantics of the mid-layer recovery engine are not
+	 * well defined, so this may change in time.
+	 */
+	retval = SUCCESS;
+done:
+	if (paused)
+		unpause_sequencer(ahc);
+	if (wait) {
+    		struct timer_list timer;
+		int ret;
+
+		ahc_unlock(ahc, &s);
+		init_timer(&timer);
+		timer.data = 0;
+		timer.expires = jiffies + (5 * HZ);
+		timer.function = ahc_eh_cmd_done;
+		printf("Recovery code sleeping\n");
+		down(&eh_sem);
+		printf("Recovery code awake\n");
+        	ret = del_timer(&timer);
+		if (ret != 0) {
+			printf("Timer Expired\n");
+			retval = FAILED;
+		}
+		ahc_lock(ahc, &s);
+	}
+	acmd = TAILQ_FIRST(&ahc->platform_data->completeq);
+	TAILQ_INIT(&ahc->platform_data->completeq);
+	ahc_unlock(ahc, &s);
+	if (acmd != NULL)
+		ahc_run_complete_queue(ahc, acmd);
+	spin_lock_irq(&io_request_lock);
+	return (retval);
+}
+
+/*
+ * Abort the current SCSI command(s).
+ */
+int
+aic7xxx_abort(Scsi_Cmnd *cmd)
+{
+	int error;
+
+	error = aic7xxx_queue_recovery_cmd(cmd, SCB_ABORT);
+	if (error != 0)
+		printf("aic7xxx_abort returns %d\n", error);
+	return (error);
+}
+
+/*
+ * Attempt to send a target reset message to the device that timed out.
+ */
+int
+aic7xxx_dev_reset(Scsi_Cmnd *cmd)
+{
+	int error;
+
+	error = aic7xxx_queue_recovery_cmd(cmd, SCB_DEVICE_RESET);
+	if (error != 0)
+		printf("aic7xxx_dev_reset returns %d\n", error);
+	return (error);
+}
+
+/*
+ * Reset the SCSI bus.
+ */
+int
+aic7xxx_bus_reset(Scsi_Cmnd *cmd)
+{
+	struct ahc_softc *ahc;
+	struct ahc_cmd *acmd;
+	u_long s;
+	int    found;
+
+	/*
+	 * It is a bug that the upper layer takes
+	 * this lock just prior to calling us.
+	 */
+	spin_unlock_irq(&io_request_lock);
+
+	ahc = *(struct ahc_softc **)cmd->host->hostdata;
+	ahc_lock(ahc, &s);
+	found = ahc_reset_channel(ahc, cmd->channel + 'A',
+				  /*initiate reset*/TRUE);
+	acmd = TAILQ_FIRST(&ahc->platform_data->completeq);
+	TAILQ_INIT(&ahc->platform_data->completeq);
+	ahc_unlock(ahc, &s);
+	if (bootverbose)
+		printf("%s: SCSI bus reset delivered. "
+		       "%d SCBs aborted.\n", ahc_name(ahc), found);
+
+	if (acmd != NULL)
+		ahc_run_complete_queue(ahc, acmd);
+
+	spin_lock_irq(&io_request_lock);
+	return SUCCESS;
+}
+
+/*
+ * Return the disk geometry for the given SCSI device.
+ */
+int
+aic7xxx_biosparam(Disk *disk, kdev_t dev, int geom[])
+{
+	int	heads;
+	int	sectors;
+	int	cylinders;
+	int	ret;
+	int	extended;
+	struct	ahc_softc *ahc;
+	struct	buffer_head *bh;
+
+	ahc = *((struct ahc_softc **)disk->device->host->hostdata);
+	bh = bread(MKDEV(MAJOR(dev), MINOR(dev) & ~0xf), 0, 1024);
+
+	if (bh) {
+		ret = scsi_partsize(bh, disk->capacity,
+				    &geom[2], &geom[0], &geom[1]);
+		brelse(bh);
+		if (ret != -1)
+			return (ret);
+	}
+	heads = 64;
+	sectors = 32;
+	cylinders = disk->capacity / (heads * sectors);
+
+	if (disk->device->channel == 0)
+		extended = (ahc->flags & AHC_EXTENDED_TRANS_A) != 0;
+	else
+		extended = (ahc->flags & AHC_EXTENDED_TRANS_B) != 0;
+	if (extended && cylinders >= 1024) {
+		heads = 255;
+		sectors = 63;
+		cylinders = disk->capacity / (heads * sectors);
+	}
+	geom[0] = heads;
+	geom[1] = sectors;
+	geom[2] = cylinders;
+	return (0);
+}
+
+/*
+ * Free the passed in Scsi_Host memory structures prior to unloading the
+ * module.
+ */
+int
+aic7xxx_release(struct Scsi_Host * host)
+{
+	struct ahc_softc *ahc;
+
+	if (host != NULL) {
+		ahc = *(struct ahc_softc **)host->hostdata;
+		ahc_free(ahc);
+	}
+	return (0);
+}
+
+void
+ahc_platform_dump_card_state(struct ahc_softc *ahc)
+{
+	struct ahc_linux_device *dev;
+	int channel;
+	int maxchannel;
+	int target;
+	int maxtarget;
+	int lun;
+	int i;
+
+	maxchannel = (ahc->features & AHC_TWIN) ? 1 : 0;
+	maxtarget = (ahc->features & AHC_WIDE) ? 15 : 7;
+	for (channel = 0; channel <= maxchannel; channel++) {
+		for (target = 0; target <=maxtarget; target++) {
+			for (lun = 0; lun < AHC_NUM_LUNS; lun++) {
+				struct ahc_cmd *acmd;
+
+				dev = ahc_get_device(ahc, channel, target,
+						     lun, /*alloc*/FALSE);
+				if (dev == NULL)
+					continue;
+
+				printf("DevQ(%d:%d:%d): ",
+				       channel, target, lun);
+				i = 0;
+				TAILQ_FOREACH(acmd, &dev->busyq,
+					      acmd_links.tqe) {
+					if (i++ > 256)
+						break;
+				}
+				printf("%d waiting\n", i);
+			}
+		}
+	}
+}
+
+#if defined(MODULE) || LINUX_VERSION_CODE >= KERNEL_VERSION(2,4,0)
+Scsi_Host_Template driver_template = AIC7XXX;
+#include "../scsi_module.c"
+#endif
diff -urN linux/drivers/scsi/aic7xxx/aic7xxx_linux_host.h /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_linux_host.h
--- linux/drivers/scsi/aic7xxx/aic7xxx_linux_host.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_linux_host.h	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,87 @@
+/*
+ * Adaptec AIC7xxx device driver host template for Linux.
+ *
+ * Copyright (c) 2000 Adaptec Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $Id: //depot/src/linux/drivers/scsi/aic7xxx/aic7xxx_linux_host.h#2 $
+ */
+
+#ifndef _AIC7XXX_LINUX_HOST_H_
+#define _AIC7XXX_LINUX_HOST_H_
+
+int		 aic7xxx_proc_info(char *, char **, off_t, int, int, int);
+int		 aic7xxx_queue(Scsi_Cmnd *, void (*)(Scsi_Cmnd *));
+int		 aic7xxx_detect(Scsi_Host_Template *);
+int		 aic7xxx_release(struct Scsi_Host *);
+const char	*aic7xxx_info(struct Scsi_Host *);
+int		 aic7xxx_biosparam(Disk *, kdev_t, int[]);
+int		 aic7xxx_bus_reset(Scsi_Cmnd *);
+int		 aic7xxx_dev_reset(Scsi_Cmnd *);
+int		 aic7xxx_abort(Scsi_Cmnd *);
+
+#if defined(__i386__)
+#  define AIC7XXX_BIOSPARAM aic7xxx_biosparam
+#else
+#  define AIC7XXX_BIOSPARAM NULL
+#endif
+
+/*
+ * Scsi_Host_Template (see hosts.h) for AIC-7xxx - some fields
+ * to do with card config are filled in after the card is detected.
+ */
+#define AIC7XXX	{						\
+	next: NULL,						\
+	module: NULL,						\
+	proc_dir: NULL,						\
+	proc_info: aic7xxx_proc_info,				\
+	name: NULL,						\
+	detect: aic7xxx_detect,					\
+	release: aic7xxx_release,				\
+	info: aic7xxx_info,					\
+	command: NULL,						\
+	queuecommand: aic7xxx_queue,				\
+	eh_strategy_handler: NULL,				\
+	eh_abort_handler: aic7xxx_abort,			\
+	eh_device_reset_handler: aic7xxx_dev_reset,		\
+	eh_bus_reset_handler: aic7xxx_bus_reset,		\
+	eh_host_reset_handler: NULL,				\
+	abort: NULL,						\
+	reset: NULL,						\
+	slave_attach: NULL,					\
+	bios_param: AIC7XXX_BIOSPARAM,				\
+	can_queue: 254,		/* max simultaneous cmds      */\
+	this_id: -1,		/* scsi id of host adapter    */\
+	sg_tablesize: 0,	/* max scatter-gather cmds    */\
+	cmd_per_lun: 2,		/* cmds per lun		      */\
+	present: 0,		/* number of 7xxx's present   */\
+	unchecked_isa_dma: 0,	/* no memory DMA restrictions */\
+	use_clustering: ENABLE_CLUSTERING,			\
+	use_new_eh_code: 1					\
+}
+
+#endif /* _AIC7XXX_LINUX_HOST_H_ */
diff -urN linux/drivers/scsi/aic7xxx/aic7xxx_linux_pci.c /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_linux_pci.c
--- linux/drivers/scsi/aic7xxx/aic7xxx_linux_pci.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_linux_pci.c	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,315 @@
+/*
+ * Linux driver attachment glue for PCI based controllers.
+ *
+ * Copyright (c) 2000 Adaptec Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $Id: //depot/src/linux/drivers/scsi/aic7xxx/aic7xxx_linux_pci.c#13 $
+ */
+
+#include "aic7xxx_osm.h"
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,4,0)
+struct pci_device_id
+{
+};
+#endif
+
+static int	ahc_linux_pci_dev_probe(struct pci_dev *pdev,
+					const struct pci_device_id *ent);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,4,0)
+static void	ahc_linux_pci_dev_remove(struct pci_dev *pdev);
+
+/* We do our own ID filtering.  So, grab all SCSI storage class devices. */
+static struct pci_device_id ahc_linux_pci_id_table[] = {
+	{
+		0x9004, PCI_ANY_ID, PCI_ANY_ID, PCI_ANY_ID,
+		PCI_CLASS_STORAGE_SCSI << 8, 0xFFFF00, 0
+	},
+	{
+		0x9005, PCI_ANY_ID, PCI_ANY_ID, PCI_ANY_ID,
+		PCI_CLASS_STORAGE_SCSI << 8, 0xFFFF00, 0
+	},
+	{ 0 }
+};
+
+static struct pci_driver aic7xxx_pci_driver = {
+	name:		"aic7xxx",
+	probe:		ahc_linux_pci_dev_probe,
+	remove:		ahc_linux_pci_dev_remove,
+	id_table:	ahc_linux_pci_id_table
+};
+
+static void
+ahc_linux_pci_dev_remove(struct pci_dev *pdev)
+{
+	struct ahc_softc *ahc;
+	struct ahc_softc *list_ahc;
+
+	/*
+	 * We should be able to just perform
+	 * the free directly, but check our
+	 * list for extra sanity.
+	 */
+	ahc = (struct ahc_softc *)pdev->driver_data;
+	TAILQ_FOREACH(list_ahc, &ahc_tailq, links) {
+		if (list_ahc == ahc) {
+			ahc_free(ahc);
+			break;
+		}
+	}
+}
+#endif /* !LINUX_VERSION_CODE < KERNEL_VERSION(2,4,0) */
+
+static int
+ahc_linux_pci_dev_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
+{
+	char	 buf[80];
+	struct	 ahc_softc *ahc;
+	ahc_dev_softc_t pci;
+	struct	 ahc_pci_identity *entry;
+	char	*name;
+	int	 error;
+
+	pci = pdev;
+	entry = ahc_find_pci_device(pci);
+	if (entry == NULL)
+		return (-ENODEV);
+
+	/*
+	 * Allocate a softc for this card and
+	 * set it up for attachment by our
+	 * common detect routine.
+	 */
+	sprintf(buf, "ahc_pci:%d:%d:%d",
+		ahc_get_pci_bus(pci),
+		ahc_get_pci_slot(pci),
+		ahc_get_pci_function(pci));
+	name = malloc(strlen(buf) + 1, M_DEVBUF, M_NOWAIT);
+	if (name == NULL)
+		return (-ENOMEM);
+	strcpy(name, buf);
+	ahc = ahc_alloc(NULL, name);
+	if (ahc == NULL)
+		return (-ENOMEM);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,4,0)
+	if (pci_enable_device(pdev)) {
+		ahc_free(ahc);
+		return (-ENODEV);
+	}
+	pci_set_master(pdev);
+#endif
+	ahc->dev_softc = pci;
+	ahc->platform_data->irq = pdev->irq;
+	error = ahc_pci_config(ahc, entry);
+	if (error != 0) {
+		ahc_free(ahc);
+		return (-error);
+	}
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,4,0)
+	pdev->driver_data = ahc;
+	if (aic7xxx_detect_complete)
+		aic7xxx_register_host(ahc, &driver_template);
+#endif
+	return (0);
+}
+
+int
+ahc_linux_pci_probe(Scsi_Host_Template *template)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,4,0)
+	return (pci_module_init(&aic7xxx_pci_driver));
+#else
+	struct pci_dev *pdev;
+	u_int class;
+	int found;
+
+	/* If we don't have a PCI bus, we can't find any adapters. */
+	if (pci_present() == 0)
+		return (0);
+
+	found = 0;
+	pdev = NULL;
+	class = PCI_CLASS_STORAGE_SCSI << 8;
+	while ((pdev = pci_find_class(class, pdev)) != NULL) {
+		struct ahc_softc *ahc;
+		ahc_dev_softc_t pci;
+		int error;
+
+		pci = pdev;
+
+		/*
+		 * Some BIOSen report the same device multiple times.
+		 */
+		TAILQ_FOREACH(ahc, &ahc_tailq, links) {
+			struct pci_dev *probed_pdev;
+
+			probed_pdev = ahc->dev_softc;
+			if (probed_pdev->bus->number == pdev->bus->number
+			 && probed_pdev->devfn == pdev->devfn)
+				break;
+		}
+		if (ahc != NULL) {
+			/* Skip duplicate. */
+			continue;
+		}
+
+		error = ahc_linux_pci_dev_probe(pdev, /*pci_devid*/NULL);
+		if (error == 0)
+			found++;
+	}
+	return (found);
+#endif
+}
+
+int
+ahc_pci_map_registers(struct ahc_softc *ahc)
+{
+	uint32_t command;
+	u_long	 base;
+	uint8_t *maddr;
+
+	command = ahc_pci_read_config(ahc->dev_softc, PCIR_COMMAND, 4);
+	base = 0;
+	maddr = NULL;
+#ifdef MMAPIO
+	if ((command & PCIM_CMD_MEMEN) != 0) {
+		u_long start;
+		u_long base_page;
+		u_long base_offset;
+		
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,3,0)
+		start = pci_resource_start(ahc->dev_softc, 1);
+		base_page = start & PAGE_MASK;
+		base_offset = start - base_page;
+#else
+		start = ahc_pci_read_config(ahc->dev_softc, PCIR_MAPS+4, 4);
+		base_offset = start & PCI_BASE_ADDRESS_MEM_MASK;
+		base_page = base_offset & PAGE_MASK;
+		base_offset -= base_page;
+#endif
+		ahc->platform_data->mem_busaddr = start;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,4,0)
+		if (request_mem_region(start, 0x1000, "aic7xxx") == 0) {
+			printf("aic7xxx: PCI%d:%d:%d MEM region 0x%lx "
+			       "in use. Cannot map device.\n",
+			       ahc_get_pci_bus(ahc->dev_softc),
+			       ahc_get_pci_slot(ahc->dev_softc),
+			       ahc_get_pci_function(ahc->dev_softc),
+			       start);
+		} else
+#endif
+			maddr = ioremap_nocache(base_page, base_offset + 256);
+		if (maddr != NULL) {
+			ahc->tag = BUS_SPACE_MEMIO;
+			ahc->bsh.maddr = maddr + base_offset;
+
+			/*
+			 * Do a quick test to see if memory mapped
+			 * I/O is functioning correctly.
+			 */
+			if (ahc_inb(ahc, HCNTRL) == 0xFF) {
+				printf("aic7xxx: PCI Device %d:%d:%d "
+				       "failed memory mapped test\n",
+				       ahc_get_pci_bus(ahc->dev_softc),
+				       ahc_get_pci_slot(ahc->dev_softc),
+				       ahc_get_pci_function(ahc->dev_softc));
+				iounmap((void *)base_page);
+				maddr = NULL;
+			} else {
+				command &= ~PCIM_CMD_PORTEN;
+				ahc_pci_write_config(ahc->dev_softc,
+						    PCIR_COMMAND, command, 4);
+			}
+		}
+	}
+#endif
+
+	/*
+	 * We always prefer memory mapped access.  Only
+	 * complain about our ioport conflicting with
+	 * another device if we are going to use it.
+	 */
+	if (maddr == NULL) {
+		ahc->tag = BUS_SPACE_PIO;
+		if ((command & PCIM_CMD_PORTEN) != 0) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,3,0)
+			base = pci_resource_start(ahc->dev_softc, 0);
+#else
+			base = ahc_pci_read_config(ahc->dev_softc,
+						   PCIR_MAPS, 4);
+			base &= PCI_BASE_ADDRESS_IO_MASK;
+#endif
+		}
+		if (base == 0) {
+			printf("aic7xxx: PCI%d:%d:%d No mapping available. "
+			       "Cannot map device.\n",
+			       ahc_get_pci_bus(ahc->dev_softc),
+			       ahc_get_pci_slot(ahc->dev_softc),
+			       ahc_get_pci_function(ahc->dev_softc));
+			return (ENXIO);
+		} else {
+			command &= ~PCIM_CMD_MEMEN;
+			ahc_pci_write_config(ahc->dev_softc,
+					    PCIR_COMMAND, command, 4);
+		}
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,4,0)
+		if (check_region(base, 256) != 0) {
+#else
+		if (request_region(base, 256, "aic7xxx") == 0) {
+#endif
+			printf("aic7xxx: PCI%d:%d:%d IO region 0x%lx[0..255] "
+			       "in use. Cannot map device.\n",
+			       ahc_get_pci_bus(ahc->dev_softc),
+			       ahc_get_pci_slot(ahc->dev_softc),
+			       ahc_get_pci_function(ahc->dev_softc),
+			       base);
+			base = 0;
+			return (EBUSY);
+		}
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,4,0)
+		request_region(base, 256, "aic7xxx");
+#endif
+		ahc->bsh.ioport = base;
+	}
+	return (0);
+}
+
+int
+ahc_pci_map_int(struct ahc_softc *ahc)
+{
+	int error;
+
+	ahc->platform_data->irq = ahc->dev_softc->irq;
+	error = request_irq(ahc->platform_data->irq, aic7xxx_isr,
+			    SA_INTERRUPT|SA_SHIRQ, "aic7xxx", ahc);
+	if (error < 0)
+		error = request_irq(ahc->platform_data->irq, aic7xxx_isr,
+				    SA_SHIRQ, "aic7xxx", ahc);
+	
+	return (-error);
+}
diff -urN linux/drivers/scsi/aic7xxx/aic7xxx_osm.h /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_osm.h
--- linux/drivers/scsi/aic7xxx/aic7xxx_osm.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_osm.h	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,1052 @@
+/*
+ * Adaptec AIC7xxx device driver for Linux.
+ *
+ * Copyright (c) 1994 John Aycock
+ *   The University of Calgary Department of Computer Science.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2, or (at your option)
+ * any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; see the file COPYING.  If not, write to
+ * the Free Software Foundation, 675 Mass Ave, Cambridge, MA 02139, USA.
+ * 
+ * $Id: //depot/src/linux/drivers/scsi/aic7xxx/aic7xxx_linux.h#39 $
+ *
+ * Copyright (c) 2000, 2001 Adaptec Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $Id: //depot/src/linux/drivers/scsi/aic7xxx/aic7xxx_linux.h#39 $
+ *
+ */
+#ifndef _AIC7XXX_LINUX_H_
+#define _AIC7XXX_LINUX_H_
+
+#include <linux/types.h>
+#include <linux/blk.h>
+#include <linux/blkdev.h>
+#include <linux/delay.h>
+#include <linux/ioport.h>
+#include <linux/malloc.h>
+#include <linux/pci.h>
+#include <linux/version.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,4,0)
+#include <linux/config.h>
+#endif
+
+/* Core SCSI definitions */
+#include "../scsi.h"
+#include "../hosts.h"
+
+/* Name space conflict with BSD queue macros */
+#ifdef LIST_HEAD
+#undef LIST_HEAD
+#endif
+
+#include "cam.h"
+#include "queue.h"
+#include "scsi_message.h"
+
+/*
+ * We never have to reference the current task, and the driver core
+ * makes ample use of this "name".
+ */
+#ifdef current
+#undef current
+#endif
+
+/************************* Forward Declarations *******************************/
+struct ahc_softc;
+typedef struct pci_dev *ahc_dev_softc_t;
+typedef Scsi_Cmnd      *ahc_io_ctx_t;
+
+/******************************* Byte Order ***********************************/
+#define ahc_htobe16(x)	cpu_to_be16(x)
+#define ahc_htobe32(x)	cpu_to_be32(x)
+#define ahc_htobe64(x)	cpu_to_be64(x)
+#define ahc_htole16(x)	cpu_to_le16(x)
+#define ahc_htole32(x)	cpu_to_le32(x)
+#define ahc_htole64(x)	cpu_to_le64(x)
+
+#define ahc_be16toh(x)	be16_to_cpu(x)
+#define ahc_be32toh(x)	be32_to_cpu(x)
+#define ahc_be64toh(x)	be64_to_cpu(x)
+#define ahc_le16toh(x)	le16_to_cpu(x)
+#define ahc_le32toh(x)	le32_to_cpu(x)
+#define ahc_le64toh(x)	le64_to_cpu(x)
+
+/************************* Configuration Data *********************************/
+extern int aic7xxx_no_probe;
+extern int aic7xxx_detect_complete;
+extern Scsi_Host_Template driver_template;
+
+/***************************** Bus Space/DMA **********************************/
+typedef uint32_t bus_addr_t;
+typedef uint32_t bus_size_t;
+
+typedef enum {
+	BUS_SPACE_MEMIO,
+	BUS_SPACE_PIO
+} bus_space_tag_t;
+
+typedef union {
+	u_long		  ioport;
+	volatile uint8_t *maddr;
+} bus_space_handle_t;
+
+typedef struct bus_dma_segment
+{
+	bus_addr_t	ds_addr;
+	bus_size_t	ds_len;
+} bus_dma_segment_t;
+
+struct ahc_linux_dma_tag
+{
+	bus_size_t	alignment;
+	bus_size_t	boundary;
+	bus_size_t	maxsize;
+};
+typedef struct ahc_linux_dma_tag* bus_dma_tag_t;
+
+struct ahc_linux_dmamap
+{
+	bus_addr_t	bus_addr;
+};
+typedef struct ahc_linux_dmamap* bus_dmamap_t;
+
+typedef int bus_dma_filter_t(void*, bus_addr_t);
+typedef void bus_dmamap_callback_t(void *, bus_dma_segment_t *, int, int);
+
+#define BUS_DMA_WAITOK		0x0
+#define BUS_DMA_NOWAIT		0x1
+#define BUS_DMA_ALLOCNOW	0x2
+#define BUS_DMA_LOAD_SEGS	0x4	/*
+					 * Argument is an S/G list not
+					 * a single buffer.
+					 */
+
+#define BUS_SPACE_MAXADDR	0xFFFFFFFF
+#define BUS_SPACE_MAXSIZE_32BIT	0xFFFFFFFF
+
+int	ahc_dma_tag_create(struct ahc_softc *, bus_dma_tag_t /*parent*/,
+			   bus_size_t /*alignment*/, bus_size_t /*boundary*/,
+			   bus_addr_t /*lowaddr*/, bus_addr_t /*highaddr*/,
+			   bus_dma_filter_t*/*filter*/, void */*filterarg*/,
+			   bus_size_t /*maxsize*/, int /*nsegments*/,
+			   bus_size_t /*maxsegsz*/, int /*flags*/,
+			   bus_dma_tag_t */*dma_tagp*/);
+
+void	ahc_dma_tag_destroy(struct ahc_softc *, bus_dma_tag_t /*tag*/);
+
+int	ahc_dmamem_alloc(struct ahc_softc *, bus_dma_tag_t /*dmat*/,
+			 void** /*vaddr*/, int /*flags*/,
+			 bus_dmamap_t* /*mapp*/);
+
+void	ahc_dmamem_free(struct ahc_softc *, bus_dma_tag_t /*dmat*/,
+			void* /*vaddr*/, bus_dmamap_t /*map*/);
+
+void	ahc_dmamap_destroy(struct ahc_softc *, bus_dma_tag_t /*tag*/,
+			   bus_dmamap_t /*map*/);
+
+int	ahc_dmamap_load(struct ahc_softc *ahc, bus_dma_tag_t /*dmat*/,
+			bus_dmamap_t /*map*/, void * /*buf*/,
+			bus_size_t /*buflen*/, bus_dmamap_callback_t *,
+			void */*callback_arg*/, int /*flags*/);
+
+int	ahc_dmamap_unload(struct ahc_softc *, bus_dma_tag_t, bus_dmamap_t);
+
+/* XXX May do selective memory barrier operations on certain platforms */
+#define ahc_dmamap_sync(ahc, dma_tag, dmamap, op)
+
+/************************** SCSI Constants/Structures *************************/
+#define SCSI_REV_2 2
+#define	SCSI_STATUS_OK			0x00
+#define	SCSI_STATUS_CHECK_COND		0x02
+#define	SCSI_STATUS_COND_MET		0x04
+#define	SCSI_STATUS_BUSY		0x08
+#define SCSI_STATUS_INTERMED		0x10
+#define SCSI_STATUS_INTERMED_COND_MET	0x14
+#define SCSI_STATUS_RESERV_CONFLICT	0x18
+#define SCSI_STATUS_CMD_TERMINATED	0x22
+#define SCSI_STATUS_QUEUE_FULL		0x28
+
+/*
+ * 6 byte request sense CDB format.
+ */
+struct scsi_sense
+{
+	uint8_t opcode;
+	uint8_t byte2;
+	uint8_t unused[2];
+	uint8_t length;
+	uint8_t control;
+};
+
+struct scsi_sense_data
+{
+	uint8_t	error_code;
+	uint8_t	segment;
+	uint8_t	flags;
+	uint8_t	info[4];
+	uint8_t	extra_len;
+	uint8_t	cmd_spec_info[4];
+	uint8_t add_sense_code;
+	uint8_t add_sense_code_qual;
+	uint8_t	fru;
+	uint8_t	sense_key_spec[3];
+	uint8_t	extra_bytes[14];
+};
+
+struct scsi_inquiry_data
+{
+	uint8_t device;
+#define	SID_TYPE(inq_data) ((inq_data)->device & 0x1f)
+#define	SID_QUAL(inq_data) (((inq_data)->device & 0xE0) >> 5)
+#define	SID_QUAL_LU_CONNECTED	0x00	/*
+					 * The specified peripheral device
+					 * type is currently connected to
+					 * logical unit.  If the target cannot
+					 * determine whether or not a physical
+					 * device is currently connected, it
+					 * shall also use this peripheral
+					 * qualifier when returning the INQUIRY
+					 * data.  This peripheral qualifier
+					 * does not mean that the device is
+					 * ready for access by the initiator.
+					 */
+#define	SID_QUAL_LU_OFFLINE	0x01	/*
+					 * The target is capable of supporting
+					 * the specified peripheral device type
+					 * on this logical unit; however, the
+					 * physical device is not currently
+					 * connected to this logical unit.
+					 */
+#define SID_QUAL_RSVD		0x02
+#define	SID_QUAL_BAD_LU		0x03	/*
+					 * The target is not capable of
+					 * supporting a physical device on
+					 * this logical unit. For this
+					 * peripheral qualifier the peripheral
+					 * device type shall be set to 1Fh to
+					 * provide compatibility with previous
+					 * versions of SCSI. All other
+					 * peripheral device type values are
+					 * reserved for this peripheral
+					 * qualifier.
+					 */
+#define	SID_QUAL_IS_VENDOR_UNIQUE(inq_data) ((SID_QUAL(inq_data) & 0x08) != 0)
+	uint8_t dev_qual2;
+#define	SID_QUAL2	0x7F
+#define	SID_IS_REMOVABLE(inq_data) (((inq_data)->dev_qual2 & 0x80) != 0)
+	uint8_t version;
+#define SID_ANSI_REV(inq_data) ((inq_data)->version & 0x07)
+#define		SCSI_REV_0		0
+#define		SCSI_REV_CCS		1
+#define		SCSI_REV_2		2
+#define		SCSI_REV_SPC		3
+#define		SCSI_REV_SPC2		4
+
+#define SID_ECMA	0x38
+#define SID_ISO		0xC0
+	uint8_t response_format;
+#define SID_AENC	0x80
+#define SID_TrmIOP	0x40
+	uint8_t additional_length;
+	uint8_t reserved[2];
+	uint8_t flags;
+#define	SID_SftRe	0x01
+#define	SID_CmdQue	0x02
+#define	SID_Linked	0x08
+#define	SID_Sync	0x10
+#define	SID_WBus16	0x20
+#define	SID_WBus32	0x40
+#define	SID_RelAdr	0x80
+#define SID_VENDOR_SIZE   8
+	char	 vendor[SID_VENDOR_SIZE];
+#define SID_PRODUCT_SIZE  16
+	char	 product[SID_PRODUCT_SIZE];
+#define SID_REVISION_SIZE 4
+	char	 revision[SID_REVISION_SIZE];
+	/*
+	 * The following fields were taken from SCSI Primary Commands - 2
+	 * (SPC-2) Revision 14, Dated 11 November 1999
+	 */
+#define	SID_VENDOR_SPECIFIC_0_SIZE	20
+	u_int8_t vendor_specific0[SID_VENDOR_SPECIFIC_0_SIZE];
+	/*
+	 * An extension of SCSI Parallel Specific Values
+	 */
+#define	SID_SPI_IUS		0x01
+#define	SID_SPI_QAS		0x02
+#define	SID_SPI_CLOCK_ST	0x00
+#define	SID_SPI_CLOCK_DT	0x04
+#define	SID_SPI_CLOCK_DT_ST	0x0C
+#define	SID_SPI_MASK		0x0F
+	uint8_t spi3data;
+	uint8_t reserved2;
+	/*
+	 * Version Descriptors, stored 2 byte values.
+	 */
+	uint8_t version1[2];
+	uint8_t version2[2];
+	uint8_t version3[2];
+	uint8_t version4[2];
+	uint8_t version5[2];
+	uint8_t version6[2];
+	uint8_t version7[2];
+	uint8_t version8[2];
+
+	uint8_t reserved3[22];
+
+#define	SID_VENDOR_SPECIFIC_1_SIZE	160
+	uint8_t vendor_specific1[SID_VENDOR_SPECIFIC_1_SIZE];
+};
+
+/********************************** Includes **********************************/
+/* Host template and function declarations referenced by the template. */
+#include "aic7xxx_linux_host.h"
+
+/* Core driver definitions */
+#include "aic7xxx.h"
+
+/* SMP support */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,3,17)
+#include <linux/spinlock.h>
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2,1,93)
+#include <linux/smp.h>
+#endif
+
+#define AIC7XXX_DRIVER_VERSION  "6.1.0"
+
+#ifndef LINUX_VERSION_CODE
+#include <linux/version.h>
+#endif
+
+#ifndef KERNEL_VERSION
+#define KERNEL_VERSION(x,y,z) (((x)<<16)+((y)<<8)+(z))
+#endif
+
+/**************************** Front End Queues ********************************/
+/*
+ * Data structure used to cast the Linux struct scsi_cmnd to something
+ * that allows us to use the queue macros.  The linux structure has
+ * plenty of space to hold the links fields as required by the queue
+ * macros, but the queue macors require them to have the correct type.
+ */
+struct ahc_cmd_internal {
+	/* Area owned by the Linux scsi layer. */
+	uint8_t	private[offsetof(struct scsi_cmnd, SCp.Status)];
+	union {
+		STAILQ_ENTRY(ahc_cmd)	ste;
+		LIST_ENTRY(ahc_cmd)	le;
+		TAILQ_ENTRY(ahc_cmd)	tqe;
+	} links;
+	uint32_t			end;
+};
+
+struct ahc_cmd {
+	union {
+		struct ahc_cmd_internal	icmd;
+		struct scsi_cmnd	scsi_cmd;
+	} un;
+};
+
+#define acmd_icmd(cmd) ((cmd)->un.icmd)
+#define acmd_scsi_cmd(cmd) ((cmd)->un.scsi_cmd)
+#define acmd_links un.icmd.links
+
+/*************************** Device Data Structures ***************************/
+/*
+ * A per probed device structure used to deal with some error recovery
+ * scenarios that the Linux mid-layer code just doesn't know how to
+ * handle.  The structure allocated for a device only becomes persistant
+ * after a successfully completed inquiry command to the target when
+ * that inquiry data indicates a lun is present.
+ */
+TAILQ_HEAD(ahc_busyq, ahc_cmd);
+typedef enum {
+	AHC_DEV_UNCONFIGURED	 = 0x01,
+	AHC_DEV_FREEZE_TIL_EMPTY = 0x02, /* Freeze queue until active == 0 */
+	AHC_DEV_TIMER_ACTIVE	 = 0x04, /* Our timer is active */
+	AHC_DEV_ON_RUN_LIST	 = 0x08	 /* Queued to be run later */
+} ahc_dev_flags;
+
+struct ahc_linux_target;
+struct ahc_linux_device {
+	LIST_ENTRY(ahc_linux_device) links;
+	struct		ahc_busyq busyq;
+
+	/*
+	 * The number of transactions currently
+	 * queued to the device.
+	 */
+	int		active;
+
+	/*
+	 * The currently allowed number of 
+	 * transactions that can be queued to
+	 * the device.  Must be signed for
+	 * conversion from tagged to untagged
+	 * mode where the device may have more
+	 * than one outstanding active transaction.
+	 */
+	int		openings;
+
+	/*
+	 * A positive count indicates that this
+	 * device's queue is halted.
+	 */
+	u_int		qfrozen;
+	
+	/*
+	 * Cumulative command counter.
+	 */
+	u_int		num_commands;
+
+	/*
+	 * The number of tagged transactions when
+	 * running at our current opening level
+	 * that have been successfully received by
+	 * this device since the last QUEUE FULL.
+	 */
+	u_int		tag_success_count;
+#define AHC_TAG_SUCCESS_INTERVAL 50
+
+	ahc_dev_flags	flags;
+
+	/*
+	 * The high limit for the tags variable.
+	 */
+	u_int		maxtags;
+
+	/*
+	 * The computed number of tags outstanding
+	 * at the time of the last QUEUE FULL event.
+	 */
+	u_int		tags_on_last_queuefull;
+
+	/*
+	 * How many times we have seen a queue full
+	 * with the same number of tags.  This is used
+	 * to stop our adaptive queue depth algorithm
+	 * on devices with a fixed number of tags.
+	 */
+	u_int		last_queuefull_same_count;
+
+#define AHC_LOCK_TAGS_COUNT 50
+	int		lun;
+	struct		ahc_linux_target *target;
+};
+
+struct ahc_linux_target {
+	struct	ahc_linux_device *devices[AHC_NUM_LUNS];
+	int	channel;
+	int	target;
+	int	refcount;
+};
+
+/********************* Definitions Required by the Core ***********************/
+/*
+ * Number of SG segments we require.  So long as the S/G segments for
+ * a particular transaction are allocated in a physically contiguous
+ * manner and are allocated below 4GB, the number of S/G segments is
+ * unrestricted.
+ */
+#define        AHC_NSEG 128
+
+/*
+ * Per-SCB OSM storage.
+ */
+struct scb_platform_data {
+	struct ahc_linux_device	*dev;
+	uint32_t		 xfer_len;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,3,0)
+	uint32_t		 resid;		/* Transfer residual */
+#endif
+};
+
+/*
+ * Define a structure used for each host adapter.  All members are
+ * aligned on a boundary >= the size of the member to honor the
+ * alignment restrictions of the various platforms supported by
+ * this driver.
+ */
+TAILQ_HEAD(ahc_completeq, ahc_cmd);
+struct ahc_platform_data {
+	/*
+	 * Fields accessed from interrupt context.
+	 */
+	struct ahc_linux_target *targets[AHC_NUM_TARGETS]; 
+	LIST_HEAD(, ahc_linux_device) device_runq;
+	struct ahc_completeq	 completeq;
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,1,0)
+	spinlock_t		 spin_lock;
+#endif
+	struct Scsi_Host        *host;		/* pointer to scsi host */
+	uint32_t		 irq;		/* IRQ for this adapter */
+	uint32_t		 bios_address;
+	uint32_t		 mem_busaddr;	/* Mem Base Addr */
+	int                      host_no;	/* SCSI host number */
+};
+
+/************************** OS Utility Wrappers *******************************/
+#define printf printk
+#define M_NOWAIT GFP_ATOMIC
+#define M_WAITOK 0
+#define malloc(size, type, flags) kmalloc(size, flags)
+#define free(ptr, type) kfree(ptr)
+
+static __inline void ahc_delay(long);
+static __inline void
+ahc_delay(long usec)
+{
+	/*
+	 * udelay on Linux can have problems for
+	 * multi-millisecond waits.  Wait at most
+	 * 1024us per call.
+	 */
+	while (usec > 0) {
+		udelay(usec % 1024);
+		usec -= 1024;
+	}
+}
+
+
+/***************************** Low Level I/O **********************************/
+#if defined(__powerpc__)
+#define MMAPIO
+#ifdef mb
+#undef mb
+#endif
+#define mb() \
+	__asm__ __volatile__("eieio" ::: "memory")
+#elif defined(__i386__)
+#define MMAPIO
+#ifdef mb
+#undef mb
+#endif
+#define mb() \
+	do { ; } while(0)
+#elif defined(__alpha__)
+#ifdef mb
+#undef mb
+#endif
+#define mb() \
+	__asm__ __volatile__("mb": : :"memory")
+#endif
+
+static __inline uint8_t ahc_inb(struct ahc_softc * ahc, long port);
+static __inline void ahc_outb(struct ahc_softc * ahc, long port, uint8_t val);
+static __inline void ahc_outsb(struct ahc_softc * ahc, long port,
+			       uint8_t *, int count);
+static __inline void ahc_insb(struct ahc_softc * ahc, long port,
+			       uint8_t *, int count);
+
+static __inline uint8_t
+ahc_inb(struct ahc_softc * ahc, long port)
+{
+#ifdef MMAPIO
+	uint8_t x;
+
+	if (ahc->tag == BUS_SPACE_MEMIO) {
+		x = readb(ahc->bsh.maddr + port);
+	} else {
+		x = inb(ahc->bsh.ioport + port);
+	}
+	mb();
+	return (x);
+#else
+	return (inb(ahc->bsh.ioport + port));
+#endif
+}
+
+static __inline void
+ahc_outb(struct ahc_softc * ahc, long port, uint8_t val)
+{
+#ifdef MMAPIO
+	if (ahc->tag == BUS_SPACE_MEMIO) {
+		writeb(val, ahc->bsh.maddr + port);
+	} else {
+		outb(val, ahc->bsh.ioport + port);
+	}
+	mb();
+#else
+	outb(val, ahc->bsh.ioport + port);
+#endif
+}
+
+static __inline void
+ahc_outsb(struct ahc_softc * ahc, long port, uint8_t *array, int count)
+{
+	int i;
+
+	/*
+	 * There is probably a more efficient way to do this on Linux
+	 * but we don't use this for anything speed critical and this
+	 * should work.
+	 */
+	for (i = 0; i < count; i++)
+		ahc_outb(ahc, port, *array++);
+}
+
+static __inline void
+ahc_insb(struct ahc_softc * ahc, long port, uint8_t *array, int count)
+{
+	int i;
+
+	/*
+	 * There is probably a more efficient way to do this on Linux
+	 * but we don't use this for anything speed critical and this
+	 * should work.
+	 */
+	for (i = 0; i < count; i++)
+		*array++ = ahc_inb(ahc, port);
+}
+
+/**************************** Initialization **********************************/
+int	aic7xxx_register_host(struct ahc_softc *ahc,
+			      Scsi_Host_Template *template);
+
+/******************************** Locking *************************************/
+/* Lock protecting internal data structures */
+static __inline void ahc_lockinit(struct ahc_softc *);
+static __inline void ahc_lock(struct ahc_softc *, unsigned long *flags);
+static __inline void ahc_unlock(struct ahc_softc *, unsigned long *flags);
+
+/* Lock held during command compeletion to the upper layer */
+static __inline void ahc_done_lockinit(struct ahc_softc *);
+static __inline void ahc_done_lock(struct ahc_softc *, unsigned long *flags);
+static __inline void ahc_done_unlock(struct ahc_softc *, unsigned long *flags);
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,1,93)
+static __inline void
+ahc_lockinit(struct ahc_softc *ahc)
+{
+	spin_lock_init(&ahc->platform_data->spin_lock);
+}
+
+static __inline void
+ahc_lock(struct ahc_softc *ahc, unsigned long *flags)
+{
+	*flags = 0;
+	spin_lock_irqsave(&ahc->platform_data->spin_lock, *flags);
+}
+
+static __inline void
+ahc_unlock(struct ahc_softc *ahc, unsigned long *flags)
+{
+	spin_unlock_irqrestore(&ahc->platform_data->spin_lock, *flags);
+}
+
+static __inline void
+ahc_done_lockinit(struct ahc_softc *ahc)
+{
+	/* We don't own the iorequest lock, so we don't initialize it. */
+}
+
+static __inline void
+ahc_done_lock(struct ahc_softc *ahc, unsigned long *flags)
+{
+	*flags = 0;
+	spin_lock_irqsave(&io_request_lock, *flags);
+}
+
+static __inline void
+ahc_done_unlock(struct ahc_softc *ahc, unsigned long *flags)
+{
+	spin_unlock_irqrestore(&io_request_lock, *flags);
+}
+
+#else /* LINUX_VERSION_CODE < KERNEL_VERSION(2,1,0) */
+
+ahc_lockinit(struct ahc_softc *ahc)
+{
+}
+
+static __inline void
+ahc_lock(struct ahc_softc *ahc, unsigned long *flags)
+{
+	*flags = 0;
+	save_flags(*flags);
+	cli();
+}
+
+static __inline void
+ahc_unlock(struct ahc_softc *ahc, unsigned long *flags)
+{
+	restore_flags(*flags);
+}
+
+ahc_done_lockinit(struct ahc_softc *ahc)
+{
+}
+
+static __inline void
+ahc_done_lock(struct ahc_softc *ahc, unsigned long *flags)
+{
+	/*
+	 * The done lock is always held while
+	 * the ahc lock is held so blocking
+	 * interrupts again would have no effect.
+	 */
+}
+
+static __inline void
+ahc_done_unlock(struct ahc_softc *ahc, unsigned long *flags)
+{
+}
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2,1,0) */
+
+/******************************* PCI Definitions ******************************/
+/*
+ * PCIM_xxx: mask to locate subfield in register
+ * PCIR_xxx: config register offset
+ * PCIC_xxx: device class
+ * PCIS_xxx: device subclass
+ * PCIP_xxx: device programming interface
+ * PCIV_xxx: PCI vendor ID (only required to fixup ancient devices)
+ * PCID_xxx: device ID
+ */
+#define PCIR_DEVVENDOR		0x00
+#define PCIR_VENDOR		0x00
+#define PCIR_DEVICE		0x02
+#define PCIR_COMMAND		0x04
+#define PCIM_CMD_PORTEN		0x0001
+#define PCIM_CMD_MEMEN		0x0002
+#define PCIM_CMD_BUSMASTEREN	0x0004
+#define PCIM_CMD_MWRICEN	0x0010
+#define PCIM_CMD_PERRESPEN	0x0040
+#define PCIR_STATUS		0x06
+#define PCIR_REVID		0x08
+#define PCIR_PROGIF		0x09
+#define PCIR_SUBCLASS		0x0a
+#define PCIR_CLASS		0x0b
+#define PCIR_CACHELNSZ		0x0c
+#define PCIR_LATTIMER		0x0d
+#define PCIR_HEADERTYPE		0x0e
+#define PCIM_MFDEV		0x80
+#define PCIR_BIST		0x0f
+#define PCIR_CAP_PTR		0x34
+
+/* config registers for header type 0 devices */
+#define PCIR_MAPS	0x10
+#define PCIR_SUBVEND_0	0x2c
+#define PCIR_SUBDEV_0	0x2e
+
+/**************************** VL/EISA Routines ********************************/
+int			 aic7770_linux_probe(Scsi_Host_Template *);
+int			 aic7770_map_registers(struct ahc_softc *ahc);
+int			 aic7770_map_int(struct ahc_softc *ahc,
+					 u_int irq, int shared);
+
+/******************************* PCI Routines *********************************/
+/*
+ * We need to use the bios32.h routines if we are kernel version 2.1.92 or less.
+ */
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,1,92)
+#if defined(__sparc_v9__) || defined(__powerpc__)
+#error "PPC and Sparc platforms are only support under 2.1.92 and above"
+#endif
+#include <linux/bios32.h>
+#endif
+
+int			 ahc_linux_pci_probe(Scsi_Host_Template *);
+int			 ahc_pci_map_registers(struct ahc_softc *ahc);
+int			 ahc_pci_map_int(struct ahc_softc *ahc);
+
+static __inline uint32_t ahc_pci_read_config(ahc_dev_softc_t pci,
+					     int reg, int width);
+
+static __inline uint32_t
+ahc_pci_read_config(ahc_dev_softc_t pci, int reg, int width)
+{
+	switch (width) {
+	case 1:
+	{
+		uint8_t retval;
+
+		pci_read_config_byte(pci, reg, &retval);
+		return (retval);
+	}
+	case 2:
+	{
+		uint16_t retval;
+		pci_read_config_word(pci, reg, &retval);
+		return (retval);
+	}
+	case 4:
+	{
+		uint32_t retval;
+		pci_read_config_dword(pci, reg, &retval);
+		return (retval);
+	}
+	default:
+		panic("ahc_pci_read_config: Read size too big");
+		/* NOTREACHED */
+	}
+}
+
+static __inline void ahc_pci_write_config(ahc_dev_softc_t pci,
+					  int reg, uint32_t value,
+					  int width);
+
+static __inline void
+ahc_pci_write_config(ahc_dev_softc_t pci, int reg, uint32_t value, int width)
+{
+	switch (width) {
+	case 1:
+		pci_write_config_byte(pci, reg, value);
+		break;
+	case 2:
+		pci_write_config_word(pci, reg, value);
+		break;
+	case 4:
+		pci_write_config_dword(pci, reg, value);
+		break;
+	default:
+		panic("ahc_pci_write_config: Write size too big");
+		/* NOTREACHED */
+	}
+}
+
+static __inline int ahc_get_pci_function(ahc_dev_softc_t);
+static __inline int
+ahc_get_pci_function(ahc_dev_softc_t pci)
+{
+	return (PCI_FUNC(pci->devfn));
+}
+
+static __inline int ahc_get_pci_slot(ahc_dev_softc_t);
+static __inline int
+ahc_get_pci_slot(ahc_dev_softc_t pci)
+{
+	return (PCI_SLOT(pci->devfn));
+}
+
+static __inline int ahc_get_pci_bus(ahc_dev_softc_t);
+static __inline int
+ahc_get_pci_bus(ahc_dev_softc_t pci)
+{
+	return (pci->bus->number);
+}
+
+static __inline void ahc_flush_device_writes(struct ahc_softc *);
+static __inline void
+ahc_flush_device_writes(struct ahc_softc *ahc)
+{
+	/* XXX Is this sufficient for all architectures??? */
+	ahc_inb(ahc, INTSTAT);
+}
+
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,3,0)
+#define pci_map_sg(pdev, sg_list, nseg, direction) (nseg)
+#define pci_unmap_sg(pdev, sg_list, nseg, direction)
+#define sg_dma_address(sg) (VIRT_TO_BUS((sg)->address))
+#define sg_dma_len(sg) ((sg)->length)
+#define pci_map_single(pdev, buffer, bufflen, direction) \
+	(VIRT_TO_BUS(buffer))
+#define pci_unmap_single(pdev, buffer, buflen, direction)
+#endif
+
+/*********************** Transaction Access Wrappers **************************/
+static __inline void ahc_set_transaction_status(struct scb *, uint32_t);
+static __inline
+void ahc_set_transaction_status(struct scb *scb, uint32_t status)
+{
+	scb->io_ctx->result &= ~(CAM_STATUS_MASK << 16);
+	scb->io_ctx->result |= status << 16;
+}
+
+static __inline void ahc_set_scsi_status(struct scb *, uint32_t);
+static __inline
+void ahc_set_scsi_status(struct scb *scb, uint32_t status)
+{
+	scb->io_ctx->result &= ~0xFFFF;
+	scb->io_ctx->result |= status;
+}
+
+static __inline uint32_t ahc_get_transaction_status(struct scb *);
+static __inline
+uint32_t ahc_get_transaction_status(struct scb *scb)
+{
+	return ((scb->io_ctx->result >> 16) & CAM_STATUS_MASK);
+}
+
+static __inline uint32_t ahc_get_scsi_status(struct scb *);
+static __inline
+uint32_t ahc_get_scsi_status(struct scb *scb)
+{
+	return (scb->io_ctx->result & 0xFFFF);
+}
+
+static __inline void ahc_set_transaction_tag(struct scb *, int, u_int);
+static __inline
+void ahc_set_transaction_tag(struct scb *scb, int enabled, u_int type)
+{
+	/*
+	 * Nothing to do for linux as the incoming transaction
+	 * has no concept of tag/non tagged, etc.
+	 */
+}
+
+static __inline u_long ahc_get_transfer_length(struct scb *);
+static __inline
+u_long ahc_get_transfer_length(struct scb *scb)
+{
+	return (scb->platform_data->xfer_len);
+}
+
+static __inline int ahc_get_transfer_dir(struct scb *);
+static __inline
+int ahc_get_transfer_dir(struct scb *scb)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,3,40)
+	return (scb->io_ctx->sc_data_direction);
+#else
+	if (scb->io_ctx->bufflen == 0)
+		return (CAM_DIR_NONE);
+
+	switch(scb->io_ctx->cmnd[0]) {
+	case 0x08:  /* READ(6)  */
+	case 0x28:  /* READ(10) */
+	case 0xA8:  /* READ(12) */
+		return (CAM_DIR_IN);
+        case 0x0A:  /* WRITE(6)  */
+        case 0x2A:  /* WRITE(10) */
+        case 0xAA:  /* WRITE(12) */
+		return (CAM_DIR_OUT);
+        default:
+		return (CAM_DIR_NONE);
+        }
+#endif
+}
+
+static __inline void ahc_set_residual(struct scb *, u_long);
+static __inline
+void ahc_set_residual(struct scb *scb, u_long resid)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,3,0)
+	scb->io_ctx->resid = resid;
+#else
+	scb->platform_data->resid = resid;
+#endif
+}
+
+static __inline void ahc_set_sense_residual(struct scb *, u_long);
+static __inline
+void ahc_set_sense_residual(struct scb *scb, u_long resid)
+{
+	/* This can't be reported in Linux */
+}
+
+static __inline u_long ahc_get_residual(struct scb *);
+static __inline
+u_long ahc_get_residual(struct scb *scb)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,3,0)
+	return (scb->io_ctx->resid);
+#else
+	return (scb->platform_data->resid);
+#endif
+}
+
+static __inline int ahc_perform_autosense(struct scb *);
+static __inline
+int ahc_perform_autosense(struct scb *scb)
+{
+	/*
+	 * We always perform autosense in Linux.
+	 * On other platforms this is set on a
+	 * per-transaction basis.
+	 */
+	return (1);
+}
+
+static __inline uint32_t
+ahc_get_sense_bufsize(struct ahc_softc *ahc, struct scb *scb)
+{
+	return (sizeof(struct scsi_sense_data));
+}
+
+static __inline void ahc_notify_xfer_settings_change(struct ahc_softc *,
+						     struct ahc_devinfo *);
+static __inline void
+ahc_notify_xfer_settings_change(struct ahc_softc *ahc,
+				struct ahc_devinfo *devinfo)
+{
+	/* Nothing to do here for linux */
+}
+
+static __inline void ahc_platform_scb_free(struct ahc_softc *ahc,
+					   struct scb *scb);
+static __inline void
+ahc_platform_scb_free(struct ahc_softc *ahc, struct scb *scb)
+{
+	ahc->flags &= ~AHC_RESOURCE_SHORTAGE;
+}
+
+int	ahc_platform_alloc(struct ahc_softc *ahc, void *platform_arg);
+void	ahc_platform_free(struct ahc_softc *ahc);
+void	ahc_platform_freeze_devq(struct ahc_softc *ahc, struct scb *scb);
+static __inline void	ahc_freeze_scb(struct scb *scb);
+static __inline void
+ahc_freeze_scb(struct scb *scb)
+{
+	/* Noting to do here for linux */
+}
+
+void	ahc_platform_set_tags(struct ahc_softc *ahc,
+			      struct ahc_devinfo *devinfo, int enable);
+int	ahc_platform_abort_scbs(struct ahc_softc *ahc, int target,
+				char channel, int lun, u_int tag,
+				role_t role, uint32_t status);
+void	aic7xxx_isr(int irq, void *dev_id, struct pt_regs * regs);
+void	ahc_platform_flushwork(struct ahc_softc *ahc);
+int	ahc_softc_comp(struct ahc_softc *, struct ahc_softc *);
+void	ahc_done(struct ahc_softc*, struct scb*);
+void	ahc_send_async(struct ahc_softc *, char channel,
+		       u_int target, u_int lun, ac_code);
+void	ahc_print_path(struct ahc_softc *, struct scb *);
+void	ahc_platform_dump_card_state(struct ahc_softc *ahc);
+
+#ifdef CONFIG_PCI
+#define AHC_PCI_CONFIG 1
+#else
+#define AHC_PCI_CONFIG 0
+#endif
+#define bootverbose aic7xxx_verbose
+extern int aic7xxx_verbose;
+#endif /* _AIC7XXX_LINUX_H_ */
diff -urN linux/drivers/scsi/aic7xxx/aic7xxx_pci.c /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_pci.c
--- linux/drivers/scsi/aic7xxx/aic7xxx_pci.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_pci.c	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,2080 @@
+/*
+ * Product specific probe and attach routines for:
+ *      3940, 2940, aic7895, aic7890, aic7880,
+ *	aic7870, aic7860 and aic7850 SCSI controllers
+ *
+ * Copyright (c) 1995-2000 Justin T. Gibbs
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification, immediately at the beginning of the file.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $Id: //depot/src/aic7xxx/aic7xxx_pci.c#14 $
+ *
+ * $FreeBSD: src/sys/dev/aic7xxx/aic7xxx_pci.c,v 1.6 2000/11/10 20:13:41 gibbs Exp $
+ */
+
+#include "aic7xxx_osm.h"
+#include "aic7xxx_inline.h"
+#include "aic7xxx_93cx6.h"
+
+#define AHC_PCI_IOADDR	PCIR_MAPS	/* I/O Address */
+#define AHC_PCI_MEMADDR	(PCIR_MAPS + 4)	/* Mem I/O Address */
+
+static __inline uint64_t
+ahc_compose_id(u_int device, u_int vendor, u_int subdevice, u_int subvendor)
+{
+	uint64_t id;
+
+	id = subvendor
+	   | (subdevice << 16)
+	   | ((uint64_t)vendor << 32)
+	   | ((uint64_t)device << 48);
+
+	return (id);
+}
+
+#define ID_ALL_MASK		0xFFFFFFFFFFFFFFFFull
+#define ID_DEV_VENDOR_MASK	0xFFFFFFFF00000000ull
+#define ID_9005_GENERIC_MASK	0xFFF0FFFF00000000ull
+#define ID_AIC7850		0x5078900400000000ull
+#define ID_AHA_2910_15_20_30C	0x5078900478509004ull
+#define ID_AIC7855		0x5578900400000000ull
+#define ID_AIC7859		0x3860900400000000ull
+#define ID_AHA_2930CU		0x3860900438699004ull
+#define ID_AIC7860		0x6078900400000000ull
+#define ID_AIC7860C		0x6078900478609004ull
+#define ID_AHA_1480A		0x6075900400000000ull
+#define ID_AHA_2940AU_0		0x6178900400000000ull
+#define ID_AHA_2940AU_1		0x6178900478619004ull
+#define ID_AHA_2940AU_CN	0x2178900478219004ull
+#define ID_AHA_2930C_VAR	0x6038900438689004ull
+
+#define ID_AIC7870		0x7078900400000000ull
+#define ID_AHA_2940		0x7178900400000000ull
+#define ID_AHA_3940		0x7278900400000000ull
+#define ID_AHA_398X		0x7378900400000000ull
+#define ID_AHA_2944		0x7478900400000000ull
+#define ID_AHA_3944		0x7578900400000000ull
+#define ID_AHA_4944		0x7678900400000000ull
+
+#define ID_AIC7880		0x8078900400000000ull
+#define ID_AIC7880_B		0x8078900478809004ull
+#define ID_AHA_2940U		0x8178900400000000ull
+#define ID_AHA_3940U		0x8278900400000000ull
+#define ID_AHA_2944U		0x8478900400000000ull
+#define ID_AHA_3944U		0x8578900400000000ull
+#define ID_AHA_398XU		0x8378900400000000ull
+#define ID_AHA_4944U		0x8678900400000000ull
+#define ID_AHA_2940UB		0x8178900478819004ull
+#define ID_AHA_2930U		0x8878900478889004ull
+#define ID_AHA_2940U_PRO	0x8778900478879004ull
+#define ID_AHA_2940U_CN		0x0078900478009004ull
+
+#define ID_AIC7895		0x7895900478959004ull
+#define ID_AIC7895_RAID_PORT	0x7893900478939004ull
+#define ID_AHA_2940U_DUAL	0x7895900478919004ull
+#define ID_AHA_3940AU		0x7895900478929004ull
+#define ID_AHA_3944AU		0x7895900478949004ull
+
+#define ID_AIC7890		0x001F9005000F9005ull
+#define ID_AAA_131U2		0x0013900500039005ull
+#define ID_AHA_2930U2		0x0011900501819005ull
+#define ID_AHA_2940U2B		0x00109005A1009005ull
+#define ID_AHA_2940U2_OEM	0x0010900521809005ull
+#define ID_AHA_2940U2		0x00109005A1809005ull
+#define ID_AHA_2950U2B		0x00109005E1009005ull
+
+#define ID_AIC7892		0x008F9005FFFF9005ull
+#define ID_AHA_29160		0x00809005E2A09005ull
+#define ID_AHA_29160_CPQ	0x00809005E2A00E11ull
+#define ID_AHA_29160N		0x0080900562A09005ull
+#define ID_AHA_29160C		0x0080900562209005ull
+#define ID_AHA_29160B		0x00809005E2209005ull
+#define ID_AHA_19160B		0x0081900562A19005ull
+
+#define ID_AIC7896		0x005F9005FFFF9005ull
+#define ID_AHA_3950U2B_0	0x00509005FFFF9005ull
+#define ID_AHA_3950U2B_1	0x00509005F5009005ull
+#define ID_AHA_3950U2D_0	0x00519005FFFF9005ull
+#define ID_AHA_3950U2D_1	0x00519005B5009005ull
+
+#define ID_AIC7899		0x00CF9005FFFF9005ull
+#define ID_AHA_3960D		0x00C09005F6209005ull /* AKA AHA-39160 */
+#define ID_AHA_3960D_CPQ	0x00C09005F6200E11ull
+
+#define ID_AIC7810		0x1078900400000000ull
+#define ID_AIC7815		0x7815900400000000ull
+
+#define DEVID_9005_TYPE(id) ((id) & 0xF)
+#define		DEVID_9005_TYPE_HBA		0x0	/* Standard Card */
+#define		DEVID_9005_TYPE_AAA		0x3	/* RAID Card */
+#define		DEVID_9005_TYPE_SISL		0x5	/* Low Cost Card */
+#define		DEVID_9005_TYPE_MB		0xF	/* On Motherboard */
+
+#define DEVID_9005_MAXRATE(id) (((id) & 0x30) >> 4)
+#define		DEVID_9005_MAXRATE_U160		0x0
+#define		DEVID_9005_MAXRATE_ULTRA2	0x1
+#define		DEVID_9005_MAXRATE_ULTRA	0x2
+#define		DEVID_9005_MAXRATE_FAST		0x3
+
+#define DEVID_9005_MFUNC(id) (((id) & 0x40) >> 6)
+
+#define DEVID_9005_CLASS(id) (((id) & 0xFF00) >> 8)
+#define		DEVID_9005_CLASS_SPI		0x0	/* Parallel SCSI */
+
+#define SUBID_9005_TYPE(id) ((id) & 0xF)
+#define		SUBID_9005_TYPE_MB		0xF	/* On Motherboard */
+#define		SUBID_9005_TYPE_CARD		0x0	/* Standard Card */
+#define		SUBID_9005_TYPE_LCCARD		0x1	/* Low Cost Card */
+#define		SUBID_9005_TYPE_RAID		0x3	/* Combined with Raid */
+
+#define SUBID_9005_MAXRATE(id) (((id) & 0x30) >> 4)
+#define		SUBID_9005_MAXRATE_ULTRA2	0x0
+#define		SUBID_9005_MAXRATE_ULTRA	0x1
+#define		SUBID_9005_MAXRATE_U160		0x2
+#define		SUBID_9005_MAXRATE_RESERVED	0x3
+
+#define SUBID_9005_SEEPTYPE(id)						\
+	((SUBID_9005_TYPE(id) == SUBID_9005_TYPE_MB)			\
+	 ? ((id) & 0xC0) >> 6						\
+	 : ((id) & 0x300) >> 8)
+#define		SUBID_9005_SEEPTYPE_NONE	0x0
+#define		SUBID_9005_SEEPTYPE_1K		0x1
+#define		SUBID_9005_SEEPTYPE_2K_4K	0x2
+#define		SUBID_9005_SEEPTYPE_RESERVED	0x3
+#define SUBID_9005_AUTOTERM(id)						\
+	((SUBID_9005_TYPE(id) == SUBID_9005_TYPE_MB)			\
+	 ? (((id) & 0x400) >> 10) == 0					\
+	 : (((id) & 0x40) >> 6) == 0)
+
+#define SUBID_9005_NUMCHAN(id)						\
+	((SUBID_9005_TYPE(id) == SUBID_9005_TYPE_MB)			\
+	 ? ((id) & 0x300) >> 8						\
+	 : ((id) & 0xC00) >> 10)
+
+#define SUBID_9005_LEGACYCONN(id)					\
+	((SUBID_9005_TYPE(id) == SUBID_9005_TYPE_MB)			\
+	 ? 0								\
+	 : ((id) & 0x80) >> 7)
+
+#define SUBID_9005_MFUNCENB(id)						\
+	((SUBID_9005_TYPE(id) == SUBID_9005_TYPE_MB)			\
+	 ? ((id) & 0x800) >> 11						\
+	 : ((id) & 0x1000) >> 12)
+/*
+ * Informational only. Should use chip register to be
+ * ceratian, but may be use in identification strings.
+ */
+#define SUBID_9005_CARD_SCSIWIDTH_MASK	0x2000
+#define SUBID_9005_CARD_PCIWIDTH_MASK	0x4000
+#define SUBID_9005_CARD_SEDIFF_MASK	0x8000
+
+static ahc_device_setup_t ahc_aic7850_setup;
+static ahc_device_setup_t ahc_aic7855_setup;
+static ahc_device_setup_t ahc_aic7860_setup;
+static ahc_device_setup_t ahc_apa1480_setup;
+static ahc_device_setup_t ahc_aic7870_setup;
+static ahc_device_setup_t ahc_aha394X_setup;
+static ahc_device_setup_t ahc_aha494X_setup;
+static ahc_device_setup_t ahc_aha398X_setup;
+static ahc_device_setup_t ahc_aic7880_setup;
+static ahc_device_setup_t ahc_aha2940Pro_setup;
+static ahc_device_setup_t ahc_aha394XU_setup;
+static ahc_device_setup_t ahc_aha398XU_setup;
+static ahc_device_setup_t ahc_aic7890_setup;
+static ahc_device_setup_t ahc_aic7892_setup;
+static ahc_device_setup_t ahc_aic7895_setup;
+static ahc_device_setup_t ahc_aic7896_setup;
+static ahc_device_setup_t ahc_aic7899_setup;
+static ahc_device_setup_t ahc_aha29160C_setup;
+static ahc_device_setup_t ahc_raid_setup;
+static ahc_device_setup_t ahc_aha394XX_setup;
+static ahc_device_setup_t ahc_aha494XX_setup;
+static ahc_device_setup_t ahc_aha398XX_setup;
+
+struct ahc_pci_identity ahc_pci_ident_table [] =
+{
+	/* aic7850 based controllers */
+	{
+		ID_AHA_2910_15_20_30C,
+		ID_ALL_MASK,
+		"Adaptec 2910/15/20/30C SCSI adapter",
+		ahc_aic7850_setup
+	},
+	/* aic7860 based controllers */
+	{
+		ID_AHA_2930CU,
+		ID_ALL_MASK,
+		"Adaptec 2930CU SCSI adapter",
+		ahc_aic7860_setup
+	},
+	{
+		ID_AHA_1480A & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec 1480A Ultra SCSI adapter",
+		ahc_apa1480_setup
+	},
+	{
+		ID_AHA_2940AU_0 & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec 2940A Ultra SCSI adapter",
+		ahc_aic7860_setup
+	},
+	{
+		ID_AHA_2940AU_CN & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec 2940A/CN Ultra SCSI adapter",
+		ahc_aic7860_setup
+	},
+	{
+		ID_AHA_2930C_VAR & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec 2930C SCSI adapter (VAR)",
+		ahc_aic7860_setup
+	},
+	/* aic7870 based controllers */
+	{
+		ID_AHA_2940,
+		ID_ALL_MASK,
+		"Adaptec 2940 SCSI adapter",
+		ahc_aic7870_setup
+	},
+	{
+		ID_AHA_3940,
+		ID_ALL_MASK,
+		"Adaptec 3940 SCSI adapter",
+		ahc_aha394X_setup
+	},
+	{
+		ID_AHA_398X,
+		ID_ALL_MASK,
+		"Adaptec 398X SCSI RAID adapter",
+		ahc_aha398X_setup
+	},
+	{
+		ID_AHA_2944,
+		ID_ALL_MASK,
+		"Adaptec 2944 SCSI adapter",
+		ahc_aic7870_setup
+	},
+	{
+		ID_AHA_3944,
+		ID_ALL_MASK,
+		"Adaptec 3944 SCSI adapter",
+		ahc_aha394X_setup
+	},
+	{
+		ID_AHA_4944,
+		ID_ALL_MASK,
+		"Adaptec 4944 SCSI adapter",
+		ahc_aha494X_setup
+	},
+	/* aic7880 based controllers */
+	{
+		ID_AHA_2940U & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec 2940 Ultra SCSI adapter",
+		ahc_aic7880_setup
+	},
+	{
+		ID_AHA_3940U & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec 3940 Ultra SCSI adapter",
+		ahc_aha394XU_setup
+	},
+	{
+		ID_AHA_2944U & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec 2944 Ultra SCSI adapter",
+		ahc_aic7880_setup
+	},
+	{
+		ID_AHA_3944U & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec 3944 Ultra SCSI adapter",
+		ahc_aha394XU_setup
+	},
+	{
+		ID_AHA_398XU & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec 398X Ultra SCSI RAID adapter",
+		ahc_aha398XU_setup
+	},
+	{
+		/*
+		 * XXX Don't know the slot numbers
+		 * so we can't identify channels
+		 */
+		ID_AHA_4944U & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec 4944 Ultra SCSI adapter",
+		ahc_aic7880_setup
+	},
+	{
+		ID_AHA_2930U & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec 2930 Ultra SCSI adapter",
+		ahc_aic7880_setup
+	},
+	{
+		ID_AHA_2940U_PRO & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec 2940 Pro Ultra SCSI adapter",
+		ahc_aha2940Pro_setup
+	},
+	{
+		ID_AHA_2940U_CN & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec 2940/CN Ultra SCSI adapter",
+		ahc_aic7880_setup
+	},
+	/* aic7890 based controllers */
+	{
+		ID_AHA_2930U2,
+		ID_ALL_MASK,
+		"Adaptec 2930 Ultra2 SCSI adapter",
+		ahc_aic7890_setup
+	},
+	{
+		ID_AHA_2940U2B,
+		ID_ALL_MASK,
+		"Adaptec 2940B Ultra2 SCSI adapter",
+		ahc_aic7890_setup
+	},
+	{
+		ID_AHA_2940U2_OEM,
+		ID_ALL_MASK,
+		"Adaptec 2940 Ultra2 SCSI adapter (OEM)",
+		ahc_aic7890_setup
+	},
+	{
+		ID_AHA_2940U2,
+		ID_ALL_MASK,
+		"Adaptec 2940 Ultra2 SCSI adapter",
+		ahc_aic7890_setup
+	},
+	{
+		ID_AHA_2950U2B,
+		ID_ALL_MASK,
+		"Adaptec 2950 Ultra2 SCSI adapter",
+		ahc_aic7890_setup
+	},
+	{
+		ID_AAA_131U2,
+		ID_ALL_MASK,
+		"Adaptec AAA-131 Ultra2 RAID adapter",
+		ahc_aic7890_setup
+	},
+	/* aic7892 based controllers */
+	{
+		ID_AHA_29160,
+		ID_ALL_MASK,
+		"Adaptec 29160 Ultra160 SCSI adapter",
+		ahc_aic7892_setup
+	},
+	{
+		ID_AHA_29160_CPQ,
+		ID_ALL_MASK,
+		"Adaptec (Compaq OEM) 29160 Ultra160 SCSI adapter",
+		ahc_aic7892_setup
+	},
+	{
+		ID_AHA_29160N,
+		ID_ALL_MASK,
+		"Adaptec 29160N Ultra160 SCSI adapter",
+		ahc_aic7892_setup
+	},
+	{
+		ID_AHA_29160C,
+		ID_ALL_MASK,
+		"Adaptec 29160C Ultra160 SCSI adapter",
+		ahc_aha29160C_setup
+	},
+	{
+		ID_AHA_29160B,
+		ID_ALL_MASK,
+		"Adaptec 29160B Ultra160 SCSI adapter",
+		ahc_aic7892_setup
+	},
+	{
+		ID_AHA_19160B,
+		ID_ALL_MASK,
+		"Adaptec 19160B Ultra160 SCSI adapter",
+		ahc_aic7892_setup
+	},
+	/* aic7895 based controllers */	
+	{
+		ID_AHA_2940U_DUAL,
+		ID_ALL_MASK,
+		"Adaptec 2940/DUAL Ultra SCSI adapter",
+		ahc_aic7895_setup
+	},
+	{
+		ID_AHA_3940AU,
+		ID_ALL_MASK,
+		"Adaptec 3940A Ultra SCSI adapter",
+		ahc_aic7895_setup
+	},
+	{
+		ID_AHA_3944AU,
+		ID_ALL_MASK,
+		"Adaptec 3944A Ultra SCSI adapter",
+		ahc_aic7895_setup
+	},
+	/* aic7896/97 based controllers */	
+	{
+		ID_AHA_3950U2B_0,
+		ID_ALL_MASK,
+		"Adaptec 3950B Ultra2 SCSI adapter",
+		ahc_aic7896_setup
+	},
+	{
+		ID_AHA_3950U2B_1,
+		ID_ALL_MASK,
+		"Adaptec 3950B Ultra2 SCSI adapter",
+		ahc_aic7896_setup
+	},
+	{
+		ID_AHA_3950U2D_0,
+		ID_ALL_MASK,
+		"Adaptec 3950D Ultra2 SCSI adapter",
+		ahc_aic7896_setup
+	},
+	{
+		ID_AHA_3950U2D_1,
+		ID_ALL_MASK,
+		"Adaptec 3950D Ultra2 SCSI adapter",
+		ahc_aic7896_setup
+	},
+	/* aic7899 based controllers */	
+	{
+		ID_AHA_3960D,
+		ID_ALL_MASK,
+		"Adaptec 3960D Ultra160 SCSI adapter",
+		ahc_aic7899_setup
+	},
+	{
+		ID_AHA_3960D_CPQ,
+		ID_ALL_MASK,
+		"Adaptec (Compaq OEM) 3960D Ultra160 SCSI adapter",
+		ahc_aic7899_setup
+	},
+	/* Generic chip probes for devices we don't know 'exactly' */
+	{
+		ID_AIC7850 & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec aic7850 SCSI adapter",
+		ahc_aic7850_setup
+	},
+	{
+		ID_AIC7855 & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec aic7855 SCSI adapter",
+		ahc_aic7855_setup
+	},
+	{
+		ID_AIC7859 & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec aic7859 Ultra SCSI adapter",
+		ahc_aic7860_setup
+	},
+	{
+		ID_AIC7860 & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec aic7860 SCSI adapter",
+		ahc_aic7860_setup
+	},
+	{
+		ID_AIC7870 & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec aic7870 SCSI adapter",
+		ahc_aic7870_setup
+	},
+	{
+		ID_AIC7880 & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec aic7880 Ultra SCSI adapter",
+		ahc_aic7880_setup
+	},
+	{
+		ID_AIC7890 & ID_9005_GENERIC_MASK,
+		ID_9005_GENERIC_MASK,
+		"Adaptec aic7890/91 Ultra2 SCSI adapter",
+		ahc_aic7890_setup
+	},
+	{
+		ID_AIC7892 & ID_9005_GENERIC_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec aic7892 Ultra160 SCSI adapter",
+		ahc_aic7892_setup
+	},
+	{
+		ID_AIC7895 & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec aic7895 Ultra SCSI adapter",
+		ahc_aic7895_setup
+	},
+	{
+		ID_AIC7895_RAID_PORT & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec aic7895 Ultra SCSI adapter (RAID PORT)",
+		ahc_aic7895_setup
+	},
+	{
+		ID_AIC7896 & ID_9005_GENERIC_MASK,
+		ID_9005_GENERIC_MASK,
+		"Adaptec aic7896/97 Ultra2 SCSI adapter",
+		ahc_aic7896_setup
+	},
+	{
+		ID_AIC7899 & ID_9005_GENERIC_MASK,
+		ID_9005_GENERIC_MASK,
+		"Adaptec aic7899 Ultra160 SCSI adapter",
+		ahc_aic7899_setup
+	},
+	{
+		ID_AIC7810 & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec aic7810 RAID memory controller",
+		ahc_raid_setup
+	},
+	{
+		ID_AIC7815 & ID_DEV_VENDOR_MASK,
+		ID_DEV_VENDOR_MASK,
+		"Adaptec aic7815 RAID memory controller",
+		ahc_raid_setup
+	}
+};
+
+const u_int ahc_num_pci_devs = NUM_ELEMENTS(ahc_pci_ident_table);
+		
+#define AHC_394X_SLOT_CHANNEL_A	4
+#define AHC_394X_SLOT_CHANNEL_B	5
+
+#define AHC_398X_SLOT_CHANNEL_A	4
+#define AHC_398X_SLOT_CHANNEL_B	8
+#define AHC_398X_SLOT_CHANNEL_C	12
+
+#define AHC_494X_SLOT_CHANNEL_A	4
+#define AHC_494X_SLOT_CHANNEL_B	5
+#define AHC_494X_SLOT_CHANNEL_C	6
+#define AHC_494X_SLOT_CHANNEL_D	7
+
+#define	DEVCONFIG		0x40
+#define		SCBSIZE32	0x00010000ul	/* aic789X only */
+#define		MPORTMODE	0x00000400ul	/* aic7870 only */
+#define		RAMPSM		0x00000200ul	/* aic7870 only */
+#define		VOLSENSE	0x00000100ul
+#define		SCBRAMSEL	0x00000080ul
+#define		MRDCEN		0x00000040ul
+#define		EXTSCBTIME	0x00000020ul	/* aic7870 only */
+#define		EXTSCBPEN	0x00000010ul	/* aic7870 only */
+#define		BERREN		0x00000008ul
+#define		DACEN		0x00000004ul
+#define		STPWLEVEL	0x00000002ul
+#define		DIFACTNEGEN	0x00000001ul	/* aic7870 only */
+
+#define	CSIZE_LATTIME		0x0c
+#define		CACHESIZE	0x0000003ful	/* only 5 bits */
+#define		LATTIME		0x0000ff00ul
+
+typedef enum
+{
+	AHC_POWER_STATE_D0,
+	AHC_POWER_STATE_D1,
+	AHC_POWER_STATE_D2,
+	AHC_POWER_STATE_D3
+} ahc_power_state;
+
+static void ahc_power_state_change(struct ahc_softc *ahc,
+				   ahc_power_state new_state);
+static int ahc_ext_scbram_present(struct ahc_softc *ahc);
+static void ahc_scbram_config(struct ahc_softc *ahc, int enable,
+				  int pcheck, int fast, int large);
+static void ahc_probe_ext_scbram(struct ahc_softc *ahc);
+static void check_extport(struct ahc_softc *ahc, u_int *sxfrctl1);
+static void configure_termination(struct ahc_softc *ahc,
+				  struct seeprom_descriptor *sd,
+				  u_int adapter_control,
+	 			  u_int *sxfrctl1);
+
+static void ahc_new_term_detect(struct ahc_softc *ahc,
+				int *enableSEC_low,
+				int *enableSEC_high,
+				int *enablePRI_low,
+				int *enablePRI_high,
+				int *eeprom_present);
+static void aic787X_cable_detect(struct ahc_softc *ahc, int *internal50_present,
+				 int *internal68_present,
+				 int *externalcable_present,
+				 int *eeprom_present);
+static void aic785X_cable_detect(struct ahc_softc *ahc, int *internal50_present,
+				 int *externalcable_present,
+				 int *eeprom_present);
+static int acquire_seeprom(struct ahc_softc *ahc,
+			   struct seeprom_descriptor *sd);
+static void release_seeprom(struct seeprom_descriptor *sd);
+static void write_brdctl(struct ahc_softc *ahc, uint8_t value);
+static uint8_t read_brdctl(struct ahc_softc *ahc);
+
+struct ahc_pci_identity *
+ahc_find_pci_device(ahc_dev_softc_t pci)
+{
+	uint64_t  full_id;
+	uint16_t  device;
+	uint16_t  vendor;
+	uint16_t  subdevice;
+	uint16_t  subvendor;
+	struct	  ahc_pci_identity *entry;
+	u_int	  i;
+
+	vendor = ahc_pci_read_config(pci, PCIR_DEVVENDOR, /*bytes*/2);
+	device = ahc_pci_read_config(pci, PCIR_DEVICE, /*bytes*/2);
+	subvendor = ahc_pci_read_config(pci, PCIR_SUBVEND_0, /*bytes*/2);
+	subdevice = ahc_pci_read_config(pci, PCIR_SUBDEV_0, /*bytes*/2);
+	full_id = ahc_compose_id(device,
+				 vendor,
+				 subdevice,
+				 subvendor);
+
+	/* If the second function is not hooked up, ignore it. */
+	if (ahc_get_pci_function(pci) > 0
+	 && subvendor == 0x9005
+	 && SUBID_9005_MFUNCENB(subdevice) == 0)
+		return (NULL);
+
+	for (i = 0; i < ahc_num_pci_devs; i++) {
+		entry = &ahc_pci_ident_table[i];
+		if (entry->full_id == (full_id & entry->id_mask))
+			return (entry);
+	}
+	return (NULL);
+}
+
+int
+ahc_pci_config(struct ahc_softc *ahc, struct ahc_pci_identity *entry)
+{
+	struct		 ahc_probe_config probe_config;
+	struct scb_data *shared_scb_data;
+	u_int		 command;
+	u_int		 our_id = 0;
+	u_int		 sxfrctl1;
+	u_int		 scsiseq;
+	u_int		 dscommand0;
+	int		 error;
+	uint8_t		 sblkctl;
+
+	shared_scb_data = NULL;
+	ahc_init_probe_config(&probe_config);
+	error = entry->setup(ahc->dev_softc, &probe_config);
+	if (error != 0)
+		return (error);
+	probe_config.chip |= AHC_PCI;
+	probe_config.description = entry->name;
+
+	error = ahc_pci_map_registers(ahc);
+	if (error != 0)
+		return (error);
+
+	ahc_power_state_change(ahc, AHC_POWER_STATE_D0);
+
+	/* Ensure busmastering is enabled */
+	command = ahc_pci_read_config(ahc->dev_softc, PCIR_COMMAND, /*bytes*/1);
+	command |= PCIM_CMD_BUSMASTEREN;
+	ahc_pci_write_config(ahc->dev_softc, PCIR_COMMAND, command, /*bytes*/1);
+
+	/* On all PCI adapters, we allow SCB paging */
+	probe_config.flags |= AHC_PAGESCBS;
+
+	error = ahc_softc_init(ahc, &probe_config);
+	if (error != 0)
+		return (error);
+
+	/* Remeber how the card was setup in case there is no SEEPROM */
+	if ((ahc_inb(ahc, HCNTRL) & POWRDN) == 0) {
+		pause_sequencer(ahc);
+		if ((ahc->features & AHC_ULTRA2) != 0)
+			our_id = ahc_inb(ahc, SCSIID_ULTRA2) & OID;
+		else
+			our_id = ahc_inb(ahc, SCSIID) & OID;
+		sxfrctl1 = ahc_inb(ahc, SXFRCTL1) & STPWEN;
+		scsiseq = ahc_inb(ahc, SCSISEQ);
+	} else {
+		sxfrctl1 = STPWEN;
+		our_id = 7;
+		scsiseq = 0;
+	}
+
+	error = ahc_reset(ahc);
+	if (error != 0)
+		return (ENXIO);
+
+	if ((ahc->features & AHC_DT) != 0) {
+		u_int sfunct;
+
+		/* Perform ALT-Mode Setup */
+		sfunct = ahc_inb(ahc, SFUNCT) & ~ALT_MODE;
+		ahc_outb(ahc, SFUNCT, sfunct | ALT_MODE);
+		ahc_outb(ahc, OPTIONMODE, OPTIONMODE_DEFAULTS);
+		ahc_outb(ahc, SFUNCT, sfunct);
+
+		/* Normal mode setup */
+		ahc_outb(ahc, CRCCONTROL1, CRCVALCHKEN|CRCENDCHKEN|CRCREQCHKEN
+					  |TARGCRCENDEN);
+	}
+
+	error = ahc_pci_map_int(ahc);
+	if (error != 0)
+		return (error);
+	dscommand0 = ahc_inb(ahc, DSCOMMAND0);
+	dscommand0 |= MPARCKEN|CACHETHEN;
+	if ((ahc->features & AHC_ULTRA2) != 0) {
+
+		/*
+		 * DPARCKEN doesn't work correctly on
+		 * some MBs so don't use it.
+		 */
+		dscommand0 &= ~DPARCKEN;
+	}
+
+	/*
+	 * Handle chips that must have cache line
+	 * streaming (dis/en)abled.
+	 */
+	if ((ahc->bugs & AHC_CACHETHEN_DIS_BUG) != 0)
+		dscommand0 |= CACHETHEN;
+
+	if ((ahc->bugs & AHC_CACHETHEN_BUG) != 0)
+		dscommand0 &= ~CACHETHEN;
+
+	ahc_outb(ahc, DSCOMMAND0, dscommand0);
+
+	ahc->pci_cachesize =
+	    ahc_pci_read_config(ahc->dev_softc, CSIZE_LATTIME,
+				/*bytes*/1) & CACHESIZE;
+	ahc->pci_cachesize *= 4;
+
+	/* See if we have a SEEPROM and perform auto-term */
+	check_extport(ahc, &sxfrctl1);
+
+	/*
+	 * Take the LED out of diagnostic mode
+	 */
+	sblkctl = ahc_inb(ahc, SBLKCTL);
+	ahc_outb(ahc, SBLKCTL, (sblkctl & ~(DIAGLEDEN|DIAGLEDON)));
+
+	if ((ahc->features & AHC_ULTRA2) != 0) {
+		ahc_outb(ahc, DFF_THRSH, RD_DFTHRSH_MAX|WR_DFTHRSH_MAX);
+	} else {
+		ahc_outb(ahc, DSPCISTATUS, DFTHRSH_100);
+	}
+
+	if (ahc->flags & AHC_USEDEFAULTS) {
+		/*
+		 * PCI Adapter default setup
+		 * Should only be used if the adapter does not have
+		 * a SEEPROM.
+		 */
+		/* See if someone else set us up already */
+		if (scsiseq != 0) {
+			printf("%s: Using left over BIOS settings\n",
+				ahc_name(ahc));
+			ahc->flags &= ~AHC_USEDEFAULTS;
+			ahc->flags |= AHC_BIOS_ENABLED;
+		} else {
+			/*
+			 * Assume only one connector and always turn
+			 * on termination.
+			 */
+ 			our_id = 0x07;
+			sxfrctl1 = STPWEN;
+		}
+		ahc_outb(ahc, SCSICONF, our_id|ENSPCHK|RESET_SCSI);
+
+		ahc->our_id = our_id;
+	}
+
+	/*
+	 * Take a look to see if we have external SRAM.
+	 * We currently do not attempt to use SRAM that is
+	 * shared among multiple controllers.
+	 */
+	ahc_probe_ext_scbram(ahc);
+
+	/*
+	 * Record our termination setting for the
+	 * generic initialization routine.
+	 */
+	if ((sxfrctl1 & STPWEN) != 0)
+		ahc->flags |= AHC_TERM_ENB_A;
+
+	/* Core initialization */
+	error = ahc_init(ahc);
+	if (error != 0)
+		return (error);
+
+	/*
+	 * Link this softc in with all other ahc instances.
+	 */
+	ahc_softc_insert(ahc);
+
+	return (0);
+}
+
+static void
+ahc_power_state_change(struct ahc_softc *ahc, ahc_power_state new_state)
+{
+	uint32_t cap;
+	u_int cap_offset;
+
+	/*
+	 * Traverse the capability list looking for
+	 * the power management capability.
+	 */
+	cap = 0;
+	cap_offset = ahc_pci_read_config(ahc->dev_softc,
+					 PCIR_CAP_PTR, /*bytes*/1);
+	while (cap_offset != 0) {
+
+		cap = ahc_pci_read_config(ahc->dev_softc,
+					  cap_offset, /*bytes*/4);
+		if ((cap & 0xFF) == 1
+		 && ((cap >> 16) & 0x3) > 0) {
+			uint32_t pm_control;
+
+			pm_control = ahc_pci_read_config(ahc->dev_softc,
+							 cap_offset + 4,
+							 /*bytes*/4);
+			pm_control &= ~0x3;
+			pm_control |= new_state;
+			ahc_pci_write_config(ahc->dev_softc,
+					     cap_offset + 4,
+					     pm_control, /*bytes*/2);
+			break;
+		}
+		cap_offset = (cap >> 8) & 0xFF;
+	}
+}
+
+/*
+ * Test for the presense of external sram in an
+ * "unshared" configuration.
+ */
+static int
+ahc_ext_scbram_present(struct ahc_softc *ahc)
+{
+	u_int chip;
+	int ramps;
+	int single_user;
+	uint32_t devconfig;
+
+	chip = ahc->chip & AHC_CHIPID_MASK;
+	devconfig = ahc_pci_read_config(ahc->dev_softc,
+					DEVCONFIG, /*bytes*/4);
+	single_user = (devconfig & MPORTMODE) != 0;
+
+	if ((ahc->features & AHC_ULTRA2) != 0)
+		ramps = (ahc_inb(ahc, DSCOMMAND0) & RAMPS) != 0;
+	else if (chip >= AHC_AIC7870)
+		ramps = (devconfig & RAMPSM) != 0;
+	else
+		ramps = 0;
+
+	if (ramps && single_user)
+		return (1);
+	return (0);
+}
+
+/*
+ * Enable external scbram.
+ */
+static void
+ahc_scbram_config(struct ahc_softc *ahc, int enable, int pcheck,
+		  int fast, int large)
+{
+	uint32_t devconfig;
+
+	if (ahc->features & AHC_MULTI_FUNC) {
+		/*
+		 * Set the SCB Base addr (highest address bit)
+		 * depending on which channel we are.
+		 */
+		ahc_outb(ahc, SCBBADDR, ahc_get_pci_function(ahc->dev_softc));
+	}
+
+	devconfig = ahc_pci_read_config(ahc->dev_softc, DEVCONFIG, /*bytes*/4);
+	if ((ahc->features & AHC_ULTRA2) != 0) {
+		u_int dscommand0;
+
+		dscommand0 = ahc_inb(ahc, DSCOMMAND0);
+		if (enable)
+			dscommand0 &= ~INTSCBRAMSEL;
+		else
+			dscommand0 |= INTSCBRAMSEL;
+		if (large)
+			dscommand0 &= ~USCBSIZE32;
+		else
+			dscommand0 |= USCBSIZE32;
+		ahc_outb(ahc, DSCOMMAND0, dscommand0);
+	} else {
+		if (fast)
+			devconfig &= ~EXTSCBTIME;
+		else
+			devconfig |= EXTSCBTIME;
+		if (enable)
+			devconfig &= ~SCBRAMSEL;
+		else
+			devconfig |= SCBRAMSEL;
+		if (large)
+			devconfig &= ~SCBSIZE32;
+		else
+			devconfig |= SCBSIZE32;
+	}
+	if (pcheck)
+		devconfig |= EXTSCBPEN;
+	else
+		devconfig &= ~EXTSCBPEN;
+
+	ahc_pci_write_config(ahc->dev_softc, DEVCONFIG, devconfig, /*bytes*/4);
+}
+
+/*
+ * Take a look to see if we have external SRAM.
+ * We currently do not attempt to use SRAM that is
+ * shared among multiple controllers.
+ */
+static void
+ahc_probe_ext_scbram(struct ahc_softc *ahc)
+{
+	int num_scbs;
+	int test_num_scbs;
+	int enable;
+	int pcheck;
+	int fast;
+	int large;
+
+	enable = FALSE;
+	pcheck = FALSE;
+	fast = FALSE;
+	large = FALSE;
+	num_scbs = 0;
+	
+	if (ahc_ext_scbram_present(ahc) == 0)
+		goto done;
+
+	/*
+	 * Probe for the best parameters to use.
+	 */
+	ahc_scbram_config(ahc, /*enable*/TRUE, pcheck, fast, large);
+	num_scbs = ahc_probe_scbs(ahc);
+	if (num_scbs == 0) {
+		/* The SRAM wasn't really present. */
+		goto done;
+	}
+	enable = TRUE;
+
+	/*
+	 * Clear any outstanding parity error
+	 * and ensure that parity error reporting
+	 * is enabled.
+	 */
+	ahc_outb(ahc, SEQCTL, 0);
+	ahc_outb(ahc, CLRINT, CLRPARERR);
+	ahc_outb(ahc, CLRINT, CLRBRKADRINT);
+
+	/* Now see if we can do parity */
+	ahc_scbram_config(ahc, enable, /*pcheck*/TRUE, fast, large);
+	num_scbs = ahc_probe_scbs(ahc);
+	if ((ahc_inb(ahc, INTSTAT) & BRKADRINT) == 0
+	 || (ahc_inb(ahc, ERROR) & MPARERR) == 0)
+		pcheck = TRUE;
+
+	/* Clear any resulting parity error */
+	ahc_outb(ahc, CLRINT, CLRPARERR);
+	ahc_outb(ahc, CLRINT, CLRBRKADRINT);
+
+	/* Now see if we can do fast timing */
+	ahc_scbram_config(ahc, enable, pcheck, /*fast*/TRUE, large);
+	test_num_scbs = ahc_probe_scbs(ahc);
+	if (test_num_scbs == num_scbs
+	 && ((ahc_inb(ahc, INTSTAT) & BRKADRINT) == 0
+	  || (ahc_inb(ahc, ERROR) & MPARERR) == 0))
+		fast = TRUE;
+
+	/*
+	 * See if we can use large SCBs and still maintain
+	 * the same overall count of SCBs.
+	 */
+	if ((ahc->features & AHC_LARGE_SCBS) != 0) {
+		ahc_scbram_config(ahc, enable, pcheck, fast, /*large*/TRUE);
+		test_num_scbs = ahc_probe_scbs(ahc);
+		if (test_num_scbs >= num_scbs) {
+			large = TRUE;
+			num_scbs = test_num_scbs;
+	 		if (num_scbs >= 64) {
+				/*
+				 * We have enough space to move the
+				 * "busy targets table" into SCB space
+				 * and make it qualify all the way to the
+				 * lun level.
+				 */
+				ahc->flags |= AHC_SCB_BTT;
+			}
+		}
+	}
+done:
+	/*
+	 * Disable parity error reporting until we
+	 * can load instruction ram.
+	 */
+	ahc_outb(ahc, SEQCTL, PERRORDIS|FAILDIS);
+	/* Clear any latched parity error */
+	ahc_outb(ahc, CLRINT, CLRPARERR);
+	ahc_outb(ahc, CLRINT, CLRBRKADRINT);
+	if (bootverbose && enable) {
+		printf("%s: External SRAM, %s access%s, %dbytes/SCB\n",
+		       ahc_name(ahc), fast ? "fast" : "slow", 
+		       pcheck ? ", parity checking enabled" : "",
+		       large ? 64 : 32);
+	}
+	ahc_scbram_config(ahc, enable, pcheck, fast, large);
+}
+
+/*
+ * Check the external port logic for a serial eeprom
+ * and termination/cable detection contrls.
+ */
+static void
+check_extport(struct ahc_softc *ahc, u_int *sxfrctl1)
+{
+	struct	seeprom_descriptor sd;
+	struct	seeprom_config sc;
+	u_int	scsi_conf;
+	u_int	adapter_control;
+	int	have_seeprom;
+	int	have_autoterm;
+
+	sd.sd_ahc = ahc;
+	sd.sd_control_offset = SEECTL;		
+	sd.sd_status_offset = SEECTL;		
+	sd.sd_dataout_offset = SEECTL;		
+
+	/*
+	 * For some multi-channel devices, the c46 is simply too
+	 * small to work.  For the other controller types, we can
+	 * get our information from either SEEPROM type.  Set the
+	 * type to start our probe with accordingly.
+	 */
+	if (ahc->flags & AHC_LARGE_SEEPROM)
+		sd.sd_chip = C56_66;
+	else
+		sd.sd_chip = C46;
+
+	sd.sd_MS = SEEMS;
+	sd.sd_RDY = SEERDY;
+	sd.sd_CS = SEECS;
+	sd.sd_CK = SEECK;
+	sd.sd_DO = SEEDO;
+	sd.sd_DI = SEEDI;
+
+	have_seeprom = acquire_seeprom(ahc, &sd);
+	if (have_seeprom) {
+
+		if (bootverbose) 
+			printf("%s: Reading SEEPROM...", ahc_name(ahc));
+
+		for (;;) {
+			u_int start_addr;
+
+			start_addr = 32 * (ahc->channel - 'A');
+
+			have_seeprom = read_seeprom(&sd, (uint16_t *)&sc,
+						    start_addr, sizeof(sc)/2);
+
+			if (have_seeprom)
+				have_seeprom = verify_cksum(&sc);
+
+			if (have_seeprom != 0 || sd.sd_chip == C56_66) {
+				if (bootverbose) {
+					if (have_seeprom == 0)
+						printf ("checksum error\n");
+					else
+						printf ("done.\n");
+				}
+				break;
+			}
+			sd.sd_chip = C56_66;
+		}
+	}
+
+#if 0
+	if (!have_seeprom) {
+		/*
+		 * Pull scratch ram settings and treat them as
+		 * if they are the contents of an seeprom if
+		 * the 'ADPT' signature is found in SCB2.
+		 */
+		ahc_outb(ahc, SCBPTR, 2);
+		if (ahc_inb(ahc, SCB_BASE) == 'A'
+		 && ahc_inb(ahc, SCB_BASE + 1) == 'D'
+		 && ahc_inb(ahc, SCB_BASE + 2) == 'P'
+		 && ahc_inb(ahc, SCB_BASE + 3) == 'T') {
+			uint8_t *sc_bytes;
+			int	  i;
+
+			sc_bytes = (uint8_t *)&sc;
+			for (i = 0; i < 64; i++)
+				sc_bytes[i] = ahc_inb(ahc, TARG_SCSIRATE + i);
+			/* Byte 0x1c is stored in byte 4 of SCB2 */
+			sc_bytes[0x1c] = ahc_inb(ahc, SCB_BASE + 4);
+			have_seeprom = verify_cksum(&sc);
+		}
+	}
+#endif
+
+	if (!have_seeprom) {
+		if (bootverbose)
+			printf("%s: No SEEPROM available.\n", ahc_name(ahc));
+		ahc->flags |= AHC_USEDEFAULTS;
+	} else {
+		/*
+		 * Put the data we've collected down into SRAM
+		 * where ahc_init will find it.
+		 */
+		int i;
+		int max_targ = sc.max_targets & CFMAXTARG;
+		uint16_t discenable;
+		uint16_t ultraenb;
+
+		discenable = 0;
+		ultraenb = 0;
+		if ((sc.adapter_control & CFULTRAEN) != 0) {
+			/*
+			 * Determine if this adapter has a "newstyle"
+			 * SEEPROM format.
+			 */
+			for (i = 0; i < max_targ; i++) {
+				if ((sc.device_flags[i] & CFSYNCHISULTRA) != 0){
+					ahc->flags |= AHC_NEWEEPROM_FMT;
+					break;
+				}
+			}
+		}
+
+		for (i = 0; i < max_targ; i++) {
+			u_int     scsirate;
+			uint16_t target_mask;
+
+			target_mask = 0x01 << i;
+			if (sc.device_flags[i] & CFDISC)
+				discenable |= target_mask;
+			if ((ahc->flags & AHC_NEWEEPROM_FMT) != 0) {
+				if ((sc.device_flags[i] & CFSYNCHISULTRA) != 0)
+					ultraenb |= target_mask;
+			} else if ((sc.adapter_control & CFULTRAEN) != 0) {
+				ultraenb |= target_mask;
+			}
+			if ((sc.device_flags[i] & CFXFER) == 0x04
+			 && (ultraenb & target_mask) != 0) {
+				/* Treat 10MHz as a non-ultra speed */
+				sc.device_flags[i] &= ~CFXFER;
+			 	ultraenb &= ~target_mask;
+			}
+			if ((ahc->features & AHC_ULTRA2) != 0) {
+				u_int offset;
+
+				if (sc.device_flags[i] & CFSYNCH)
+					offset = MAX_OFFSET_ULTRA2;
+				else 
+					offset = 0;
+				ahc_outb(ahc, TARG_OFFSET + i, offset);
+
+				/*
+				 * The ultra enable bits contain the
+				 * high bit of the ultra2 sync rate
+				 * field.
+				 */
+				scsirate = (sc.device_flags[i] & CFXFER)
+					 | ((ultraenb & target_mask)
+					    ? 0x8 : 0x0);
+				if (sc.device_flags[i] & CFWIDEB)
+					scsirate |= WIDEXFER;
+			} else {
+				scsirate = (sc.device_flags[i] & CFXFER) << 4;
+				if (sc.device_flags[i] & CFSYNCH)
+					scsirate |= SOFS;
+				if (sc.device_flags[i] & CFWIDEB)
+					scsirate |= WIDEXFER;
+			}
+			ahc_outb(ahc, TARG_SCSIRATE + i, scsirate);
+		}
+		ahc->our_id = sc.brtime_id & CFSCSIID;
+
+		scsi_conf = (ahc->our_id & 0x7);
+		if (sc.adapter_control & CFSPARITY)
+			scsi_conf |= ENSPCHK;
+		if (sc.adapter_control & CFRESETB)
+			scsi_conf |= RESET_SCSI;
+
+		if ((sc.adapter_control & CFCHNLBPRIMARY) != 0
+		 && (ahc->features & AHC_MULTI_FUNC) != 0)
+			ahc->flags |= AHC_CHANNEL_B_PRIMARY;
+
+		if (sc.bios_control & CFEXTEND)
+			ahc->flags |= AHC_EXTENDED_TRANS_A;
+
+		if (sc.bios_control & CFBIOSEN)
+			ahc->flags |= AHC_BIOS_ENABLED;
+		if (ahc->features & AHC_ULTRA
+		 && (ahc->flags & AHC_NEWEEPROM_FMT) == 0) {
+			/* Should we enable Ultra mode? */
+			if (!(sc.adapter_control & CFULTRAEN))
+				/* Treat us as a non-ultra card */
+				ultraenb = 0;
+		}
+
+		if (sc.signature == CFSIGNATURE) {
+			uint32_t devconfig;
+
+			/* Honor the STPWLEVEL settings */
+			devconfig = ahc_pci_read_config(ahc->dev_softc,
+							DEVCONFIG, /*bytes*/4);
+			devconfig &= ~STPWLEVEL;
+			if ((sc.bios_control & CFSTPWLEVEL) != 0)
+				devconfig |= STPWLEVEL;
+			ahc_pci_write_config(ahc->dev_softc, DEVCONFIG,
+					     devconfig, /*bytes*/4);
+		}
+		/* Set SCSICONF info */
+		ahc_outb(ahc, SCSICONF, scsi_conf);
+		ahc_outb(ahc, DISC_DSB, ~(discenable & 0xff));
+		ahc_outb(ahc, DISC_DSB + 1, ~((discenable >> 8) & 0xff));
+		ahc_outb(ahc, ULTRA_ENB, ultraenb & 0xff);
+		ahc_outb(ahc, ULTRA_ENB + 1, (ultraenb >> 8) & 0xff);
+	}
+
+	/*
+	 * Cards that have the external logic necessary to talk to
+	 * a SEEPROM, are almost certain to have the remaining logic
+	 * necessary for auto-termination control.  This assumption
+	 * hasn't failed yet...
+	 */
+	have_autoterm = have_seeprom;
+	if (have_seeprom)
+		adapter_control = sc.adapter_control;
+	else
+		adapter_control = CFAUTOTERM;
+
+	/*
+	 * Some low-cost chips have SEEPROM and auto-term control built
+	 * in, instead of using a GAL.  They can tell us directly
+	 * if the termination logic is enabled.
+	 */
+	if ((ahc->features & AHC_SPIOCAP) != 0) {
+		if ((ahc_inb(ahc, SPIOCAP) & SSPIOCPS) != 0)
+			have_autoterm = TRUE;
+		else
+			have_autoterm = FALSE;
+	}
+
+	if (have_autoterm)
+		configure_termination(ahc, &sd, adapter_control, sxfrctl1);
+
+	release_seeprom(&sd);
+}
+
+static void
+configure_termination(struct ahc_softc *ahc,
+		      struct seeprom_descriptor *sd,
+		      u_int adapter_control,
+		      u_int *sxfrctl1)
+{
+	uint8_t brddat;
+	
+	brddat = 0;
+
+	/*
+	 * Update the settings in sxfrctl1 to match the
+	 * termination settings 
+	 */
+	*sxfrctl1 = 0;
+	
+	/*
+	 * SEECS must be on for the GALS to latch
+	 * the data properly.  Be sure to leave MS
+	 * on or we will release the seeprom.
+	 */
+	SEEPROM_OUTB(sd, sd->sd_MS | sd->sd_CS);
+	if ((adapter_control & CFAUTOTERM) != 0
+	 || (ahc->features & AHC_NEW_TERMCTL) != 0) {
+		int internal50_present;
+		int internal68_present;
+		int externalcable_present;
+		int eeprom_present;
+		int enableSEC_low;
+		int enableSEC_high;
+		int enablePRI_low;
+		int enablePRI_high;
+		int sum;
+
+		enableSEC_low = 0;
+		enableSEC_high = 0;
+		enablePRI_low = 0;
+		enablePRI_high = 0;
+		if ((ahc->features & AHC_NEW_TERMCTL) != 0) {
+			ahc_new_term_detect(ahc, &enableSEC_low,
+					       &enableSEC_high,
+					       &enablePRI_low,
+					       &enablePRI_high,
+					       &eeprom_present);
+			if ((adapter_control & CFSEAUTOTERM) == 0) {
+				if (bootverbose)
+					printf("%s: Manual SE Termination\n",
+					       ahc_name(ahc));
+				enableSEC_low = (adapter_control & CFSELOWTERM);
+				enableSEC_high =
+				    (adapter_control & CFSEHIGHTERM);
+			}
+			if ((adapter_control & CFAUTOTERM) == 0) {
+				if (bootverbose)
+					printf("%s: Manual LVD Termination\n",
+					       ahc_name(ahc));
+				enablePRI_low = (adapter_control & CFSTERM);
+				enablePRI_high = (adapter_control & CFWSTERM);
+			}
+			/* Make the table calculations below happy */
+			internal50_present = 0;
+			internal68_present = 1;
+			externalcable_present = 1;
+		} else if ((ahc->features & AHC_SPIOCAP) != 0) {
+			aic785X_cable_detect(ahc, &internal50_present,
+					     &externalcable_present,
+					     &eeprom_present);
+		} else {
+			aic787X_cable_detect(ahc, &internal50_present,
+					     &internal68_present,
+					     &externalcable_present,
+					     &eeprom_present);
+		}
+
+		if ((ahc->features & AHC_WIDE) == 0)
+			internal68_present = 0;
+
+		if (bootverbose
+		 && (ahc->features & AHC_ULTRA2) == 0) {
+			printf("%s: internal 50 cable %s present",
+			       ahc_name(ahc),
+			       internal50_present ? "is":"not");
+
+			if ((ahc->features & AHC_WIDE) != 0)
+				printf(", internal 68 cable %s present",
+				       internal68_present ? "is":"not");
+			printf("\n%s: external cable %s present\n",
+			       ahc_name(ahc),
+			       externalcable_present ? "is":"not");
+		}
+		if (bootverbose)
+			printf("%s: BIOS eeprom %s present\n",
+			       ahc_name(ahc), eeprom_present ? "is" : "not");
+
+		if ((ahc->flags & AHC_INT50_SPEEDFLEX) != 0) {
+			/*
+			 * The 50 pin connector is a separate bus,
+			 * so force it to always be terminated.
+			 * In the future, perform current sensing
+			 * to determine if we are in the middle of
+			 * a properly terminated bus.
+			 */
+			internal50_present = 0;
+		}
+
+		/*
+		 * Now set the termination based on what
+		 * we found.
+		 * Flash Enable = BRDDAT7
+		 * Secondary High Term Enable = BRDDAT6
+		 * Secondary Low Term Enable = BRDDAT5 (7890)
+		 * Primary High Term Enable = BRDDAT4 (7890)
+		 */
+		if ((ahc->features & AHC_ULTRA2) == 0
+		 && (internal50_present != 0)
+		 && (internal68_present != 0)
+		 && (externalcable_present != 0)) {
+			printf("%s: Illegal cable configuration!!. "
+			       "Only two connectors on the "
+			       "adapter may be used at a "
+			       "time!\n", ahc_name(ahc));
+		}
+
+		if ((ahc->features & AHC_WIDE) != 0
+		 && ((externalcable_present == 0)
+		  || (internal68_present == 0)
+		  || (enableSEC_high != 0))) {
+			brddat |= BRDDAT6;
+			if (bootverbose) {
+				if ((ahc->flags & AHC_INT50_SPEEDFLEX) != 0)
+					printf("%s: 68 pin termination "
+					       "Enabled\n", ahc_name(ahc));
+				else
+					printf("%s: %sHigh byte termination "
+					       "Enabled\n", ahc_name(ahc),
+					       enableSEC_high ? "Secondary "
+							      : "");
+			}
+		}
+
+		sum = internal50_present + internal68_present
+		    + externalcable_present;
+		if (sum < 2 || (enableSEC_low != 0)) {
+			if ((ahc->features & AHC_ULTRA2) != 0)
+				brddat |= BRDDAT5;
+			else
+				*sxfrctl1 |= STPWEN;
+			if (bootverbose) {
+				if ((ahc->flags & AHC_INT50_SPEEDFLEX) != 0)
+					printf("%s: 50 pin termination "
+					       "Enabled\n", ahc_name(ahc));
+				else
+					printf("%s: %sLow byte termination "
+					       "Enabled\n", ahc_name(ahc),
+					       enableSEC_low ? "Secondary "
+							     : "");
+			}
+		}
+
+		if (enablePRI_low != 0) {
+			*sxfrctl1 |= STPWEN;
+			if (bootverbose)
+				printf("%s: Primary Low Byte termination "
+				       "Enabled\n", ahc_name(ahc));
+		}
+
+		/*
+		 * Setup STPWEN before setting up the rest of
+		 * the termination per the tech note on the U160 cards.
+		 */
+		ahc_outb(ahc, SXFRCTL1, *sxfrctl1);
+
+		if (enablePRI_high != 0) {
+			brddat |= BRDDAT4;
+			if (bootverbose)
+				printf("%s: Primary High Byte "
+				       "termination Enabled\n",
+				       ahc_name(ahc));
+		}
+		
+		write_brdctl(ahc, brddat);
+
+	} else {
+		if ((adapter_control & CFSTERM) != 0) {
+			*sxfrctl1 |= STPWEN;
+
+			if (bootverbose)
+				printf("%s: %sLow byte termination Enabled\n",
+				       ahc_name(ahc),
+				       (ahc->features & AHC_ULTRA2) ? "Primary "
+								    : "");
+		}
+
+		if ((adapter_control & CFWSTERM) != 0
+		 && (ahc->features & AHC_WIDE) != 0) {
+			brddat |= BRDDAT6;
+			if (bootverbose)
+				printf("%s: %sHigh byte termination Enabled\n",
+				       ahc_name(ahc),
+				       (ahc->features & AHC_ULTRA2)
+				     ? "Secondary " : "");
+		}
+
+		/*
+		 * Setup STPWEN before setting up the rest of
+		 * the termination per the tech note on the U160 cards.
+		 */
+		ahc_outb(ahc, SXFRCTL1, *sxfrctl1);
+
+		if ((ahc->features & AHC_WIDE) != 0)
+			write_brdctl(ahc, brddat);
+	}
+	SEEPROM_OUTB(sd, sd->sd_MS); /* Clear CS */
+}
+
+static void
+ahc_new_term_detect(struct ahc_softc *ahc, int *enableSEC_low,
+		    int *enableSEC_high, int *enablePRI_low,
+		    int *enablePRI_high, int *eeprom_present)
+{
+	uint8_t brdctl;
+
+	/*
+	 * BRDDAT7 = Eeprom
+	 * BRDDAT6 = Enable Secondary High Byte termination
+	 * BRDDAT5 = Enable Secondary Low Byte termination
+	 * BRDDAT4 = Enable Primary high byte termination
+	 * BRDDAT3 = Enable Primary low byte termination
+	 */
+	brdctl = read_brdctl(ahc);
+	*eeprom_present = brdctl & BRDDAT7;
+	*enableSEC_high = (brdctl & BRDDAT6);
+	*enableSEC_low = (brdctl & BRDDAT5);
+	*enablePRI_high = (brdctl & BRDDAT4);
+	*enablePRI_low = (brdctl & BRDDAT3);
+}
+
+static void
+aic787X_cable_detect(struct ahc_softc *ahc, int *internal50_present,
+		     int *internal68_present, int *externalcable_present,
+		     int *eeprom_present)
+{
+	uint8_t brdctl;
+
+	/*
+	 * First read the status of our cables.
+	 * Set the rom bank to 0 since the
+	 * bank setting serves as a multiplexor
+	 * for the cable detection logic.
+	 * BRDDAT5 controls the bank switch.
+	 */
+	write_brdctl(ahc, 0);
+
+	/*
+	 * Now read the state of the internal
+	 * connectors.  BRDDAT6 is INT50 and
+	 * BRDDAT7 is INT68.
+	 */
+	brdctl = read_brdctl(ahc);
+	*internal50_present = (brdctl & BRDDAT6) ? 0 : 1;
+	*internal68_present = (brdctl & BRDDAT7) ? 0 : 1;
+
+	/*
+	 * Set the rom bank to 1 and determine
+	 * the other signals.
+	 */
+	write_brdctl(ahc, BRDDAT5);
+
+	/*
+	 * Now read the state of the external
+	 * connectors.  BRDDAT6 is EXT68 and
+	 * BRDDAT7 is EPROMPS.
+	 */
+	brdctl = read_brdctl(ahc);
+	*externalcable_present = (brdctl & BRDDAT6) ? 0 : 1;
+	*eeprom_present = (brdctl & BRDDAT7) ? 1 : 0;
+}
+
+static void
+aic785X_cable_detect(struct ahc_softc *ahc, int *internal50_present,
+		     int *externalcable_present, int *eeprom_present)
+{
+	uint8_t brdctl;
+
+	ahc_outb(ahc, BRDCTL, BRDRW|BRDCS);
+	ahc_outb(ahc, BRDCTL, 0);
+	brdctl = ahc_inb(ahc, BRDCTL);
+	*internal50_present = (brdctl & BRDDAT5) ? 0 : 1;
+	*externalcable_present = (brdctl & BRDDAT6) ? 0 : 1;
+
+	*eeprom_present = (ahc_inb(ahc, SPIOCAP) & EEPROM) ? 1 : 0;
+}
+	
+static int
+acquire_seeprom(struct ahc_softc *ahc, struct seeprom_descriptor *sd)
+{
+	int wait;
+
+	if ((ahc->features & AHC_SPIOCAP) != 0
+	 && (ahc_inb(ahc, SPIOCAP) & SEEPROM) == 0)
+		return (0);
+
+	/*
+	 * Request access of the memory port.  When access is
+	 * granted, SEERDY will go high.  We use a 1 second
+	 * timeout which should be near 1 second more than
+	 * is needed.  Reason: after the chip reset, there
+	 * should be no contention.
+	 */
+	SEEPROM_OUTB(sd, sd->sd_MS);
+	wait = 1000;  /* 1 second timeout in msec */
+	while (--wait && ((SEEPROM_STATUS_INB(sd) & sd->sd_RDY) == 0)) {
+		ahc_delay(1000);  /* delay 1 msec */
+	}
+	if ((SEEPROM_STATUS_INB(sd) & sd->sd_RDY) == 0) {
+		SEEPROM_OUTB(sd, 0); 
+		return (0);
+	}
+	return(1);
+}
+
+static void
+release_seeprom(struct seeprom_descriptor *sd)
+{
+	/* Release access to the memory port and the serial EEPROM. */
+	SEEPROM_OUTB(sd, 0);
+}
+
+static void
+write_brdctl(struct ahc_softc *ahc, uint8_t value)
+{
+	uint8_t brdctl;
+
+	if ((ahc->chip & AHC_CHIPID_MASK) == AHC_AIC7895) {
+		brdctl = BRDSTB;
+	 	if (ahc->channel == 'B')
+			brdctl |= BRDCS;
+	} else if ((ahc->features & AHC_ULTRA2) != 0) {
+		brdctl = 0;
+	} else {
+		brdctl = BRDSTB|BRDCS;
+	}
+	ahc_outb(ahc, BRDCTL, brdctl);
+	ahc_flush_device_writes(ahc);
+	brdctl |= value;
+	ahc_outb(ahc, BRDCTL, brdctl);
+	ahc_flush_device_writes(ahc);
+	if ((ahc->features & AHC_ULTRA2) != 0)
+		brdctl |= BRDSTB_ULTRA2;
+	else
+		brdctl &= ~BRDSTB;
+	ahc_outb(ahc, BRDCTL, brdctl);
+	ahc_flush_device_writes(ahc);
+	if ((ahc->features & AHC_ULTRA2) != 0)
+		brdctl = 0;
+	else
+		brdctl &= ~BRDCS;
+	ahc_outb(ahc, BRDCTL, brdctl);
+}
+
+static uint8_t
+read_brdctl(ahc)
+	struct 	ahc_softc *ahc;
+{
+	uint8_t brdctl;
+	uint8_t value;
+
+	if ((ahc->chip & AHC_CHIPID_MASK) == AHC_AIC7895) {
+		brdctl = BRDRW;
+	 	if (ahc->channel == 'B')
+			brdctl |= BRDCS;
+	} else if ((ahc->features & AHC_ULTRA2) != 0) {
+		brdctl = BRDRW_ULTRA2;
+	} else {
+		brdctl = BRDRW|BRDCS;
+	}
+	ahc_outb(ahc, BRDCTL, brdctl);
+	ahc_flush_device_writes(ahc);
+	value = ahc_inb(ahc, BRDCTL);
+	ahc_outb(ahc, BRDCTL, 0);
+	return (value);
+}
+
+#define	DPE	0x80
+#define SSE	0x40
+#define	RMA	0x20
+#define	RTA	0x10
+#define STA	0x08
+#define DPR	0x01
+
+void
+ahc_pci_intr(struct ahc_softc *ahc)
+{
+	u_int error;
+	u_int status1;
+
+	error = ahc_inb(ahc, ERROR);
+	if ((error & PCIERRSTAT) == 0)
+		return;
+
+	status1 = ahc_pci_read_config(ahc->dev_softc,
+				      PCIR_STATUS + 1, /*bytes*/1);
+
+	printf("%s: PCI error Interrupt at seqaddr = 0x%x\n",
+	      ahc_name(ahc),
+	      ahc_inb(ahc, SEQADDR0) | (ahc_inb(ahc, SEQADDR1) << 8));
+
+	if (status1 & DPE) {
+		printf("%s: Data Parity Error Detected during address "
+		       "or write data phase\n", ahc_name(ahc));
+	}
+	if (status1 & SSE) {
+		printf("%s: Signal System Error Detected\n", ahc_name(ahc));
+	}
+	if (status1 & RMA) {
+		printf("%s: Received a Master Abort\n", ahc_name(ahc));
+	}
+	if (status1 & RTA) {
+		printf("%s: Received a Target Abort\n", ahc_name(ahc));
+	}
+	if (status1 & STA) {
+		printf("%s: Signaled a Target Abort\n", ahc_name(ahc));
+	}
+	if (status1 & DPR) {
+		printf("%s: Data Parity Error has been reported via PERR#\n",
+		       ahc_name(ahc));
+	}
+	if ((status1 & (DPE|SSE|RMA|RTA|STA|DPR)) == 0) {
+		printf("%s: Latched PCIERR interrupt with "
+		       "no status bits set\n", ahc_name(ahc)); 
+	}
+	ahc_pci_write_config(ahc->dev_softc, PCIR_STATUS + 1,
+			     status1, /*bytes*/1);
+
+	if (status1 & (DPR|RMA|RTA)) {
+		ahc_outb(ahc, CLRINT, CLRPARERR);
+	}
+
+	unpause_sequencer(ahc);
+}
+
+static int
+ahc_aic7850_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	probe_config->channel = 'A';
+	probe_config->chip = AHC_AIC7850;
+	probe_config->features = AHC_AIC7850_FE;
+	probe_config->bugs |= AHC_TMODE_WIDEODD_BUG|AHC_CACHETHEN_BUG
+			   |  AHC_PCI_MWI_BUG;
+	return (0);
+}
+
+static int
+ahc_aic7855_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	probe_config->channel = 'A';
+	probe_config->chip = AHC_AIC7855;
+	probe_config->features = AHC_AIC7855_FE;
+	probe_config->bugs |= AHC_TMODE_WIDEODD_BUG|AHC_CACHETHEN_BUG
+			   |  AHC_PCI_MWI_BUG;
+	return (0);
+}
+
+static int
+ahc_aic7860_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	uint8_t rev;
+
+	probe_config->channel = 'A';
+	probe_config->chip = AHC_AIC7860;
+	probe_config->features = AHC_AIC7860_FE;
+	probe_config->bugs |= AHC_TMODE_WIDEODD_BUG|AHC_CACHETHEN_BUG
+			   |  AHC_PCI_MWI_BUG;
+	rev = ahc_pci_read_config(pci, PCIR_REVID, /*bytes*/1);
+	if (rev >= 1)
+		probe_config->bugs |= AHC_PCI_2_1_RETRY_BUG;
+	return (0);
+}
+
+static int
+ahc_apa1480_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	int error;
+
+	error = ahc_aic7860_setup(pci, probe_config);
+	if (error != 0)
+		return (error);
+	probe_config->features |= AHC_REMOVABLE;
+	return (0);
+}
+
+static int
+ahc_aic7870_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	probe_config->channel = 'A';
+	probe_config->chip = AHC_AIC7870;
+	probe_config->features = AHC_AIC7870_FE;
+	probe_config->bugs |= AHC_TMODE_WIDEODD_BUG|AHC_CACHETHEN_BUG
+			   |  AHC_PCI_MWI_BUG;
+	return (0);
+}
+
+static int
+ahc_aha394X_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	int error;
+
+	error = ahc_aic7870_setup(pci, probe_config);
+	if (error == 0)
+		error = ahc_aha394XX_setup(pci, probe_config);
+	return (error);
+}
+
+static int
+ahc_aha398X_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	int error;
+
+	error = ahc_aic7870_setup(pci, probe_config);
+	if (error == 0)
+		error = ahc_aha398XX_setup(pci, probe_config);
+	return (error);
+}
+
+static int
+ahc_aha494X_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	int error;
+
+	error = ahc_aic7870_setup(pci, probe_config);
+	if (error == 0)
+		error = ahc_aha494XX_setup(pci, probe_config);
+	return (error);
+}
+
+static int
+ahc_aic7880_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	uint8_t rev;
+
+	probe_config->channel = 'A';
+	probe_config->chip = AHC_AIC7880;
+	probe_config->features = AHC_AIC7880_FE;
+	probe_config->bugs |= AHC_TMODE_WIDEODD_BUG;
+	rev = ahc_pci_read_config(pci, PCIR_REVID, /*bytes*/1);
+	if (rev >= 1) {
+		probe_config->bugs |= AHC_PCI_2_1_RETRY_BUG;
+	} else {
+		probe_config->bugs |= AHC_CACHETHEN_BUG|AHC_PCI_MWI_BUG;
+	}
+	return (0);
+}
+
+static int
+ahc_aha2940Pro_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	int error;
+
+	probe_config->flags |= AHC_INT50_SPEEDFLEX;
+	error = ahc_aic7880_setup(pci, probe_config);
+	return (0);
+}
+
+static int
+ahc_aha394XU_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	int error;
+
+	error = ahc_aic7880_setup(pci, probe_config);
+	if (error == 0)
+		error = ahc_aha394XX_setup(pci, probe_config);
+	return (error);
+}
+
+static int
+ahc_aha398XU_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	int error;
+
+	error = ahc_aic7880_setup(pci, probe_config);
+	if (error == 0)
+		error = ahc_aha398XX_setup(pci, probe_config);
+	return (error);
+}
+
+static int
+ahc_aic7890_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	uint8_t rev;
+
+	probe_config->channel = 'A';
+	probe_config->chip = AHC_AIC7890;
+	probe_config->features = AHC_AIC7890_FE;
+	probe_config->flags |= AHC_NEWEEPROM_FMT;
+	rev = ahc_pci_read_config(pci, PCIR_REVID, /*bytes*/1);
+	if (rev == 0)
+		probe_config->bugs |= AHC_AUTOFLUSH_BUG|AHC_CACHETHEN_BUG;
+	return (0);
+}
+
+static int
+ahc_aic7892_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	probe_config->channel = 'A';
+	probe_config->chip = AHC_AIC7892;
+	probe_config->features = AHC_AIC7892_FE;
+	probe_config->flags |= AHC_NEWEEPROM_FMT;
+	probe_config->bugs |= AHC_SCBCHAN_UPLOAD_BUG;
+	return (0);
+}
+
+static int
+ahc_aic7895_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	uint8_t rev;
+
+	probe_config->channel = ahc_get_pci_function(pci) == 1 ? 'B' : 'A';
+	/*
+	 * The 'C' revision of the aic7895 has a few additional features.
+	 */
+	rev = ahc_pci_read_config(pci, PCIR_REVID, /*bytes*/1);
+	if (rev >= 4) {
+		probe_config->chip = AHC_AIC7895C;
+		probe_config->features = AHC_AIC7895C_FE;
+	} else  {
+		u_int command;
+
+		probe_config->chip = AHC_AIC7895;
+		probe_config->features = AHC_AIC7895_FE;
+
+		/*
+		 * The BIOS disables the use of MWI transactions
+		 * since it does not have the MWI bug work around
+		 * we have.  Disabling MWI reduces performance, so
+		 * turn it on again.
+		 */
+		command = ahc_pci_read_config(pci, PCIR_COMMAND, /*bytes*/1);
+		command |= PCIM_CMD_MWRICEN;
+		ahc_pci_write_config(pci, PCIR_COMMAND, command, /*bytes*/1);
+		probe_config->bugs |= AHC_PCI_MWI_BUG;
+	}
+	/*
+	 * XXX Does CACHETHEN really not work???  What about PCI retry?
+	 * on C level chips.  Need to test, but for now, play it safe.
+	 */
+	probe_config->bugs |= AHC_TMODE_WIDEODD_BUG|AHC_PCI_2_1_RETRY_BUG
+			   |  AHC_CACHETHEN_BUG;
+
+#if 0
+	uint32_t devconfig;
+
+	/*
+	 * Cachesize must also be zero due to stray DAC
+	 * problem when sitting behind some bridges.
+	 */
+	ahc_pci_write_config(pci, CSIZE_LATTIME, 0, /*bytes*/1);
+	devconfig = ahc_pci_read_config(pci, DEVCONFIG, /*bytes*/1);
+	devconfig |= MRDCEN;
+	ahc_pci_write_config(pci, DEVCONFIG, devconfig, /*bytes*/1);
+#endif
+	probe_config->flags |= AHC_NEWEEPROM_FMT;
+	return (0);
+}
+
+static int
+ahc_aic7896_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	probe_config->channel = ahc_get_pci_function(pci) == 1 ? 'B' : 'A';
+	probe_config->chip = AHC_AIC7896;
+	probe_config->features = AHC_AIC7896_FE;
+	probe_config->flags |= AHC_NEWEEPROM_FMT;
+	probe_config->bugs |= AHC_CACHETHEN_DIS_BUG;
+	return (0);
+}
+
+static int
+ahc_aic7899_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	probe_config->channel = ahc_get_pci_function(pci) == 1 ? 'B' : 'A';
+	probe_config->chip = AHC_AIC7899;
+	probe_config->features = AHC_AIC7899_FE;
+	probe_config->flags |= AHC_NEWEEPROM_FMT;
+	probe_config->bugs |= AHC_SCBCHAN_UPLOAD_BUG;
+	return (0);
+}
+
+static int
+ahc_aha29160C_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	int error;
+
+	error = ahc_aic7899_setup(pci, probe_config);
+	if (error != 0)
+		return (error);
+	probe_config->features |= AHC_REMOVABLE;
+	return (0);
+}
+
+static int
+ahc_raid_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	printf("RAID functionality unsupported\n");
+	return (ENXIO);
+}
+
+static int
+ahc_aha394XX_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	switch (ahc_get_pci_slot(pci)) {
+	case AHC_394X_SLOT_CHANNEL_A:
+		probe_config->channel = 'A';
+		break;
+	case AHC_394X_SLOT_CHANNEL_B:
+		probe_config->channel = 'B';
+		break;
+	default:
+		printf("adapter at unexpected slot %d\n"
+		       "unable to map to a channel\n",
+		       ahc_get_pci_slot(pci));
+		probe_config->channel = 'A';
+	}
+	return (0);
+}
+
+static int
+ahc_aha398XX_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	switch (ahc_get_pci_slot(pci)) {
+	case AHC_398X_SLOT_CHANNEL_A:
+		probe_config->channel = 'A';
+		break;
+	case AHC_398X_SLOT_CHANNEL_B:
+		probe_config->channel = 'B';
+		break;
+	case AHC_398X_SLOT_CHANNEL_C:
+		probe_config->channel = 'C';
+		break;
+	default:
+		printf("adapter at unexpected slot %d\n"
+		       "unable to map to a channel\n",
+		       ahc_get_pci_slot(pci));
+		probe_config->channel = 'A';
+		break;
+	}
+	probe_config->flags |= AHC_LARGE_SEEPROM;
+	return (0);
+}
+
+static int
+ahc_aha494XX_setup(ahc_dev_softc_t pci, struct ahc_probe_config *probe_config)
+{
+	switch (ahc_get_pci_slot(pci)) {
+	case AHC_494X_SLOT_CHANNEL_A:
+		probe_config->channel = 'A';
+		break;
+	case AHC_494X_SLOT_CHANNEL_B:
+		probe_config->channel = 'B';
+		break;
+	case AHC_494X_SLOT_CHANNEL_C:
+		probe_config->channel = 'C';
+		break;
+	case AHC_494X_SLOT_CHANNEL_D:
+		probe_config->channel = 'D';
+		break;
+	default:
+		printf("adapter at unexpected slot %d\n"
+		       "unable to map to a channel\n",
+		       ahc_get_pci_slot(pci));
+		probe_config->channel = 'A';
+	}
+	probe_config->flags |= AHC_LARGE_SEEPROM;
+	return (0);
+}
diff -urN linux/drivers/scsi/aic7xxx/aic7xxx_proc.c /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_proc.c
--- linux/drivers/scsi/aic7xxx/aic7xxx_proc.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_proc.c	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,293 @@
+/*
+ * Copyright (c) 2000, 2001 Adaptec Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * String handling code courtesy of Gerard Roudier's <groudier@club-internet.fr>
+ * sym driver.
+ *
+ * $Id: //depot/src/linux/drivers/scsi/aic7xxx/aic7xxx_proc.c#6 $
+ */
+#include "aic7xxx_osm.h"
+#include "aic7xxx_inline.h"
+
+struct info_str {
+	char *buffer;
+	int length;
+	off_t offset;
+	int pos;
+};
+
+static void	copy_mem_info(struct info_str *info, char *data, int len);
+static int	copy_info(struct info_str *info, char *fmt, ...);
+static u_int	scsi_calc_syncsrate(u_int period_factor);
+static void	ahc_format_transinfo(struct info_str *info,
+				     struct ahc_transinfo *tinfo);
+static void	ahc_dump_target_state(struct ahc_softc *ahc,
+				      struct info_str *info,
+				      u_int our_id, char channel,
+				      u_int target_id, u_int target_offset);
+static void	ahc_dump_device_state(struct info_str *info,
+				      struct ahc_linux_device *dev);
+
+static void
+copy_mem_info(struct info_str *info, char *data, int len)
+{
+	if (info->pos + len > info->offset + info->length)
+		len = info->offset + info->length - info->pos;
+
+	if (info->pos + len < info->offset) {
+		info->pos += len;
+		return;
+	}
+
+	if (info->pos < info->offset) {
+		off_t partial;
+
+		partial = info->offset - info->pos;
+		data += partial;
+		info->pos += partial;
+		len  -= partial;
+	}
+
+	if (len > 0) {
+		memcpy(info->buffer, data, len);
+		info->pos += len;
+		info->buffer += len;
+	}
+}
+
+static int
+copy_info(struct info_str *info, char *fmt, ...)
+{
+	va_list args;
+	char buf[256];
+	int len;
+
+	va_start(args, fmt);
+	len = vsprintf(buf, fmt, args);
+	va_end(args);
+
+	copy_mem_info(info, buf, len);
+	return (len);
+}
+
+/*
+ * Table of syncrates that don't follow the "divisible by 4"
+ * rule. This table will be expanded in future SCSI specs.
+ */
+static struct {
+	u_int period_factor;
+	u_int period;	/* in 10ths of ns */
+} scsi_syncrates[] = {
+	{ 0x09, 125 },	/* FAST-80 */
+	{ 0x0a, 250 },	/* FAST-40 40MHz */
+	{ 0x0b, 303 },	/* FAST-40 33MHz */
+	{ 0x0c, 500 }	/* FAST-20 */
+};
+ 
+/*
+ * Return the frequency in kHz corresponding to the given
+ * sync period factor.
+ */
+static u_int
+scsi_calc_syncsrate(u_int period_factor)
+{
+	int i; 
+	int num_syncrates;
+ 
+	num_syncrates = sizeof(scsi_syncrates) / sizeof(scsi_syncrates[0]);
+	/* See if the period is in the "exception" table */
+	for (i = 0; i < num_syncrates; i++) {
+
+		if (period_factor == scsi_syncrates[i].period_factor) {
+       			/* Period in kHz */
+			return (10000000 / scsi_syncrates[i].period);
+		}
+	}
+
+	/*
+	 * Wasn't in the table, so use the standard
+	 * 4 times conversion.
+	 */
+	return (10000000 / (period_factor * 4 * 10));
+}
+
+static void
+ahc_format_transinfo(struct info_str *info, struct ahc_transinfo *tinfo)
+{
+	u_int speed;
+	u_int freq;
+	u_int mb;
+
+        speed = 3300;
+        freq = 0;
+	if (tinfo->offset != 0) {
+		freq = scsi_calc_syncsrate(tinfo->period);
+		speed = freq;
+	}
+	speed *= (0x01 << tinfo->width);
+        mb = speed / 1000;
+        if (mb > 0)
+		copy_info(info, "%d.%03dMB/s transfers", mb, speed % 1000);
+        else
+		copy_info(info, "%dKB/s transfers", speed);
+
+	if (freq != 0) {
+		copy_info(info, " (%d.%03dMHz%s, offset %d",
+			 freq / 1000, freq % 1000,
+			 (tinfo->ppr_options & MSG_EXT_PPR_DT_REQ) != 0
+			 ? " DT" : "", tinfo->offset);
+	}
+
+	if (tinfo->width > 0) {
+		if (freq != 0) {
+			copy_info(info, ", ");
+		} else {
+			copy_info(info, " (");
+		}
+		copy_info(info, "%dbit)", 8 * (0x01 << tinfo->width));
+	} else if (freq != 0) {
+		copy_info(info, ")");
+	}
+	copy_info(info, "\n");
+}
+
+static void
+ahc_dump_target_state(struct ahc_softc *ahc, struct info_str *info,
+		      u_int our_id, char channel, u_int target_id,
+		      u_int target_offset)
+{
+	struct	ahc_linux_target *targ;
+	struct	ahc_initiator_tinfo *tinfo;
+	struct	tmode_tstate *tstate;
+	int	lun;
+
+	tinfo = ahc_fetch_transinfo(ahc, channel, our_id,
+				    target_id, &tstate);
+	copy_info(info, "Channel %c Target %d Negotiation Settings\n",
+		  channel, target_id);
+	copy_info(info, "\tUser: ");
+	ahc_format_transinfo(info, &tinfo->user);
+	targ = ahc->platform_data->targets[target_offset];
+	if (targ == NULL)
+		return;
+
+	copy_info(info, "\tGoal: ");
+	ahc_format_transinfo(info, &tinfo->goal);
+	copy_info(info, "\tCurr: ");
+	ahc_format_transinfo(info, &tinfo->current);
+
+	for (lun = 0; lun < AHC_NUM_LUNS; lun++) {
+		struct ahc_linux_device *dev;
+
+		dev = targ->devices[lun];
+
+		if (dev == NULL)
+			continue;
+
+		ahc_dump_device_state(info, dev);
+	}
+}
+
+static void
+ahc_dump_device_state(struct info_str *info, struct ahc_linux_device *dev)
+{
+	copy_info(info, "\tChannel %c Target %d Lun %d Settings\n",
+		  dev->target->channel + 'A', dev->target->target, dev->lun);
+
+	copy_info(info, "\t\tCommands Queued %d\n", dev->num_commands);
+	copy_info(info, "\t\tCommands Active %d\n", dev->active);
+	copy_info(info, "\t\tCommand Openings %d\n", dev->openings);
+	copy_info(info, "\t\tMax Tagged Openings %d\n", dev->maxtags);
+	copy_info(info, "\t\tDevice Queue Frozen Count %d\n", dev->qfrozen);
+}
+
+/*+F*************************************************************************
+ * Function:
+ *   aic7xxx_proc_info
+ *
+ * Description:
+ *   Return information to handle /proc support for the driver.
+ *-F*************************************************************************/
+int
+aic7xxx_proc_info(char *buffer, char **start, off_t offset,
+		  int length, int hostno, int inout)
+{
+	struct	ahc_softc *ahc;
+	struct	info_str info;
+	char	ahc_info[256];
+	u_int	max_targ;
+	u_int	i;
+
+	TAILQ_FOREACH(ahc, &ahc_tailq, links) {
+		if (ahc->platform_data->host_no == hostno)
+			break;
+	}
+
+	if (ahc == NULL)
+		return (-EINVAL);
+
+	 /* Has data been written to the file? */ 
+	if (inout == TRUE)
+		return (-ENOSYS);
+
+	if (start)
+		*start = buffer;
+
+	info.buffer	= buffer;
+	info.length	= length;
+	info.offset	= offset;
+	info.pos	= 0;
+
+	copy_info(&info, "Adaptec AIC7xxx driver version: %s\n",
+		  AIC7XXX_DRIVER_VERSION);
+	ahc_controller_info(ahc, ahc_info);
+	copy_info(&info, "%s\n", ahc_info);
+
+	max_targ = 15;
+	if ((ahc->features & (AHC_WIDE|AHC_TWIN)) == 0)
+		max_targ = 7;
+
+	for (i = 0; i <= max_targ; i++) {
+		u_int our_id;
+		u_int target_id;
+		char channel;
+
+		channel = 'A';
+		our_id = ahc->our_id;
+		target_id = i;
+		if (i > 7 && (ahc->features & AHC_TWIN) != 0) {
+			channel = 'B';
+			our_id = ahc->our_id_b;
+			target_id = i % 8;
+		}
+
+		ahc_dump_target_state(ahc, &info, our_id,
+				      channel, target_id, i);
+	}
+	return (info.pos > info.offset ? info.pos - info.offset : 0);
+}
diff -urN linux/drivers/scsi/aic7xxx/aic7xxx_reg.h /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_reg.h
--- linux/drivers/scsi/aic7xxx/aic7xxx_reg.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_reg.h	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,701 @@
+/*
+  * DO NOT EDIT - This file is automatically generated.
+  */
+
+#define	SCSISEQ         		0x00
+#define		TEMODE          	0x80
+#define		SCSIRSTO        	0x01
+
+#define	SXFRCTL0        		0x01
+#define		DFON            	0x80
+#define		DFPEXP          	0x40
+#define		FAST20          	0x20
+#define		CLRSTCNT        	0x10
+#define		SPIOEN          	0x08
+#define		SCAMEN          	0x04
+#define		CLRCHN          	0x02
+
+#define	SXFRCTL1        		0x02
+#define		BITBUCKET       	0x80
+#define		SWRAPEN         	0x40
+#define		STIMESEL        	0x18
+#define		ENSTIMER        	0x04
+#define		ACTNEGEN        	0x02
+#define		STPWEN          	0x01
+
+#define	SCSISIGO        		0x03
+#define		CDO             	0x80
+#define		IOO             	0x40
+#define		MSGO            	0x20
+#define		ATNO            	0x10
+#define		SELO            	0x08
+#define		BSYO            	0x04
+#define		REQO            	0x02
+#define		ACKO            	0x01
+
+#define	SCSISIGI        		0x03
+#define		P_DATAIN_DT     	0x60
+#define		P_DATAOUT_DT    	0x20
+#define		ATNI            	0x10
+#define		SELI            	0x08
+#define		BSYI            	0x04
+#define		REQI            	0x02
+#define		ACKI            	0x01
+
+#define	SCSIRATE        		0x04
+#define		WIDEXFER        	0x80
+#define		SXFR            	0x70
+#define		ENABLE_CRC      	0x40
+#define		SINGLE_EDGE     	0x10
+#define		SOFS            	0x0f
+#define		SXFR_ULTRA2     	0x0f
+
+#define	SCSIID          		0x05
+#define	SCSIOFFSET      		0x05
+#define		SOFS_ULTRA2     	0x7f
+
+#define	SCSIDATL        		0x06
+
+#define	SCSIDATH        		0x07
+
+#define	STCNT           		0x08
+
+#define	OPTIONMODE      		0x08
+#define		AUTORATEEN      	0x80
+#define		AUTOACKEN       	0x40
+#define		ATNMGMNTEN      	0x20
+#define		BUSFREEREV      	0x10
+#define		EXPPHASEDIS     	0x08
+#define		SCSIDATL_IMGEN  	0x04
+#define		OPTIONMODE_DEFAULTS	0x03
+#define		AUTO_MSGOUT_DE  	0x02
+#define		DIS_MSGIN_DUALEDGE	0x01
+
+#define	TARGCRCCNT      		0x0a
+
+#define	CLRSINT0        		0x0b
+#define		CLRSELDO        	0x40
+#define		CLRSELDI        	0x20
+#define		CLRSELINGO      	0x10
+#define		CLRIOERR        	0x08
+#define		CLRSWRAP        	0x08
+#define		CLRSPIORDY      	0x02
+
+#define	SSTAT0          		0x0b
+#define		TARGET          	0x80
+#define		SELDO           	0x40
+#define		SELDI           	0x20
+#define		SELINGO         	0x10
+#define		IOERR           	0x08
+#define		SWRAP           	0x08
+#define		SDONE           	0x04
+#define		SPIORDY         	0x02
+#define		DMADONE         	0x01
+
+#define	CLRSINT1        		0x0c
+#define		CLRSELTIMEO     	0x80
+#define		CLRATNO         	0x40
+#define		CLRSCSIRSTI     	0x20
+#define		CLRBUSFREE      	0x08
+#define		CLRSCSIPERR     	0x04
+#define		CLRPHASECHG     	0x02
+#define		CLRREQINIT      	0x01
+
+#define	SSTAT1          		0x0c
+#define		SELTO           	0x80
+#define		ATNTARG         	0x40
+#define		SCSIRSTI        	0x20
+#define		PHASEMIS        	0x10
+#define		BUSFREE         	0x08
+#define		SCSIPERR        	0x04
+#define		PHASECHG        	0x02
+#define		REQINIT         	0x01
+
+#define	SSTAT2          		0x0d
+#define		OVERRUN         	0x80
+#define		SHVALID         	0x40
+#define		SFCNT           	0x1f
+#define		EXP_ACTIVE      	0x10
+#define		CRCVALERR       	0x08
+#define		CRCENDERR       	0x04
+#define		CRCREQERR       	0x02
+#define		DUAL_EDGE_ERR   	0x01
+
+#define	SSTAT3          		0x0e
+#define		SCSICNT         	0xf0
+#define		OFFCNT          	0x0f
+
+#define	SCSIID_ULTRA2   		0x0f
+
+#define	SIMODE0         		0x10
+#define		ENSELDO         	0x40
+#define		ENSELDI         	0x20
+#define		ENSELINGO       	0x10
+#define		ENIOERR         	0x08
+#define		ENSWRAP         	0x08
+#define		ENSDONE         	0x04
+#define		ENSPIORDY       	0x02
+#define		ENDMADONE       	0x01
+
+#define	SIMODE1         		0x11
+#define		ENSELTIMO       	0x80
+#define		ENATNTARG       	0x40
+#define		ENSCSIRST       	0x20
+#define		ENPHASEMIS      	0x10
+#define		ENBUSFREE       	0x08
+#define		ENSCSIPERR      	0x04
+#define		ENPHASECHG      	0x02
+#define		ENREQINIT       	0x01
+
+#define	SCSIBUSL        		0x12
+
+#define	SCSIBUSH        		0x13
+
+#define	SHADDR          		0x14
+
+#define	SELTIMER        		0x18
+#define	TARGIDIN        		0x18
+#define		STAGE6          	0x20
+#define		STAGE5          	0x10
+#define		STAGE4          	0x08
+#define		STAGE3          	0x04
+#define		STAGE2          	0x02
+#define		STAGE1          	0x01
+
+#define	SELID           		0x19
+#define		SELID_MASK      	0xf0
+#define		ONEBIT          	0x08
+
+#define	SCAMCTL         		0x1a
+#define		ENSCAMSELO      	0x80
+#define		CLRSCAMSELID    	0x40
+#define		ALTSTIM         	0x20
+#define		DFLTTID         	0x10
+#define		SCAMLVL         	0x03
+
+#define	TARGID          		0x1b
+
+#define	SPIOCAP         		0x1b
+#define		SOFT1           	0x80
+#define		SOFT0           	0x40
+#define		SOFTCMDEN       	0x20
+#define		HAS_BRDCTL      	0x10
+#define		SEEPROM         	0x08
+#define		EEPROM          	0x04
+#define		ROM             	0x02
+#define		SSPIOCPS        	0x01
+
+#define	BRDCTL          		0x1d
+#define		BRDDAT7         	0x80
+#define		BRDDAT6         	0x40
+#define		BRDDAT5         	0x20
+#define		BRDDAT4         	0x10
+#define		BRDSTB          	0x10
+#define		BRDCS           	0x08
+#define		BRDDAT3         	0x08
+#define		BRDDAT2         	0x04
+#define		BRDRW           	0x04
+#define		BRDRW_ULTRA2    	0x02
+#define		BRDCTL1         	0x02
+#define		BRDCTL0         	0x01
+#define		BRDSTB_ULTRA2   	0x01
+
+#define	SEECTL          		0x1e
+#define		EXTARBACK       	0x80
+#define		EXTARBREQ       	0x40
+#define		SEEMS           	0x20
+#define		SEERDY          	0x10
+#define		SEECS           	0x08
+#define		SEECK           	0x04
+#define		SEEDO           	0x02
+#define		SEEDI           	0x01
+
+#define	SBLKCTL         		0x1f
+#define		DIAGLEDEN       	0x80
+#define		DIAGLEDON       	0x40
+#define		AUTOFLUSHDIS    	0x20
+#define		ENAB40          	0x08
+#define		SELBUSB         	0x08
+#define		ENAB20          	0x04
+#define		SELWIDE         	0x02
+#define		XCVR            	0x01
+
+#define	BUSY_TARGETS    		0x20
+#define	TARG_SCSIRATE   		0x20
+
+#define	SRAM_BASE       		0x20
+
+#define	ULTRA_ENB       		0x30
+#define	CMDSIZE_TABLE   		0x30
+
+#define	DISC_DSB        		0x32
+
+#define	CMDSIZE_TABLE_TAIL		0x34
+
+#define	MWI_RESIDUAL    		0x38
+
+#define	NEXT_QUEUED_SCB 		0x39
+
+#define	MSG_OUT         		0x3a
+
+#define	DMAPARAMS       		0x3b
+#define		PRELOADEN       	0x80
+#define		WIDEODD         	0x40
+#define		SCSIEN          	0x20
+#define		SDMAENACK       	0x10
+#define		SDMAEN          	0x10
+#define		HDMAEN          	0x08
+#define		HDMAENACK       	0x08
+#define		DIRECTION       	0x04
+#define		FIFOFLUSH       	0x02
+#define		FIFORESET       	0x01
+
+#define	SEQ_FLAGS       		0x3c
+#define		IDENTIFY_SEEN   	0x80
+#define		TARGET_CMD_IS_TAGGED	0x40
+#define		DPHASE          	0x20
+#define		TARG_CMD_PENDING	0x10
+#define		CMDPHASE_PENDING	0x08
+#define		DPHASE_PENDING  	0x04
+#define		SPHASE_PENDING  	0x02
+#define		NO_DISCONNECT   	0x01
+
+#define	SAVED_SCSIID    		0x3d
+
+#define	SAVED_LUN       		0x3e
+
+#define	LASTPHASE       		0x3f
+#define		P_MESGIN        	0xe0
+#define		PHASE_MASK      	0xe0
+#define		P_STATUS        	0xc0
+#define		P_MESGOUT       	0xa0
+#define		P_COMMAND       	0x80
+#define		CDI             	0x80
+#define		IOI             	0x40
+#define		P_DATAIN        	0x40
+#define		MSGI            	0x20
+#define		P_BUSFREE       	0x01
+#define		P_DATAOUT       	0x00
+
+#define	WAITING_SCBH    		0x40
+
+#define	DISCONNECTED_SCBH		0x41
+
+#define	FREE_SCBH       		0x42
+
+#define	HSCB_ADDR       		0x43
+
+#define	SHARED_DATA_ADDR		0x47
+
+#define	KERNEL_QINPOS   		0x4b
+
+#define	QINPOS          		0x4c
+
+#define	QOUTPOS         		0x4d
+
+#define	KERNEL_TQINPOS  		0x4e
+
+#define	TQINPOS         		0x4f
+
+#define	ARG_1           		0x50
+#define	RETURN_1        		0x50
+#define		SEND_MSG        	0x80
+#define		SEND_SENSE      	0x40
+#define		SEND_REJ        	0x20
+#define		MSGOUT_PHASEMIS 	0x10
+#define		EXIT_MSG_LOOP   	0x08
+#define		CONT_MSG_LOOP   	0x04
+#define		CONT_TARG_SESSION	0x02
+
+#define	ARG_2           		0x51
+#define	RETURN_2        		0x51
+
+#define	LAST_MSG        		0x52
+
+#define	TARGET_MSG_REQUEST		0x53
+
+#define	SCSISEQ_TEMPLATE		0x55
+#define		ENSELO          	0x40
+#define		ENSELI          	0x20
+#define		ENRSELI         	0x10
+#define		ENAUTOATNO      	0x08
+#define		ENAUTOATNI      	0x04
+#define		ENAUTOATNP      	0x02
+
+#define	DATA_COUNT_ODD  		0x56
+
+#define	INITIATOR_TAG   		0x57
+
+#define	SCSICONF        		0x5a
+#define		TERM_ENB        	0x80
+#define		RESET_SCSI      	0x40
+#define		ENSPCHK         	0x20
+#define		HWSCSIID        	0x0f
+#define		HSCSIID         	0x07
+
+#define	INTDEF          		0x5c
+#define		EDGE_TRIG       	0x80
+#define		VECTOR          	0x0f
+
+#define	HOSTCONF        		0x5d
+
+#define	HA_274_BIOSCTRL 		0x5f
+#define		BIOSMODE        	0x30
+#define		BIOSDISABLED    	0x30
+#define		CHANNEL_B_PRIMARY	0x08
+
+#define	SEQCTL          		0x60
+#define		PERRORDIS       	0x80
+#define		PAUSEDIS        	0x40
+#define		FAILDIS         	0x20
+#define		FASTMODE        	0x10
+#define		BRKADRINTEN     	0x08
+#define		STEP            	0x04
+#define		SEQRESET        	0x02
+#define		LOADRAM         	0x01
+
+#define	SEQRAM          		0x61
+
+#define	SEQADDR0        		0x62
+
+#define	SEQADDR1        		0x63
+#define		SEQADDR1_MASK   	0x01
+
+#define	ACCUM           		0x64
+
+#define	SINDEX          		0x65
+
+#define	DINDEX          		0x66
+
+#define	ALLONES         		0x69
+
+#define	ALLZEROS        		0x6a
+
+#define	NONE            		0x6a
+
+#define	FLAGS           		0x6b
+#define		ZERO            	0x02
+#define		CARRY           	0x01
+
+#define	SINDIR          		0x6c
+
+#define	DINDIR          		0x6d
+
+#define	FUNCTION1       		0x6e
+
+#define	STACK           		0x6f
+
+#define	TARG_OFFSET     		0x70
+
+#define	BCTL            		0x84
+#define		ACE             	0x08
+#define		ENABLE          	0x01
+
+#define	DSCOMMAND0      		0x84
+#define		CACHETHEN       	0x80
+#define		DPARCKEN        	0x40
+#define		MPARCKEN        	0x20
+#define		EXTREQLCK       	0x10
+#define		INTSCBRAMSEL    	0x08
+#define		RAMPS           	0x04
+#define		USCBSIZE32      	0x02
+#define		CIOPARCKEN      	0x01
+
+#define	BUSTIME         		0x85
+#define		BOFF            	0xf0
+#define		BON             	0x0f
+
+#define	BUSSPD          		0x86
+#define		DFTHRSH         	0xc0
+#define		DFTHRSH_75      	0x80
+#define		STBOFF          	0x38
+#define		STBON           	0x07
+
+#define	HS_MAILBOX      		0x86
+#define		HOST_MAILBOX    	0xf0
+#define		HOST_TQINPOS    	0x80
+#define		SEQ_MAILBOX     	0x0f
+
+#define	DSPCISTATUS     		0x86
+#define		DFTHRSH_100     	0xc0
+
+#define	HCNTRL          		0x87
+#define		POWRDN          	0x40
+#define		SWINT           	0x10
+#define		IRQMS           	0x08
+#define		PAUSE           	0x04
+#define		INTEN           	0x02
+#define		CHIPRST         	0x01
+#define		CHIPRSTACK      	0x01
+
+#define	HADDR           		0x88
+
+#define	HCNT            		0x8c
+
+#define	SCBPTR          		0x90
+
+#define	INTSTAT         		0x91
+#define		SEQINT_MASK     	0xf1
+#define		OUT_OF_RANGE    	0xe1
+#define		NO_FREE_SCB     	0xd1
+#define		SCB_MISMATCH    	0xc1
+#define		MISSED_BUSFREE  	0xb1
+#define		MKMSG_FAILED    	0xa1
+#define		DATA_OVERRUN    	0x91
+#define		PERR_DETECTED   	0x81
+#define		BAD_STATUS      	0x71
+#define		HOST_MSG_LOOP   	0x61
+#define		RESIDUAL        	0x51
+#define		IGN_WIDE_RES    	0x41
+#define		NO_MATCH        	0x31
+#define		NO_IDENT        	0x21
+#define		SEND_REJECT     	0x11
+#define		INT_PEND        	0x0f
+#define		BRKADRINT       	0x08
+#define		SCSIINT         	0x04
+#define		CMDCMPLT        	0x02
+#define		BAD_PHASE       	0x01
+#define		SEQINT          	0x01
+
+#define	CLRINT          		0x92
+#define		CLRPARERR       	0x10
+#define		CLRBRKADRINT    	0x08
+#define		CLRSCSIINT      	0x04
+#define		CLRCMDINT       	0x02
+#define		CLRSEQINT       	0x01
+
+#define	ERROR           		0x92
+#define		CIOPARERR       	0x80
+#define		PCIERRSTAT      	0x40
+#define		MPARERR         	0x20
+#define		DPARERR         	0x10
+#define		SQPARERR        	0x08
+#define		ILLOPCODE       	0x04
+#define		ILLSADDR        	0x02
+#define		ILLHADDR        	0x01
+
+#define	DFCNTRL         		0x93
+
+#define	DFSTATUS        		0x94
+#define		PRELOAD_AVAIL   	0x80
+#define		DWORDEMP        	0x20
+#define		MREQPEND        	0x10
+#define		HDONE           	0x08
+#define		DFTHRESH        	0x04
+#define		FIFOFULL        	0x02
+#define		FIFOEMP         	0x01
+
+#define	DFWADDR         		0x95
+
+#define	DFRADDR         		0x97
+
+#define	DFDAT           		0x99
+
+#define	SCBCNT          		0x9a
+#define		SCBAUTO         	0x80
+#define		SCBCNT_MASK     	0x1f
+
+#define	QINFIFO         		0x9b
+
+#define	QINCNT          		0x9c
+
+#define	QOUTFIFO        		0x9d
+
+#define	CRCCONTROL1     		0x9d
+#define		CRCONSEEN       	0x80
+#define		CRCVALCHKEN     	0x40
+#define		CRCENDCHKEN     	0x20
+#define		CRCREQCHKEN     	0x10
+#define		TARGCRCENDEN    	0x08
+#define		TARGCRCCNTEN    	0x04
+
+#define	QOUTCNT         		0x9e
+
+#define	SCSIPHASE       		0x9e
+#define		STATUS_PHASE    	0x20
+#define		COMMAND_PHASE   	0x10
+#define		MSG_IN_PHASE    	0x08
+#define		MSG_OUT_PHASE   	0x04
+#define		DATA_IN_PHASE   	0x02
+#define		DATA_OUT_PHASE  	0x01
+
+#define	SFUNCT          		0x9f
+#define		ALT_MODE        	0x80
+
+#define	SCB_CDB_PTR     		0xa0
+#define	SCB_CDB_STORE   		0xa0
+#define	SCB_TARGET_INFO 		0xa0
+#define	SCB_RESIDUAL_DATACNT		0xa0
+
+#define	SCB_BASE        		0xa0
+
+#define	SCB_RESIDUAL_SGPTR		0xa4
+
+#define	SCB_SCSI_STATUS 		0xa8
+
+#define	SCB_CDB_STORE_PAD		0xa9
+
+#define	SCB_DATAPTR     		0xac
+
+#define	SCB_DATACNT     		0xb0
+#define		SG_LAST_SEG     	0x80
+#define		SG_HIGH_ADDR_BITS	0x7f
+
+#define	SCB_SGPTR       		0xb4
+#define		SG_RESID_VALID  	0x04
+#define		SG_FULL_RESID   	0x02
+#define		SG_LIST_NULL    	0x01
+
+#define	SCB_CONTROL     		0xb8
+#define		TARGET_SCB      	0x80
+#define		DISCENB         	0x40
+#define		TAG_ENB         	0x20
+#define		MK_MESSAGE      	0x10
+#define		ULTRAENB        	0x08
+#define		DISCONNECTED    	0x04
+#define		SCB_TAG_TYPE    	0x03
+
+#define	SCB_SCSIID      		0xb9
+#define		TID             	0xf0
+#define		TWIN_CHNLB      	0x80
+#define		TWIN_TID        	0x70
+#define		OID             	0x0f
+
+#define	SCB_LUN         		0xba
+#define		LID             	0xff
+
+#define	SCB_TAG         		0xbb
+
+#define	SCB_CDB_LEN     		0xbc
+
+#define	SCB_SCSIRATE    		0xbd
+
+#define	SCB_SCSIOFFSET  		0xbe
+
+#define	SCB_NEXT        		0xbf
+
+#define	SEECTL_2840     		0xc0
+#define		CS_2840         	0x04
+#define		CK_2840         	0x02
+#define		DO_2840         	0x01
+
+#define	SCB_64_SPARE    		0xc0
+
+#define	STATUS_2840     		0xc1
+#define		EEPROM_TF       	0x80
+#define		BIOS_SEL        	0x60
+#define		ADSEL           	0x1e
+#define		DI_2840         	0x01
+
+#define	SCB_64_BTT      		0xd0
+
+#define	CCHADDR         		0xe0
+
+#define	CCHCNT          		0xe8
+
+#define	CCSGRAM         		0xe9
+
+#define	CCSGADDR        		0xea
+
+#define	CCSGCTL         		0xeb
+#define		CCSGDONE        	0x80
+#define		CCSGEN          	0x08
+#define		SG_FETCH_NEEDED 	0x02
+#define		CCSGRESET       	0x01
+
+#define	CCSCBRAM        		0xec
+
+#define	CCSCBADDR       		0xed
+
+#define	CCSCBCTL        		0xee
+#define		CCSCBDONE       	0x80
+#define		ARRDONE         	0x40
+#define		CCARREN         	0x10
+#define		CCSCBEN         	0x08
+#define		CCSCBDIR        	0x04
+#define		CCSCBRESET      	0x01
+
+#define	CCSCBCNT        		0xef
+
+#define	SCBBADDR        		0xf0
+
+#define	CCSCBPTR        		0xf1
+
+#define	HNSCB_QOFF      		0xf4
+
+#define	SNSCB_QOFF      		0xf6
+
+#define	SDSCB_QOFF      		0xf8
+
+#define	QOFF_CTLSTA     		0xfa
+#define		SCB_AVAIL       	0x40
+#define		SNSCB_ROLLOVER  	0x20
+#define		SDSCB_ROLLOVER  	0x10
+#define		SCB_QSIZE       	0x07
+#define		SCB_QSIZE_256   	0x06
+
+#define	DFF_THRSH       		0xfb
+#define		WR_DFTHRSH      	0x70
+#define		WR_DFTHRSH_MAX  	0x70
+#define		WR_DFTHRSH_90   	0x60
+#define		WR_DFTHRSH_85   	0x50
+#define		WR_DFTHRSH_75   	0x40
+#define		WR_DFTHRSH_63   	0x30
+#define		WR_DFTHRSH_50   	0x20
+#define		WR_DFTHRSH_25   	0x10
+#define		RD_DFTHRSH_MAX  	0x07
+#define		RD_DFTHRSH      	0x07
+#define		RD_DFTHRSH_90   	0x06
+#define		RD_DFTHRSH_85   	0x05
+#define		RD_DFTHRSH_75   	0x04
+#define		RD_DFTHRSH_63   	0x03
+#define		RD_DFTHRSH_50   	0x02
+#define		RD_DFTHRSH_25   	0x01
+#define		RD_DFTHRSH_MIN  	0x00
+#define		WR_DFTHRSH_MIN  	0x00
+
+#define	SG_CACHE_SHADOW 		0xfc
+#define		SG_ADDR_MASK    	0xf8
+#define		ODD_SEG         	0x04
+#define		LAST_SEG        	0x02
+#define		LAST_SEG_DONE   	0x01
+
+#define	SG_CACHE_PRE    		0xfc
+
+
+#define	CMD_GROUP_CODE_SHIFT	0x05
+#define	BUS_8_BIT	0x00
+#define	CCSGRAM_MAXSEGS	0x10
+#define	TARGET_DATA_IN	0x01
+#define	STATUS_QUEUE_FULL	0x28
+#define	STATUS_BUSY	0x08
+#define	MAX_OFFSET_8BIT	0x0f
+#define	BUS_16_BIT	0x01
+#define	TID_SHIFT	0x04
+#define	SCB_DOWNLOAD_SIZE_64	0x30
+#define	SCB_UPLOAD_SIZE	0x20
+#define	HOST_MAILBOX_SHIFT	0x04
+#define	SCB_INITIATOR_TAG	0x03
+#define	SCB_TARGET_STATUS	0x02
+#define	SCB_TARGET_DATA_DIR	0x01
+#define	SCB_TARGET_PHASES	0x00
+#define	MAX_OFFSET_ULTRA2	0x7f
+#define	MAX_OFFSET_16BIT	0x08
+#define	TARGET_CMD_CMPLT	0xfe
+#define	SCB_LIST_NULL	0xff
+#define	SG_SIZEOF	0x08
+#define	SCB_DOWNLOAD_SIZE	0x20
+#define	SEQ_MAILBOX_SHIFT	0x00
+#define	HOST_MSG	0xff
+#define	BUS_32_BIT	0x02
+#define	CCSGADDR_MAX	0x80
+
+
+/* Downloaded Constant Definitions */
+#define	SG_PREFETCH_ADDR_MASK	0x06
+#define	SG_PREFETCH_ALIGN_MASK	0x05
+#define	QOUTFIFO_OFFSET	0x00
+#define	SG_PREFETCH_CNT	0x04
+#define	INVERTED_CACHESIZE_MASK	0x03
+#define	CACHESIZE_MASK	0x02
+#define	QINFIFO_OFFSET	0x01
diff -urN linux/drivers/scsi/aic7xxx/aic7xxx_seq.h /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_seq.h
--- linux/drivers/scsi/aic7xxx/aic7xxx_seq.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aic7xxx_seq.h	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,1221 @@
+/*
+  * DO NOT EDIT - This file is automatically generated.
+  */
+static uint8_t seqprog[] = {
+	0xb2, 0x00, 0x00, 0x08,
+	0xf7, 0x11, 0x22, 0x08,
+	0x00, 0x65, 0xda, 0x59,
+	0xf7, 0x01, 0x02, 0x08,
+	0xff, 0x6a, 0x24, 0x08,
+	0x60, 0x0b, 0x6a, 0x68,
+	0x40, 0x00, 0x0a, 0x68,
+	0x08, 0x1f, 0x3e, 0x10,
+	0x60, 0x0b, 0x6a, 0x68,
+	0x08, 0x1f, 0x3e, 0x10,
+	0xff, 0x40, 0x38, 0x60,
+	0x40, 0xfa, 0x0a, 0x78,
+	0x01, 0x4c, 0xc8, 0x30,
+	0x00, 0x4b, 0x0a, 0x70,
+	0x01, 0x39, 0xa0, 0x30,
+	0x00, 0x6a, 0x74, 0x5e,
+	0x01, 0x50, 0x20, 0x31,
+	0x0d, 0x6a, 0x76, 0x00,
+	0x00, 0x50, 0xd2, 0x5d,
+	0x01, 0x50, 0xc8, 0x30,
+	0x00, 0x39, 0x3e, 0x60,
+	0x00, 0xbb, 0x2e, 0x70,
+	0xc1, 0x6a, 0x8c, 0x5e,
+	0x01, 0xbf, 0x72, 0x30,
+	0x01, 0x40, 0x7e, 0x31,
+	0x01, 0x90, 0x80, 0x30,
+	0x01, 0xf6, 0xd4, 0x30,
+	0x01, 0x4c, 0x98, 0x18,
+	0x01, 0x40, 0x20, 0x31,
+	0x00, 0x65, 0x42, 0x58,
+	0x00, 0x65, 0x0a, 0x40,
+	0x00, 0x65, 0x6c, 0x5e,
+	0x00, 0x65, 0x0a, 0x40,
+	0x20, 0x11, 0x48, 0x68,
+	0x20, 0x6a, 0x18, 0x00,
+	0x20, 0x11, 0x22, 0x00,
+	0xf7, 0x1f, 0xca, 0x08,
+	0x80, 0xb9, 0x4e, 0x78,
+	0x08, 0x65, 0xca, 0x00,
+	0x01, 0x65, 0x3e, 0x30,
+	0x01, 0xb9, 0x1e, 0x30,
+	0x7f, 0xb9, 0x0a, 0x08,
+	0x01, 0xb9, 0x0a, 0x30,
+	0x01, 0x55, 0xca, 0x30,
+	0x80, 0xb8, 0x5c, 0x78,
+	0x80, 0x65, 0xca, 0x00,
+	0x01, 0x65, 0x00, 0x34,
+	0x01, 0x55, 0x00, 0x34,
+	0x1a, 0x01, 0x02, 0x00,
+	0x08, 0xb8, 0x66, 0x78,
+	0x20, 0x01, 0x02, 0x00,
+	0x02, 0xbd, 0x08, 0x34,
+	0x01, 0xbd, 0x08, 0x34,
+	0x08, 0x6a, 0x18, 0x00,
+	0x08, 0x11, 0x22, 0x00,
+	0x60, 0x0b, 0x00, 0x78,
+	0x40, 0x0b, 0x0e, 0x69,
+	0x80, 0x0b, 0xfc, 0x78,
+	0x20, 0x6a, 0x16, 0x00,
+	0x20, 0x11, 0x7c, 0x68,
+	0x20, 0x6a, 0x18, 0x00,
+	0x20, 0x11, 0x22, 0x00,
+	0xa4, 0x6a, 0x06, 0x00,
+	0x08, 0x3c, 0x78, 0x00,
+	0x01, 0x4f, 0xc8, 0x30,
+	0xe0, 0x6a, 0xcc, 0x00,
+	0x47, 0x6a, 0xbc, 0x5d,
+	0x01, 0x6a, 0xdc, 0x01,
+	0x88, 0x6a, 0xcc, 0x00,
+	0x47, 0x6a, 0xbc, 0x5d,
+	0x01, 0x6a, 0x26, 0x01,
+	0xf0, 0x19, 0x7a, 0x08,
+	0x0f, 0x18, 0xc8, 0x08,
+	0x0f, 0x0f, 0xc8, 0x08,
+	0x0f, 0x05, 0xc8, 0x08,
+	0x00, 0x3d, 0x7a, 0x00,
+	0x08, 0x1f, 0x9c, 0x78,
+	0x80, 0x3d, 0x7a, 0x00,
+	0x01, 0x3d, 0xd8, 0x31,
+	0x01, 0x3d, 0x32, 0x31,
+	0x10, 0x03, 0x52, 0x79,
+	0x00, 0x65, 0xf4, 0x58,
+	0x80, 0x66, 0xd0, 0x78,
+	0x01, 0x66, 0xd8, 0x31,
+	0x01, 0x66, 0x32, 0x31,
+	0x40, 0x66, 0xae, 0x68,
+	0x01, 0x3c, 0x78, 0x00,
+	0x10, 0x03, 0xd8, 0x78,
+	0x00, 0x65, 0xf4, 0x58,
+	0xe0, 0x66, 0xc8, 0x18,
+	0x00, 0x65, 0xd8, 0x50,
+	0xdd, 0x66, 0xc8, 0x18,
+	0x00, 0x65, 0xd8, 0x48,
+	0x01, 0x66, 0xd8, 0x31,
+	0x01, 0x66, 0x32, 0x31,
+	0x10, 0x03, 0x52, 0x79,
+	0x00, 0x65, 0xf4, 0x58,
+	0x01, 0x66, 0xd8, 0x31,
+	0x01, 0x66, 0x32, 0x31,
+	0x01, 0x66, 0xae, 0x30,
+	0x40, 0x3c, 0x78, 0x00,
+	0x10, 0x03, 0xce, 0x78,
+	0x00, 0x65, 0xf4, 0x58,
+	0x00, 0x65, 0xd8, 0x40,
+	0x61, 0x6a, 0x8c, 0x5e,
+	0x08, 0x50, 0x2a, 0x71,
+	0x02, 0x0b, 0xd4, 0x78,
+	0x00, 0x65, 0xd0, 0x40,
+	0x80, 0x86, 0xc8, 0x08,
+	0x01, 0x4e, 0xc8, 0x30,
+	0x00, 0x4f, 0xea, 0x60,
+	0xc4, 0x6a, 0x2a, 0x5d,
+	0x40, 0x3c, 0xe6, 0x78,
+	0x28, 0x6a, 0x40, 0x5d,
+	0x00, 0x65, 0x50, 0x41,
+	0x08, 0x6a, 0x40, 0x5d,
+	0x00, 0x65, 0x50, 0x41,
+	0xff, 0x6a, 0xd8, 0x01,
+	0xff, 0x6a, 0x32, 0x01,
+	0x90, 0x3c, 0x78, 0x00,
+	0x10, 0x03, 0x46, 0x69,
+	0x00, 0x65, 0x2a, 0x41,
+	0x08, 0x01, 0x02, 0x00,
+	0x02, 0x0b, 0xf6, 0x78,
+	0xf7, 0x01, 0x02, 0x08,
+	0x01, 0x06, 0xcc, 0x34,
+	0x1a, 0x01, 0x02, 0x00,
+	0xf0, 0x19, 0x7a, 0x08,
+	0x0f, 0x0f, 0xc8, 0x08,
+	0x0f, 0x05, 0xc8, 0x08,
+	0x00, 0x3d, 0x7a, 0x00,
+	0x08, 0x1f, 0x0a, 0x79,
+	0x80, 0x3d, 0x7a, 0x00,
+	0x20, 0x6a, 0x16, 0x00,
+	0x00, 0x65, 0xba, 0x41,
+	0xb2, 0x00, 0x00, 0x08,
+	0x40, 0x6a, 0x16, 0x00,
+	0x01, 0x40, 0x20, 0x31,
+	0x01, 0xbf, 0x80, 0x30,
+	0x01, 0xb9, 0x7a, 0x30,
+	0x01, 0xba, 0x7c, 0x30,
+	0x00, 0x65, 0x60, 0x58,
+	0x80, 0x0b, 0xb6, 0x79,
+	0xe4, 0x6a, 0x2a, 0x5d,
+	0x80, 0xba, 0x40, 0x5d,
+	0x20, 0xb8, 0x28, 0x79,
+	0x20, 0x6a, 0x40, 0x5d,
+	0x00, 0xa3, 0x40, 0x5d,
+	0x01, 0xa0, 0x78, 0x30,
+	0x10, 0x03, 0x42, 0x69,
+	0x08, 0x3c, 0x5e, 0x69,
+	0x04, 0x3c, 0x84, 0x69,
+	0x02, 0x3c, 0x8a, 0x69,
+	0x01, 0x3c, 0x48, 0x79,
+	0x00, 0x6a, 0x74, 0x5e,
+	0x01, 0x6a, 0xa0, 0x30,
+	0x00, 0x65, 0x96, 0x59,
+	0x04, 0x50, 0x3a, 0x61,
+	0x0d, 0x6a, 0x76, 0x00,
+	0x00, 0xbb, 0xd2, 0x5d,
+	0x00, 0x65, 0x28, 0x41,
+	0xa4, 0x6a, 0x06, 0x00,
+	0x00, 0x65, 0xf4, 0x58,
+	0x00, 0x65, 0xd0, 0x40,
+	0xe4, 0x6a, 0x2a, 0x5d,
+	0x20, 0x3c, 0x4e, 0x79,
+	0x02, 0x6a, 0x40, 0x5d,
+	0x04, 0x6a, 0x40, 0x5d,
+	0x01, 0x03, 0x50, 0x69,
+	0xf7, 0x11, 0x22, 0x08,
+	0xff, 0x6a, 0x24, 0x08,
+	0xff, 0x6a, 0x06, 0x08,
+	0x01, 0x6a, 0x7e, 0x00,
+	0x00, 0x65, 0x96, 0x59,
+	0x00, 0x65, 0x04, 0x40,
+	0x84, 0x6a, 0x2a, 0x5d,
+	0x00, 0x65, 0xf4, 0x58,
+	0x01, 0x66, 0xc8, 0x30,
+	0x01, 0x64, 0xd8, 0x31,
+	0x01, 0x64, 0x32, 0x31,
+	0x5b, 0x64, 0xc8, 0x28,
+	0x30, 0x64, 0xca, 0x18,
+	0x01, 0x6c, 0xc8, 0x30,
+	0xff, 0x64, 0x80, 0x79,
+	0x08, 0x01, 0x02, 0x00,
+	0x02, 0x0b, 0x72, 0x79,
+	0x01, 0x64, 0x78, 0x61,
+	0xf7, 0x01, 0x02, 0x08,
+	0x01, 0x06, 0xd8, 0x31,
+	0x01, 0x06, 0x32, 0x31,
+	0xff, 0x64, 0xc8, 0x18,
+	0xff, 0x64, 0x72, 0x69,
+	0xf7, 0x3c, 0x78, 0x08,
+	0x00, 0x65, 0x2a, 0x41,
+	0x40, 0xa1, 0x7e, 0x10,
+	0x04, 0xa1, 0x2a, 0x5d,
+	0x00, 0x65, 0x56, 0x42,
+	0xc4, 0x6a, 0x2a, 0x5d,
+	0xc0, 0x6a, 0x7e, 0x00,
+	0x00, 0xa2, 0x40, 0x5d,
+	0xe4, 0x6a, 0x06, 0x00,
+	0x00, 0x6a, 0x40, 0x5d,
+	0x00, 0x65, 0x50, 0x41,
+	0x10, 0x3c, 0x9a, 0x69,
+	0x00, 0xbb, 0x60, 0x44,
+	0x18, 0x6a, 0xda, 0x01,
+	0x01, 0x69, 0xd8, 0x31,
+	0x1c, 0x6a, 0xd0, 0x01,
+	0x09, 0xee, 0xdc, 0x01,
+	0x80, 0xee, 0xa2, 0x79,
+	0xff, 0x6a, 0xdc, 0x09,
+	0x01, 0x93, 0x26, 0x01,
+	0x03, 0x6a, 0x2a, 0x01,
+	0x01, 0x69, 0x32, 0x31,
+	0x1c, 0x6a, 0xa0, 0x5d,
+	0x0a, 0x93, 0x26, 0x01,
+	0x00, 0x65, 0x64, 0x5e,
+	0x01, 0x4f, 0x9e, 0x18,
+	0x02, 0x6a, 0x22, 0x05,
+	0x80, 0x6a, 0x74, 0x00,
+	0x80, 0x3c, 0x78, 0x00,
+	0x00, 0x65, 0x98, 0x5d,
+	0x01, 0x3f, 0xc8, 0x30,
+	0xbf, 0x64, 0x56, 0x7a,
+	0x80, 0x64, 0x7e, 0x73,
+	0xa0, 0x64, 0xd4, 0x73,
+	0xc0, 0x64, 0xcc, 0x73,
+	0xe0, 0x64, 0x14, 0x74,
+	0x01, 0x6a, 0x8c, 0x5e,
+	0x00, 0x65, 0xba, 0x41,
+	0xf7, 0x11, 0x22, 0x08,
+	0x01, 0x06, 0xd4, 0x30,
+	0xff, 0x6a, 0x24, 0x08,
+	0xf7, 0x01, 0x02, 0x08,
+	0x09, 0x0c, 0xd4, 0x79,
+	0x08, 0x0c, 0x04, 0x68,
+	0xb1, 0x6a, 0x8c, 0x5e,
+	0xff, 0x6a, 0x26, 0x09,
+	0x12, 0x01, 0x02, 0x00,
+	0x02, 0x6a, 0x08, 0x30,
+	0xff, 0x6a, 0x08, 0x08,
+	0xdf, 0x01, 0x02, 0x08,
+	0x01, 0x6a, 0x7e, 0x00,
+	0xff, 0x6a, 0x78, 0x0c,
+	0xff, 0x6a, 0xc8, 0x08,
+	0x08, 0xa4, 0x48, 0x19,
+	0x00, 0xa5, 0x4a, 0x21,
+	0x00, 0xa6, 0x4c, 0x21,
+	0x00, 0xa7, 0x4e, 0x25,
+	0x88, 0xeb, 0x04, 0x72,
+	0x08, 0xeb, 0x90, 0x6e,
+	0x80, 0xa3, 0x90, 0x6e,
+	0x04, 0xea, 0x0a, 0xe2,
+	0x08, 0xee, 0x90, 0x6e,
+	0x04, 0x6a, 0xd0, 0x81,
+	0x05, 0xa4, 0xc0, 0x89,
+	0x03, 0xa5, 0xc2, 0x31,
+	0x09, 0x6a, 0xd6, 0x05,
+	0xff, 0x6a, 0xd6, 0x09,
+	0x08, 0xeb, 0x06, 0x6a,
+	0x06, 0xa4, 0xd4, 0x89,
+	0x80, 0x94, 0x90, 0x7e,
+	0x04, 0xe9, 0x10, 0x31,
+	0x01, 0xe9, 0xca, 0x30,
+	0x01, 0x65, 0x14, 0x7a,
+	0x01, 0x56, 0xac, 0x10,
+	0x01, 0x65, 0x18, 0x31,
+	0x02, 0xe9, 0x1a, 0x31,
+	0x01, 0xe9, 0x46, 0x31,
+	0x00, 0x65, 0xe8, 0x59,
+	0x01, 0xa4, 0xca, 0x30,
+	0x01, 0x56, 0x22, 0x7a,
+	0x04, 0x65, 0xca, 0x00,
+	0x80, 0xa3, 0x26, 0x7a,
+	0x02, 0x65, 0xca, 0x00,
+	0x01, 0x65, 0xf8, 0x31,
+	0x01, 0x3b, 0x26, 0x31,
+	0xff, 0x6a, 0xd4, 0x0c,
+	0x01, 0x8c, 0xc8, 0x30,
+	0x00, 0x88, 0xc8, 0x18,
+	0x02, 0x64, 0xc8, 0x88,
+	0xff, 0x64, 0x90, 0x7e,
+	0xff, 0x8d, 0x3c, 0x6a,
+	0xff, 0x8e, 0x3c, 0x6a,
+	0x03, 0x8c, 0xd4, 0x98,
+	0x00, 0x65, 0x90, 0x56,
+	0x01, 0x64, 0x70, 0x30,
+	0xff, 0x64, 0xc8, 0x10,
+	0x01, 0x64, 0xc8, 0x18,
+	0x00, 0x8c, 0x18, 0x19,
+	0xff, 0x8d, 0x1a, 0x21,
+	0xff, 0x8e, 0x1c, 0x25,
+	0x04, 0x14, 0x10, 0x31,
+	0x03, 0xa0, 0x18, 0x31,
+	0x03, 0xa0, 0x10, 0x30,
+	0x08, 0x6a, 0xcc, 0x00,
+	0xa0, 0x6a, 0xb6, 0x5d,
+	0x01, 0xa0, 0xac, 0x08,
+	0x00, 0x65, 0x7c, 0x42,
+	0xa8, 0x6a, 0x76, 0x00,
+	0x79, 0x6a, 0x76, 0x00,
+	0x40, 0x3f, 0x5e, 0x6a,
+	0x04, 0x3b, 0x76, 0x00,
+	0x00, 0x65, 0x4a, 0x5d,
+	0x04, 0x6a, 0xd4, 0x81,
+	0x20, 0x3c, 0x48, 0x6a,
+	0x20, 0x3c, 0x78, 0x00,
+	0x07, 0xac, 0x10, 0x31,
+	0x05, 0xb3, 0x46, 0x31,
+	0x88, 0x6a, 0xcc, 0x00,
+	0xac, 0x6a, 0xae, 0x5d,
+	0xa3, 0x6a, 0xcc, 0x00,
+	0xb3, 0x6a, 0xb2, 0x5d,
+	0x00, 0x65, 0x2c, 0x5a,
+	0xfd, 0xa4, 0x48, 0x09,
+	0x01, 0x8c, 0xac, 0x08,
+	0x03, 0x8c, 0x10, 0x30,
+	0x00, 0x65, 0xa6, 0x5d,
+	0x01, 0xa4, 0x8c, 0x7a,
+	0x04, 0x3b, 0x76, 0x08,
+	0x01, 0x3b, 0x26, 0x31,
+	0x80, 0x02, 0x04, 0x00,
+	0x10, 0x0c, 0x84, 0x7a,
+	0x7f, 0x02, 0x04, 0x08,
+	0x91, 0x6a, 0x8c, 0x5e,
+	0x00, 0x65, 0xba, 0x41,
+	0x01, 0xa4, 0xca, 0x30,
+	0x80, 0xa3, 0x92, 0x7a,
+	0x02, 0x65, 0xca, 0x00,
+	0x01, 0x56, 0x96, 0x7a,
+	0x04, 0x65, 0xca, 0x00,
+	0x01, 0x65, 0xf8, 0x31,
+	0x01, 0x3b, 0x26, 0x31,
+	0x00, 0x65, 0xf2, 0x59,
+	0x01, 0xfc, 0xa2, 0x6a,
+	0x80, 0x0b, 0x9a, 0x6a,
+	0x10, 0x0c, 0x9a, 0x7a,
+	0x04, 0x93, 0xb6, 0x6a,
+	0xdf, 0x93, 0x26, 0x09,
+	0x20, 0x93, 0xa6, 0x6a,
+	0x01, 0x94, 0xb6, 0x6a,
+	0x02, 0x93, 0x26, 0x01,
+	0x01, 0x94, 0xaa, 0x7a,
+	0x01, 0x94, 0xaa, 0x7a,
+	0x01, 0x94, 0xaa, 0x7a,
+	0x01, 0x94, 0xaa, 0x7a,
+	0x01, 0x94, 0xaa, 0x7a,
+	0x10, 0x94, 0xb6, 0x6a,
+	0xd7, 0x93, 0x26, 0x09,
+	0x08, 0x93, 0xba, 0x6a,
+	0x02, 0xfc, 0xc4, 0x7a,
+	0x01, 0xfc, 0x44, 0x7b,
+	0x01, 0xa4, 0x48, 0x01,
+	0x00, 0x65, 0x44, 0x43,
+	0x40, 0x0d, 0xca, 0x6a,
+	0x00, 0x65, 0xf2, 0x59,
+	0x00, 0x65, 0xbc, 0x42,
+	0x80, 0xfc, 0xd4, 0x7a,
+	0x80, 0xa4, 0xd4, 0x6a,
+	0xff, 0xa5, 0x4a, 0x19,
+	0xff, 0xa6, 0x4c, 0x21,
+	0xff, 0xa7, 0x4e, 0x21,
+	0xf8, 0xfc, 0x48, 0x09,
+	0xff, 0x6a, 0xac, 0x08,
+	0x04, 0xfc, 0xdc, 0x7a,
+	0x01, 0x56, 0xac, 0x00,
+	0xff, 0x6a, 0x46, 0x09,
+	0xff, 0x38, 0xe8, 0x6a,
+	0x80, 0xa3, 0xe8, 0x7a,
+	0x80, 0x0b, 0xe6, 0x7a,
+	0x04, 0x3b, 0xe8, 0x7a,
+	0xbf, 0x3b, 0x76, 0x08,
+	0x01, 0x3b, 0x26, 0x31,
+	0x00, 0x65, 0xf2, 0x59,
+	0x01, 0x0b, 0xf6, 0x6a,
+	0x10, 0x0c, 0xea, 0x7a,
+	0x04, 0x93, 0xf4, 0x6a,
+	0x01, 0x94, 0xf2, 0x7a,
+	0x10, 0x94, 0xf4, 0x6a,
+	0xc7, 0x93, 0x26, 0x09,
+	0x01, 0x99, 0xd4, 0x30,
+	0x38, 0x93, 0xf8, 0x6a,
+	0xff, 0x08, 0x44, 0x6b,
+	0xff, 0x09, 0x44, 0x6b,
+	0xff, 0x0a, 0x44, 0x6b,
+	0xff, 0x38, 0x14, 0x7b,
+	0x04, 0x14, 0x10, 0x31,
+	0x01, 0x38, 0x18, 0x31,
+	0x02, 0x6a, 0x1a, 0x31,
+	0x88, 0x6a, 0xcc, 0x00,
+	0x14, 0x6a, 0xb4, 0x5d,
+	0x00, 0x38, 0xa0, 0x5d,
+	0xff, 0x6a, 0x70, 0x08,
+	0x00, 0x65, 0x3a, 0x43,
+	0x80, 0xa3, 0x1a, 0x7b,
+	0x01, 0xa4, 0x48, 0x01,
+	0x00, 0x65, 0x44, 0x43,
+	0x08, 0xeb, 0x20, 0x7b,
+	0x00, 0x65, 0xf2, 0x59,
+	0x08, 0xeb, 0x1c, 0x6b,
+	0x07, 0xe9, 0x10, 0x31,
+	0x80, 0xe9, 0x26, 0x7b,
+	0x80, 0xa3, 0x46, 0x01,
+	0x88, 0x6a, 0xcc, 0x00,
+	0xa4, 0x6a, 0xb4, 0x5d,
+	0x08, 0x6a, 0xa0, 0x5d,
+	0x0d, 0x93, 0x26, 0x01,
+	0x00, 0x65, 0x64, 0x5e,
+	0x88, 0x6a, 0xcc, 0x00,
+	0x00, 0x65, 0x46, 0x5e,
+	0x01, 0x99, 0x46, 0x31,
+	0x00, 0x65, 0x2c, 0x5a,
+	0x00, 0x65, 0xe8, 0x59,
+	0x03, 0x8c, 0x10, 0x30,
+	0x00, 0x65, 0xa6, 0x5d,
+	0x01, 0x8c, 0x42, 0x7b,
+	0x01, 0x56, 0xac, 0x10,
+	0x80, 0x0b, 0x7c, 0x6a,
+	0x80, 0x0b, 0x4e, 0x6b,
+	0x01, 0x0c, 0x46, 0x7b,
+	0x10, 0x0c, 0x7c, 0x7a,
+	0xff, 0x6a, 0xd6, 0x09,
+	0x08, 0xeb, 0x4c, 0x6b,
+	0xff, 0x6a, 0xd6, 0x09,
+	0x08, 0xeb, 0x50, 0x6b,
+	0xff, 0x38, 0x62, 0x7b,
+	0x01, 0x38, 0xc8, 0x30,
+	0x00, 0x08, 0x40, 0x19,
+	0xff, 0x6a, 0xc8, 0x08,
+	0x00, 0x09, 0x42, 0x21,
+	0x00, 0x0a, 0x44, 0x21,
+	0xff, 0x6a, 0x70, 0x08,
+	0x00, 0x65, 0x64, 0x43,
+	0x03, 0x08, 0x40, 0x31,
+	0x03, 0x08, 0x40, 0x31,
+	0x01, 0x08, 0x40, 0x31,
+	0x01, 0x09, 0x42, 0x31,
+	0x01, 0x0a, 0x44, 0x31,
+	0xfd, 0xb4, 0x68, 0x09,
+	0x12, 0x01, 0x02, 0x00,
+	0x12, 0x01, 0x02, 0x00,
+	0x04, 0x3c, 0xba, 0x79,
+	0xfb, 0x3c, 0x78, 0x08,
+	0x04, 0x93, 0x2a, 0x79,
+	0x01, 0x0c, 0x78, 0x6b,
+	0x00, 0x65, 0x2a, 0x41,
+	0x00, 0x65, 0xba, 0x41,
+	0x00, 0x65, 0x4a, 0x5d,
+	0x01, 0xbc, 0x18, 0x31,
+	0x02, 0x6a, 0x1a, 0x31,
+	0x02, 0x6a, 0xf8, 0x01,
+	0x01, 0xbc, 0x10, 0x30,
+	0x02, 0x6a, 0x12, 0x30,
+	0x01, 0xbc, 0x10, 0x30,
+	0xff, 0x6a, 0x12, 0x08,
+	0xff, 0x6a, 0x14, 0x08,
+	0xf3, 0xbc, 0xd4, 0x18,
+	0xa0, 0x6a, 0xa6, 0x53,
+	0x04, 0xa0, 0x10, 0x31,
+	0xac, 0x6a, 0x26, 0x01,
+	0x04, 0xa0, 0x10, 0x31,
+	0x03, 0x08, 0x18, 0x31,
+	0x88, 0x6a, 0xcc, 0x00,
+	0xa0, 0x6a, 0xb4, 0x5d,
+	0x00, 0xbc, 0xa0, 0x5d,
+	0x3d, 0x6a, 0x26, 0x01,
+	0x00, 0x65, 0xbe, 0x43,
+	0xff, 0x6a, 0x10, 0x09,
+	0xa4, 0x6a, 0x26, 0x01,
+	0x0c, 0xa0, 0x32, 0x31,
+	0x05, 0x6a, 0x26, 0x01,
+	0x35, 0x6a, 0x26, 0x01,
+	0x0c, 0xa0, 0x32, 0x31,
+	0x36, 0x6a, 0x26, 0x01,
+	0x02, 0x93, 0x26, 0x01,
+	0x35, 0x6a, 0x26, 0x01,
+	0x00, 0x65, 0x58, 0x5e,
+	0x00, 0x65, 0x58, 0x5e,
+	0x02, 0x93, 0x26, 0x01,
+	0x04, 0x0b, 0xc2, 0x6b,
+	0x10, 0x0c, 0xbe, 0x7b,
+	0x01, 0x03, 0xc2, 0x6b,
+	0xc7, 0x93, 0x26, 0x09,
+	0x38, 0x93, 0xc6, 0x6b,
+	0x10, 0x01, 0x02, 0x00,
+	0x00, 0x65, 0xba, 0x41,
+	0x00, 0x65, 0x4a, 0x5d,
+	0x01, 0x06, 0x50, 0x31,
+	0x00, 0x65, 0xba, 0x41,
+	0x10, 0x3f, 0x06, 0x00,
+	0x01, 0x3a, 0xca, 0x30,
+	0x80, 0x65, 0x00, 0x64,
+	0x10, 0xb8, 0x24, 0x6c,
+	0x01, 0xb9, 0xdc, 0x30,
+	0x01, 0x6e, 0xc8, 0x30,
+	0x01, 0x53, 0xca, 0x30,
+	0x80, 0xb9, 0xe4, 0x7b,
+	0x01, 0x54, 0xca, 0x30,
+	0x80, 0xb9, 0xe8, 0x7b,
+	0x01, 0x54, 0xca, 0x30,
+	0x00, 0x65, 0x24, 0x6c,
+	0xc0, 0xba, 0xca, 0x00,
+	0x40, 0xb8, 0xf0, 0x6b,
+	0xbf, 0x65, 0xca, 0x08,
+	0x20, 0xb8, 0x04, 0x7c,
+	0x01, 0x65, 0x0c, 0x30,
+	0x00, 0x65, 0x98, 0x5d,
+	0xa0, 0x3f, 0x0c, 0x64,
+	0x23, 0xb8, 0x0c, 0x08,
+	0x00, 0x65, 0x98, 0x5d,
+	0xa0, 0x3f, 0x0c, 0x64,
+	0x00, 0xbb, 0x04, 0x44,
+	0xff, 0x65, 0x04, 0x64,
+	0x00, 0x65, 0x24, 0x44,
+	0x40, 0x6a, 0x18, 0x00,
+	0x01, 0x65, 0x0c, 0x30,
+	0x00, 0x65, 0x98, 0x5d,
+	0xa0, 0x3f, 0xd2, 0x73,
+	0x40, 0x6a, 0x18, 0x00,
+	0x01, 0x3a, 0xa4, 0x30,
+	0x08, 0x6a, 0x74, 0x00,
+	0x00, 0x65, 0xba, 0x41,
+	0x64, 0x6a, 0x24, 0x5d,
+	0x80, 0x64, 0x9a, 0x6c,
+	0x04, 0x64, 0x70, 0x74,
+	0x02, 0x64, 0x7e, 0x74,
+	0x00, 0x6a, 0x40, 0x74,
+	0x03, 0x64, 0x8c, 0x74,
+	0x23, 0x64, 0x2c, 0x74,
+	0x08, 0x64, 0x3c, 0x74,
+	0x61, 0x6a, 0x8c, 0x5e,
+	0x00, 0x65, 0x98, 0x5d,
+	0x08, 0x50, 0xbc, 0x71,
+	0x00, 0x65, 0x24, 0x44,
+	0x80, 0x04, 0x3a, 0x7c,
+	0x50, 0x6a, 0x1a, 0x5d,
+	0x01, 0x50, 0x3a, 0x64,
+	0x01, 0xa4, 0x36, 0x7c,
+	0x01, 0x56, 0x3c, 0x7c,
+	0x41, 0x6a, 0x8c, 0x5e,
+	0x00, 0x65, 0x3c, 0x44,
+	0x07, 0x6a, 0x12, 0x5d,
+	0x01, 0x06, 0xd4, 0x30,
+	0x00, 0x65, 0xba, 0x41,
+	0x10, 0xb8, 0x44, 0x7c,
+	0xa1, 0x6a, 0x8c, 0x5e,
+	0x01, 0xb4, 0x4a, 0x6c,
+	0x02, 0xb4, 0x4c, 0x6c,
+	0x01, 0xa4, 0x4c, 0x7c,
+	0xff, 0xa8, 0x5c, 0x7c,
+	0x04, 0xb4, 0x68, 0x01,
+	0x01, 0x6a, 0x76, 0x00,
+	0x00, 0xbb, 0xd2, 0x5d,
+	0xff, 0xa8, 0x5c, 0x7c,
+	0x71, 0x6a, 0x8c, 0x5e,
+	0x40, 0x50, 0x5c, 0x64,
+	0x00, 0x65, 0x6c, 0x5e,
+	0x00, 0x65, 0xcc, 0x41,
+	0x00, 0xbb, 0x60, 0x5c,
+	0x00, 0x65, 0xcc, 0x41,
+	0x00, 0x65, 0x6c, 0x5e,
+	0x01, 0x65, 0xa0, 0x30,
+	0x01, 0xf8, 0xc8, 0x30,
+	0x01, 0x4d, 0xc8, 0x30,
+	0x00, 0x6a, 0x76, 0xdd,
+	0x00, 0x50, 0x88, 0x5d,
+	0x01, 0x4d, 0x9a, 0x18,
+	0x02, 0x6a, 0x22, 0x05,
+	0x04, 0xb8, 0x70, 0x01,
+	0x00, 0x65, 0x88, 0x5e,
+	0x20, 0xb8, 0xcc, 0x69,
+	0x01, 0xbb, 0xa0, 0x30,
+	0x01, 0xba, 0x7c, 0x30,
+	0x00, 0xb9, 0x90, 0x5c,
+	0x00, 0x65, 0xcc, 0x41,
+	0x20, 0x3c, 0x3c, 0x7c,
+	0x04, 0x14, 0x58, 0x31,
+	0x08, 0xa0, 0x60, 0x31,
+	0xac, 0x6a, 0xcc, 0x00,
+	0x14, 0x6a, 0xb4, 0x5d,
+	0xa0, 0x6a, 0xac, 0x5d,
+	0x00, 0x65, 0x3c, 0x44,
+	0xdf, 0x3c, 0x78, 0x08,
+	0x00, 0x65, 0x3c, 0x44,
+	0x4c, 0x65, 0xcc, 0x28,
+	0x01, 0x3e, 0x20, 0x31,
+	0xd0, 0x66, 0xcc, 0x18,
+	0x20, 0x66, 0xcc, 0x18,
+	0x01, 0x50, 0xda, 0x34,
+	0x4c, 0x19, 0xca, 0x28,
+	0x3f, 0x64, 0x7c, 0x08,
+	0xd0, 0x65, 0xca, 0x18,
+	0x01, 0x3e, 0x20, 0x31,
+	0x30, 0x65, 0xd4, 0x18,
+	0x00, 0x65, 0xa8, 0x4c,
+	0xe1, 0x6a, 0x22, 0x01,
+	0xff, 0x6a, 0xd4, 0x08,
+	0x20, 0x65, 0xd4, 0x18,
+	0x00, 0x65, 0xb0, 0x54,
+	0xe1, 0x6a, 0x22, 0x01,
+	0xff, 0x6a, 0xd4, 0x08,
+	0x20, 0x65, 0xca, 0x18,
+	0xe0, 0x65, 0xd4, 0x18,
+	0x00, 0x65, 0xba, 0x4c,
+	0xe1, 0x6a, 0x22, 0x01,
+	0xff, 0x6a, 0xd4, 0x08,
+	0xd0, 0x65, 0xd4, 0x18,
+	0x00, 0x65, 0xc2, 0x54,
+	0xe1, 0x6a, 0x22, 0x01,
+	0xff, 0x6a, 0xd4, 0x08,
+	0x01, 0x6c, 0xa0, 0x30,
+	0xff, 0x50, 0xd0, 0x74,
+	0x00, 0x50, 0x4e, 0x5d,
+	0x01, 0x50, 0x20, 0x31,
+	0x00, 0x65, 0xf2, 0x44,
+	0x00, 0x65, 0xee, 0x44,
+	0x80, 0x3c, 0x78, 0x00,
+	0x01, 0x06, 0xd4, 0x30,
+	0x00, 0x65, 0x98, 0x5d,
+	0x01, 0x3c, 0x78, 0x00,
+	0xe0, 0x3f, 0x0e, 0x65,
+	0x02, 0x3c, 0x78, 0x00,
+	0x20, 0x12, 0x0e, 0x65,
+	0x50, 0x6a, 0x1a, 0x5d,
+	0x00, 0x50, 0x4e, 0x5d,
+	0x50, 0x6a, 0x1a, 0x5d,
+	0x01, 0x50, 0x20, 0x31,
+	0x04, 0x3c, 0x78, 0x00,
+	0x01, 0xb9, 0xc8, 0x30,
+	0x00, 0x3d, 0x0c, 0x65,
+	0x08, 0x3c, 0x78, 0x00,
+	0x01, 0xba, 0xc8, 0x30,
+	0x00, 0x3e, 0x0c, 0x65,
+	0x10, 0x3c, 0x78, 0x00,
+	0x04, 0xb8, 0x0c, 0x7d,
+	0xfb, 0xb8, 0x70, 0x09,
+	0x20, 0xb8, 0x02, 0x6d,
+	0x01, 0x90, 0xc8, 0x30,
+	0xff, 0x6a, 0xa0, 0x00,
+	0x00, 0x3d, 0x90, 0x5c,
+	0x01, 0x64, 0x20, 0x31,
+	0x80, 0x6a, 0x78, 0x00,
+	0x00, 0x65, 0x62, 0x58,
+	0x10, 0xb8, 0x3c, 0x7c,
+	0xff, 0x6a, 0x12, 0x5d,
+	0x00, 0x65, 0x3c, 0x44,
+	0x00, 0x65, 0x6c, 0x5e,
+	0x31, 0x6a, 0x8c, 0x5e,
+	0x00, 0x65, 0x3c, 0x44,
+	0x10, 0x3f, 0x06, 0x00,
+	0x01, 0x65, 0x74, 0x34,
+	0x81, 0x6a, 0x8c, 0x5e,
+	0x00, 0x65, 0x1c, 0x45,
+	0x01, 0x06, 0xd4, 0x30,
+	0x01, 0x0c, 0x1c, 0x7d,
+	0x04, 0x0c, 0x16, 0x6d,
+	0xe0, 0x03, 0x7e, 0x08,
+	0xe0, 0x3f, 0xba, 0x61,
+	0x01, 0x65, 0xcc, 0x30,
+	0x01, 0x12, 0xda, 0x34,
+	0x01, 0x06, 0xd4, 0x34,
+	0x01, 0x03, 0x2a, 0x6d,
+	0x40, 0x03, 0xcc, 0x08,
+	0x01, 0x65, 0x06, 0x30,
+	0x40, 0x65, 0xc8, 0x08,
+	0x00, 0x66, 0x38, 0x75,
+	0x40, 0x65, 0x38, 0x7d,
+	0x00, 0x65, 0x38, 0x5d,
+	0xff, 0x6a, 0xd4, 0x08,
+	0xff, 0x6a, 0xd4, 0x08,
+	0xff, 0x6a, 0xd4, 0x08,
+	0xff, 0x6a, 0xd4, 0x0c,
+	0x08, 0x01, 0x02, 0x00,
+	0x02, 0x0b, 0x42, 0x7d,
+	0x01, 0x65, 0x0c, 0x30,
+	0x02, 0x0b, 0x46, 0x7d,
+	0xf7, 0x01, 0x02, 0x0c,
+	0x80, 0x3c, 0x90, 0x6e,
+	0x21, 0x6a, 0x8c, 0x46,
+	0x01, 0x65, 0xc8, 0x30,
+	0xff, 0x41, 0x6e, 0x75,
+	0x01, 0x41, 0x20, 0x31,
+	0xff, 0x6a, 0xa2, 0x00,
+	0x00, 0x65, 0x5e, 0x45,
+	0xff, 0xbf, 0x6e, 0x75,
+	0x01, 0x90, 0xa2, 0x30,
+	0x01, 0xbf, 0x20, 0x31,
+	0x00, 0xbb, 0x58, 0x65,
+	0xff, 0x51, 0x6c, 0x75,
+	0x01, 0xbf, 0xcc, 0x30,
+	0x01, 0x90, 0xca, 0x30,
+	0x01, 0x51, 0x20, 0x31,
+	0x01, 0x66, 0x7e, 0x31,
+	0x01, 0x65, 0x20, 0x35,
+	0x01, 0xbf, 0x82, 0x34,
+	0x01, 0x64, 0xa0, 0x30,
+	0x00, 0x6a, 0x74, 0x5e,
+	0x0d, 0x6a, 0x76, 0x00,
+	0x00, 0x50, 0xd2, 0x45,
+	0x01, 0x65, 0xa2, 0x30,
+	0xe0, 0x6a, 0xcc, 0x00,
+	0x47, 0x6a, 0xc6, 0x5d,
+	0x01, 0x6a, 0xd0, 0x01,
+	0x01, 0x6a, 0xdc, 0x05,
+	0x88, 0x6a, 0xcc, 0x00,
+	0x47, 0x6a, 0xc6, 0x5d,
+	0x01, 0x6a, 0xa0, 0x5d,
+	0x01, 0x6a, 0x26, 0x05,
+	0x01, 0x65, 0xd8, 0x31,
+	0x09, 0xee, 0xdc, 0x01,
+	0x80, 0xee, 0x8c, 0x7d,
+	0xff, 0x6a, 0xdc, 0x0d,
+	0x01, 0x65, 0x32, 0x31,
+	0x0a, 0x93, 0x26, 0x01,
+	0x00, 0x65, 0x64, 0x46,
+	0x81, 0x6a, 0x8c, 0x5e,
+	0x01, 0x0c, 0x98, 0x7d,
+	0x04, 0x0c, 0x96, 0x6d,
+	0xe0, 0x03, 0x06, 0x08,
+	0xe0, 0x03, 0x7e, 0x0c,
+	0x01, 0x65, 0x18, 0x31,
+	0xff, 0x6a, 0x1a, 0x09,
+	0xff, 0x6a, 0x1c, 0x0d,
+	0x01, 0x8c, 0x10, 0x30,
+	0x01, 0x8d, 0x12, 0x30,
+	0x01, 0x8e, 0x14, 0x34,
+	0x01, 0x6c, 0xda, 0x30,
+	0x01, 0x6c, 0xda, 0x30,
+	0x01, 0x6c, 0xda, 0x30,
+	0x01, 0x6c, 0xda, 0x30,
+	0x01, 0x6c, 0xda, 0x30,
+	0x01, 0x6c, 0xda, 0x30,
+	0x01, 0x6c, 0xda, 0x30,
+	0x01, 0x6c, 0xda, 0x34,
+	0x3d, 0x64, 0xa2, 0x28,
+	0x55, 0x64, 0xc8, 0x28,
+	0x00, 0x65, 0xc6, 0x45,
+	0x2e, 0x64, 0xa2, 0x28,
+	0x66, 0x64, 0xc8, 0x28,
+	0x00, 0x6c, 0xda, 0x18,
+	0x01, 0x51, 0xc8, 0x30,
+	0x00, 0x6c, 0xda, 0x20,
+	0xff, 0x6a, 0xc8, 0x08,
+	0x00, 0x6c, 0xda, 0x20,
+	0x00, 0x6c, 0xda, 0x24,
+	0x01, 0x65, 0xc8, 0x30,
+	0xe0, 0x6a, 0xcc, 0x00,
+	0x43, 0x6a, 0xc2, 0x5d,
+	0x01, 0x90, 0xe2, 0x31,
+	0x04, 0x3b, 0xe6, 0x7d,
+	0x30, 0x6a, 0xd0, 0x01,
+	0x20, 0x6a, 0xd0, 0x01,
+	0x1d, 0x6a, 0xdc, 0x01,
+	0xdc, 0xee, 0xe2, 0x65,
+	0x00, 0x65, 0xfe, 0x45,
+	0x20, 0x6a, 0xd0, 0x01,
+	0x01, 0x6a, 0xdc, 0x01,
+	0x20, 0xa0, 0xd8, 0x31,
+	0x09, 0xee, 0xdc, 0x01,
+	0x80, 0xee, 0xee, 0x7d,
+	0x11, 0x6a, 0xdc, 0x01,
+	0x50, 0xee, 0xf2, 0x65,
+	0x20, 0x6a, 0xd0, 0x01,
+	0x09, 0x6a, 0xdc, 0x01,
+	0x88, 0xee, 0xf8, 0x65,
+	0x19, 0x6a, 0xdc, 0x01,
+	0xd8, 0xee, 0xfc, 0x65,
+	0xff, 0x6a, 0xdc, 0x09,
+	0x18, 0xee, 0x00, 0x6e,
+	0xff, 0x6a, 0xd4, 0x0c,
+	0x88, 0x6a, 0xcc, 0x00,
+	0x43, 0x6a, 0xc2, 0x5d,
+	0x20, 0x6a, 0xa0, 0x5d,
+	0x01, 0x3b, 0x26, 0x31,
+	0x04, 0x3b, 0x1a, 0x6e,
+	0xa0, 0x6a, 0xca, 0x00,
+	0x20, 0x65, 0xc8, 0x18,
+	0x00, 0x65, 0x54, 0x5e,
+	0x00, 0x65, 0x12, 0x66,
+	0x0a, 0x93, 0x26, 0x01,
+	0x00, 0x65, 0x64, 0x46,
+	0xa0, 0x6a, 0xcc, 0x00,
+	0xe8, 0x6a, 0xc8, 0x00,
+	0x01, 0x94, 0x1e, 0x6e,
+	0x10, 0x94, 0x20, 0x6e,
+	0x08, 0x94, 0x32, 0x6e,
+	0x08, 0x94, 0x32, 0x6e,
+	0x08, 0x94, 0x32, 0x6e,
+	0x00, 0x65, 0x44, 0x5e,
+	0x08, 0x64, 0xc8, 0x18,
+	0x00, 0x8c, 0xca, 0x18,
+	0x00, 0x65, 0x28, 0x4e,
+	0x00, 0x65, 0x1e, 0x46,
+	0xf7, 0x93, 0x26, 0x09,
+	0x08, 0x93, 0x34, 0x6e,
+	0x00, 0x65, 0x44, 0x5e,
+	0x08, 0x64, 0xc8, 0x18,
+	0x08, 0x64, 0x36, 0x66,
+	0x00, 0x65, 0x64, 0x5e,
+	0x00, 0x65, 0x44, 0x5e,
+	0x00, 0x65, 0x44, 0x5e,
+	0x00, 0x65, 0x44, 0x5e,
+	0x01, 0x99, 0xda, 0x30,
+	0x01, 0x99, 0xda, 0x30,
+	0x01, 0x99, 0xda, 0x30,
+	0x01, 0x99, 0xda, 0x30,
+	0x01, 0x99, 0xda, 0x30,
+	0x01, 0x99, 0xda, 0x30,
+	0x01, 0x99, 0xda, 0x30,
+	0x01, 0x99, 0xda, 0x34,
+	0x01, 0x6c, 0x32, 0x31,
+	0x01, 0x6c, 0x32, 0x31,
+	0x01, 0x6c, 0x32, 0x31,
+	0x01, 0x6c, 0x32, 0x31,
+	0x01, 0x6c, 0x32, 0x31,
+	0x01, 0x6c, 0x32, 0x31,
+	0x01, 0x6c, 0x32, 0x31,
+	0x01, 0x6c, 0x32, 0x35,
+	0x08, 0x94, 0x64, 0x7e,
+	0xf7, 0x93, 0x26, 0x09,
+	0x08, 0x93, 0x68, 0x6e,
+	0xff, 0x6a, 0xd4, 0x0c,
+	0x01, 0x42, 0x7e, 0x31,
+	0xff, 0x6a, 0x76, 0x01,
+	0x01, 0x90, 0x84, 0x34,
+	0xff, 0x6a, 0x76, 0x05,
+	0xff, 0x42, 0x84, 0x66,
+	0xff, 0x41, 0x7c, 0x66,
+	0xd1, 0x6a, 0x8c, 0x5e,
+	0xff, 0x6a, 0xca, 0x04,
+	0x01, 0x41, 0x20, 0x31,
+	0x01, 0xbf, 0x82, 0x30,
+	0x01, 0x6a, 0x76, 0x00,
+	0x00, 0xbb, 0xd2, 0x45,
+	0x01, 0x42, 0x20, 0x31,
+	0x01, 0xbf, 0x84, 0x34,
+	0x01, 0x41, 0x7e, 0x31,
+	0x01, 0x90, 0x82, 0x34,
+	0x01, 0x65, 0x22, 0x31,
+	0xff, 0x6a, 0xd4, 0x08,
+	0xff, 0x6a, 0xd4, 0x0c
+};
+
+static int ahc_patch21_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch21_func(struct ahc_softc *ahc)
+{
+	return ((ahc->bugs & AHC_SCBCHAN_UPLOAD_BUG) != 0);
+}
+
+static int ahc_patch20_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch20_func(struct ahc_softc *ahc)
+{
+	return ((ahc->features & AHC_CMD_CHAN) == 0);
+}
+
+static int ahc_patch19_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch19_func(struct ahc_softc *ahc)
+{
+	return ((ahc->features & AHC_QUEUE_REGS) == 0);
+}
+
+static int ahc_patch18_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch18_func(struct ahc_softc *ahc)
+{
+	return ((ahc->features & AHC_WIDE) != 0);
+}
+
+static int ahc_patch17_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch17_func(struct ahc_softc *ahc)
+{
+	return ((ahc->flags & AHC_SCB_BTT) != 0);
+}
+
+static int ahc_patch16_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch16_func(struct ahc_softc *ahc)
+{
+	return ((ahc->bugs & AHC_PCI_2_1_RETRY_BUG) != 0);
+}
+
+static int ahc_patch15_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch15_func(struct ahc_softc *ahc)
+{
+	return ((ahc->flags & AHC_TMODE_WIDEODD_BUG) != 0);
+}
+
+static int ahc_patch14_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch14_func(struct ahc_softc *ahc)
+{
+	return ((ahc->bugs & AHC_AUTOFLUSH_BUG) != 0);
+}
+
+static int ahc_patch13_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch13_func(struct ahc_softc *ahc)
+{
+	return ((ahc->features & AHC_ULTRA2) == 0);
+}
+
+static int ahc_patch12_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch12_func(struct ahc_softc *ahc)
+{
+	return ((ahc->bugs & AHC_PCI_MWI_BUG) != 0 && ahc->pci_cachesize != 0);
+}
+
+static int ahc_patch11_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch11_func(struct ahc_softc *ahc)
+{
+	return ((ahc->features & AHC_HS_MAILBOX) != 0);
+}
+
+static int ahc_patch10_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch10_func(struct ahc_softc *ahc)
+{
+	return ((ahc->features & AHC_MULTI_TID) != 0);
+}
+
+static int ahc_patch9_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch9_func(struct ahc_softc *ahc)
+{
+	return ((ahc->features & AHC_CMD_CHAN) != 0);
+}
+
+static int ahc_patch8_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch8_func(struct ahc_softc *ahc)
+{
+	return ((ahc->flags & AHC_INITIATORROLE) != 0);
+}
+
+static int ahc_patch7_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch7_func(struct ahc_softc *ahc)
+{
+	return ((ahc->features & AHC_ULTRA) != 0);
+}
+
+static int ahc_patch6_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch6_func(struct ahc_softc *ahc)
+{
+	return ((ahc->flags & AHC_TARGETROLE) != 0);
+}
+
+static int ahc_patch5_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch5_func(struct ahc_softc *ahc)
+{
+	return ((ahc->flags & AHC_SEQUENCER_DEBUG) != 0);
+}
+
+static int ahc_patch4_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch4_func(struct ahc_softc *ahc)
+{
+	return ((ahc->flags & AHC_PAGESCBS) != 0);
+}
+
+static int ahc_patch3_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch3_func(struct ahc_softc *ahc)
+{
+	return ((ahc->features & AHC_QUEUE_REGS) != 0);
+}
+
+static int ahc_patch2_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch2_func(struct ahc_softc *ahc)
+{
+	return ((ahc->features & AHC_TWIN) != 0);
+}
+
+static int ahc_patch1_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch1_func(struct ahc_softc *ahc)
+{
+	return ((ahc->features & AHC_ULTRA2) != 0);
+}
+
+static int ahc_patch0_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch0_func(struct ahc_softc *ahc)
+{
+	return (0);
+}
+
+typedef int patch_func_t (struct ahc_softc *);
+struct patch {
+	patch_func_t	*patch_func;
+	uint32_t	begin	   :10,
+			skip_instr :10,
+			skip_patch :12;
+} patches[] = {
+	{ ahc_patch1_func, 4, 1, 1 },
+	{ ahc_patch2_func, 7, 3, 1 },
+	{ ahc_patch3_func, 11, 1, 2 },
+	{ ahc_patch0_func, 12, 2, 1 },
+	{ ahc_patch4_func, 15, 1, 2 },
+	{ ahc_patch0_func, 16, 1, 1 },
+	{ ahc_patch5_func, 21, 2, 1 },
+	{ ahc_patch3_func, 26, 1, 2 },
+	{ ahc_patch0_func, 27, 1, 1 },
+	{ ahc_patch2_func, 36, 4, 1 },
+	{ ahc_patch1_func, 40, 1, 2 },
+	{ ahc_patch0_func, 41, 2, 3 },
+	{ ahc_patch2_func, 41, 1, 2 },
+	{ ahc_patch0_func, 42, 1, 1 },
+	{ ahc_patch6_func, 43, 4, 2 },
+	{ ahc_patch0_func, 47, 1, 1 },
+	{ ahc_patch7_func, 49, 2, 1 },
+	{ ahc_patch1_func, 51, 1, 2 },
+	{ ahc_patch0_func, 52, 1, 1 },
+	{ ahc_patch6_func, 57, 69, 21 },
+	{ ahc_patch8_func, 57, 1, 1 },
+	{ ahc_patch9_func, 65, 3, 2 },
+	{ ahc_patch0_func, 68, 3, 1 },
+	{ ahc_patch10_func, 72, 1, 2 },
+	{ ahc_patch0_func, 73, 2, 3 },
+	{ ahc_patch1_func, 73, 1, 2 },
+	{ ahc_patch0_func, 74, 1, 1 },
+	{ ahc_patch2_func, 76, 2, 1 },
+	{ ahc_patch9_func, 78, 1, 2 },
+	{ ahc_patch0_func, 79, 1, 1 },
+	{ ahc_patch9_func, 83, 1, 2 },
+	{ ahc_patch0_func, 84, 1, 1 },
+	{ ahc_patch9_func, 93, 1, 2 },
+	{ ahc_patch0_func, 94, 1, 1 },
+	{ ahc_patch9_func, 97, 1, 2 },
+	{ ahc_patch0_func, 98, 1, 1 },
+	{ ahc_patch11_func, 108, 1, 2 },
+	{ ahc_patch0_func, 109, 1, 1 },
+	{ ahc_patch9_func, 117, 1, 2 },
+	{ ahc_patch0_func, 118, 1, 1 },
+	{ ahc_patch8_func, 126, 9, 4 },
+	{ ahc_patch1_func, 128, 1, 2 },
+	{ ahc_patch0_func, 129, 1, 1 },
+	{ ahc_patch2_func, 131, 2, 1 },
+	{ ahc_patch6_func, 142, 77, 9 },
+	{ ahc_patch4_func, 154, 1, 1 },
+	{ ahc_patch1_func, 170, 1, 1 },
+	{ ahc_patch9_func, 178, 1, 2 },
+	{ ahc_patch0_func, 179, 1, 1 },
+	{ ahc_patch9_func, 188, 1, 2 },
+	{ ahc_patch0_func, 189, 1, 1 },
+	{ ahc_patch9_func, 205, 6, 2 },
+	{ ahc_patch0_func, 211, 6, 1 },
+	{ ahc_patch8_func, 219, 18, 2 },
+	{ ahc_patch1_func, 232, 1, 1 },
+	{ ahc_patch1_func, 239, 1, 2 },
+	{ ahc_patch0_func, 240, 2, 2 },
+	{ ahc_patch7_func, 241, 1, 1 },
+	{ ahc_patch9_func, 249, 29, 2 },
+	{ ahc_patch1_func, 261, 16, 1 },
+	{ ahc_patch12_func, 278, 14, 1 },
+	{ ahc_patch1_func, 292, 2, 2 },
+	{ ahc_patch0_func, 294, 3, 3 },
+	{ ahc_patch9_func, 294, 1, 2 },
+	{ ahc_patch0_func, 295, 2, 1 },
+	{ ahc_patch1_func, 299, 1, 2 },
+	{ ahc_patch0_func, 300, 1, 1 },
+	{ ahc_patch9_func, 304, 1, 1 },
+	{ ahc_patch9_func, 307, 2, 2 },
+	{ ahc_patch0_func, 309, 4, 1 },
+	{ ahc_patch12_func, 313, 1, 1 },
+	{ ahc_patch13_func, 316, 2, 3 },
+	{ ahc_patch9_func, 316, 1, 2 },
+	{ ahc_patch0_func, 317, 1, 1 },
+	{ ahc_patch1_func, 326, 41, 6 },
+	{ ahc_patch6_func, 335, 1, 1 },
+	{ ahc_patch8_func, 336, 1, 1 },
+	{ ahc_patch14_func, 340, 1, 1 },
+	{ ahc_patch14_func, 341, 5, 1 },
+	{ ahc_patch0_func, 367, 51, 15 },
+	{ ahc_patch12_func, 367, 1, 1 },
+	{ ahc_patch6_func, 369, 2, 2 },
+	{ ahc_patch15_func, 370, 1, 1 },
+	{ ahc_patch9_func, 373, 1, 1 },
+	{ ahc_patch16_func, 380, 1, 1 },
+	{ ahc_patch12_func, 385, 9, 3 },
+	{ ahc_patch9_func, 386, 3, 2 },
+	{ ahc_patch0_func, 389, 3, 1 },
+	{ ahc_patch9_func, 397, 6, 2 },
+	{ ahc_patch0_func, 403, 8, 1 },
+	{ ahc_patch12_func, 411, 1, 1 },
+	{ ahc_patch9_func, 413, 1, 2 },
+	{ ahc_patch0_func, 414, 1, 1 },
+	{ ahc_patch6_func, 417, 1, 1 },
+	{ ahc_patch6_func, 418, 1, 1 },
+	{ ahc_patch8_func, 419, 4, 2 },
+	{ ahc_patch9_func, 421, 2, 1 },
+	{ ahc_patch9_func, 423, 2, 1 },
+	{ ahc_patch12_func, 425, 9, 4 },
+	{ ahc_patch9_func, 425, 1, 1 },
+	{ ahc_patch9_func, 432, 2, 1 },
+	{ ahc_patch0_func, 434, 4, 3 },
+	{ ahc_patch9_func, 434, 1, 2 },
+	{ ahc_patch0_func, 435, 3, 1 },
+	{ ahc_patch1_func, 439, 2, 1 },
+	{ ahc_patch6_func, 441, 5, 2 },
+	{ ahc_patch0_func, 446, 1, 1 },
+	{ ahc_patch8_func, 447, 113, 22 },
+	{ ahc_patch1_func, 448, 3, 2 },
+	{ ahc_patch0_func, 451, 5, 3 },
+	{ ahc_patch9_func, 451, 2, 2 },
+	{ ahc_patch0_func, 453, 3, 1 },
+	{ ahc_patch1_func, 458, 2, 2 },
+	{ ahc_patch0_func, 460, 6, 3 },
+	{ ahc_patch9_func, 460, 2, 2 },
+	{ ahc_patch0_func, 462, 3, 1 },
+	{ ahc_patch1_func, 468, 2, 2 },
+	{ ahc_patch0_func, 470, 9, 7 },
+	{ ahc_patch9_func, 470, 5, 6 },
+	{ ahc_patch17_func, 470, 1, 2 },
+	{ ahc_patch0_func, 471, 1, 1 },
+	{ ahc_patch17_func, 473, 1, 2 },
+	{ ahc_patch0_func, 474, 1, 1 },
+	{ ahc_patch0_func, 475, 4, 1 },
+	{ ahc_patch1_func, 484, 1, 1 },
+	{ ahc_patch2_func, 496, 2, 2 },
+	{ ahc_patch0_func, 498, 2, 2 },
+	{ ahc_patch18_func, 498, 2, 1 },
+	{ ahc_patch18_func, 534, 7, 1 },
+	{ ahc_patch3_func, 562, 1, 2 },
+	{ ahc_patch0_func, 563, 1, 1 },
+	{ ahc_patch19_func, 566, 1, 1 },
+	{ ahc_patch8_func, 568, 93, 23 },
+	{ ahc_patch4_func, 569, 1, 1 },
+	{ ahc_patch9_func, 576, 2, 2 },
+	{ ahc_patch0_func, 578, 3, 1 },
+	{ ahc_patch17_func, 585, 2, 2 },
+	{ ahc_patch0_func, 587, 1, 1 },
+	{ ahc_patch17_func, 591, 10, 3 },
+	{ ahc_patch5_func, 593, 8, 1 },
+	{ ahc_patch0_func, 601, 9, 2 },
+	{ ahc_patch5_func, 602, 8, 1 },
+	{ ahc_patch4_func, 612, 1, 2 },
+	{ ahc_patch0_func, 613, 1, 1 },
+	{ ahc_patch17_func, 614, 1, 2 },
+	{ ahc_patch0_func, 615, 1, 1 },
+	{ ahc_patch5_func, 616, 1, 1 },
+	{ ahc_patch5_func, 619, 1, 1 },
+	{ ahc_patch5_func, 621, 1, 1 },
+	{ ahc_patch4_func, 623, 2, 2 },
+	{ ahc_patch0_func, 625, 2, 1 },
+	{ ahc_patch5_func, 627, 1, 1 },
+	{ ahc_patch5_func, 630, 1, 1 },
+	{ ahc_patch5_func, 633, 1, 1 },
+	{ ahc_patch4_func, 646, 1, 1 },
+	{ ahc_patch6_func, 661, 16, 1 },
+	{ ahc_patch4_func, 679, 20, 1 },
+	{ ahc_patch9_func, 700, 4, 2 },
+	{ ahc_patch0_func, 704, 4, 1 },
+	{ ahc_patch9_func, 708, 4, 2 },
+	{ ahc_patch0_func, 712, 3, 1 },
+	{ ahc_patch20_func, 720, 14, 1 },
+	{ ahc_patch6_func, 734, 3, 1 },
+	{ ahc_patch9_func, 746, 24, 8 },
+	{ ahc_patch17_func, 750, 1, 2 },
+	{ ahc_patch0_func, 751, 1, 1 },
+	{ ahc_patch13_func, 756, 4, 2 },
+	{ ahc_patch0_func, 760, 7, 3 },
+	{ ahc_patch21_func, 760, 5, 2 },
+	{ ahc_patch0_func, 765, 2, 1 },
+	{ ahc_patch0_func, 770, 40, 3 },
+	{ ahc_patch16_func, 782, 16, 2 },
+	{ ahc_patch0_func, 798, 4, 1 },
+	{ ahc_patch4_func, 822, 3, 2 },
+	{ ahc_patch0_func, 825, 1, 1 },
+	{ ahc_patch4_func, 826, 12, 1 }
+};
+struct cs {
+	u_int16_t	begin;
+	u_int16_t	end;
+} critical_sections[] = {
+	{ 11, 15 },
+	{ 20, 28 },
+	{ 679, 695 },
+	{ 822, 825 },
+	{ 826, 832 },
+	{ 834, 836 },
+	{ 836, 838 }
+};
+const int num_critical_sections = sizeof(critical_sections)
+				 / sizeof(*critical_sections);
diff -urN linux/drivers/scsi/aic7xxx/aicasm/Makefile /tmp/linux/drivers/scsi/aic7xxx/aicasm/Makefile
--- linux/drivers/scsi/aic7xxx/aicasm/Makefile	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aicasm/Makefile	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,37 @@
+PROG=	aicasm
+
+CSRCS=	aicasm.c aicasm_symbol.c
+GENSRCS= aicasm_gram.c aicasm_scan.c
+
+GENHDRS=	y.tab.h
+
+SRCS=	${GENSRCS} ${CSRCS}
+CLEANFILES= ${GENSRCS} ${GENHDRS} y.output
+CFLAGS= -I/usr/include -ldb1
+YFLAGS= -d
+
+NOMAN=	noman
+
+ifdef DEBUG
+CFLAGS+= -DDEBUG -g
+YFLAGS+= -t -v
+LFLAGS= -d
+endif
+
+.SUFFIXES= .l .y .c
+
+$(PROG): $(SRCS)
+
+clean:
+	rm -f $(CLEANFILES) $(PROG)
+.y.o:
+	$(YACC) $(YFLAGS) $(.IMPSRC)
+	$(CC) $(CFLAGS) -c y.tab.c
+	rm -f y.tab.c
+	mv y.tab.o $(.TARGET)
+
+.l.o:
+	$(LEX) $(LFLAGS) $(.IMPSRC)
+	$(CC) $(CFLAGS) -c lex.yy.c
+	rm -f lex.yy.c
+	mv lex.yy.o $(.TARGET)
diff -urN linux/drivers/scsi/aic7xxx/aicasm/aicasm.c /tmp/linux/drivers/scsi/aic7xxx/aicasm/aicasm.c
--- linux/drivers/scsi/aic7xxx/aicasm/aicasm.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aicasm/aicasm.c	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,777 @@
+/*
+ * Aic7xxx SCSI host adapter firmware asssembler
+ *
+ * Copyright (c) 1997, 1998, 2000 Justin T. Gibbs.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification, immediately at the beginning of the file.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $Id: //depot/src/aic7xxx/aicasm/aicasm.c#6 $
+ *
+ * $FreeBSD: src/sys/dev/aic7xxx/aicasm/aicasm.c,v 1.29 2000/10/05 04:25:42 gibbs Exp $
+ */
+#include <sys/types.h>
+#include <sys/mman.h>
+
+#include <ctype.h>
+#include <inttypes.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <sysexits.h>
+#include <unistd.h>
+
+#include "aicasm.h"
+#include "aicasm_symbol.h"
+#include "aicasm_insformat.h"
+
+typedef struct patch {
+	STAILQ_ENTRY(patch) links;
+	int		patch_func;
+	u_int		begin;
+	u_int		skip_instr;
+	u_int		skip_patch;
+} patch_t;
+
+STAILQ_HEAD(patch_list, patch) patches;
+
+static void usage(void);
+static void back_patch(void);
+static void output_code(void);
+static void output_listing(char *ifilename);
+static void dump_scope(scope_t *scope);
+static void emit_patch(scope_t *scope, int patch);
+static int check_patch(patch_t **start_patch, int start_instr,
+		       int *skip_addr, int *func_vals);
+
+struct path_list search_path;
+int includes_search_curdir;
+char *appname;
+FILE *ofile;
+char *ofilename;
+char *regfilename;
+FILE *regfile;
+char *listfilename;
+FILE *listfile;
+
+static STAILQ_HEAD(,instruction) seq_program;
+struct cs_tailq cs_tailq;
+struct scope_list scope_stack;
+symlist_t patch_functions;
+
+#if DEBUG
+extern int yy_flex_debug;
+extern int yydebug;
+#endif
+extern FILE *yyin;
+extern int yyparse(void);
+
+int main(int argc, char *argv[]);
+
+int
+main(int argc, char *argv[])
+{
+	extern char *optarg;
+	extern int optind;
+	int  ch;
+	int  retval;
+	char *inputfilename;
+	scope_t *sentinal;
+
+	STAILQ_INIT(&patches);
+	SLIST_INIT(&search_path);
+	STAILQ_INIT(&seq_program);
+	TAILQ_INIT(&cs_tailq);
+	SLIST_INIT(&scope_stack);
+
+	/* Set Sentinal scope node */
+	sentinal = scope_alloc();
+	sentinal->type = SCOPE_ROOT;
+	
+	includes_search_curdir = 1;
+	appname = *argv;
+	regfile = NULL;
+	listfile = NULL;
+#if DEBUG
+	yy_flex_debug = 0;
+	yydebug = 0;
+#endif
+	while ((ch = getopt(argc, argv, "d:l:n:o:r:I:O:")) != -1) {
+		switch(ch) {
+		case 'd':
+#if DEBUG
+			if (strcmp(optarg, "s") == 0) {
+				yy_flex_debug = 1;
+			} else if (strcmp(optarg, "p") == 0) {
+				yydebug = 1;
+			} else {
+				fprintf(stderr, "%s: -d Requires either an "
+					"'s' or 'p' argument\n", appname);
+				usage();
+			}
+#else
+			stop("-d: Assembler not built with debugging "
+			     "information", EX_SOFTWARE);
+#endif
+			break;
+		case 'l':
+			/* Create a program listing */
+			if ((listfile = fopen(optarg, "w")) == NULL) {
+				perror(optarg);
+				stop(NULL, EX_CANTCREAT);
+			}
+			listfilename = optarg;
+			break;
+		case 'n':
+			/* Don't complain about the -nostdinc directrive */
+			if (strcmp(optarg, "ostdinc")) {
+				fprintf(stderr, "%s: Unknown option -%c%s\n",
+					appname, ch, optarg);
+				usage();
+				/* NOTREACHED */
+			}
+			break;
+		case 'o':
+			if ((ofile = fopen(optarg, "w")) == NULL) {
+				perror(optarg);
+				stop(NULL, EX_CANTCREAT);
+			}
+			ofilename = optarg;
+			break;
+		case 'r':
+			if ((regfile = fopen(optarg, "w")) == NULL) {
+				perror(optarg);
+				stop(NULL, EX_CANTCREAT);
+			}
+			regfilename = optarg;
+			break;
+		case 'I':
+		{
+			path_entry_t include_dir;
+
+			if (strcmp(optarg, "-") == 0) {
+				if (includes_search_curdir == 0) {
+					fprintf(stderr, "%s: Warning - '-I-' "
+							"specified multiple "
+							"times\n", appname);
+				}
+				includes_search_curdir = 0;
+				for (include_dir = search_path.slh_first;
+				     include_dir != NULL;
+				     include_dir = include_dir->links.sle_next)
+					/*
+					 * All entries before a '-I-' only
+					 * apply to includes specified with
+					 * quotes instead of "<>".
+					 */
+					include_dir->quoted_includes_only = 1;
+			} else {
+				include_dir =
+				    (path_entry_t)malloc(sizeof(*include_dir));
+				if (include_dir == NULL) {
+					perror(optarg);
+					stop(NULL, EX_OSERR);
+				}
+				include_dir->directory = strdup(optarg);
+				if (include_dir->directory == NULL) {
+					perror(optarg);
+					stop(NULL, EX_OSERR);
+				}
+				include_dir->quoted_includes_only = 0;
+				SLIST_INSERT_HEAD(&search_path, include_dir,
+						  links);
+			}
+			break;
+		}
+		case '?':
+		default:
+			usage();
+			/* NOTREACHED */
+		}
+	}
+	argc -= optind;
+	argv += optind;
+
+	if (argc != 1) {
+		fprintf(stderr, "%s: No input file specifiled\n", appname);
+		usage();
+		/* NOTREACHED */
+	}
+
+	symtable_open();
+	inputfilename = *argv;
+	include_file(*argv, SOURCE_FILE);
+	retval = yyparse();
+	if (retval == 0) {
+		if (SLIST_FIRST(&scope_stack) == NULL
+		 || SLIST_FIRST(&scope_stack)->type != SCOPE_ROOT) {
+			stop("Unterminated conditional expression",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+
+		/* Process outmost scope */
+		process_scope(SLIST_FIRST(&scope_stack));
+		/*
+		 * Decend the tree of scopes and insert/emit
+		 * patches as appropriate.  We perform a depth first
+		 * tranversal, recursively handling each scope.
+		 */
+		/* start at the root scope */
+		dump_scope(SLIST_FIRST(&scope_stack));
+
+		/* Patch up forward jump addresses */
+		back_patch();
+
+		if (ofile != NULL)
+			output_code();
+		if (regfile != NULL) {
+			symtable_dump(regfile);
+		}
+		if (listfile != NULL)
+			output_listing(inputfilename);
+	}
+
+	stop(NULL, 0);
+	/* NOTREACHED */
+	return (0);
+}
+
+static void
+usage()
+{
+
+	(void)fprintf(stderr,
+"usage: %-16s [-nostdinc] [-I-] [-I directory] [-o output_file]
+			[-r register_output_file] [-l program_list_file]
+			input_file\n",
+			appname);
+	exit(EX_USAGE);
+}
+
+static void
+back_patch()
+{
+	struct instruction *cur_instr;
+
+	for(cur_instr = seq_program.stqh_first;
+	    cur_instr != NULL;
+	    cur_instr = cur_instr->links.stqe_next) {
+		if (cur_instr->patch_label != NULL) {
+			struct ins_format3 *f3_instr;
+			u_int address;
+
+			if (cur_instr->patch_label->type != LABEL) {
+				char buf[255];
+
+				snprintf(buf, sizeof(buf),
+					 "Undefined label %s",
+					 cur_instr->patch_label->name);
+				stop(buf, EX_DATAERR);
+				/* NOTREACHED */
+			}
+			f3_instr = &cur_instr->format.format3;
+			address = f3_instr->address;
+			address += cur_instr->patch_label->info.linfo->address;
+			f3_instr->address = address;
+		}
+	}
+}
+
+static void
+output_code()
+{
+	struct instruction *cur_instr;
+	patch_t *cur_patch;
+	critical_section_t *cs;
+	symbol_node_t *cur_node;
+	int instrcount;
+
+	instrcount = 0;
+	fprintf(ofile,
+"/*
+  * DO NOT EDIT - This file is automatically generated.
+  */\n");
+
+	fprintf(ofile, "static uint8_t seqprog[] = {\n");
+	for(cur_instr = seq_program.stqh_first;
+	    cur_instr != NULL;
+	    cur_instr = cur_instr->links.stqe_next) {
+
+		fprintf(ofile, "%s\t0x%02x, 0x%02x, 0x%02x, 0x%02x",
+			cur_instr == seq_program.stqh_first ? "" : ",\n",
+#if BYTE_ORDER == LITTLE_ENDIAN
+			cur_instr->format.bytes[0],
+			cur_instr->format.bytes[1],
+			cur_instr->format.bytes[2],
+			cur_instr->format.bytes[3]);
+#else
+			cur_instr->format.bytes[3],
+			cur_instr->format.bytes[2],
+			cur_instr->format.bytes[1],
+			cur_instr->format.bytes[0]);
+#endif
+		instrcount++;
+	}
+	fprintf(ofile, "\n};\n\n");
+
+	/*
+	 *  Output patch information.  Patch functions first.
+	 */
+	for(cur_node = SLIST_FIRST(&patch_functions);
+	    cur_node != NULL;
+	    cur_node = SLIST_NEXT(cur_node,links)) {
+		fprintf(ofile,
+"static int ahc_patch%d_func(struct ahc_softc *ahc);
+
+static int
+ahc_patch%d_func(struct ahc_softc *ahc)
+{
+	return (%s);
+}\n\n",
+			cur_node->symbol->info.condinfo->func_num,
+			cur_node->symbol->info.condinfo->func_num,
+			cur_node->symbol->name);
+	}
+
+	fprintf(ofile,
+"typedef int patch_func_t (struct ahc_softc *);
+struct patch {
+	patch_func_t	*patch_func;
+	uint32_t	begin	   :10,
+			skip_instr :10,
+			skip_patch :12;
+} patches[] = {\n");
+
+	for(cur_patch = STAILQ_FIRST(&patches);
+	    cur_patch != NULL;
+	    cur_patch = STAILQ_NEXT(cur_patch,links)) {
+		fprintf(ofile, "%s\t{ ahc_patch%d_func, %d, %d, %d }",
+			cur_patch == STAILQ_FIRST(&patches) ? "" : ",\n",
+			cur_patch->patch_func, cur_patch->begin,
+			cur_patch->skip_instr, cur_patch->skip_patch);
+	}
+
+	fprintf(ofile, "\n};\n");
+
+	fprintf(ofile,
+"struct cs {
+	u_int16_t	begin;
+	u_int16_t	end;
+} critical_sections[] = {\n");
+
+	for(cs = TAILQ_FIRST(&cs_tailq);
+	    cs != NULL;
+	    cs = TAILQ_NEXT(cs, links)) {
+		fprintf(ofile, "%s\t{ %d, %d }",
+			cs == TAILQ_FIRST(&cs_tailq) ? "" : ",\n",
+			cs->begin_addr, cs->end_addr);
+	}
+
+	fprintf(ofile, "\n};\n");
+
+	fprintf(ofile,
+"const int num_critical_sections = sizeof(critical_sections)
+				 / sizeof(*critical_sections);\n");
+
+	fprintf(stderr, "%s: %d instructions used\n", appname, instrcount);
+}
+
+static void
+dump_scope(scope_t *scope)
+{
+	scope_t *cur_scope;
+
+	/*
+	 * Emit the first patch for this scope
+	 */
+	emit_patch(scope, 0);
+
+	/*
+	 * Dump each scope within this one.
+	 */
+	cur_scope = TAILQ_FIRST(&scope->inner_scope);
+
+	while (cur_scope != NULL) {
+
+		dump_scope(cur_scope);
+
+		cur_scope = TAILQ_NEXT(cur_scope, scope_links);
+	}
+
+	/*
+	 * Emit the second, closing, patch for this scope
+	 */
+	emit_patch(scope, 1);
+}
+
+void
+emit_patch(scope_t *scope, int patch)
+{
+	patch_info_t *pinfo;
+	patch_t *new_patch;
+
+	pinfo = &scope->patches[patch];
+
+	if (pinfo->skip_instr == 0)
+		/* No-Op patch */
+		return;
+
+	new_patch = (patch_t *)malloc(sizeof(*new_patch));
+
+	if (new_patch == NULL)
+		stop("Could not malloc patch structure", EX_OSERR);
+
+	memset(new_patch, 0, sizeof(*new_patch));
+
+	if (patch == 0) {
+		new_patch->patch_func = scope->func_num;
+		new_patch->begin = scope->begin_addr;
+	} else {
+		new_patch->patch_func = 0;
+		new_patch->begin = scope->end_addr;
+	}
+	new_patch->skip_instr = pinfo->skip_instr;
+	new_patch->skip_patch = pinfo->skip_patch;
+	STAILQ_INSERT_TAIL(&patches, new_patch, links);
+}
+
+void
+output_listing(char *ifilename)
+{
+	char buf[1024];
+	FILE *ifile;
+	struct instruction *cur_instr;
+	patch_t *cur_patch;
+	symbol_node_t *cur_func;
+	int *func_values;
+	int instrcount;
+	int instrptr;
+	int line;
+	int func_count;
+	int skip_addr;
+
+	instrcount = 0;
+	instrptr = 0;
+	line = 1;
+	skip_addr = 0;
+	if ((ifile = fopen(ifilename, "r")) == NULL) {
+		perror(ifilename);
+		stop(NULL, EX_DATAERR);
+	}
+
+	/*
+	 * Determine which options to apply to this listing.
+	 */
+	for (func_count = 0, cur_func = SLIST_FIRST(&patch_functions);
+	    cur_func != NULL;
+	    cur_func = SLIST_NEXT(cur_func, links))
+		func_count++;
+
+	func_values = NULL;
+	if (func_count != 0) {
+		func_values = (int *)malloc(func_count * sizeof(int));
+
+		if (func_values == NULL)
+			stop("Could not malloc", EX_OSERR);
+		
+		func_values[0] = 0; /* FALSE func */
+		func_count--;
+
+		/*
+		 * Ask the user to fill in the return values for
+		 * the rest of the functions.
+		 */
+		
+		
+		for (cur_func = SLIST_FIRST(&patch_functions);
+		     cur_func != NULL && SLIST_NEXT(cur_func, links) != NULL;
+		     cur_func = SLIST_NEXT(cur_func, links), func_count--) {
+			int input;
+			
+			fprintf(stdout, "\n(%s)\n", cur_func->symbol->name);
+			fprintf(stdout,
+				"Enter the return value for "
+				"this expression[T/F]:");
+
+			while (1) {
+
+				input = getchar();
+				input = toupper(input);
+
+				if (input == 'T') {
+					func_values[func_count] = 1;
+					break;
+				} else if (input == 'F') {
+					func_values[func_count] = 0;
+					break;
+				}
+			}
+			if (isatty(fileno(stdin)) == 0)
+				putchar(input);
+		}
+		fprintf(stdout, "\nThanks!\n");
+	}
+
+	/* Now output the listing */
+	cur_patch = STAILQ_FIRST(&patches);
+	for(cur_instr = STAILQ_FIRST(&seq_program);
+	    cur_instr != NULL;
+	    cur_instr = STAILQ_NEXT(cur_instr, links), instrcount++) {
+
+		if (check_patch(&cur_patch, instrcount,
+				&skip_addr, func_values) == 0) {
+			/* Don't count this instruction as it is in a patch
+			 * that was removed.
+			 */
+                        continue;
+		}
+
+		while (line < cur_instr->srcline) {
+			fgets(buf, sizeof(buf), ifile);
+				fprintf(listfile, "\t\t%s", buf);
+				line++;
+		}
+		fprintf(listfile, "%03x %02x%02x%02x%02x", instrptr,
+#if BYTE_ORDER == LITTLE_ENDIAN
+			cur_instr->format.bytes[0],
+			cur_instr->format.bytes[1],
+			cur_instr->format.bytes[2],
+			cur_instr->format.bytes[3]);
+#else
+			cur_instr->format.bytes[3],
+			cur_instr->format.bytes[2],
+			cur_instr->format.bytes[1],
+			cur_instr->format.bytes[0]);
+#endif
+		fgets(buf, sizeof(buf), ifile);
+		fprintf(listfile, "\t%s", buf);
+		line++;
+		instrptr++;
+	}
+	/* Dump the remainder of the file */
+	while(fgets(buf, sizeof(buf), ifile) != NULL)
+		fprintf(listfile, "\t\t%s", buf);
+
+	fclose(ifile);
+}
+
+static int
+check_patch(patch_t **start_patch, int start_instr,
+	    int *skip_addr, int *func_vals)
+{
+	patch_t *cur_patch;
+
+	cur_patch = *start_patch;
+
+	while (cur_patch != NULL && start_instr == cur_patch->begin) {
+		if (func_vals[cur_patch->patch_func] == 0) {
+			int skip;
+
+			/* Start rejecting code */
+			*skip_addr = start_instr + cur_patch->skip_instr;
+			for (skip = cur_patch->skip_patch;
+			     skip > 0 && cur_patch != NULL;
+			     skip--)
+				cur_patch = STAILQ_NEXT(cur_patch, links);
+		} else {
+			/* Accepted this patch.  Advance to the next
+			 * one and wait for our intruction pointer to
+			 * hit this point.
+			 */
+			cur_patch = STAILQ_NEXT(cur_patch, links);
+		}
+	}
+
+	*start_patch = cur_patch;
+	if (start_instr < *skip_addr)
+		/* Still skipping */
+		return (0);
+
+	return (1);
+}
+
+/*
+ * Print out error information if appropriate, and clean up before
+ * terminating the program.
+ */
+void
+stop(const char *string, int err_code)
+{
+	if (string != NULL) {
+		fprintf(stderr, "%s: ", appname);
+		if (yyfilename != NULL) {
+			fprintf(stderr, "Stopped at file %s, line %d - ",
+				yyfilename, yylineno);
+		}
+		fprintf(stderr, "%s\n", string);
+	}
+
+	if (ofile != NULL) {
+		fclose(ofile);
+		if (err_code != 0) {
+			fprintf(stderr, "%s: Removing %s due to error\n",
+				appname, ofilename);
+			unlink(ofilename);
+		}
+	}
+
+	if (regfile != NULL) {
+		fclose(regfile);
+		if (err_code != 0) {
+			fprintf(stderr, "%s: Removing %s due to error\n",
+				appname, regfilename);
+			unlink(regfilename);
+		}
+	}
+
+	if (listfile != NULL) {
+		fclose(listfile);
+		if (err_code != 0) {
+			fprintf(stderr, "%s: Removing %s due to error\n",
+				appname, listfilename);
+			unlink(listfilename);
+		}
+	}
+
+	symlist_free(&patch_functions);
+	symtable_close();
+
+	exit(err_code);
+}
+
+struct instruction *
+seq_alloc()
+{
+	struct instruction *new_instr;
+
+	new_instr = (struct instruction *)malloc(sizeof(struct instruction));
+	if (new_instr == NULL)
+		stop("Unable to malloc instruction object", EX_SOFTWARE);
+	memset(new_instr, 0, sizeof(*new_instr));
+	STAILQ_INSERT_TAIL(&seq_program, new_instr, links);
+	new_instr->srcline = yylineno;
+	return new_instr;
+}
+
+critical_section_t *
+cs_alloc()
+{
+	critical_section_t *new_cs;
+
+	new_cs= (critical_section_t *)malloc(sizeof(critical_section_t));
+	if (new_cs == NULL)
+		stop("Unable to malloc critical_section object", EX_SOFTWARE);
+	memset(new_cs, 0, sizeof(*new_cs));
+	
+	TAILQ_INSERT_TAIL(&cs_tailq, new_cs, links);
+	return new_cs;
+}
+
+scope_t *
+scope_alloc()
+{
+	scope_t *new_scope;
+
+	new_scope = (scope_t *)malloc(sizeof(scope_t));
+	if (new_scope == NULL)
+		stop("Unable to malloc scope object", EX_SOFTWARE);
+	memset(new_scope, 0, sizeof(*new_scope));
+	TAILQ_INIT(&new_scope->inner_scope);
+	
+	if (SLIST_FIRST(&scope_stack) != NULL) {
+		TAILQ_INSERT_TAIL(&SLIST_FIRST(&scope_stack)->inner_scope,
+				  new_scope, scope_links);
+	}
+	/* This patch is now the current scope */
+	SLIST_INSERT_HEAD(&scope_stack, new_scope, scope_stack_links);
+	return new_scope;
+}
+
+void
+process_scope(scope_t *scope)
+{
+	/*
+	 * We are "leaving" this scope.  We should now have
+	 * enough information to process the lists of scopes
+	 * we encapsulate.
+	 */
+	scope_t *cur_scope;
+	u_int skip_patch_count;
+	u_int skip_instr_count;
+
+	cur_scope = TAILQ_LAST(&scope->inner_scope, scope_tailq);
+	skip_patch_count = 0;
+	skip_instr_count = 0;
+	while (cur_scope != NULL) {
+		u_int patch0_patch_skip;
+
+		patch0_patch_skip = 0;
+		switch (cur_scope->type) {
+		case SCOPE_IF:
+		case SCOPE_ELSE_IF:
+			if (skip_instr_count != 0) {
+				/* Create a tail patch */
+				patch0_patch_skip++;
+				cur_scope->patches[1].skip_patch =
+				    skip_patch_count + 1;
+				cur_scope->patches[1].skip_instr =
+				    skip_instr_count;
+			}
+
+			/* Count Head patch */
+			patch0_patch_skip++;
+
+			/* Count any patches contained in our inner scope */
+			patch0_patch_skip += cur_scope->inner_scope_patches;
+
+			cur_scope->patches[0].skip_patch = patch0_patch_skip;
+			cur_scope->patches[0].skip_instr =
+			    cur_scope->end_addr - cur_scope->begin_addr;
+
+			skip_instr_count += cur_scope->patches[0].skip_instr;
+
+			skip_patch_count += patch0_patch_skip;
+			if (cur_scope->type == SCOPE_IF) {
+				scope->inner_scope_patches += skip_patch_count;
+				skip_patch_count = 0;
+			        skip_instr_count = 0;
+			}
+			break;
+		case SCOPE_ELSE:
+			/* Count any patches contained in our innter scope */
+			skip_patch_count += cur_scope->inner_scope_patches;
+
+			skip_instr_count += cur_scope->end_addr
+					  - cur_scope->begin_addr;
+			break;
+		case SCOPE_ROOT:
+			stop("Unexpected scope type encountered", EX_SOFTWARE);
+			/* NOTREACHED */
+		}
+
+		cur_scope = TAILQ_PREV(cur_scope, scope_tailq, scope_links);
+	}
+}
diff -urN linux/drivers/scsi/aic7xxx/aicasm/aicasm.h /tmp/linux/drivers/scsi/aic7xxx/aicasm/aicasm.h
--- linux/drivers/scsi/aic7xxx/aicasm/aicasm.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aicasm/aicasm.h	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,78 @@
+/*
+ * Assembler for the sequencer program downloaded to Aic7xxx SCSI host adapters
+ *
+ * Copyright (c) 1997 Justin T. Gibbs.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $Id: //depot/src/aic7xxx/aicasm/aicasm.h#4 $
+ *
+ * $FreeBSD: src/sys/dev/aic7xxx/aicasm/aicasm.h,v 1.11 2000/09/22 22:19:54 gibbs Exp $
+ */
+
+#ifdef __linux__
+#include "../queue.h"
+#else
+#include <sys/queue.h>
+#endif
+
+#ifndef TRUE
+#define TRUE 1
+#endif
+
+#ifndef FALSE
+#define FALSE 0
+#endif
+
+typedef struct path_entry {
+	char	*directory;
+	int	quoted_includes_only;
+	SLIST_ENTRY(path_entry) links;
+} *path_entry_t;
+
+typedef enum {  
+	QUOTED_INCLUDE,
+	BRACKETED_INCLUDE,
+	SOURCE_FILE
+} include_type;
+
+SLIST_HEAD(path_list, path_entry);
+
+extern struct path_list search_path;
+extern struct cs_tailq cs_tailq;
+extern struct scope_list scope_stack;
+extern struct symlist patch_functions;
+extern int includes_search_curdir;		/* False if we've seen -I- */
+extern char *appname;
+extern int yylineno;
+extern char *yyfilename;
+
+void stop(const char *errstring, int err_code);
+void include_file(char *file_name, include_type type);
+struct instruction *seq_alloc(void);
+struct critical_section *cs_alloc(void);
+struct scope *scope_alloc(void);
+void process_scope(struct scope *);
diff -urN linux/drivers/scsi/aic7xxx/aicasm/aicasm_gram.c /tmp/linux/drivers/scsi/aic7xxx/aicasm/aicasm_gram.c
--- linux/drivers/scsi/aic7xxx/aicasm/aicasm_gram.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aicasm/aicasm_gram.c	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,2058 @@
+#ifndef lint
+static char yysccsid[] = "@(#)yaccpar	1.9 (Berkeley) 02/21/93";
+#endif
+#define YYBYACC 1
+#define YYMAJOR 1
+#define YYMINOR 9
+#define yyclearin (yychar=(-1))
+#define yyerrok (yyerrflag=0)
+#define YYRECOVERING (yyerrflag!=0)
+#define YYPREFIX "yy"
+#line 2 "aicasm_gram.y"
+/*
+ * Parser for the Aic7xxx SCSI Host adapter sequencer assembler.
+ *
+ * Copyright (c) 1997, 1998, 2000 Justin T. Gibbs.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $Id: //depot/src/aic7xxx/aicasm/aicasm_gram.y#6 $
+ *
+ * $FreeBSD: src/sys/dev/aic7xxx/aicasm/aicasm_gram.y,v 1.12 2000/10/31 18:44:32 gibbs Exp $
+ */
+
+#include <inttypes.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <sysexits.h>
+
+#include <sys/types.h>
+#ifdef __linux__
+#include "../queue.h"
+#else
+#include <sys/queue.h>
+#endif
+
+#include "aicasm.h"
+#include "aicasm_symbol.h"
+#include "aicasm_insformat.h"
+
+int yylineno;
+char *yyfilename;
+static symbol_t *cur_symbol;
+static symtype cur_symtype;
+static symbol_t *accumulator;
+static symbol_ref_t allones;
+static symbol_ref_t allzeros;
+static symbol_ref_t none;
+static symbol_ref_t sindex;
+static int instruction_ptr;
+static int sram_or_scb_offset;
+static int download_constant_count;
+static int in_critical_section;
+
+static void process_bitmask(int mask_type, symbol_t *sym, int mask);
+static void initialize_symbol(symbol_t *symbol);
+static void process_register(symbol_t **p_symbol);
+static void format_1_instr(int opcode, symbol_ref_t *dest,
+			   expression_t *immed, symbol_ref_t *src, int ret);
+static void format_2_instr(int opcode, symbol_ref_t *dest,
+			   expression_t *places, symbol_ref_t *src, int ret);
+static void format_3_instr(int opcode, symbol_ref_t *src,
+			   expression_t *immed, symbol_ref_t *address);
+static void test_readable_symbol(symbol_t *symbol);
+static void test_writable_symbol(symbol_t *symbol);
+static void type_check(symbol_t *symbol, expression_t *expression, int and_op);
+static void make_expression(expression_t *immed, int value);
+static void add_conditional(symbol_t *symbol);
+static int  is_download_const(expression_t *immed);
+
+#define YYDEBUG 1
+#define SRAM_SYMNAME "SRAM_BASE"
+#define SCB_SYMNAME "SCB_BASE"
+#line 89 "aicasm_gram.y"
+typedef union {
+	int		value;
+	char		*str;
+	symbol_t	*sym;
+	symbol_ref_t	sym_ref;
+	expression_t	expression;
+} YYSTYPE;
+#line 106 "y.tab.c"
+#define T_REGISTER 257
+#define T_CONST 258
+#define T_DOWNLOAD 259
+#define T_SCB 260
+#define T_SRAM 261
+#define T_ALIAS 262
+#define T_SIZE 263
+#define T_ADDRESS 264
+#define T_ACCESS_MODE 265
+#define T_MODE 266
+#define T_BEGIN_CS 267
+#define T_END_CS 268
+#define T_BIT 269
+#define T_MASK 270
+#define T_NUMBER 271
+#define T_PATH 272
+#define T_CEXPR 273
+#define T_EOF 274
+#define T_INCLUDE 275
+#define T_SHR 276
+#define T_SHL 277
+#define T_ROR 278
+#define T_ROL 279
+#define T_MVI 280
+#define T_MOV 281
+#define T_CLR 282
+#define T_BMOV 283
+#define T_JMP 284
+#define T_JC 285
+#define T_JNC 286
+#define T_JE 287
+#define T_JNE 288
+#define T_JNZ 289
+#define T_JZ 290
+#define T_CALL 291
+#define T_ADD 292
+#define T_ADC 293
+#define T_INC 294
+#define T_DEC 295
+#define T_STC 296
+#define T_CLC 297
+#define T_CMP 298
+#define T_NOT 299
+#define T_XOR 300
+#define T_TEST 301
+#define T_AND 302
+#define T_OR 303
+#define T_RET 304
+#define T_NOP 305
+#define T_ACCUM 306
+#define T_ALLONES 307
+#define T_ALLZEROS 308
+#define T_NONE 309
+#define T_SINDEX 310
+#define T_A 311
+#define T_SYMBOL 312
+#define T_NL 313
+#define T_IF 314
+#define T_ELSE 315
+#define T_ELSE_IF 316
+#define T_ENDIF 317
+#define UMINUS 318
+#define YYERRCODE 256
+short yylhs[] = {                                        -1,
+    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+   16,   16,   27,   17,   29,   26,   28,   28,   30,   30,
+   30,   30,   30,   30,   30,   30,   30,   30,   30,   31,
+   32,   33,   34,   35,   36,   37,   38,   39,   40,   41,
+    6,    6,    6,    6,    6,    6,    6,    6,    6,   18,
+   18,   15,   15,   42,   44,   19,   45,   46,   20,   43,
+   43,    1,    1,    1,    1,    3,    7,    8,    8,    4,
+    5,    5,    9,    9,   22,   23,   21,    2,    2,    2,
+    2,    2,    2,   24,   24,   24,   24,   10,   10,   10,
+   10,   25,   25,   25,   25,   25,   25,   25,   25,   25,
+   25,   25,   25,   25,   25,   25,   11,   11,   11,   11,
+   25,   12,   12,   12,   12,   13,   13,   14,   14,   25,
+   25,   25,   25,   25,   25,
+};
+short yylen[] = {                                         2,
+    1,    2,    1,    2,    1,    2,    1,    2,    1,    2,
+    1,    2,    1,    2,    1,    2,    1,    2,    1,    2,
+    4,    4,    0,    3,    0,    5,    1,    2,    1,    1,
+    1,    1,    1,    1,    1,    1,    1,    1,    1,    2,
+    2,    2,    3,    3,    2,    1,    1,    1,    1,    1,
+    3,    3,    3,    3,    3,    2,    2,    1,    1,    3,
+    3,    1,    2,    0,    0,    7,    0,    0,    7,    1,
+    2,    1,    4,    4,    1,    1,    1,    1,    1,    1,
+    0,    2,    0,    1,    1,    1,    2,    1,    3,    3,
+    1,    3,    3,    3,    4,    2,    1,    1,    1,    1,
+    1,    7,    7,    5,    5,    3,    7,    3,    4,    8,
+    6,    6,    5,    4,    3,    2,    1,    1,    1,    1,
+    7,    1,    1,    1,    1,    1,    1,    1,    1,    3,
+    7,    7,    7,    5,    5,
+};
+short yydefred[] = {                                      0,
+   23,    0,    0,    0,   85,   86,    0,  118,  117,  120,
+  119,    0,    0,    0,    0,  122,  123,  124,  125,  100,
+  101,    0,    0,    0,    0,    0,    0,   99,    0,   98,
+    0,    0,    0,    0,    0,    0,   97,    0,    0,    0,
+    0,    1,    3,    5,    7,    9,   11,   13,   15,   17,
+   19,    0,    0,   67,   64,    0,    0,   58,   75,    0,
+    0,    0,    0,   76,    0,    0,    0,    0,    0,    0,
+    0,    0,    0,    0,    0,   84,    0,    0,    0,    0,
+   80,    0,    0,    0,    0,  116,    0,   87,    0,    0,
+   96,    2,    4,    6,    8,   10,   12,   14,   16,   18,
+   20,    0,    0,    0,    0,    0,    0,   24,   61,   62,
+    0,   60,    0,    0,    0,    0,    0,   59,   57,   56,
+    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,    0,    0,    0,  108,    0,  106,    0,    0,
+    0,    0,  115,   94,    0,    0,    0,    0,    0,    0,
+    0,  130,   25,   63,    0,   68,   65,   21,   22,    0,
+    0,   55,   79,    0,    0,    0,    0,   53,   54,    0,
+    0,    0,  114,    0,   82,    0,    0,  109,    0,    0,
+    0,    0,    0,    0,    0,   95,    0,    0,   89,   90,
+   92,   93,    0,   40,    0,    0,   74,   73,    0,  135,
+    0,  134,    0,  104,  105,    0,  128,  129,    0,  113,
+  127,  126,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,   46,   47,   48,   49,   50,    0,   27,   29,
+   30,   31,   32,   33,   34,   35,   36,   37,   38,   39,
+   70,    0,    0,  112,  111,    0,    0,    0,    0,    0,
+    0,    0,    0,   45,   41,   42,    0,    0,   26,   28,
+   69,   71,   66,    0,  107,  133,  132,  131,  103,  102,
+  121,   43,    0,  110,
+};
+short yydgoto[] = {                                      38,
+   64,  106,   65,   71,  133,  164,   67,  165,   78,   39,
+   40,   41,  213,  209,  112,   42,   43,   44,   45,   46,
+   47,   48,   49,   50,   51,  241,   52,  228,  193,  229,
+  230,  231,  232,  233,  234,  235,  236,  237,  238,  239,
+  240,  114,  242,  196,  113,  195,
+};
+short yysindex[] = {                                    186,
+    0, -277,  -80,  -71,    0,    0,   24,    0,    0,    0,
+    0,  -36, -245, -245, -245,    0,    0,    0,    0,    0,
+    0, -245, -245, -240, -253, -245, -245,    0, -245,    0,
+ -245,    1, -241,   17, -195, -115,    0,  186, -245, -245,
+  -45,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+    0, -220,  -43,    0,    0, -177, -153,    0,    0,   30,
+  -35,  -35,  -35,    0,   76,    2, -185,   30,    0,   81,
+ -185, -241,   84,   85,   85,    0, -241,   79, -245,   87,
+    0,   91,   85,  103,  109,    0,   96,    0,   33, -116,
+    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,  114,  115,   40,   53,  102,   39,    0,    0,    0,
+ -108,    0, -100, -100,  104,  131, -230,    0,    0,    0,
+   -7,  -33,  -35,  -35,  -35,  -35,  -45, -245,  -45,  112,
+ -245, -245, -241, -241,  113,    0,  129,    0,  -33, -241,
+  -33,  -33,    0,    0,   54,  -33,  -35,  -93,  -91,  -90,
+  -89,    0,    0,    0,  -88,    0,    0,    0,    0,   83,
+   93,    0,    0,    2, -241,   10,   73,    0,    0,  125,
+ -241,  126,    0,  143,    0,  130,  132,    0,  -33, -180,
+  133, -187,    2, -185,   85,    0,   85,   36,    0,    0,
+    0,    0,   78,    0, -220, -220,    0,    0,  134,    0,
+  135,    0,  -35,    0,    0, -241,    0,    0,  -45,    0,
+    0,    0,  -45,  -45, -241, -241, -241, -124,  -81,  -70,
+ -117, -112,    0,    0,    0,    0,    0,  -95,    0,    0,
+    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+    0, -105, -103,    0,    0, -241,  138,  139,  142,  146,
+  147,  149,  159,    0,    0,    0,  -69,  -35,    0,    0,
+    0,    0,    0,  160,    0,    0,    0,    0,    0,    0,
+    0,    0,    2,    0,
+};
+short yyrindex[] = {                                      0,
+    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,    0,  161,  161,    0,    0,    0,    0,    0,
+    0,    0,  161,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,    0,    0,    0,    0,    0,    0,    0,  -27,
+    0,    0,    0,    0,    0,   45,    0,  -30,   32,    0,
+    0,  161,    0,  -44,  -44,    0,  161,    0,    0,    0,
+    0,    0,  -44,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,    0,  162,  163,    0,    0,    0,    0,    0,
+    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,  161,  161,    0,    0,    0,    0,    0,  161,
+    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,    0,    6,  161,   18,  -38,    0,    0,    0,
+  161,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,   29,    0,  -44,    0,  -44,  -44,    0,    0,
+    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,    0,    0,    0,  161,    0,    0,    0,    0,
+    0,    0,    0,    0,  161,  161,  161,    0,    0,    0,
+    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,    0,    0,    0,  161,    0,    0,    0,    0,
+    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,   92,    0,
+};
+short yygindex[] = {                                      0,
+   13, -104,  100,   20,  -51,    7, -114,  -85,  212,    0,
+    0,  -34,    0,    0,    0,  185,  191,  192,  195,  196,
+  201,  202,  203,  204,  205,  -39,    0,    0,    0,   16,
+   -2,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,   66,    0,    0,    0,
+};
+#define YYTABLESIZE 501
+short yytable[] = {                                      52,
+  105,  111,   52,   63,   63,   52,   63,   91,   61,   61,
+   59,   61,  108,   72,   81,   59,   72,   59,   66,  261,
+   52,  263,  170,  134,  172,   69,   79,  184,   72,  259,
+  124,  140,  127,  162,   53,  125,  129,  126,   81,  124,
+  160,   81,   54,   85,  125,   82,  126,  124,   84,   78,
+   76,   55,  125,  180,  126,  182,  185,   57,   51,   86,
+  187,   51,   76,   76,   78,   59,   68,  119,  120,  121,
+   59,   68,   78,  124,   88,   76,   51,   89,  125,  132,
+  126,  161,  148,   56,  149,   52,   52,   78,  246,   62,
+   62,  107,   62,  206,  115,  150,   59,  151,   16,   17,
+   18,  211,  212,   77,  248,   19,  207,  208,  249,  250,
+  156,  157,   70,   72,   73,  125,  123,  126,  116,  122,
+  117,   74,   75,   77,  128,  123,   83,  131,  132,  166,
+  167,  168,  169,  215,  139,  216,  217,  136,  102,  103,
+   81,   51,   51,   81,   81,  138,  141,  171,  183,  214,
+  174,  175,  142,  188,  143,  144,  145,  146,  147,  123,
+  152,  153,  154,  155,  159,  158,  218,  219,  155,  220,
+  173,  178,  179,  221,  222,  197,  186,  189,  137,  190,
+  191,  192,  194,  200,  202,  198,  203,  254,  204,  255,
+  205,  210,  244,  245,  257,  256,  265,  266,   90,  258,
+  267,  272,  262,  262,  268,  269,  107,  270,  107,   66,
+  223,  224,  225,  226,  227,  109,   44,  271,  274,   83,
+   88,   91,   92,   52,   52,   52,   52,  110,   93,   94,
+   52,   52,   95,   96,   58,   58,   80,   58,   97,   98,
+   99,  100,  101,  260,   87,   52,   52,   52,   52,   52,
+   52,   52,   52,   72,   72,   72,   59,   59,   59,   81,
+   72,  243,    0,   59,  273,   52,  104,   52,   52,   52,
+   52,   52,    0,   72,   59,   60,  118,  163,  118,   51,
+   51,   51,   51,  130,    0,    0,   51,   51,  135,    0,
+    0,    0,   78,   78,   78,   78,    0,    0,    0,    0,
+    0,   51,   51,   51,   51,   51,   51,   51,   51,   78,
+   37,    0,   77,   77,   77,   80,   80,   80,    0,   77,
+    0,   51,   80,   51,   51,   51,   51,   51,   77,   77,
+   77,    0,   78,    0,    0,   77,    0,    0,    0,  218,
+  219,  155,  220,    0,  176,  177,  221,  222,   77,    0,
+    0,  181,    0,   44,   44,   44,   44,    0,    0,    0,
+   44,   44,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,    0,    0,    0,    0,  199,    0,    0,    0,
+    0,    0,  201,  223,  224,  225,  226,  227,    0,    0,
+    0,    0,    0,    0,    0,    0,    0,   44,   44,   44,
+   44,   44,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,    0,    0,    0,    0,    0,  247,    0,    0,
+    0,    0,    0,    0,    0,    0,  251,  252,  253,    0,
+    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+    0,    0,    1,    2,    0,    3,    4,    0,    0,    0,
+    0,    0,    5,    6,    0,    0,    0,  264,    0,    0,
+    7,    8,    9,   10,   11,   12,   13,   14,   15,   16,
+   17,   18,    0,    0,    0,    0,   19,   20,   21,   22,
+   23,   24,   25,   26,   27,   28,   29,   30,   31,   32,
+   33,    0,    0,    0,    0,    0,    0,   34,    0,   35,
+   36,
+};
+short yycheck[] = {                                      38,
+   46,   45,   41,   40,   40,   44,   40,  123,   45,   45,
+   38,   45,   52,   44,   59,   43,   44,   45,   12,  125,
+   59,  125,  127,   75,  129,   13,  280,  142,   59,  125,
+   38,   83,   67,   41,  312,   43,   71,   45,   26,   38,
+  271,   29,  123,   31,   43,   26,   45,   38,   29,   44,
+  304,  123,   43,  139,   45,  141,  142,   34,   41,   59,
+  146,   44,  304,  304,   59,  311,  312,   61,   62,   63,
+  311,  312,   44,   38,   58,   44,   59,  273,   43,   44,
+   45,  312,   43,   60,   45,  124,  125,   59,  203,  126,
+  126,  312,  126,  179,  272,   43,  124,   45,  284,  285,
+  286,  289,  290,   59,  209,  291,  287,  288,  213,  214,
+  113,  114,   13,   14,   15,   43,  124,   45,  272,   44,
+   91,   22,   23,   24,   44,  124,   27,   44,   44,  123,
+  124,  125,  126,  185,   44,  187,  188,   59,   39,   40,
+  128,  124,  125,  131,  132,   59,   44,  128,  142,  184,
+  131,  132,   44,  147,   59,  123,  273,   44,   44,  124,
+   59,  123,  271,  264,   34,   62,  262,  263,  264,  265,
+   59,   59,   44,  269,  270,   93,  123,  271,   79,  271,
+  271,  271,  271,   59,   59,   93,   44,  312,   59,  271,
+   59,   59,   59,   59,  312,  266,   59,   59,  314,  312,
+   59,  271,  242,  243,   59,   59,  312,   59,  312,  203,
+  306,  307,  308,  309,  310,  259,  125,   59,   59,   59,
+   59,   59,   38,  262,  263,  264,  265,  271,   38,   38,
+  269,  270,   38,   38,  271,  271,   25,  271,   38,   38,
+   38,   38,   38,  228,   33,  284,  285,  286,  287,  288,
+  289,  290,  291,  284,  285,  286,  284,  285,  286,  304,
+  291,  196,   -1,  291,  258,  304,  312,  306,  307,  308,
+  309,  310,   -1,  304,  311,  312,  312,  311,  312,  262,
+  263,  264,  265,   72,   -1,   -1,  269,  270,   77,   -1,
+   -1,   -1,  287,  288,  289,  290,   -1,   -1,   -1,   -1,
+   -1,  284,  285,  286,  287,  288,  289,  290,  291,  304,
+  125,   -1,  284,  285,  286,  284,  285,  286,   -1,  291,
+   -1,  304,  291,  306,  307,  308,  309,  310,  284,  285,
+  286,   -1,  304,   -1,   -1,  291,   -1,   -1,   -1,  262,
+  263,  264,  265,   -1,  133,  134,  269,  270,  304,   -1,
+   -1,  140,   -1,  262,  263,  264,  265,   -1,   -1,   -1,
+  269,  270,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
+   -1,   -1,   -1,   -1,   -1,   -1,  165,   -1,   -1,   -1,
+   -1,   -1,  171,  306,  307,  308,  309,  310,   -1,   -1,
+   -1,   -1,   -1,   -1,   -1,   -1,   -1,  306,  307,  308,
+  309,  310,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
+   -1,   -1,   -1,   -1,   -1,   -1,   -1,  206,   -1,   -1,
+   -1,   -1,   -1,   -1,   -1,   -1,  215,  216,  217,   -1,
+   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
+   -1,   -1,  257,  258,   -1,  260,  261,   -1,   -1,   -1,
+   -1,   -1,  267,  268,   -1,   -1,   -1,  246,   -1,   -1,
+  275,  276,  277,  278,  279,  280,  281,  282,  283,  284,
+  285,  286,   -1,   -1,   -1,   -1,  291,  292,  293,  294,
+  295,  296,  297,  298,  299,  300,  301,  302,  303,  304,
+  305,   -1,   -1,   -1,   -1,   -1,   -1,  312,   -1,  314,
+  315,
+};
+#define YYFINAL 38
+#ifndef YYDEBUG
+#define YYDEBUG 0
+#endif
+#define YYMAXTOKEN 318
+#if YYDEBUG
+char *yyname[] = {
+"end-of-file",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
+"'\"'",0,0,0,"'&'",0,"'('","')'",0,"'+'","','","'-'","'.'",0,0,0,0,0,0,0,0,0,0,
+0,"':'","';'","'<'",0,"'>'",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
+0,0,"'['",0,"']'",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
+"'{'","'|'","'}'","'~'",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
+0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
+0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
+0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,"T_REGISTER","T_CONST","T_DOWNLOAD",
+"T_SCB","T_SRAM","T_ALIAS","T_SIZE","T_ADDRESS","T_ACCESS_MODE","T_MODE",
+"T_BEGIN_CS","T_END_CS","T_BIT","T_MASK","T_NUMBER","T_PATH","T_CEXPR","T_EOF",
+"T_INCLUDE","T_SHR","T_SHL","T_ROR","T_ROL","T_MVI","T_MOV","T_CLR","T_BMOV",
+"T_JMP","T_JC","T_JNC","T_JE","T_JNE","T_JNZ","T_JZ","T_CALL","T_ADD","T_ADC",
+"T_INC","T_DEC","T_STC","T_CLC","T_CMP","T_NOT","T_XOR","T_TEST","T_AND","T_OR",
+"T_RET","T_NOP","T_ACCUM","T_ALLONES","T_ALLZEROS","T_NONE","T_SINDEX","T_A",
+"T_SYMBOL","T_NL","T_IF","T_ELSE","T_ELSE_IF","T_ENDIF","UMINUS",
+};
+char *yyrule[] = {
+"$accept : program",
+"program : include",
+"program : program include",
+"program : register",
+"program : program register",
+"program : constant",
+"program : program constant",
+"program : scratch_ram",
+"program : program scratch_ram",
+"program : scb",
+"program : program scb",
+"program : label",
+"program : program label",
+"program : critical_section_start",
+"program : program critical_section_start",
+"program : critical_section_end",
+"program : program critical_section_end",
+"program : conditional",
+"program : program conditional",
+"program : code",
+"program : program code",
+"include : T_INCLUDE '<' T_PATH '>'",
+"include : T_INCLUDE '\"' T_PATH '\"'",
+"$$1 :",
+"register : T_REGISTER $$1 reg_definition",
+"$$2 :",
+"reg_definition : T_SYMBOL '{' $$2 reg_attribute_list '}'",
+"reg_attribute_list : reg_attribute",
+"reg_attribute_list : reg_attribute_list reg_attribute",
+"reg_attribute : reg_address",
+"reg_attribute : size",
+"reg_attribute : access_mode",
+"reg_attribute : bit_defn",
+"reg_attribute : mask_defn",
+"reg_attribute : alias",
+"reg_attribute : accumulator",
+"reg_attribute : allones",
+"reg_attribute : allzeros",
+"reg_attribute : none",
+"reg_attribute : sindex",
+"reg_address : T_ADDRESS T_NUMBER",
+"size : T_SIZE T_NUMBER",
+"access_mode : T_ACCESS_MODE T_MODE",
+"bit_defn : T_BIT T_SYMBOL T_NUMBER",
+"mask_defn : T_MASK T_SYMBOL expression",
+"alias : T_ALIAS T_SYMBOL",
+"accumulator : T_ACCUM",
+"allones : T_ALLONES",
+"allzeros : T_ALLZEROS",
+"none : T_NONE",
+"sindex : T_SINDEX",
+"expression : expression '|' expression",
+"expression : expression '&' expression",
+"expression : expression '+' expression",
+"expression : expression '-' expression",
+"expression : '(' expression ')'",
+"expression : '~' expression",
+"expression : '-' expression",
+"expression : T_NUMBER",
+"expression : T_SYMBOL",
+"constant : T_CONST T_SYMBOL numerical_value",
+"constant : T_CONST T_SYMBOL T_DOWNLOAD",
+"numerical_value : T_NUMBER",
+"numerical_value : '-' T_NUMBER",
+"$$3 :",
+"$$4 :",
+"scratch_ram : T_SRAM '{' $$3 reg_address $$4 scb_or_sram_reg_list '}'",
+"$$5 :",
+"$$6 :",
+"scb : T_SCB '{' $$5 reg_address $$6 scb_or_sram_reg_list '}'",
+"scb_or_sram_reg_list : reg_definition",
+"scb_or_sram_reg_list : scb_or_sram_reg_list reg_definition",
+"reg_symbol : T_SYMBOL",
+"reg_symbol : T_SYMBOL '[' T_SYMBOL ']'",
+"reg_symbol : T_SYMBOL '[' T_NUMBER ']'",
+"reg_symbol : T_A",
+"destination : reg_symbol",
+"immediate : expression",
+"immediate_or_a : expression",
+"immediate_or_a : T_A",
+"source : reg_symbol",
+"opt_source :",
+"opt_source : ',' source",
+"ret :",
+"ret : T_RET",
+"critical_section_start : T_BEGIN_CS",
+"critical_section_end : T_END_CS",
+"label : T_SYMBOL ':'",
+"address : T_SYMBOL",
+"address : T_SYMBOL '+' T_NUMBER",
+"address : T_SYMBOL '-' T_NUMBER",
+"address : '.'",
+"address : '.' '+' T_NUMBER",
+"address : '.' '-' T_NUMBER",
+"conditional : T_IF T_CEXPR '{'",
+"conditional : T_ELSE T_IF T_CEXPR '{'",
+"conditional : T_ELSE '{'",
+"conditional : '}'",
+"f1_opcode : T_AND",
+"f1_opcode : T_XOR",
+"f1_opcode : T_ADD",
+"f1_opcode : T_ADC",
+"code : f1_opcode destination ',' immediate_or_a opt_source ret ';'",
+"code : T_OR reg_symbol ',' immediate_or_a opt_source ret ';'",
+"code : T_INC destination opt_source ret ';'",
+"code : T_DEC destination opt_source ret ';'",
+"code : T_CLC ret ';'",
+"code : T_CLC T_MVI destination ',' immediate_or_a ret ';'",
+"code : T_STC ret ';'",
+"code : T_STC destination ret ';'",
+"code : T_BMOV destination ',' source ',' immediate ret ';'",
+"code : T_MOV destination ',' source ret ';'",
+"code : T_MVI destination ',' immediate_or_a ret ';'",
+"code : T_NOT destination opt_source ret ';'",
+"code : T_CLR destination ret ';'",
+"code : T_NOP ret ';'",
+"code : T_RET ';'",
+"f2_opcode : T_SHL",
+"f2_opcode : T_SHR",
+"f2_opcode : T_ROL",
+"f2_opcode : T_ROR",
+"code : f2_opcode destination ',' expression opt_source ret ';'",
+"jmp_jc_jnc_call : T_JMP",
+"jmp_jc_jnc_call : T_JC",
+"jmp_jc_jnc_call : T_JNC",
+"jmp_jc_jnc_call : T_CALL",
+"jz_jnz : T_JZ",
+"jz_jnz : T_JNZ",
+"je_jne : T_JE",
+"je_jne : T_JNE",
+"code : jmp_jc_jnc_call address ';'",
+"code : T_OR reg_symbol ',' immediate jmp_jc_jnc_call address ';'",
+"code : T_TEST source ',' immediate_or_a jz_jnz address ';'",
+"code : T_CMP source ',' immediate_or_a je_jne address ';'",
+"code : T_MOV source jmp_jc_jnc_call address ';'",
+"code : T_MVI immediate jmp_jc_jnc_call address ';'",
+};
+#endif
+#ifdef YYSTACKSIZE
+#undef YYMAXDEPTH
+#define YYMAXDEPTH YYSTACKSIZE
+#else
+#ifdef YYMAXDEPTH
+#define YYSTACKSIZE YYMAXDEPTH
+#else
+#define YYSTACKSIZE 500
+#define YYMAXDEPTH 500
+#endif
+#endif
+int yydebug;
+int yynerrs;
+int yyerrflag;
+int yychar;
+short *yyssp;
+YYSTYPE *yyvsp;
+YYSTYPE yyval;
+YYSTYPE yylval;
+short yyss[YYSTACKSIZE];
+YYSTYPE yyvs[YYSTACKSIZE];
+#define yystacksize YYSTACKSIZE
+#line 1043 "aicasm_gram.y"
+
+static void
+process_bitmask(int mask_type, symbol_t *sym, int mask)
+{
+	/*
+	 * Add the current register to its
+	 * symbol list, if it already exists,
+	 * warn if we are setting it to a
+	 * different value, or in the bit to
+	 * the "allowed bits" of this register.
+	 */
+	if (sym->type == UNINITIALIZED) {
+		sym->type = mask_type;
+		initialize_symbol(sym);
+		if (mask_type == BIT) {
+			if (mask == 0) {
+				stop("Bitmask with no bits set", EX_DATAERR);
+				/* NOTREACHED */
+			}
+			if ((mask & ~(0x01 << (ffs(mask) - 1))) != 0) {
+				stop("Bitmask with more than one bit set",
+				     EX_DATAERR);
+				/* NOTREACHED */
+			}
+		}
+		sym->info.minfo->mask = mask;
+	} else if (sym->type != mask_type) {
+		stop("Bit definition mirrors a definition of the same "
+		     " name, but a different type", EX_DATAERR);
+		/* NOTREACHED */
+	} else if (mask != sym->info.minfo->mask) {
+		stop("Bitmask redefined with a conflicting value", EX_DATAERR);
+		/* NOTREACHED */
+	}
+	/* Fail if this symbol is already listed */
+	if (symlist_search(&(sym->info.minfo->symrefs),
+			   cur_symbol->name) != NULL) {
+		stop("Bitmask defined multiple times for register", EX_DATAERR);
+		/* NOTREACHED */
+	}
+	symlist_add(&(sym->info.minfo->symrefs), cur_symbol,
+		    SYMLIST_INSERT_HEAD);
+	cur_symbol->info.rinfo->valid_bitmask |= mask;
+	cur_symbol->info.rinfo->typecheck_masks = TRUE;
+}
+
+static void
+initialize_symbol(symbol_t *symbol)
+{
+	switch (symbol->type) {
+        case UNINITIALIZED:
+		stop("Call to initialize_symbol with type field unset",
+		     EX_SOFTWARE);
+		/* NOTREACHED */
+		break;
+        case REGISTER:
+        case SRAMLOC:
+        case SCBLOC:
+		symbol->info.rinfo =
+		    (struct reg_info *)malloc(sizeof(struct reg_info));
+		if (symbol->info.rinfo == NULL) {
+			stop("Can't create register info", EX_SOFTWARE);
+			/* NOTREACHED */
+		}
+		memset(symbol->info.rinfo, 0,
+		       sizeof(struct reg_info));
+		break;
+        case ALIAS:
+		symbol->info.ainfo =
+		    (struct alias_info *)malloc(sizeof(struct alias_info));
+		if (symbol->info.ainfo == NULL) {
+			stop("Can't create alias info", EX_SOFTWARE);
+			/* NOTREACHED */
+		}
+		memset(symbol->info.ainfo, 0,
+		       sizeof(struct alias_info));
+		break;
+        case MASK:
+        case BIT:
+		symbol->info.minfo =
+		    (struct mask_info *)malloc(sizeof(struct mask_info));
+		if (symbol->info.minfo == NULL) {
+			stop("Can't create bitmask info", EX_SOFTWARE);
+			/* NOTREACHED */
+		}
+		memset(symbol->info.minfo, 0, sizeof(struct mask_info));
+		SLIST_INIT(&(symbol->info.minfo->symrefs));
+		break;
+        case CONST:
+        case DOWNLOAD_CONST:
+		symbol->info.cinfo =
+		    (struct const_info *)malloc(sizeof(struct const_info));
+		if (symbol->info.cinfo == NULL) {
+			stop("Can't create alias info", EX_SOFTWARE);
+			/* NOTREACHED */
+		}
+		memset(symbol->info.cinfo, 0,
+		       sizeof(struct const_info));
+		break;
+	case LABEL:
+		symbol->info.linfo =
+		    (struct label_info *)malloc(sizeof(struct label_info));
+		if (symbol->info.linfo == NULL) {
+			stop("Can't create label info", EX_SOFTWARE);
+			/* NOTREACHED */
+		}
+		memset(symbol->info.linfo, 0,
+		       sizeof(struct label_info));
+		break;
+	case CONDITIONAL:
+		symbol->info.condinfo =
+		    (struct cond_info *)malloc(sizeof(struct cond_info));
+		if (symbol->info.condinfo == NULL) {
+			stop("Can't create conditional info", EX_SOFTWARE);
+			/* NOTREACHED */
+		}
+		memset(symbol->info.condinfo, 0,
+		       sizeof(struct cond_info));
+		break;
+	default:
+		stop("Call to initialize_symbol with invalid symbol type",
+		     EX_SOFTWARE);
+		/* NOTREACHED */
+		break;
+	}
+}
+
+static void
+process_register(symbol_t **p_symbol)
+{
+	char buf[255];
+	symbol_t *symbol = *p_symbol;
+
+	if (symbol->type == UNINITIALIZED) {
+		snprintf(buf, sizeof(buf), "Undefined register %s",
+			 symbol->name);
+		stop(buf, EX_DATAERR);
+		/* NOTREACHED */
+	} else if (symbol->type == ALIAS) {
+		*p_symbol = symbol->info.ainfo->parent;
+	} else if ((symbol->type != REGISTER)
+		&& (symbol->type != SCBLOC)
+		&& (symbol->type != SRAMLOC)) {
+		snprintf(buf, sizeof(buf),
+			 "Specified symbol %s is not a register",
+			 symbol->name);
+		stop(buf, EX_DATAERR);
+	}
+}
+
+static void
+format_1_instr(int opcode, symbol_ref_t *dest, expression_t *immed,
+	       symbol_ref_t *src, int ret)
+{
+	struct instruction *instr;
+	struct ins_format1 *f1_instr;
+
+	if (src->symbol == NULL)
+		src = dest;
+
+	/* Test register permissions */
+	test_writable_symbol(dest->symbol);
+	test_readable_symbol(src->symbol);
+
+	/* Ensure that immediate makes sense for this destination */
+	type_check(dest->symbol, immed, opcode);
+
+	/* Allocate sequencer space for the instruction and fill it out */
+	instr = seq_alloc();
+	f1_instr = &instr->format.format1;
+	f1_instr->ret = ret ? 1 : 0;
+	f1_instr->opcode = opcode;
+	f1_instr->destination = dest->symbol->info.rinfo->address
+			      + dest->offset;
+	f1_instr->source = src->symbol->info.rinfo->address
+			 + src->offset;
+	f1_instr->immediate = immed->value;
+
+	if (is_download_const(immed))
+		f1_instr->parity = 1;
+
+	symlist_free(&immed->referenced_syms);
+	instruction_ptr++;
+}
+
+static void
+format_2_instr(int opcode, symbol_ref_t *dest, expression_t *places,
+	       symbol_ref_t *src, int ret)
+{
+	struct instruction *instr;
+	struct ins_format2 *f2_instr;
+	uint8_t shift_control;
+
+	if (src->symbol == NULL)
+		src = dest;
+
+	/* Test register permissions */
+	test_writable_symbol(dest->symbol);
+	test_readable_symbol(src->symbol);
+
+	/* Allocate sequencer space for the instruction and fill it out */
+	instr = seq_alloc();
+	f2_instr = &instr->format.format2;
+	f2_instr->ret = ret ? 1 : 0;
+	f2_instr->opcode = AIC_OP_ROL;
+	f2_instr->destination = dest->symbol->info.rinfo->address
+			      + dest->offset;
+	f2_instr->source = src->symbol->info.rinfo->address
+			 + src->offset;
+	if (places->value > 8 || places->value <= 0) {
+		stop("illegal shift value", EX_DATAERR);
+		/* NOTREACHED */
+	}
+	switch (opcode) {
+	case AIC_OP_SHL:
+		if (places->value == 8)
+			shift_control = 0xf0;
+		else
+			shift_control = (places->value << 4) | places->value;
+		break;
+	case AIC_OP_SHR:
+		if (places->value == 8) {
+			shift_control = 0xf8;
+		} else {
+			shift_control = (places->value << 4)
+				      | (8 - places->value)
+				      | 0x08;
+		}
+		break;
+	case AIC_OP_ROL:
+		shift_control = places->value & 0x7;
+		break;
+	case AIC_OP_ROR:
+		shift_control = (8 - places->value) | 0x08;
+		break;
+	default:
+		shift_control = 0; /* Quiet Compiler */
+		stop("Invalid shift operation specified", EX_SOFTWARE);
+		/* NOTREACHED */
+		break;
+	};
+	f2_instr->shift_control = shift_control;
+	symlist_free(&places->referenced_syms);
+	instruction_ptr++;
+}
+
+static void
+format_3_instr(int opcode, symbol_ref_t *src,
+	       expression_t *immed, symbol_ref_t *address)
+{
+	struct instruction *instr;
+	struct ins_format3 *f3_instr;
+	int addr;
+
+	/* Test register permissions */
+	test_readable_symbol(src->symbol);
+
+	/* Ensure that immediate makes sense for this source */
+	type_check(src->symbol, immed, opcode);
+
+	/* Allocate sequencer space for the instruction and fill it out */
+	instr = seq_alloc();
+	f3_instr = &instr->format.format3;
+	if (address->symbol == NULL) {
+		/* 'dot' referrence.  Use the current instruction pointer */
+		addr = instruction_ptr + address->offset;
+	} else if (address->symbol->type == UNINITIALIZED) {
+		/* forward reference */
+		addr = address->offset;
+		instr->patch_label = address->symbol;
+	} else
+		addr = address->symbol->info.linfo->address + address->offset;
+	f3_instr->opcode = opcode;
+	f3_instr->address = addr;
+	f3_instr->source = src->symbol->info.rinfo->address
+			 + src->offset;
+	f3_instr->immediate = immed->value;
+
+	if (is_download_const(immed))
+		f3_instr->parity = 1;
+
+	symlist_free(&immed->referenced_syms);
+	instruction_ptr++;
+}
+
+static void
+test_readable_symbol(symbol_t *symbol)
+{
+	if (symbol->info.rinfo->mode == WO) {
+		stop("Write Only register specified as source",
+		     EX_DATAERR);
+		/* NOTREACHED */
+	}
+}
+
+static void
+test_writable_symbol(symbol_t *symbol)
+{
+	if (symbol->info.rinfo->mode == RO) {
+		stop("Read Only register specified as destination",
+		     EX_DATAERR);
+		/* NOTREACHED */
+	}
+}
+
+static void
+type_check(symbol_t *symbol, expression_t *expression, int opcode)
+{
+	symbol_node_t *node;
+	int and_op;
+	char buf[255];
+
+	and_op = FALSE;
+	if (opcode == AIC_OP_AND || opcode == AIC_OP_JNZ || AIC_OP_JZ)
+		and_op = TRUE;
+
+	/*
+	 * Make sure that we aren't attempting to write something
+	 * that hasn't been defined.  If this is an and operation,
+	 * this is a mask, so "undefined" bits are okay.
+	 */
+	if (and_op == FALSE
+	 && (expression->value & ~symbol->info.rinfo->valid_bitmask) != 0) {
+		snprintf(buf, sizeof(buf),
+			 "Invalid bit(s) 0x%x in immediate written to %s",
+			 expression->value & ~symbol->info.rinfo->valid_bitmask,
+			 symbol->name);
+		stop(buf, EX_DATAERR);
+		/* NOTREACHED */
+	}
+
+	/*
+	 * Now make sure that all of the symbols referenced by the
+	 * expression are defined for this register.
+	 */
+	if(symbol->info.rinfo->typecheck_masks != FALSE) {
+		for(node = expression->referenced_syms.slh_first;
+		    node != NULL;
+		    node = node->links.sle_next) {
+			if ((node->symbol->type == MASK
+			  || node->symbol->type == BIT)
+			 && symlist_search(&node->symbol->info.minfo->symrefs,
+					   symbol->name) == NULL) {
+				snprintf(buf, sizeof(buf),
+					 "Invalid bit or mask %s "
+					 "for register %s",
+					 node->symbol->name, symbol->name);
+				stop(buf, EX_DATAERR);
+				/* NOTREACHED */
+			}
+		}
+	}
+}
+
+static void
+make_expression(expression_t *immed, int value)
+{
+	SLIST_INIT(&immed->referenced_syms);
+	immed->value = value & 0xff;
+}
+
+static void
+add_conditional(symbol_t *symbol)
+{
+	static int numfuncs;
+
+	if (numfuncs == 0) {
+		/* add a special conditional, "0" */
+		symbol_t *false_func;
+
+		false_func = symtable_get("0");
+		if (false_func->type != UNINITIALIZED) {
+			stop("Conditional expression '0' "
+			     "conflicts with a symbol", EX_DATAERR);
+			/* NOTREACHED */
+		}
+		false_func->type = CONDITIONAL;
+		initialize_symbol(false_func);
+		false_func->info.condinfo->func_num = numfuncs++;
+		symlist_add(&patch_functions, false_func, SYMLIST_INSERT_HEAD);
+	}
+
+	/* This condition has occurred before */
+	if (symbol->type == CONDITIONAL)
+		return;
+
+	if (symbol->type != UNINITIALIZED) {
+		stop("Conditional expression conflicts with a symbol",
+		     EX_DATAERR);
+		/* NOTREACHED */
+	}
+
+	symbol->type = CONDITIONAL;
+	initialize_symbol(symbol);
+	symbol->info.condinfo->func_num = numfuncs++;
+	symlist_add(&patch_functions, symbol, SYMLIST_INSERT_HEAD);
+}
+
+void
+yyerror(const char *string)
+{
+	stop(string, EX_DATAERR);
+}
+
+static int
+is_download_const(expression_t *immed)
+{
+	if ((immed->referenced_syms.slh_first != NULL)
+	 && (immed->referenced_syms.slh_first->symbol->type == DOWNLOAD_CONST))
+		return (TRUE);
+
+	return (FALSE);
+}
+#line 1011 "y.tab.c"
+#define YYABORT goto yyabort
+#define YYREJECT goto yyabort
+#define YYACCEPT goto yyaccept
+#define YYERROR goto yyerrlab
+int
+yyparse()
+{
+    register int yym, yyn, yystate;
+#if YYDEBUG
+    register char *yys;
+    extern char *getenv();
+
+    if (yys = getenv("YYDEBUG"))
+    {
+        yyn = *yys;
+        if (yyn >= '0' && yyn <= '9')
+            yydebug = yyn - '0';
+    }
+#endif
+
+    yynerrs = 0;
+    yyerrflag = 0;
+    yychar = (-1);
+
+    yyssp = yyss;
+    yyvsp = yyvs;
+    *yyssp = yystate = 0;
+
+yyloop:
+    if (yyn = yydefred[yystate]) goto yyreduce;
+    if (yychar < 0)
+    {
+        if ((yychar = yylex()) < 0) yychar = 0;
+#if YYDEBUG
+        if (yydebug)
+        {
+            yys = 0;
+            if (yychar <= YYMAXTOKEN) yys = yyname[yychar];
+            if (!yys) yys = "illegal-symbol";
+            printf("%sdebug: state %d, reading %d (%s)\n",
+                    YYPREFIX, yystate, yychar, yys);
+        }
+#endif
+    }
+    if ((yyn = yysindex[yystate]) && (yyn += yychar) >= 0 &&
+            yyn <= YYTABLESIZE && yycheck[yyn] == yychar)
+    {
+#if YYDEBUG
+        if (yydebug)
+            printf("%sdebug: state %d, shifting to state %d\n",
+                    YYPREFIX, yystate, yytable[yyn]);
+#endif
+        if (yyssp >= yyss + yystacksize - 1)
+        {
+            goto yyoverflow;
+        }
+        *++yyssp = yystate = yytable[yyn];
+        *++yyvsp = yylval;
+        yychar = (-1);
+        if (yyerrflag > 0)  --yyerrflag;
+        goto yyloop;
+    }
+    if ((yyn = yyrindex[yystate]) && (yyn += yychar) >= 0 &&
+            yyn <= YYTABLESIZE && yycheck[yyn] == yychar)
+    {
+        yyn = yytable[yyn];
+        goto yyreduce;
+    }
+    if (yyerrflag) goto yyinrecovery;
+#ifdef lint
+    goto yynewerror;
+#endif
+yynewerror:
+    yyerror("syntax error");
+#ifdef lint
+    goto yyerrlab;
+#endif
+yyerrlab:
+    ++yynerrs;
+yyinrecovery:
+    if (yyerrflag < 3)
+    {
+        yyerrflag = 3;
+        for (;;)
+        {
+            if ((yyn = yysindex[*yyssp]) && (yyn += YYERRCODE) >= 0 &&
+                    yyn <= YYTABLESIZE && yycheck[yyn] == YYERRCODE)
+            {
+#if YYDEBUG
+                if (yydebug)
+                    printf("%sdebug: state %d, error recovery shifting\
+ to state %d\n", YYPREFIX, *yyssp, yytable[yyn]);
+#endif
+                if (yyssp >= yyss + yystacksize - 1)
+                {
+                    goto yyoverflow;
+                }
+                *++yyssp = yystate = yytable[yyn];
+                *++yyvsp = yylval;
+                goto yyloop;
+            }
+            else
+            {
+#if YYDEBUG
+                if (yydebug)
+                    printf("%sdebug: error recovery discarding state %d\n",
+                            YYPREFIX, *yyssp);
+#endif
+                if (yyssp <= yyss) goto yyabort;
+                --yyssp;
+                --yyvsp;
+            }
+        }
+    }
+    else
+    {
+        if (yychar == 0) goto yyabort;
+#if YYDEBUG
+        if (yydebug)
+        {
+            yys = 0;
+            if (yychar <= YYMAXTOKEN) yys = yyname[yychar];
+            if (!yys) yys = "illegal-symbol";
+            printf("%sdebug: state %d, error recovery discards token %d (%s)\n",
+                    YYPREFIX, yystate, yychar, yys);
+        }
+#endif
+        yychar = (-1);
+        goto yyloop;
+    }
+yyreduce:
+#if YYDEBUG
+    if (yydebug)
+        printf("%sdebug: state %d, reducing by rule %d (%s)\n",
+                YYPREFIX, yystate, yyn, yyrule[yyn]);
+#endif
+    yym = yylen[yyn];
+    yyval = yyvsp[1-yym];
+    switch (yyn)
+    {
+case 21:
+#line 205 "aicasm_gram.y"
+{ include_file(yyvsp[-1].str, BRACKETED_INCLUDE); }
+break;
+case 22:
+#line 207 "aicasm_gram.y"
+{ include_file(yyvsp[-1].str, QUOTED_INCLUDE); }
+break;
+case 23:
+#line 211 "aicasm_gram.y"
+{ cur_symtype = REGISTER; }
+break;
+case 25:
+#line 216 "aicasm_gram.y"
+{
+			if (yyvsp[-1].sym->type != UNINITIALIZED) {
+				stop("Register multiply defined", EX_DATAERR);
+				/* NOTREACHED */
+			}
+			cur_symbol = yyvsp[-1].sym; 
+			cur_symbol->type = cur_symtype;
+			initialize_symbol(cur_symbol);
+		}
+break;
+case 26:
+#line 227 "aicasm_gram.y"
+{                    
+			/*
+			 * Default to allowing everything in for registers
+			 * with no bit or mask definitions.
+			 */
+			if (cur_symbol->info.rinfo->valid_bitmask == 0)
+				cur_symbol->info.rinfo->valid_bitmask = 0xFF;
+
+			if (cur_symbol->info.rinfo->size == 0)
+				cur_symbol->info.rinfo->size = 1;
+
+			/*
+			 * This might be useful for registers too.
+			 */
+			if (cur_symbol->type != REGISTER) {
+				if (cur_symbol->info.rinfo->address == 0)
+					cur_symbol->info.rinfo->address =
+					    sram_or_scb_offset;
+				sram_or_scb_offset +=
+				    cur_symbol->info.rinfo->size;
+			}
+			cur_symbol = NULL;
+		}
+break;
+case 40:
+#line 273 "aicasm_gram.y"
+{
+		cur_symbol->info.rinfo->address = yyvsp[0].value;
+	}
+break;
+case 41:
+#line 280 "aicasm_gram.y"
+{
+		cur_symbol->info.rinfo->size = yyvsp[0].value;
+	}
+break;
+case 42:
+#line 287 "aicasm_gram.y"
+{
+		cur_symbol->info.rinfo->mode = yyvsp[0].value;
+	}
+break;
+case 43:
+#line 294 "aicasm_gram.y"
+{
+		process_bitmask(BIT, yyvsp[-1].sym, yyvsp[0].value);
+	}
+break;
+case 44:
+#line 301 "aicasm_gram.y"
+{
+		process_bitmask(MASK, yyvsp[-1].sym, yyvsp[0].expression.value);
+	}
+break;
+case 45:
+#line 308 "aicasm_gram.y"
+{
+		if (yyvsp[0].sym->type != UNINITIALIZED) {
+			stop("Re-definition of register alias",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		yyvsp[0].sym->type = ALIAS;
+		initialize_symbol(yyvsp[0].sym);
+		yyvsp[0].sym->info.ainfo->parent = cur_symbol;
+	}
+break;
+case 46:
+#line 322 "aicasm_gram.y"
+{
+		if (accumulator != NULL) {
+			stop("Only one accumulator definition allowed",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		accumulator = cur_symbol;
+	}
+break;
+case 47:
+#line 334 "aicasm_gram.y"
+{
+		if (allones.symbol != NULL) {
+			stop("Only one definition of allones allowed",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		allones.symbol = cur_symbol;
+	}
+break;
+case 48:
+#line 346 "aicasm_gram.y"
+{
+		if (allzeros.symbol != NULL) {
+			stop("Only one definition of allzeros allowed",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		allzeros.symbol = cur_symbol;
+	}
+break;
+case 49:
+#line 358 "aicasm_gram.y"
+{
+		if (none.symbol != NULL) {
+			stop("Only one definition of none allowed",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		none.symbol = cur_symbol;
+	}
+break;
+case 50:
+#line 370 "aicasm_gram.y"
+{
+		if (sindex.symbol != NULL) {
+			stop("Only one definition of sindex allowed",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		sindex.symbol = cur_symbol;
+	}
+break;
+case 51:
+#line 382 "aicasm_gram.y"
+{
+		 yyval.expression.value = yyvsp[-2].expression.value | yyvsp[0].expression.value;
+		 symlist_merge(&yyval.expression.referenced_syms,
+			       &yyvsp[-2].expression.referenced_syms,
+			       &yyvsp[0].expression.referenced_syms);
+	}
+break;
+case 52:
+#line 389 "aicasm_gram.y"
+{
+		yyval.expression.value = yyvsp[-2].expression.value & yyvsp[0].expression.value;
+		symlist_merge(&yyval.expression.referenced_syms,
+			       &yyvsp[-2].expression.referenced_syms,
+			       &yyvsp[0].expression.referenced_syms);
+	}
+break;
+case 53:
+#line 396 "aicasm_gram.y"
+{
+		yyval.expression.value = yyvsp[-2].expression.value + yyvsp[0].expression.value;
+		symlist_merge(&yyval.expression.referenced_syms,
+			       &yyvsp[-2].expression.referenced_syms,
+			       &yyvsp[0].expression.referenced_syms);
+	}
+break;
+case 54:
+#line 403 "aicasm_gram.y"
+{
+		yyval.expression.value = yyvsp[-2].expression.value - yyvsp[0].expression.value;
+		symlist_merge(&(yyval.expression.referenced_syms),
+			       &(yyvsp[-2].expression.referenced_syms),
+			       &(yyvsp[0].expression.referenced_syms));
+	}
+break;
+case 55:
+#line 410 "aicasm_gram.y"
+{
+		yyval.expression = yyvsp[-1].expression;
+	}
+break;
+case 56:
+#line 414 "aicasm_gram.y"
+{
+		yyval.expression = yyvsp[0].expression;
+		yyval.expression.value = (~yyval.expression.value) & 0xFF;
+	}
+break;
+case 57:
+#line 419 "aicasm_gram.y"
+{
+		yyval.expression = yyvsp[0].expression;
+		yyval.expression.value = -yyval.expression.value;
+	}
+break;
+case 58:
+#line 424 "aicasm_gram.y"
+{
+		yyval.expression.value = yyvsp[0].value;
+		SLIST_INIT(&yyval.expression.referenced_syms);
+	}
+break;
+case 59:
+#line 429 "aicasm_gram.y"
+{
+		symbol_t *symbol;
+
+		symbol = yyvsp[0].sym;
+		switch (symbol->type) {
+		case ALIAS:
+			symbol = yyvsp[0].sym->info.ainfo->parent;
+		case REGISTER:
+		case SCBLOC:
+		case SRAMLOC:
+			yyval.expression.value = symbol->info.rinfo->address;
+			break;
+		case MASK:
+		case BIT:
+			yyval.expression.value = symbol->info.minfo->mask;
+			break;
+		case DOWNLOAD_CONST:
+		case CONST:
+			yyval.expression.value = symbol->info.cinfo->value;
+			break;
+		case UNINITIALIZED:
+		default:
+		{
+			char buf[255];
+
+			snprintf(buf, sizeof(buf),
+				 "Undefined symbol %s referenced",
+				 symbol->name);
+			stop(buf, EX_DATAERR);
+			/* NOTREACHED */
+			break;
+		}
+		}
+		SLIST_INIT(&yyval.expression.referenced_syms);
+		symlist_add(&yyval.expression.referenced_syms, symbol, SYMLIST_INSERT_HEAD);
+	}
+break;
+case 60:
+#line 469 "aicasm_gram.y"
+{
+		if (yyvsp[-1].sym->type != UNINITIALIZED) {
+			stop("Re-definition of symbol as a constant",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		yyvsp[-1].sym->type = CONST;
+		initialize_symbol(yyvsp[-1].sym);
+		yyvsp[-1].sym->info.cinfo->value = yyvsp[0].value;
+		yyvsp[-1].sym->info.cinfo->define = yyvsp[-2].value;
+	}
+break;
+case 61:
+#line 481 "aicasm_gram.y"
+{
+		if (yyvsp[-2].value) {
+			stop("Invalid downloaded constant declaration",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		if (yyvsp[-1].sym->type != UNINITIALIZED) {
+			stop("Re-definition of symbol as a downloaded constant",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		yyvsp[-1].sym->type = DOWNLOAD_CONST;
+		initialize_symbol(yyvsp[-1].sym);
+		yyvsp[-1].sym->info.cinfo->value = download_constant_count++;
+		yyvsp[-1].sym->info.cinfo->define = FALSE;
+	}
+break;
+case 62:
+#line 501 "aicasm_gram.y"
+{
+		yyval.value = yyvsp[0].value;
+	}
+break;
+case 63:
+#line 505 "aicasm_gram.y"
+{
+		yyval.value = -yyvsp[0].value;
+	}
+break;
+case 64:
+#line 512 "aicasm_gram.y"
+{
+			cur_symbol = symtable_get(SRAM_SYMNAME);
+			cur_symtype = SRAMLOC;
+			if (cur_symbol->type != UNINITIALIZED) {
+				stop("Only one SRAM definition allowed",
+				     EX_DATAERR);
+				/* NOTREACHED */
+			}
+			cur_symbol->type = SRAMLOC;
+			initialize_symbol(cur_symbol);
+		}
+break;
+case 65:
+#line 524 "aicasm_gram.y"
+{
+			sram_or_scb_offset = cur_symbol->info.rinfo->address;
+		}
+break;
+case 66:
+#line 529 "aicasm_gram.y"
+{
+			cur_symbol = NULL;
+		}
+break;
+case 67:
+#line 536 "aicasm_gram.y"
+{
+			cur_symbol = symtable_get(SCB_SYMNAME);
+			cur_symtype = SCBLOC;
+			if (cur_symbol->type != UNINITIALIZED) {
+				stop("Only one SRAM definition allowed",
+				     EX_SOFTWARE);
+				/* NOTREACHED */
+			}
+			cur_symbol->type = SCBLOC;
+			initialize_symbol(cur_symbol);
+			/* 64 bytes of SCB space */
+			cur_symbol->info.rinfo->size = 64;
+		}
+break;
+case 68:
+#line 550 "aicasm_gram.y"
+{
+			sram_or_scb_offset = cur_symbol->info.rinfo->address;
+		}
+break;
+case 69:
+#line 555 "aicasm_gram.y"
+{
+			cur_symbol = NULL;
+		}
+break;
+case 72:
+#line 567 "aicasm_gram.y"
+{
+		process_register(&yyvsp[0].sym);
+		yyval.sym_ref.symbol = yyvsp[0].sym;
+		yyval.sym_ref.offset = 0;
+	}
+break;
+case 73:
+#line 573 "aicasm_gram.y"
+{
+		process_register(&yyvsp[-3].sym);
+		if (yyvsp[-1].sym->type != CONST) {
+			stop("register offset must be a constant", EX_DATAERR);
+			/* NOTREACHED */
+		}
+		if ((yyvsp[-1].sym->info.cinfo->value + 1) > yyvsp[-3].sym->info.rinfo->size) {
+			stop("Accessing offset beyond range of register",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		yyval.sym_ref.symbol = yyvsp[-3].sym;
+		yyval.sym_ref.offset = yyvsp[-1].sym->info.cinfo->value;
+	}
+break;
+case 74:
+#line 588 "aicasm_gram.y"
+{
+		process_register(&yyvsp[-3].sym);
+		if ((yyvsp[-1].value + 1) > yyvsp[-3].sym->info.rinfo->size) {
+			stop("Accessing offset beyond range of register",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		yyval.sym_ref.symbol = yyvsp[-3].sym;
+		yyval.sym_ref.offset = yyvsp[-1].value;
+	}
+break;
+case 75:
+#line 599 "aicasm_gram.y"
+{
+		if (accumulator == NULL) {
+			stop("No accumulator has been defined", EX_DATAERR);
+			/* NOTREACHED */
+		}
+		yyval.sym_ref.symbol = accumulator;
+		yyval.sym_ref.offset = 0;
+	}
+break;
+case 76:
+#line 611 "aicasm_gram.y"
+{
+		test_writable_symbol(yyvsp[0].sym_ref.symbol);
+		yyval.sym_ref = yyvsp[0].sym_ref;
+	}
+break;
+case 77:
+#line 619 "aicasm_gram.y"
+{ yyval.expression = yyvsp[0].expression; }
+break;
+case 78:
+#line 624 "aicasm_gram.y"
+{
+		yyval.expression = yyvsp[0].expression;
+	}
+break;
+case 79:
+#line 628 "aicasm_gram.y"
+{
+		SLIST_INIT(&yyval.expression.referenced_syms);
+		yyval.expression.value = 0;
+	}
+break;
+case 80:
+#line 636 "aicasm_gram.y"
+{
+		test_readable_symbol(yyvsp[0].sym_ref.symbol);
+		yyval.sym_ref = yyvsp[0].sym_ref;
+	}
+break;
+case 81:
+#line 643 "aicasm_gram.y"
+{
+		yyval.sym_ref.symbol = NULL;
+		yyval.sym_ref.offset = 0;
+	}
+break;
+case 82:
+#line 648 "aicasm_gram.y"
+{ yyval.sym_ref = yyvsp[0].sym_ref; }
+break;
+case 83:
+#line 652 "aicasm_gram.y"
+{ yyval.value = 0; }
+break;
+case 84:
+#line 654 "aicasm_gram.y"
+{ yyval.value = 1; }
+break;
+case 85:
+#line 659 "aicasm_gram.y"
+{
+		critical_section_t *cs;
+
+		if (in_critical_section != FALSE) {
+			stop("Critical Section within Critical Section",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		cs = cs_alloc();
+		cs->begin_addr = instruction_ptr;
+		in_critical_section = TRUE;
+	}
+break;
+case 86:
+#line 674 "aicasm_gram.y"
+{
+		critical_section_t *cs;
+
+		if (in_critical_section == FALSE) {
+			stop("Unballanced 'end_cs'", EX_DATAERR);
+			/* NOTREACHED */
+		}
+		cs = TAILQ_LAST(&cs_tailq, cs_tailq);
+		cs->end_addr = instruction_ptr;
+		in_critical_section = FALSE;
+	}
+break;
+case 87:
+#line 688 "aicasm_gram.y"
+{
+		if (yyvsp[-1].sym->type != UNINITIALIZED) {
+			stop("Program label multiply defined", EX_DATAERR);
+			/* NOTREACHED */
+		}
+		yyvsp[-1].sym->type = LABEL;
+		initialize_symbol(yyvsp[-1].sym);
+		yyvsp[-1].sym->info.linfo->address = instruction_ptr;
+	}
+break;
+case 88:
+#line 701 "aicasm_gram.y"
+{
+		yyval.sym_ref.symbol = yyvsp[0].sym;
+		yyval.sym_ref.offset = 0;
+	}
+break;
+case 89:
+#line 706 "aicasm_gram.y"
+{
+		yyval.sym_ref.symbol = yyvsp[-2].sym;
+		yyval.sym_ref.offset = yyvsp[0].value;
+	}
+break;
+case 90:
+#line 711 "aicasm_gram.y"
+{
+		yyval.sym_ref.symbol = yyvsp[-2].sym;
+		yyval.sym_ref.offset = -yyvsp[0].value;
+	}
+break;
+case 91:
+#line 716 "aicasm_gram.y"
+{
+		yyval.sym_ref.symbol = NULL;
+		yyval.sym_ref.offset = 0;
+	}
+break;
+case 92:
+#line 721 "aicasm_gram.y"
+{
+		yyval.sym_ref.symbol = NULL;
+		yyval.sym_ref.offset = yyvsp[0].value;
+	}
+break;
+case 93:
+#line 726 "aicasm_gram.y"
+{
+		yyval.sym_ref.symbol = NULL;
+		yyval.sym_ref.offset = -yyvsp[0].value;
+	}
+break;
+case 94:
+#line 734 "aicasm_gram.y"
+{
+		scope_t *new_scope;
+
+		add_conditional(yyvsp[-1].sym);
+		new_scope = scope_alloc();
+		new_scope->type = SCOPE_IF;
+		new_scope->begin_addr = instruction_ptr;
+		new_scope->func_num = yyvsp[-1].sym->info.condinfo->func_num;
+	}
+break;
+case 95:
+#line 744 "aicasm_gram.y"
+{
+		scope_t *new_scope;
+		scope_t *scope_context;
+		scope_t *last_scope;
+
+		/*
+		 * Ensure that the previous scope is either an
+		 * if or and else if.
+		 */
+		scope_context = SLIST_FIRST(&scope_stack);
+		last_scope = TAILQ_LAST(&scope_context->inner_scope,
+					scope_tailq);
+		if (last_scope == NULL
+		 || last_scope->type == T_ELSE) {
+
+			stop("'else if' without leading 'if'", EX_DATAERR);
+			/* NOTREACHED */
+		}
+		add_conditional(yyvsp[-1].sym);
+		new_scope = scope_alloc();
+		new_scope->type = SCOPE_ELSE_IF;
+		new_scope->begin_addr = instruction_ptr;
+		new_scope->func_num = yyvsp[-1].sym->info.condinfo->func_num;
+	}
+break;
+case 96:
+#line 769 "aicasm_gram.y"
+{
+		scope_t *new_scope;
+		scope_t *scope_context;
+		scope_t *last_scope;
+
+		/*
+		 * Ensure that the previous scope is either an
+		 * if or and else if.
+		 */
+		scope_context = SLIST_FIRST(&scope_stack);
+		last_scope = TAILQ_LAST(&scope_context->inner_scope,
+					scope_tailq);
+		if (last_scope == NULL
+		 || last_scope->type == SCOPE_ELSE) {
+
+			stop("'else' without leading 'if'", EX_DATAERR);
+			/* NOTREACHED */
+		}
+		new_scope = scope_alloc();
+		new_scope->type = SCOPE_ELSE;
+		new_scope->begin_addr = instruction_ptr;
+	}
+break;
+case 97:
+#line 795 "aicasm_gram.y"
+{
+		scope_t *scope_context;
+
+		scope_context = SLIST_FIRST(&scope_stack);
+		if (scope_context->type == SCOPE_ROOT) {
+			stop("Unexpected '}' encountered", EX_DATAERR);
+			/* NOTREACHED */
+		}
+
+		scope_context->end_addr = instruction_ptr;
+
+		/* Pop the scope */
+		SLIST_REMOVE_HEAD(&scope_stack, scope_stack_links);
+
+		process_scope(scope_context);
+
+		if (SLIST_FIRST(&scope_stack) == NULL) {
+			stop("Unexpected '}' encountered", EX_DATAERR);
+			/* NOTREACHED */
+		}
+	}
+break;
+case 98:
+#line 819 "aicasm_gram.y"
+{ yyval.value = AIC_OP_AND; }
+break;
+case 99:
+#line 820 "aicasm_gram.y"
+{ yyval.value = AIC_OP_XOR; }
+break;
+case 100:
+#line 821 "aicasm_gram.y"
+{ yyval.value = AIC_OP_ADD; }
+break;
+case 101:
+#line 822 "aicasm_gram.y"
+{ yyval.value = AIC_OP_ADC; }
+break;
+case 102:
+#line 827 "aicasm_gram.y"
+{
+		format_1_instr(yyvsp[-6].value, &yyvsp[-5].sym_ref, &yyvsp[-3].expression, &yyvsp[-2].sym_ref, yyvsp[-1].value);
+	}
+break;
+case 103:
+#line 834 "aicasm_gram.y"
+{
+		format_1_instr(AIC_OP_OR, &yyvsp[-5].sym_ref, &yyvsp[-3].expression, &yyvsp[-2].sym_ref, yyvsp[-1].value);
+	}
+break;
+case 104:
+#line 841 "aicasm_gram.y"
+{
+		expression_t immed;
+
+		make_expression(&immed, 1);
+		format_1_instr(AIC_OP_ADD, &yyvsp[-3].sym_ref, &immed, &yyvsp[-2].sym_ref, yyvsp[-1].value);
+	}
+break;
+case 105:
+#line 851 "aicasm_gram.y"
+{
+		expression_t immed;
+
+		make_expression(&immed, -1);
+		format_1_instr(AIC_OP_ADD, &yyvsp[-3].sym_ref, &immed, &yyvsp[-2].sym_ref, yyvsp[-1].value);
+	}
+break;
+case 106:
+#line 861 "aicasm_gram.y"
+{
+		expression_t immed;
+
+		make_expression(&immed, -1);
+		format_1_instr(AIC_OP_ADD, &none, &immed, &allzeros, yyvsp[-1].value);
+	}
+break;
+case 107:
+#line 868 "aicasm_gram.y"
+{
+		format_1_instr(AIC_OP_ADD, &yyvsp[-4].sym_ref, &yyvsp[-2].expression, &allzeros, yyvsp[-1].value);
+	}
+break;
+case 108:
+#line 875 "aicasm_gram.y"
+{
+		expression_t immed;
+
+		make_expression(&immed, 1);
+		format_1_instr(AIC_OP_ADD, &none, &immed, &allones, yyvsp[-1].value);
+	}
+break;
+case 109:
+#line 882 "aicasm_gram.y"
+{
+		expression_t immed;
+
+		make_expression(&immed, 1);
+		format_1_instr(AIC_OP_ADD, &yyvsp[-2].sym_ref, &immed, &allones, yyvsp[-1].value);
+	}
+break;
+case 110:
+#line 892 "aicasm_gram.y"
+{
+		format_1_instr(AIC_OP_BMOV, &yyvsp[-6].sym_ref, &yyvsp[-2].expression, &yyvsp[-4].sym_ref, yyvsp[-1].value);
+	}
+break;
+case 111:
+#line 899 "aicasm_gram.y"
+{
+		expression_t immed;
+
+		make_expression(&immed, 1);
+		format_1_instr(AIC_OP_BMOV, &yyvsp[-4].sym_ref, &immed, &yyvsp[-2].sym_ref, yyvsp[-1].value);
+	}
+break;
+case 112:
+#line 909 "aicasm_gram.y"
+{
+		format_1_instr(AIC_OP_OR, &yyvsp[-4].sym_ref, &yyvsp[-2].expression, &allzeros, yyvsp[-1].value);
+	}
+break;
+case 113:
+#line 916 "aicasm_gram.y"
+{
+		expression_t immed;
+
+		make_expression(&immed, 0xff);
+		format_1_instr(AIC_OP_XOR, &yyvsp[-3].sym_ref, &immed, &yyvsp[-2].sym_ref, yyvsp[-1].value);
+	}
+break;
+case 114:
+#line 926 "aicasm_gram.y"
+{
+		expression_t immed;
+
+		make_expression(&immed, 0xff);
+		format_1_instr(AIC_OP_AND, &yyvsp[-2].sym_ref, &immed, &allzeros, yyvsp[-1].value);
+	}
+break;
+case 115:
+#line 936 "aicasm_gram.y"
+{
+		expression_t immed;
+
+		make_expression(&immed, 0xff);
+		format_1_instr(AIC_OP_AND, &none, &immed, &allzeros, yyvsp[-1].value);
+	}
+break;
+case 116:
+#line 946 "aicasm_gram.y"
+{
+		expression_t immed;
+
+		make_expression(&immed, 0xff);
+		format_1_instr(AIC_OP_AND, &none, &immed, &allzeros, TRUE);
+	}
+break;
+case 117:
+#line 964 "aicasm_gram.y"
+{ yyval.value = AIC_OP_SHL; }
+break;
+case 118:
+#line 965 "aicasm_gram.y"
+{ yyval.value = AIC_OP_SHR; }
+break;
+case 119:
+#line 966 "aicasm_gram.y"
+{ yyval.value = AIC_OP_ROL; }
+break;
+case 120:
+#line 967 "aicasm_gram.y"
+{ yyval.value = AIC_OP_ROR; }
+break;
+case 121:
+#line 972 "aicasm_gram.y"
+{
+		format_2_instr(yyvsp[-6].value, &yyvsp[-5].sym_ref, &yyvsp[-3].expression, &yyvsp[-2].sym_ref, yyvsp[-1].value);
+	}
+break;
+case 122:
+#line 978 "aicasm_gram.y"
+{ yyval.value = AIC_OP_JMP; }
+break;
+case 123:
+#line 979 "aicasm_gram.y"
+{ yyval.value = AIC_OP_JC; }
+break;
+case 124:
+#line 980 "aicasm_gram.y"
+{ yyval.value = AIC_OP_JNC; }
+break;
+case 125:
+#line 981 "aicasm_gram.y"
+{ yyval.value = AIC_OP_CALL; }
+break;
+case 126:
+#line 985 "aicasm_gram.y"
+{ yyval.value = AIC_OP_JZ; }
+break;
+case 127:
+#line 986 "aicasm_gram.y"
+{ yyval.value = AIC_OP_JNZ; }
+break;
+case 128:
+#line 990 "aicasm_gram.y"
+{ yyval.value = AIC_OP_JE; }
+break;
+case 129:
+#line 991 "aicasm_gram.y"
+{ yyval.value = AIC_OP_JNE; }
+break;
+case 130:
+#line 996 "aicasm_gram.y"
+{
+		expression_t immed;
+
+		make_expression(&immed, 0);
+		format_3_instr(yyvsp[-2].value, &sindex, &immed, &yyvsp[-1].sym_ref);
+	}
+break;
+case 131:
+#line 1006 "aicasm_gram.y"
+{
+		format_3_instr(yyvsp[-2].value, &yyvsp[-5].sym_ref, &yyvsp[-3].expression, &yyvsp[-1].sym_ref);
+	}
+break;
+case 132:
+#line 1013 "aicasm_gram.y"
+{
+		format_3_instr(yyvsp[-2].value, &yyvsp[-5].sym_ref, &yyvsp[-3].expression, &yyvsp[-1].sym_ref);
+	}
+break;
+case 133:
+#line 1020 "aicasm_gram.y"
+{
+		format_3_instr(yyvsp[-2].value, &yyvsp[-5].sym_ref, &yyvsp[-3].expression, &yyvsp[-1].sym_ref);
+	}
+break;
+case 134:
+#line 1027 "aicasm_gram.y"
+{
+		expression_t immed;
+
+		make_expression(&immed, 0);
+		format_3_instr(yyvsp[-2].value, &yyvsp[-3].sym_ref, &immed, &yyvsp[-1].sym_ref);
+	}
+break;
+case 135:
+#line 1037 "aicasm_gram.y"
+{
+		format_3_instr(yyvsp[-2].value, &allzeros, &yyvsp[-3].expression, &yyvsp[-1].sym_ref);
+	}
+break;
+#line 2003 "y.tab.c"
+    }
+    yyssp -= yym;
+    yystate = *yyssp;
+    yyvsp -= yym;
+    yym = yylhs[yyn];
+    if (yystate == 0 && yym == 0)
+    {
+#if YYDEBUG
+        if (yydebug)
+            printf("%sdebug: after reduction, shifting from state 0 to\
+ state %d\n", YYPREFIX, YYFINAL);
+#endif
+        yystate = YYFINAL;
+        *++yyssp = YYFINAL;
+        *++yyvsp = yyval;
+        if (yychar < 0)
+        {
+            if ((yychar = yylex()) < 0) yychar = 0;
+#if YYDEBUG
+            if (yydebug)
+            {
+                yys = 0;
+                if (yychar <= YYMAXTOKEN) yys = yyname[yychar];
+                if (!yys) yys = "illegal-symbol";
+                printf("%sdebug: state %d, reading %d (%s)\n",
+                        YYPREFIX, YYFINAL, yychar, yys);
+            }
+#endif
+        }
+        if (yychar == 0) goto yyaccept;
+        goto yyloop;
+    }
+    if ((yyn = yygindex[yym]) && (yyn += yystate) >= 0 &&
+            yyn <= YYTABLESIZE && yycheck[yyn] == yystate)
+        yystate = yytable[yyn];
+    else
+        yystate = yydgoto[yym];
+#if YYDEBUG
+    if (yydebug)
+        printf("%sdebug: after reduction, shifting from state %d \
+to state %d\n", YYPREFIX, *yyssp, yystate);
+#endif
+    if (yyssp >= yyss + yystacksize - 1)
+    {
+        goto yyoverflow;
+    }
+    *++yyssp = yystate;
+    *++yyvsp = yyval;
+    goto yyloop;
+yyoverflow:
+    yyerror("yacc stack overflow");
+yyabort:
+    return (1);
+yyaccept:
+    return (0);
+}
diff -urN linux/drivers/scsi/aic7xxx/aicasm/aicasm_gram.y /tmp/linux/drivers/scsi/aic7xxx/aicasm/aicasm_gram.y
--- linux/drivers/scsi/aic7xxx/aicasm/aicasm_gram.y	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aicasm/aicasm_gram.y	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,1455 @@
+%{
+/*
+ * Parser for the Aic7xxx SCSI Host adapter sequencer assembler.
+ *
+ * Copyright (c) 1997, 1998, 2000 Justin T. Gibbs.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $Id: //depot/src/aic7xxx/aicasm/aicasm_gram.y#6 $
+ *
+ * $FreeBSD: src/sys/dev/aic7xxx/aicasm/aicasm_gram.y,v 1.12 2000/10/31 18:44:32 gibbs Exp $
+ */
+
+#include <inttypes.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <sysexits.h>
+
+#include <sys/types.h>
+#ifdef __linux__
+#include "../queue.h"
+#else
+#include <sys/queue.h>
+#endif
+
+#include "aicasm.h"
+#include "aicasm_symbol.h"
+#include "aicasm_insformat.h"
+
+int yylineno;
+char *yyfilename;
+static symbol_t *cur_symbol;
+static symtype cur_symtype;
+static symbol_t *accumulator;
+static symbol_ref_t allones;
+static symbol_ref_t allzeros;
+static symbol_ref_t none;
+static symbol_ref_t sindex;
+static int instruction_ptr;
+static int sram_or_scb_offset;
+static int download_constant_count;
+static int in_critical_section;
+
+static void process_bitmask(int mask_type, symbol_t *sym, int mask);
+static void initialize_symbol(symbol_t *symbol);
+static void process_register(symbol_t **p_symbol);
+static void format_1_instr(int opcode, symbol_ref_t *dest,
+			   expression_t *immed, symbol_ref_t *src, int ret);
+static void format_2_instr(int opcode, symbol_ref_t *dest,
+			   expression_t *places, symbol_ref_t *src, int ret);
+static void format_3_instr(int opcode, symbol_ref_t *src,
+			   expression_t *immed, symbol_ref_t *address);
+static void test_readable_symbol(symbol_t *symbol);
+static void test_writable_symbol(symbol_t *symbol);
+static void type_check(symbol_t *symbol, expression_t *expression, int and_op);
+static void make_expression(expression_t *immed, int value);
+static void add_conditional(symbol_t *symbol);
+static int  is_download_const(expression_t *immed);
+
+#define YYDEBUG 1
+#define SRAM_SYMNAME "SRAM_BASE"
+#define SCB_SYMNAME "SCB_BASE"
+%}
+
+%union {
+	int		value;
+	char		*str;
+	symbol_t	*sym;
+	symbol_ref_t	sym_ref;
+	expression_t	expression;
+}
+
+%token T_REGISTER
+
+%token <value> T_CONST
+
+%token T_DOWNLOAD
+
+%token T_SCB
+
+%token T_SRAM
+
+%token T_ALIAS
+
+%token T_SIZE
+
+%token <value> T_ADDRESS
+
+%token T_ACCESS_MODE
+
+%token <value> T_MODE
+
+%token T_BEGIN_CS
+
+%token T_END_CS
+
+%token T_BIT
+
+%token T_MASK
+
+%token <value> T_NUMBER
+
+%token <str> T_PATH
+
+%token <sym> T_CEXPR
+
+%token T_EOF T_INCLUDE 
+
+%token <value> T_SHR T_SHL T_ROR T_ROL
+
+%token <value> T_MVI T_MOV T_CLR T_BMOV
+
+%token <value> T_JMP T_JC T_JNC T_JE T_JNE T_JNZ T_JZ T_CALL
+
+%token <value> T_ADD T_ADC
+
+%token <value> T_INC T_DEC
+
+%token <value> T_STC T_CLC
+
+%token <value> T_CMP T_NOT T_XOR
+
+%token <value> T_TEST T_AND
+
+%token <value> T_OR
+
+%token T_RET
+
+%token T_NOP
+
+%token T_ACCUM T_ALLONES T_ALLZEROS T_NONE T_SINDEX
+
+%token T_A
+
+%token <sym> T_SYMBOL
+
+%token T_NL
+
+%token T_IF T_ELSE T_ELSE_IF T_ENDIF
+
+%type <sym_ref> reg_symbol address destination source opt_source
+
+%type <expression> expression immediate immediate_or_a
+
+%type <value> ret f1_opcode f2_opcode jmp_jc_jnc_call jz_jnz je_jne
+
+%type <value> numerical_value
+
+%left '|'
+%left '&'
+%left '+' '-'
+%right '~'
+%nonassoc UMINUS
+%%
+
+program:
+	include
+|	program include
+|	register
+|	program register
+|	constant
+|	program constant
+|	scratch_ram
+|	program scratch_ram
+|	scb
+|	program scb
+|	label
+|	program label
+|	critical_section_start
+|	program critical_section_start
+|	critical_section_end
+|	program critical_section_end
+|	conditional
+|	program conditional
+|	code
+|	program code
+;
+
+include:
+	T_INCLUDE '<' T_PATH '>'
+	{ include_file($3, BRACKETED_INCLUDE); }
+|	T_INCLUDE '"' T_PATH '"'
+	{ include_file($3, QUOTED_INCLUDE); }
+;
+
+register:
+	T_REGISTER { cur_symtype = REGISTER; } reg_definition
+;
+
+reg_definition:
+	T_SYMBOL '{'
+		{
+			if ($1->type != UNINITIALIZED) {
+				stop("Register multiply defined", EX_DATAERR);
+				/* NOTREACHED */
+			}
+			cur_symbol = $1; 
+			cur_symbol->type = cur_symtype;
+			initialize_symbol(cur_symbol);
+		}
+		reg_attribute_list
+	'}'
+		{                    
+			/*
+			 * Default to allowing everything in for registers
+			 * with no bit or mask definitions.
+			 */
+			if (cur_symbol->info.rinfo->valid_bitmask == 0)
+				cur_symbol->info.rinfo->valid_bitmask = 0xFF;
+
+			if (cur_symbol->info.rinfo->size == 0)
+				cur_symbol->info.rinfo->size = 1;
+
+			/*
+			 * This might be useful for registers too.
+			 */
+			if (cur_symbol->type != REGISTER) {
+				if (cur_symbol->info.rinfo->address == 0)
+					cur_symbol->info.rinfo->address =
+					    sram_or_scb_offset;
+				sram_or_scb_offset +=
+				    cur_symbol->info.rinfo->size;
+			}
+			cur_symbol = NULL;
+		}
+;
+
+reg_attribute_list:
+	reg_attribute
+|	reg_attribute_list reg_attribute
+;
+
+reg_attribute:		
+	reg_address
+|	size
+|	access_mode
+|	bit_defn
+|	mask_defn
+|	alias
+|	accumulator
+|	allones
+|	allzeros
+|	none
+|	sindex
+;
+
+reg_address:
+	T_ADDRESS T_NUMBER
+	{
+		cur_symbol->info.rinfo->address = $2;
+	}
+;
+
+size:
+	T_SIZE T_NUMBER
+	{
+		cur_symbol->info.rinfo->size = $2;
+	}
+;
+
+access_mode:
+	T_ACCESS_MODE T_MODE
+	{
+		cur_symbol->info.rinfo->mode = $2;
+	}
+;
+
+bit_defn:
+	T_BIT T_SYMBOL T_NUMBER
+	{
+		process_bitmask(BIT, $2, $3);
+	}
+;
+
+mask_defn:
+	T_MASK T_SYMBOL expression
+	{
+		process_bitmask(MASK, $2, $3.value);
+	}
+;
+
+alias:
+	T_ALIAS	T_SYMBOL
+	{
+		if ($2->type != UNINITIALIZED) {
+			stop("Re-definition of register alias",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		$2->type = ALIAS;
+		initialize_symbol($2);
+		$2->info.ainfo->parent = cur_symbol;
+	}
+;
+
+accumulator:
+	T_ACCUM
+	{
+		if (accumulator != NULL) {
+			stop("Only one accumulator definition allowed",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		accumulator = cur_symbol;
+	}
+;
+
+allones:
+	T_ALLONES
+	{
+		if (allones.symbol != NULL) {
+			stop("Only one definition of allones allowed",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		allones.symbol = cur_symbol;
+	}
+;
+
+allzeros:
+	T_ALLZEROS
+	{
+		if (allzeros.symbol != NULL) {
+			stop("Only one definition of allzeros allowed",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		allzeros.symbol = cur_symbol;
+	}
+;
+
+none:
+	T_NONE
+	{
+		if (none.symbol != NULL) {
+			stop("Only one definition of none allowed",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		none.symbol = cur_symbol;
+	}
+;
+
+sindex:
+	T_SINDEX
+	{
+		if (sindex.symbol != NULL) {
+			stop("Only one definition of sindex allowed",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		sindex.symbol = cur_symbol;
+	}
+;
+
+expression:
+	expression '|' expression
+	{
+		 $$.value = $1.value | $3.value;
+		 symlist_merge(&$$.referenced_syms,
+			       &$1.referenced_syms,
+			       &$3.referenced_syms);
+	}
+|	expression '&' expression
+	{
+		$$.value = $1.value & $3.value;
+		symlist_merge(&$$.referenced_syms,
+			       &$1.referenced_syms,
+			       &$3.referenced_syms);
+	}
+|	expression '+' expression
+	{
+		$$.value = $1.value + $3.value;
+		symlist_merge(&$$.referenced_syms,
+			       &$1.referenced_syms,
+			       &$3.referenced_syms);
+	}
+|	expression '-' expression
+	{
+		$$.value = $1.value - $3.value;
+		symlist_merge(&($$.referenced_syms),
+			       &($1.referenced_syms),
+			       &($3.referenced_syms));
+	}
+|	'(' expression ')'
+	{
+		$$ = $2;
+	}
+|	'~' expression
+	{
+		$$ = $2;
+		$$.value = (~$$.value) & 0xFF;
+	}
+|	'-' expression %prec UMINUS
+	{
+		$$ = $2;
+		$$.value = -$$.value;
+	}
+|	T_NUMBER
+	{
+		$$.value = $1;
+		SLIST_INIT(&$$.referenced_syms);
+	}
+|	T_SYMBOL
+	{
+		symbol_t *symbol;
+
+		symbol = $1;
+		switch (symbol->type) {
+		case ALIAS:
+			symbol = $1->info.ainfo->parent;
+		case REGISTER:
+		case SCBLOC:
+		case SRAMLOC:
+			$$.value = symbol->info.rinfo->address;
+			break;
+		case MASK:
+		case BIT:
+			$$.value = symbol->info.minfo->mask;
+			break;
+		case DOWNLOAD_CONST:
+		case CONST:
+			$$.value = symbol->info.cinfo->value;
+			break;
+		case UNINITIALIZED:
+		default:
+		{
+			char buf[255];
+
+			snprintf(buf, sizeof(buf),
+				 "Undefined symbol %s referenced",
+				 symbol->name);
+			stop(buf, EX_DATAERR);
+			/* NOTREACHED */
+			break;
+		}
+		}
+		SLIST_INIT(&$$.referenced_syms);
+		symlist_add(&$$.referenced_syms, symbol, SYMLIST_INSERT_HEAD);
+	}
+;
+
+constant:
+	T_CONST T_SYMBOL numerical_value
+	{
+		if ($2->type != UNINITIALIZED) {
+			stop("Re-definition of symbol as a constant",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		$2->type = CONST;
+		initialize_symbol($2);
+		$2->info.cinfo->value = $3;
+		$2->info.cinfo->define = $1;
+	}
+|	T_CONST T_SYMBOL T_DOWNLOAD
+	{
+		if ($1) {
+			stop("Invalid downloaded constant declaration",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		if ($2->type != UNINITIALIZED) {
+			stop("Re-definition of symbol as a downloaded constant",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		$2->type = DOWNLOAD_CONST;
+		initialize_symbol($2);
+		$2->info.cinfo->value = download_constant_count++;
+		$2->info.cinfo->define = FALSE;
+	}
+;
+
+numerical_value:
+	T_NUMBER
+	{
+		$$ = $1;
+	}
+|	'-' T_NUMBER
+	{
+		$$ = -$2;
+	}
+;
+
+scratch_ram:
+	T_SRAM '{'
+		{
+			cur_symbol = symtable_get(SRAM_SYMNAME);
+			cur_symtype = SRAMLOC;
+			if (cur_symbol->type != UNINITIALIZED) {
+				stop("Only one SRAM definition allowed",
+				     EX_DATAERR);
+				/* NOTREACHED */
+			}
+			cur_symbol->type = SRAMLOC;
+			initialize_symbol(cur_symbol);
+		}
+		reg_address
+		{
+			sram_or_scb_offset = cur_symbol->info.rinfo->address;
+		}
+		scb_or_sram_reg_list
+	'}'
+		{
+			cur_symbol = NULL;
+		}
+;
+
+scb:
+	T_SCB '{'
+		{
+			cur_symbol = symtable_get(SCB_SYMNAME);
+			cur_symtype = SCBLOC;
+			if (cur_symbol->type != UNINITIALIZED) {
+				stop("Only one SRAM definition allowed",
+				     EX_SOFTWARE);
+				/* NOTREACHED */
+			}
+			cur_symbol->type = SCBLOC;
+			initialize_symbol(cur_symbol);
+			/* 64 bytes of SCB space */
+			cur_symbol->info.rinfo->size = 64;
+		}
+		reg_address
+		{
+			sram_or_scb_offset = cur_symbol->info.rinfo->address;
+		}
+		scb_or_sram_reg_list
+	'}'
+		{
+			cur_symbol = NULL;
+		}
+;
+
+scb_or_sram_reg_list:
+	reg_definition
+|	scb_or_sram_reg_list reg_definition
+;
+
+reg_symbol:
+	T_SYMBOL
+	{
+		process_register(&$1);
+		$$.symbol = $1;
+		$$.offset = 0;
+	}
+|	T_SYMBOL '[' T_SYMBOL ']'
+	{
+		process_register(&$1);
+		if ($3->type != CONST) {
+			stop("register offset must be a constant", EX_DATAERR);
+			/* NOTREACHED */
+		}
+		if (($3->info.cinfo->value + 1) > $1->info.rinfo->size) {
+			stop("Accessing offset beyond range of register",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		$$.symbol = $1;
+		$$.offset = $3->info.cinfo->value;
+	}
+|	T_SYMBOL '[' T_NUMBER ']'
+	{
+		process_register(&$1);
+		if (($3 + 1) > $1->info.rinfo->size) {
+			stop("Accessing offset beyond range of register",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		$$.symbol = $1;
+		$$.offset = $3;
+	}
+|	T_A
+	{
+		if (accumulator == NULL) {
+			stop("No accumulator has been defined", EX_DATAERR);
+			/* NOTREACHED */
+		}
+		$$.symbol = accumulator;
+		$$.offset = 0;
+	}
+;
+
+destination:
+	reg_symbol
+	{
+		test_writable_symbol($1.symbol);
+		$$ = $1;
+	}
+;
+
+immediate:
+	expression
+	{ $$ = $1; }
+;
+
+immediate_or_a:
+	expression
+	{
+		$$ = $1;
+	}
+|	T_A
+	{
+		SLIST_INIT(&$$.referenced_syms);
+		$$.value = 0;
+	}
+;
+
+source:
+	reg_symbol
+	{
+		test_readable_symbol($1.symbol);
+		$$ = $1;
+	}
+;
+
+opt_source:
+	{
+		$$.symbol = NULL;
+		$$.offset = 0;
+	}
+|	',' source
+	{ $$ = $2; }
+;
+
+ret:
+	{ $$ = 0; }
+|	T_RET
+	{ $$ = 1; }
+;
+
+critical_section_start:
+	T_BEGIN_CS
+	{
+		critical_section_t *cs;
+
+		if (in_critical_section != FALSE) {
+			stop("Critical Section within Critical Section",
+			     EX_DATAERR);
+			/* NOTREACHED */
+		}
+		cs = cs_alloc();
+		cs->begin_addr = instruction_ptr;
+		in_critical_section = TRUE;
+	}
+
+critical_section_end:
+	T_END_CS
+	{
+		critical_section_t *cs;
+
+		if (in_critical_section == FALSE) {
+			stop("Unballanced 'end_cs'", EX_DATAERR);
+			/* NOTREACHED */
+		}
+		cs = TAILQ_LAST(&cs_tailq, cs_tailq);
+		cs->end_addr = instruction_ptr;
+		in_critical_section = FALSE;
+	}
+
+label:
+	T_SYMBOL ':'
+	{
+		if ($1->type != UNINITIALIZED) {
+			stop("Program label multiply defined", EX_DATAERR);
+			/* NOTREACHED */
+		}
+		$1->type = LABEL;
+		initialize_symbol($1);
+		$1->info.linfo->address = instruction_ptr;
+	}
+;
+
+address:
+	T_SYMBOL
+	{
+		$$.symbol = $1;
+		$$.offset = 0;
+	}
+|	T_SYMBOL '+' T_NUMBER
+	{
+		$$.symbol = $1;
+		$$.offset = $3;
+	}
+|	T_SYMBOL '-' T_NUMBER
+	{
+		$$.symbol = $1;
+		$$.offset = -$3;
+	}
+|	'.'
+	{
+		$$.symbol = NULL;
+		$$.offset = 0;
+	}
+|	'.' '+' T_NUMBER
+	{
+		$$.symbol = NULL;
+		$$.offset = $3;
+	}
+|	'.' '-' T_NUMBER
+	{
+		$$.symbol = NULL;
+		$$.offset = -$3;
+	}
+;
+
+conditional:
+	T_IF T_CEXPR '{'
+	{
+		scope_t *new_scope;
+
+		add_conditional($2);
+		new_scope = scope_alloc();
+		new_scope->type = SCOPE_IF;
+		new_scope->begin_addr = instruction_ptr;
+		new_scope->func_num = $2->info.condinfo->func_num;
+	}
+|	T_ELSE T_IF T_CEXPR '{'
+	{
+		scope_t *new_scope;
+		scope_t *scope_context;
+		scope_t *last_scope;
+
+		/*
+		 * Ensure that the previous scope is either an
+		 * if or and else if.
+		 */
+		scope_context = SLIST_FIRST(&scope_stack);
+		last_scope = TAILQ_LAST(&scope_context->inner_scope,
+					scope_tailq);
+		if (last_scope == NULL
+		 || last_scope->type == T_ELSE) {
+
+			stop("'else if' without leading 'if'", EX_DATAERR);
+			/* NOTREACHED */
+		}
+		add_conditional($3);
+		new_scope = scope_alloc();
+		new_scope->type = SCOPE_ELSE_IF;
+		new_scope->begin_addr = instruction_ptr;
+		new_scope->func_num = $3->info.condinfo->func_num;
+	}
+|	T_ELSE '{'
+	{
+		scope_t *new_scope;
+		scope_t *scope_context;
+		scope_t *last_scope;
+
+		/*
+		 * Ensure that the previous scope is either an
+		 * if or and else if.
+		 */
+		scope_context = SLIST_FIRST(&scope_stack);
+		last_scope = TAILQ_LAST(&scope_context->inner_scope,
+					scope_tailq);
+		if (last_scope == NULL
+		 || last_scope->type == SCOPE_ELSE) {
+
+			stop("'else' without leading 'if'", EX_DATAERR);
+			/* NOTREACHED */
+		}
+		new_scope = scope_alloc();
+		new_scope->type = SCOPE_ELSE;
+		new_scope->begin_addr = instruction_ptr;
+	}
+;
+
+conditional:
+	'}'
+	{
+		scope_t *scope_context;
+
+		scope_context = SLIST_FIRST(&scope_stack);
+		if (scope_context->type == SCOPE_ROOT) {
+			stop("Unexpected '}' encountered", EX_DATAERR);
+			/* NOTREACHED */
+		}
+
+		scope_context->end_addr = instruction_ptr;
+
+		/* Pop the scope */
+		SLIST_REMOVE_HEAD(&scope_stack, scope_stack_links);
+
+		process_scope(scope_context);
+
+		if (SLIST_FIRST(&scope_stack) == NULL) {
+			stop("Unexpected '}' encountered", EX_DATAERR);
+			/* NOTREACHED */
+		}
+	}
+;
+
+f1_opcode:
+	T_AND { $$ = AIC_OP_AND; }
+|	T_XOR { $$ = AIC_OP_XOR; }
+|	T_ADD { $$ = AIC_OP_ADD; }
+|	T_ADC { $$ = AIC_OP_ADC; }
+;
+
+code:
+	f1_opcode destination ',' immediate_or_a opt_source ret ';'
+	{
+		format_1_instr($1, &$2, &$4, &$5, $6);
+	}
+;
+
+code:
+	T_OR reg_symbol ',' immediate_or_a opt_source ret ';'
+	{
+		format_1_instr(AIC_OP_OR, &$2, &$4, &$5, $6);
+	}
+;
+
+code:
+	T_INC destination opt_source ret ';'
+	{
+		expression_t immed;
+
+		make_expression(&immed, 1);
+		format_1_instr(AIC_OP_ADD, &$2, &immed, &$3, $4);
+	}
+;
+
+code:
+	T_DEC destination opt_source ret ';'
+	{
+		expression_t immed;
+
+		make_expression(&immed, -1);
+		format_1_instr(AIC_OP_ADD, &$2, &immed, &$3, $4);
+	}
+;
+
+code:
+	T_CLC ret ';'
+	{
+		expression_t immed;
+
+		make_expression(&immed, -1);
+		format_1_instr(AIC_OP_ADD, &none, &immed, &allzeros, $2);
+	}
+|	T_CLC T_MVI destination ',' immediate_or_a ret ';'
+	{
+		format_1_instr(AIC_OP_ADD, &$3, &$5, &allzeros, $6);
+	}
+;
+
+code:
+	T_STC ret ';'
+	{
+		expression_t immed;
+
+		make_expression(&immed, 1);
+		format_1_instr(AIC_OP_ADD, &none, &immed, &allones, $2);
+	}
+|	T_STC destination ret ';'
+	{
+		expression_t immed;
+
+		make_expression(&immed, 1);
+		format_1_instr(AIC_OP_ADD, &$2, &immed, &allones, $3);
+	}
+;
+
+code:
+	T_BMOV destination ',' source ',' immediate ret ';'
+	{
+		format_1_instr(AIC_OP_BMOV, &$2, &$6, &$4, $7);
+	}
+;
+
+code:
+	T_MOV destination ',' source ret ';'
+	{
+		expression_t immed;
+
+		make_expression(&immed, 1);
+		format_1_instr(AIC_OP_BMOV, &$2, &immed, &$4, $5);
+	}
+;
+
+code:
+	T_MVI destination ',' immediate_or_a ret ';'
+	{
+		format_1_instr(AIC_OP_OR, &$2, &$4, &allzeros, $5);
+	}
+;
+
+code:
+	T_NOT destination opt_source ret ';'
+	{
+		expression_t immed;
+
+		make_expression(&immed, 0xff);
+		format_1_instr(AIC_OP_XOR, &$2, &immed, &$3, $4);
+	}
+;
+
+code:
+	T_CLR destination ret ';'
+	{
+		expression_t immed;
+
+		make_expression(&immed, 0xff);
+		format_1_instr(AIC_OP_AND, &$2, &immed, &allzeros, $3);
+	}
+;
+
+code:
+	T_NOP ret ';'
+	{
+		expression_t immed;
+
+		make_expression(&immed, 0xff);
+		format_1_instr(AIC_OP_AND, &none, &immed, &allzeros, $2);
+	}
+;
+
+code:
+	T_RET ';'
+	{
+		expression_t immed;
+
+		make_expression(&immed, 0xff);
+		format_1_instr(AIC_OP_AND, &none, &immed, &allzeros, TRUE);
+	}
+;
+
+	/*
+	 * This grammer differs from the one in the aic7xxx
+	 * reference manual since the grammer listed there is
+	 * ambiguous and causes a shift/reduce conflict.
+	 * It also seems more logical as the "immediate"
+	 * argument is listed as the second arg like the
+	 * other formats.
+	 */
+
+f2_opcode:
+	T_SHL { $$ = AIC_OP_SHL; }
+|	T_SHR { $$ = AIC_OP_SHR; }
+|	T_ROL { $$ = AIC_OP_ROL; }
+|	T_ROR { $$ = AIC_OP_ROR; }
+;
+
+code:
+	f2_opcode destination ',' expression opt_source ret ';'
+	{
+		format_2_instr($1, &$2, &$4, &$5, $6);
+	}
+;
+
+jmp_jc_jnc_call:
+	T_JMP	{ $$ = AIC_OP_JMP; }
+|	T_JC	{ $$ = AIC_OP_JC; }
+|	T_JNC	{ $$ = AIC_OP_JNC; }
+|	T_CALL	{ $$ = AIC_OP_CALL; }
+;
+
+jz_jnz:
+	T_JZ	{ $$ = AIC_OP_JZ; }
+|	T_JNZ	{ $$ = AIC_OP_JNZ; }
+;
+
+je_jne:
+	T_JE	{ $$ = AIC_OP_JE; }
+|	T_JNE	{ $$ = AIC_OP_JNE; }
+;
+
+code:
+	jmp_jc_jnc_call address ';'
+	{
+		expression_t immed;
+
+		make_expression(&immed, 0);
+		format_3_instr($1, &sindex, &immed, &$2);
+	}
+;
+
+code:
+	T_OR reg_symbol ',' immediate jmp_jc_jnc_call address ';'
+	{
+		format_3_instr($5, &$2, &$4, &$6);
+	}
+;
+
+code:
+	T_TEST source ',' immediate_or_a jz_jnz address ';'
+	{
+		format_3_instr($5, &$2, &$4, &$6);
+	}
+;
+
+code:
+	T_CMP source ',' immediate_or_a je_jne address ';'
+	{
+		format_3_instr($5, &$2, &$4, &$6);
+	}
+;
+
+code:
+	T_MOV source jmp_jc_jnc_call address ';'
+	{
+		expression_t immed;
+
+		make_expression(&immed, 0);
+		format_3_instr($3, &$2, &immed, &$4);
+	}
+;
+
+code:
+	T_MVI immediate jmp_jc_jnc_call address ';'
+	{
+		format_3_instr($3, &allzeros, &$2, &$4);
+	}
+;
+
+%%
+
+static void
+process_bitmask(int mask_type, symbol_t *sym, int mask)
+{
+	/*
+	 * Add the current register to its
+	 * symbol list, if it already exists,
+	 * warn if we are setting it to a
+	 * different value, or in the bit to
+	 * the "allowed bits" of this register.
+	 */
+	if (sym->type == UNINITIALIZED) {
+		sym->type = mask_type;
+		initialize_symbol(sym);
+		if (mask_type == BIT) {
+			if (mask == 0) {
+				stop("Bitmask with no bits set", EX_DATAERR);
+				/* NOTREACHED */
+			}
+			if ((mask & ~(0x01 << (ffs(mask) - 1))) != 0) {
+				stop("Bitmask with more than one bit set",
+				     EX_DATAERR);
+				/* NOTREACHED */
+			}
+		}
+		sym->info.minfo->mask = mask;
+	} else if (sym->type != mask_type) {
+		stop("Bit definition mirrors a definition of the same "
+		     " name, but a different type", EX_DATAERR);
+		/* NOTREACHED */
+	} else if (mask != sym->info.minfo->mask) {
+		stop("Bitmask redefined with a conflicting value", EX_DATAERR);
+		/* NOTREACHED */
+	}
+	/* Fail if this symbol is already listed */
+	if (symlist_search(&(sym->info.minfo->symrefs),
+			   cur_symbol->name) != NULL) {
+		stop("Bitmask defined multiple times for register", EX_DATAERR);
+		/* NOTREACHED */
+	}
+	symlist_add(&(sym->info.minfo->symrefs), cur_symbol,
+		    SYMLIST_INSERT_HEAD);
+	cur_symbol->info.rinfo->valid_bitmask |= mask;
+	cur_symbol->info.rinfo->typecheck_masks = TRUE;
+}
+
+static void
+initialize_symbol(symbol_t *symbol)
+{
+	switch (symbol->type) {
+        case UNINITIALIZED:
+		stop("Call to initialize_symbol with type field unset",
+		     EX_SOFTWARE);
+		/* NOTREACHED */
+		break;
+        case REGISTER:
+        case SRAMLOC:
+        case SCBLOC:
+		symbol->info.rinfo =
+		    (struct reg_info *)malloc(sizeof(struct reg_info));
+		if (symbol->info.rinfo == NULL) {
+			stop("Can't create register info", EX_SOFTWARE);
+			/* NOTREACHED */
+		}
+		memset(symbol->info.rinfo, 0,
+		       sizeof(struct reg_info));
+		break;
+        case ALIAS:
+		symbol->info.ainfo =
+		    (struct alias_info *)malloc(sizeof(struct alias_info));
+		if (symbol->info.ainfo == NULL) {
+			stop("Can't create alias info", EX_SOFTWARE);
+			/* NOTREACHED */
+		}
+		memset(symbol->info.ainfo, 0,
+		       sizeof(struct alias_info));
+		break;
+        case MASK:
+        case BIT:
+		symbol->info.minfo =
+		    (struct mask_info *)malloc(sizeof(struct mask_info));
+		if (symbol->info.minfo == NULL) {
+			stop("Can't create bitmask info", EX_SOFTWARE);
+			/* NOTREACHED */
+		}
+		memset(symbol->info.minfo, 0, sizeof(struct mask_info));
+		SLIST_INIT(&(symbol->info.minfo->symrefs));
+		break;
+        case CONST:
+        case DOWNLOAD_CONST:
+		symbol->info.cinfo =
+		    (struct const_info *)malloc(sizeof(struct const_info));
+		if (symbol->info.cinfo == NULL) {
+			stop("Can't create alias info", EX_SOFTWARE);
+			/* NOTREACHED */
+		}
+		memset(symbol->info.cinfo, 0,
+		       sizeof(struct const_info));
+		break;
+	case LABEL:
+		symbol->info.linfo =
+		    (struct label_info *)malloc(sizeof(struct label_info));
+		if (symbol->info.linfo == NULL) {
+			stop("Can't create label info", EX_SOFTWARE);
+			/* NOTREACHED */
+		}
+		memset(symbol->info.linfo, 0,
+		       sizeof(struct label_info));
+		break;
+	case CONDITIONAL:
+		symbol->info.condinfo =
+		    (struct cond_info *)malloc(sizeof(struct cond_info));
+		if (symbol->info.condinfo == NULL) {
+			stop("Can't create conditional info", EX_SOFTWARE);
+			/* NOTREACHED */
+		}
+		memset(symbol->info.condinfo, 0,
+		       sizeof(struct cond_info));
+		break;
+	default:
+		stop("Call to initialize_symbol with invalid symbol type",
+		     EX_SOFTWARE);
+		/* NOTREACHED */
+		break;
+	}
+}
+
+static void
+process_register(symbol_t **p_symbol)
+{
+	char buf[255];
+	symbol_t *symbol = *p_symbol;
+
+	if (symbol->type == UNINITIALIZED) {
+		snprintf(buf, sizeof(buf), "Undefined register %s",
+			 symbol->name);
+		stop(buf, EX_DATAERR);
+		/* NOTREACHED */
+	} else if (symbol->type == ALIAS) {
+		*p_symbol = symbol->info.ainfo->parent;
+	} else if ((symbol->type != REGISTER)
+		&& (symbol->type != SCBLOC)
+		&& (symbol->type != SRAMLOC)) {
+		snprintf(buf, sizeof(buf),
+			 "Specified symbol %s is not a register",
+			 symbol->name);
+		stop(buf, EX_DATAERR);
+	}
+}
+
+static void
+format_1_instr(int opcode, symbol_ref_t *dest, expression_t *immed,
+	       symbol_ref_t *src, int ret)
+{
+	struct instruction *instr;
+	struct ins_format1 *f1_instr;
+
+	if (src->symbol == NULL)
+		src = dest;
+
+	/* Test register permissions */
+	test_writable_symbol(dest->symbol);
+	test_readable_symbol(src->symbol);
+
+	/* Ensure that immediate makes sense for this destination */
+	type_check(dest->symbol, immed, opcode);
+
+	/* Allocate sequencer space for the instruction and fill it out */
+	instr = seq_alloc();
+	f1_instr = &instr->format.format1;
+	f1_instr->ret = ret ? 1 : 0;
+	f1_instr->opcode = opcode;
+	f1_instr->destination = dest->symbol->info.rinfo->address
+			      + dest->offset;
+	f1_instr->source = src->symbol->info.rinfo->address
+			 + src->offset;
+	f1_instr->immediate = immed->value;
+
+	if (is_download_const(immed))
+		f1_instr->parity = 1;
+
+	symlist_free(&immed->referenced_syms);
+	instruction_ptr++;
+}
+
+static void
+format_2_instr(int opcode, symbol_ref_t *dest, expression_t *places,
+	       symbol_ref_t *src, int ret)
+{
+	struct instruction *instr;
+	struct ins_format2 *f2_instr;
+	uint8_t shift_control;
+
+	if (src->symbol == NULL)
+		src = dest;
+
+	/* Test register permissions */
+	test_writable_symbol(dest->symbol);
+	test_readable_symbol(src->symbol);
+
+	/* Allocate sequencer space for the instruction and fill it out */
+	instr = seq_alloc();
+	f2_instr = &instr->format.format2;
+	f2_instr->ret = ret ? 1 : 0;
+	f2_instr->opcode = AIC_OP_ROL;
+	f2_instr->destination = dest->symbol->info.rinfo->address
+			      + dest->offset;
+	f2_instr->source = src->symbol->info.rinfo->address
+			 + src->offset;
+	if (places->value > 8 || places->value <= 0) {
+		stop("illegal shift value", EX_DATAERR);
+		/* NOTREACHED */
+	}
+	switch (opcode) {
+	case AIC_OP_SHL:
+		if (places->value == 8)
+			shift_control = 0xf0;
+		else
+			shift_control = (places->value << 4) | places->value;
+		break;
+	case AIC_OP_SHR:
+		if (places->value == 8) {
+			shift_control = 0xf8;
+		} else {
+			shift_control = (places->value << 4)
+				      | (8 - places->value)
+				      | 0x08;
+		}
+		break;
+	case AIC_OP_ROL:
+		shift_control = places->value & 0x7;
+		break;
+	case AIC_OP_ROR:
+		shift_control = (8 - places->value) | 0x08;
+		break;
+	default:
+		shift_control = 0; /* Quiet Compiler */
+		stop("Invalid shift operation specified", EX_SOFTWARE);
+		/* NOTREACHED */
+		break;
+	};
+	f2_instr->shift_control = shift_control;
+	symlist_free(&places->referenced_syms);
+	instruction_ptr++;
+}
+
+static void
+format_3_instr(int opcode, symbol_ref_t *src,
+	       expression_t *immed, symbol_ref_t *address)
+{
+	struct instruction *instr;
+	struct ins_format3 *f3_instr;
+	int addr;
+
+	/* Test register permissions */
+	test_readable_symbol(src->symbol);
+
+	/* Ensure that immediate makes sense for this source */
+	type_check(src->symbol, immed, opcode);
+
+	/* Allocate sequencer space for the instruction and fill it out */
+	instr = seq_alloc();
+	f3_instr = &instr->format.format3;
+	if (address->symbol == NULL) {
+		/* 'dot' referrence.  Use the current instruction pointer */
+		addr = instruction_ptr + address->offset;
+	} else if (address->symbol->type == UNINITIALIZED) {
+		/* forward reference */
+		addr = address->offset;
+		instr->patch_label = address->symbol;
+	} else
+		addr = address->symbol->info.linfo->address + address->offset;
+	f3_instr->opcode = opcode;
+	f3_instr->address = addr;
+	f3_instr->source = src->symbol->info.rinfo->address
+			 + src->offset;
+	f3_instr->immediate = immed->value;
+
+	if (is_download_const(immed))
+		f3_instr->parity = 1;
+
+	symlist_free(&immed->referenced_syms);
+	instruction_ptr++;
+}
+
+static void
+test_readable_symbol(symbol_t *symbol)
+{
+	if (symbol->info.rinfo->mode == WO) {
+		stop("Write Only register specified as source",
+		     EX_DATAERR);
+		/* NOTREACHED */
+	}
+}
+
+static void
+test_writable_symbol(symbol_t *symbol)
+{
+	if (symbol->info.rinfo->mode == RO) {
+		stop("Read Only register specified as destination",
+		     EX_DATAERR);
+		/* NOTREACHED */
+	}
+}
+
+static void
+type_check(symbol_t *symbol, expression_t *expression, int opcode)
+{
+	symbol_node_t *node;
+	int and_op;
+	char buf[255];
+
+	and_op = FALSE;
+	if (opcode == AIC_OP_AND || opcode == AIC_OP_JNZ || AIC_OP_JZ)
+		and_op = TRUE;
+
+	/*
+	 * Make sure that we aren't attempting to write something
+	 * that hasn't been defined.  If this is an and operation,
+	 * this is a mask, so "undefined" bits are okay.
+	 */
+	if (and_op == FALSE
+	 && (expression->value & ~symbol->info.rinfo->valid_bitmask) != 0) {
+		snprintf(buf, sizeof(buf),
+			 "Invalid bit(s) 0x%x in immediate written to %s",
+			 expression->value & ~symbol->info.rinfo->valid_bitmask,
+			 symbol->name);
+		stop(buf, EX_DATAERR);
+		/* NOTREACHED */
+	}
+
+	/*
+	 * Now make sure that all of the symbols referenced by the
+	 * expression are defined for this register.
+	 */
+	if(symbol->info.rinfo->typecheck_masks != FALSE) {
+		for(node = expression->referenced_syms.slh_first;
+		    node != NULL;
+		    node = node->links.sle_next) {
+			if ((node->symbol->type == MASK
+			  || node->symbol->type == BIT)
+			 && symlist_search(&node->symbol->info.minfo->symrefs,
+					   symbol->name) == NULL) {
+				snprintf(buf, sizeof(buf),
+					 "Invalid bit or mask %s "
+					 "for register %s",
+					 node->symbol->name, symbol->name);
+				stop(buf, EX_DATAERR);
+				/* NOTREACHED */
+			}
+		}
+	}
+}
+
+static void
+make_expression(expression_t *immed, int value)
+{
+	SLIST_INIT(&immed->referenced_syms);
+	immed->value = value & 0xff;
+}
+
+static void
+add_conditional(symbol_t *symbol)
+{
+	static int numfuncs;
+
+	if (numfuncs == 0) {
+		/* add a special conditional, "0" */
+		symbol_t *false_func;
+
+		false_func = symtable_get("0");
+		if (false_func->type != UNINITIALIZED) {
+			stop("Conditional expression '0' "
+			     "conflicts with a symbol", EX_DATAERR);
+			/* NOTREACHED */
+		}
+		false_func->type = CONDITIONAL;
+		initialize_symbol(false_func);
+		false_func->info.condinfo->func_num = numfuncs++;
+		symlist_add(&patch_functions, false_func, SYMLIST_INSERT_HEAD);
+	}
+
+	/* This condition has occurred before */
+	if (symbol->type == CONDITIONAL)
+		return;
+
+	if (symbol->type != UNINITIALIZED) {
+		stop("Conditional expression conflicts with a symbol",
+		     EX_DATAERR);
+		/* NOTREACHED */
+	}
+
+	symbol->type = CONDITIONAL;
+	initialize_symbol(symbol);
+	symbol->info.condinfo->func_num = numfuncs++;
+	symlist_add(&patch_functions, symbol, SYMLIST_INSERT_HEAD);
+}
+
+void
+yyerror(const char *string)
+{
+	stop(string, EX_DATAERR);
+}
+
+static int
+is_download_const(expression_t *immed)
+{
+	if ((immed->referenced_syms.slh_first != NULL)
+	 && (immed->referenced_syms.slh_first->symbol->type == DOWNLOAD_CONST))
+		return (TRUE);
+
+	return (FALSE);
+}
diff -urN linux/drivers/scsi/aic7xxx/aicasm/aicasm_insformat.h /tmp/linux/drivers/scsi/aic7xxx/aicasm/aicasm_insformat.h
--- linux/drivers/scsi/aic7xxx/aicasm/aicasm_insformat.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aicasm/aicasm_insformat.h	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,129 @@
+/*
+ * Instruction formats for the sequencer program downloaded to
+ * Aic7xxx SCSI host adapters
+ *
+ * Copyright (c) 1997, 1998, 2000 Justin T. Gibbs.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $Id: //depot/src/aic7xxx/aicasm/aicasm_insformat.h#4 $
+ *
+ * $FreeBSD: src/sys/dev/aic7xxx/aicasm/aicasm_insformat.h,v 1.3 2000/09/22 22:19:54 gibbs Exp $
+ */
+
+#if linux
+#include <endian.h>
+#else
+#include <machine/endian.h>
+#endif
+
+struct ins_format1 {
+#if BYTE_ORDER == LITTLE_ENDIAN
+	uint32_t	immediate	: 8,
+			source		: 9,
+			destination	: 9,
+			ret		: 1,
+			opcode		: 4,
+			parity		: 1;
+#else
+	uint32_t	parity		: 1,
+			opcode		: 4,
+			ret		: 1,
+			destination	: 9,
+			source		: 9,
+			immediate	: 8;
+#endif
+};
+
+struct ins_format2 {
+#if BYTE_ORDER == LITTLE_ENDIAN
+	uint32_t	shift_control	: 8,
+			source		: 9,
+			destination	: 9,
+			ret		: 1,
+			opcode		: 4,
+			parity		: 1;
+#else
+	uint32_t	parity		: 1,
+			opcode		: 4,
+			ret		: 1,
+			destination	: 9,
+			source		: 9,
+			shift_control	: 8;
+#endif
+};
+
+struct ins_format3 {
+#if BYTE_ORDER == LITTLE_ENDIAN
+	uint32_t	immediate	: 8,
+			source		: 9,
+			address		: 10,
+			opcode		: 4,
+			parity		: 1;
+#else
+	uint32_t	parity		: 1,
+			opcode		: 4,
+			address		: 10,
+			source		: 9,
+			immediate	: 8;
+#endif
+};
+
+union ins_formats {
+		struct ins_format1 format1;
+		struct ins_format2 format2;
+		struct ins_format3 format3;
+		uint8_t		   bytes[4];
+		uint32_t	   integer;
+};
+struct instruction {
+	union	ins_formats format;
+	u_int	srcline;
+	struct symbol *patch_label;
+	STAILQ_ENTRY(instruction) links;
+};
+
+#define	AIC_OP_OR	0x0
+#define	AIC_OP_AND	0x1
+#define AIC_OP_XOR	0x2
+#define	AIC_OP_ADD	0x3
+#define	AIC_OP_ADC	0x4
+#define	AIC_OP_ROL	0x5
+#define	AIC_OP_BMOV	0x6
+
+#define	AIC_OP_JMP	0x8
+#define AIC_OP_JC	0x9
+#define AIC_OP_JNC	0xa
+#define AIC_OP_CALL	0xb
+#define	AIC_OP_JNE	0xc
+#define	AIC_OP_JNZ	0xd
+#define	AIC_OP_JE	0xe
+#define	AIC_OP_JZ	0xf
+
+/* Pseudo Ops */
+#define	AIC_OP_SHL	0x10
+#define	AIC_OP_SHR	0x20
+#define	AIC_OP_ROR	0x30
diff -urN linux/drivers/scsi/aic7xxx/aicasm/aicasm_scan.c /tmp/linux/drivers/scsi/aic7xxx/aicasm/aicasm_scan.c
--- linux/drivers/scsi/aic7xxx/aicasm/aicasm_scan.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aicasm/aicasm_scan.c	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,2377 @@
+/* A lexical scanner generated by flex */
+
+/* Scanner skeleton version:
+ * $Header: /home/daffy/u0/vern/flex/RCS/flex.skl,v 2.91 96/09/10 16:58:48 vern Exp $
+ */
+
+#define FLEX_SCANNER
+#define YY_FLEX_MAJOR_VERSION 2
+#define YY_FLEX_MINOR_VERSION 5
+
+#include <stdio.h>
+
+
+/* cfront 1.2 defines "c_plusplus" instead of "__cplusplus" */
+#ifdef c_plusplus
+#ifndef __cplusplus
+#define __cplusplus
+#endif
+#endif
+
+
+#ifdef __cplusplus
+
+#include <stdlib.h>
+#include <unistd.h>
+
+/* Use prototypes in function declarations. */
+#define YY_USE_PROTOS
+
+/* The "const" storage-class-modifier is valid. */
+#define YY_USE_CONST
+
+#else	/* ! __cplusplus */
+
+#if __STDC__
+
+#define YY_USE_PROTOS
+#define YY_USE_CONST
+
+#endif	/* __STDC__ */
+#endif	/* ! __cplusplus */
+
+#ifdef __TURBOC__
+ #pragma warn -rch
+ #pragma warn -use
+#include <io.h>
+#include <stdlib.h>
+#define YY_USE_CONST
+#define YY_USE_PROTOS
+#endif
+
+#ifdef YY_USE_CONST
+#define yyconst const
+#else
+#define yyconst
+#endif
+
+
+#ifdef YY_USE_PROTOS
+#define YY_PROTO(proto) proto
+#else
+#define YY_PROTO(proto) ()
+#endif
+
+/* Returned upon end-of-file. */
+#define YY_NULL 0
+
+/* Promotes a possibly negative, possibly signed char to an unsigned
+ * integer for use as an array index.  If the signed char is negative,
+ * we want to instead treat it as an 8-bit unsigned char, hence the
+ * double cast.
+ */
+#define YY_SC_TO_UI(c) ((unsigned int) (unsigned char) c)
+
+/* Enter a start condition.  This macro really ought to take a parameter,
+ * but we do it the disgusting crufty way forced on us by the ()-less
+ * definition of BEGIN.
+ */
+#define BEGIN yy_start = 1 + 2 *
+
+/* Translate the current start state into a value that can be later handed
+ * to BEGIN to return to the state.  The YYSTATE alias is for lex
+ * compatibility.
+ */
+#define YY_START ((yy_start - 1) / 2)
+#define YYSTATE YY_START
+
+/* Action number for EOF rule of a given start state. */
+#define YY_STATE_EOF(state) (YY_END_OF_BUFFER + state + 1)
+
+/* Special action meaning "start processing a new file". */
+#define YY_NEW_FILE yyrestart( yyin )
+
+#define YY_END_OF_BUFFER_CHAR 0
+
+/* Size of default input buffer. */
+#define YY_BUF_SIZE 16384
+
+typedef struct yy_buffer_state *YY_BUFFER_STATE;
+
+extern int yyleng;
+extern FILE *yyin, *yyout;
+
+#define EOB_ACT_CONTINUE_SCAN 0
+#define EOB_ACT_END_OF_FILE 1
+#define EOB_ACT_LAST_MATCH 2
+
+/* The funky do-while in the following #define is used to turn the definition
+ * int a single C statement (which needs a semi-colon terminator).  This
+ * avoids problems with code like:
+ *
+ * 	if ( condition_holds )
+ *		yyless( 5 );
+ *	else
+ *		do_something_else();
+ *
+ * Prior to using the do-while the compiler would get upset at the
+ * "else" because it interpreted the "if" statement as being all
+ * done when it reached the ';' after the yyless() call.
+ */
+
+/* Return all but the first 'n' matched characters back to the input stream. */
+
+#define yyless(n) \
+	do \
+		{ \
+		/* Undo effects of setting up yytext. */ \
+		*yy_cp = yy_hold_char; \
+		YY_RESTORE_YY_MORE_OFFSET \
+		yy_c_buf_p = yy_cp = yy_bp + n - YY_MORE_ADJ; \
+		YY_DO_BEFORE_ACTION; /* set up yytext again */ \
+		} \
+	while ( 0 )
+
+#define unput(c) yyunput( c, yytext_ptr )
+
+/* The following is because we cannot portably get our hands on size_t
+ * (without autoconf's help, which isn't available because we want
+ * flex-generated scanners to compile on their own).
+ */
+typedef unsigned int yy_size_t;
+
+
+struct yy_buffer_state
+	{
+	FILE *yy_input_file;
+
+	char *yy_ch_buf;		/* input buffer */
+	char *yy_buf_pos;		/* current position in input buffer */
+
+	/* Size of input buffer in bytes, not including room for EOB
+	 * characters.
+	 */
+	yy_size_t yy_buf_size;
+
+	/* Number of characters read into yy_ch_buf, not including EOB
+	 * characters.
+	 */
+	int yy_n_chars;
+
+	/* Whether we "own" the buffer - i.e., we know we created it,
+	 * and can realloc() it to grow it, and should free() it to
+	 * delete it.
+	 */
+	int yy_is_our_buffer;
+
+	/* Whether this is an "interactive" input source; if so, and
+	 * if we're using stdio for input, then we want to use getc()
+	 * instead of fread(), to make sure we stop fetching input after
+	 * each newline.
+	 */
+	int yy_is_interactive;
+
+	/* Whether we're considered to be at the beginning of a line.
+	 * If so, '^' rules will be active on the next match, otherwise
+	 * not.
+	 */
+	int yy_at_bol;
+
+	/* Whether to try to fill the input buffer when we reach the
+	 * end of it.
+	 */
+	int yy_fill_buffer;
+
+	int yy_buffer_status;
+#define YY_BUFFER_NEW 0
+#define YY_BUFFER_NORMAL 1
+	/* When an EOF's been seen but there's still some text to process
+	 * then we mark the buffer as YY_EOF_PENDING, to indicate that we
+	 * shouldn't try reading from the input source any more.  We might
+	 * still have a bunch of tokens to match, though, because of
+	 * possible backing-up.
+	 *
+	 * When we actually see the EOF, we change the status to "new"
+	 * (via yyrestart()), so that the user can continue scanning by
+	 * just pointing yyin at a new input file.
+	 */
+#define YY_BUFFER_EOF_PENDING 2
+	};
+
+static YY_BUFFER_STATE yy_current_buffer = 0;
+
+/* We provide macros for accessing buffer states in case in the
+ * future we want to put the buffer states in a more general
+ * "scanner state".
+ */
+#define YY_CURRENT_BUFFER yy_current_buffer
+
+
+/* yy_hold_char holds the character lost when yytext is formed. */
+static char yy_hold_char;
+
+static int yy_n_chars;		/* number of characters read into yy_ch_buf */
+
+
+int yyleng;
+
+/* Points to current character in buffer. */
+static char *yy_c_buf_p = (char *) 0;
+static int yy_init = 1;		/* whether we need to initialize */
+static int yy_start = 0;	/* start state number */
+
+/* Flag which is used to allow yywrap()'s to do buffer switches
+ * instead of setting up a fresh yyin.  A bit of a hack ...
+ */
+static int yy_did_buffer_switch_on_eof;
+
+void yyrestart YY_PROTO(( FILE *input_file ));
+
+void yy_switch_to_buffer YY_PROTO(( YY_BUFFER_STATE new_buffer ));
+void yy_load_buffer_state YY_PROTO(( void ));
+YY_BUFFER_STATE yy_create_buffer YY_PROTO(( FILE *file, int size ));
+void yy_delete_buffer YY_PROTO(( YY_BUFFER_STATE b ));
+void yy_init_buffer YY_PROTO(( YY_BUFFER_STATE b, FILE *file ));
+void yy_flush_buffer YY_PROTO(( YY_BUFFER_STATE b ));
+#define YY_FLUSH_BUFFER yy_flush_buffer( yy_current_buffer )
+
+YY_BUFFER_STATE yy_scan_buffer YY_PROTO(( char *base, yy_size_t size ));
+YY_BUFFER_STATE yy_scan_string YY_PROTO(( yyconst char *yy_str ));
+YY_BUFFER_STATE yy_scan_bytes YY_PROTO(( yyconst char *bytes, int len ));
+
+static void *yy_flex_alloc YY_PROTO(( yy_size_t ));
+static void *yy_flex_realloc YY_PROTO(( void *, yy_size_t ));
+static void yy_flex_free YY_PROTO(( void * ));
+
+#define yy_new_buffer yy_create_buffer
+
+#define yy_set_interactive(is_interactive) \
+	{ \
+	if ( ! yy_current_buffer ) \
+		yy_current_buffer = yy_create_buffer( yyin, YY_BUF_SIZE ); \
+	yy_current_buffer->yy_is_interactive = is_interactive; \
+	}
+
+#define yy_set_bol(at_bol) \
+	{ \
+	if ( ! yy_current_buffer ) \
+		yy_current_buffer = yy_create_buffer( yyin, YY_BUF_SIZE ); \
+	yy_current_buffer->yy_at_bol = at_bol; \
+	}
+
+#define YY_AT_BOL() (yy_current_buffer->yy_at_bol)
+
+typedef unsigned char YY_CHAR;
+FILE *yyin = (FILE *) 0, *yyout = (FILE *) 0;
+typedef int yy_state_type;
+extern char *yytext;
+#define yytext_ptr yytext
+
+static yy_state_type yy_get_previous_state YY_PROTO(( void ));
+static yy_state_type yy_try_NUL_trans YY_PROTO(( yy_state_type current_state ));
+static int yy_get_next_buffer YY_PROTO(( void ));
+static void yy_fatal_error YY_PROTO(( yyconst char msg[] ));
+
+/* Done after the current pattern has been matched and before the
+ * corresponding action - sets up yytext.
+ */
+#define YY_DO_BEFORE_ACTION \
+	yytext_ptr = yy_bp; \
+	yyleng = (int) (yy_cp - yy_bp); \
+	yy_hold_char = *yy_cp; \
+	*yy_cp = '\0'; \
+	yy_c_buf_p = yy_cp;
+
+#define YY_NUM_RULES 80
+#define YY_END_OF_BUFFER 81
+static yyconst short int yy_accept[256] =
+    {   0,
+        0,    0,    5,    5,    0,    0,    0,    0,   81,   79,
+       14,    1,   66,   79,   66,   66,   77,   67,   69,   34,
+       78,   78,   78,   78,   78,   78,   78,   78,   78,   78,
+       78,   78,   78,   78,   78,   78,   78,   78,   78,    5,
+        4,    6,    7,   13,   12,   10,   11,   74,   80,   71,
+       74,   72,   73,   14,    0,    0,    0,   77,   77,   77,
+        2,   67,    0,   69,   78,   78,   78,   20,   78,   78,
+       78,   78,   78,   78,   78,   78,   78,   78,   78,   78,
+       78,   78,   78,   43,   45,   78,   78,   47,   78,   78,
+       78,   78,   62,   78,   78,   78,   78,   78,   78,   78,
+
+       78,    5,    6,    6,    8,    7,    3,    7,   13,    0,
+       72,   72,   72,    0,    0,   68,   78,   78,   78,   51,
+       50,   78,   78,   61,   23,   78,   78,   56,   41,   57,
+       78,   54,   78,   78,    0,    9,   53,   42,   44,   46,
+       48,   78,   40,   39,   78,   64,   58,   78,   63,   38,
+       37,   27,   78,   35,   36,   78,   78,   55,   78,   59,
+        0,    0,   78,   78,   78,   78,   78,   78,   78,   78,
+       52,   49,   78,   78,   65,   24,   32,   78,   78,   78,
+       26,   60,    0,    0,   78,   78,   78,   78,   78,   25,
+       78,   78,   16,   78,   78,   78,   78,    0,    0,   78,
+
+       78,   78,   78,   78,   78,   78,   78,   78,   78,   33,
+       75,    0,   78,   78,   78,   78,   18,   30,   78,   78,
+       78,   78,    0,   76,   70,   78,   78,   78,   78,   31,
+       17,   15,   78,   76,   76,   78,   78,   78,   78,   78,
+       78,   78,   78,   78,   78,   78,   78,   19,   29,   28,
+       78,   22,   78,   21,    0
+    } ;
+
+static yyconst int yy_ec[256] =
+    {   0,
+        1,    1,    1,    1,    1,    1,    1,    1,    2,    3,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    2,    4,    5,    6,    1,    1,    4,    1,    7,
+        8,    9,    4,    4,   10,   11,   12,   13,   14,   14,
+       14,   14,   14,   14,   14,   15,   15,    4,   16,    5,
+        1,    5,    1,    1,   17,   18,   19,   20,   21,   22,
+       23,   24,   25,   24,   24,   26,   24,   27,   28,   24,
+       24,   29,   24,   30,   24,   24,   31,   32,   24,   24,
+        4,    1,    4,    1,   33,    1,   34,   35,   36,   37,
+
+       38,   39,   40,   41,   42,   43,   44,   45,   46,   47,
+       48,   49,   24,   50,   51,   52,   53,   54,   55,   56,
+       24,   57,    4,    4,    4,    4,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1
+    } ;
+
+static yyconst int yy_meta[58] =
+    {   0,
+        1,    1,    2,    1,    1,    1,    3,    3,    4,    5,
+        5,    6,    5,    5,    5,    1,    5,    5,    5,    5,
+        5,    5,    5,    5,    5,    5,    5,    5,    5,    5,
+        5,    5,    5,    5,    5,    5,    5,    5,    5,    5,
+        5,    5,    5,    5,    5,    5,    5,    5,    5,    5,
+        5,    5,    5,    5,    5,    5,    5
+    } ;
+
+static yyconst short int yy_base[270] =
+    {   0,
+        0,    0,   55,   56,   63,   66,   74,   90,  229,  845,
+      211,  845,  845,   47,   49,   51,   96,   98,  102,   76,
+      107,   92,  109,  111,  113,  115,  121,  123,  126,  132,
+      134,  144,  138,  136,  147,  164,  167,  176,  182,    0,
+      845,  186,  187,    0,  845,  845,  845,  845,  845,  845,
+      154,  193,  845,  198,  151,  138,  195,  199,  204,  206,
+      845,  209,  220,  213,  232,  237,  241,  234,  239,  251,
+      253,  255,  257,  259,  261,  265,  267,  270,  272,  274,
+      279,  292,  285,  299,  301,  307,  311,  313,  315,  320,
+      322,  324,  326,  329,  332,  340,  334,  342,  347,  349,
+
+      351,    0,  293,  330,  845,  358,  845,  376,    0,  375,
+      380,  382,  384,  144,  134,    0,  386,  391,  393,  395,
+      397,  401,  403,  405,  407,  409,  411,  414,  416,  418,
+      421,  425,  427,  429,  348,  845,  431,  433,  437,  441,
+      443,  446,  450,  453,  457,  459,  464,  466,  468,  470,
+      472,  474,  476,  480,  482,  485,  487,  489,  491,  493,
+      119,  108,  500,  495,  504,  506,  508,  512,  517,  519,
+      521,  523,  525,  527,  529,  533,  536,  538,  542,  547,
+      549,  554,   93,   78,  557,  559,  562,  564,  567,  569,
+      571,  575,  580,  584,  586,  588,  590,   91,   63,  592,
+
+      594,  596,  603,  609,  611,  615,  619,  623,  625,  628,
+      634,   34,  638,  640,  632,  643,  645,  647,  653,  657,
+      660,  662,  668,  672,  845,  671,  673,  675,  677,  679,
+      681,  688,  690,  698,  700,  700,  702,  704,  706,  708,
+      713,  715,  722,  724,  732,  734,  736,  738,  740,  744,
+      746,  753,  755,  757,  845,  781,  787,  793,  795,  797,
+      799,  805,  810,  816,  822,  824,  826,  832,  838
+    } ;
+
+static yyconst short int yy_def[270] =
+    {   0,
+      255,    1,  256,  256,  257,  257,  258,  258,  255,  255,
+      255,  255,  255,  255,  259,  260,  260,  259,  259,  261,
+      261,  261,  261,  261,  261,  261,  261,  261,  261,  261,
+      261,  261,  261,  261,  261,  261,  261,  261,  261,  262,
+      255,  263,  264,  265,  255,  255,  255,  255,  255,  255,
+      266,  267,  255,  255,  255,  255,  259,  260,  260,  260,
+      255,  259,  259,  259,  261,  261,  261,  261,  261,  261,
+      261,  261,  261,  261,  261,  261,  261,  261,  261,  261,
+      261,  261,  261,  261,  261,  261,  261,  261,  261,  261,
+      261,  261,  261,  261,  261,  261,  261,  261,  261,  261,
+
+      261,  262,  263,  263,  255,  264,  255,  264,  265,  266,
+      267,  267,  267,  255,  255,   63,  261,  261,  261,  261,
+      261,  261,  261,  261,  261,  261,  261,  261,  261,  261,
+      261,  261,  261,  261,  255,  255,  261,  261,  261,  261,
+      261,  261,  261,  261,  261,  261,  261,  261,  261,  261,
+      261,  261,  261,  261,  261,  261,  261,  261,  261,  261,
+      255,  255,  261,  261,  261,  261,  261,  261,  261,  261,
+      261,  261,  261,  261,  261,  261,  261,  261,  261,  261,
+      261,  261,  255,  255,  261,  261,  261,  261,  261,  261,
+      261,  261,  261,  261,  261,  261,  261,  255,  255,  261,
+
+      261,  261,  261,  261,  261,  261,  261,  261,  261,  261,
+      268,  255,  261,  261,  261,  261,  261,  261,  261,  261,
+      261,  261,  268,  269,  255,  261,  261,  261,  261,  261,
+      261,  261,  261,  269,  269,  261,  261,  261,  261,  261,
+      261,  261,  261,  261,  261,  261,  261,  261,  261,  261,
+      261,  261,  261,  261,    0,  255,  255,  255,  255,  255,
+      255,  255,  255,  255,  255,  255,  255,  255,  255
+    } ;
+
+static yyconst short int yy_nxt[903] =
+    {   0,
+       10,   11,   12,   13,   13,   14,   13,   13,   10,   15,
+       16,   17,   18,   19,   19,   13,   20,   21,   22,   22,
+       23,   22,   22,   22,   22,   22,   22,   22,   24,   22,
+       25,   22,   22,   26,   27,   28,   29,   30,   22,   22,
+       22,   31,   32,   22,   22,   33,   34,   35,   22,   36,
+       37,   38,   22,   22,   22,   39,   22,   41,   41,   58,
+       58,   60,   60,   42,   42,   45,   43,   43,   45,   46,
+       47,  225,   46,   47,   48,   48,   49,   48,   50,   48,
+       48,   48,   48,   55,   52,   52,   58,   58,   56,   53,
+       48,   48,   49,   48,   50,   48,   48,   48,   48,  212,
+
+       52,   52,   58,   58,   61,   53,   60,   60,   58,   58,
+       62,   62,   58,   58,   64,   64,   64,   58,   58,   58,
+       58,   58,   58,   58,   58,   58,   58,   66,  211,   63,
+      199,   58,   58,   58,   58,   67,   58,   58,   68,  198,
+       68,   68,   58,   58,   58,   58,   58,   58,   58,   58,
+       69,   70,  184,   63,   58,   58,   75,   58,   58,   71,
+      183,   72,   73,   79,  111,  111,   74,   76,   77,  162,
+       78,   89,   82,   80,   58,   58,   81,   58,   58,   84,
+       83,   85,  161,   92,  115,   90,   58,   58,  114,   86,
+       87,   91,   58,   58,  104,  107,   93,  105,  108,   54,
+
+       88,   94,   96,  113,  113,   58,   58,   97,   98,   60,
+       60,   95,   54,  100,   60,   60,   60,   60,   99,   58,
+       58,   62,   62,   58,   58,   64,   64,   64,  255,  101,
+       58,   58,  116,  116,  116,  255,  116,  116,  116,  116,
+      116,  116,   58,   58,   58,   58,  255,   58,   58,   58,
+       58,   58,   58,  116,  116,  116,  116,  116,  116,  117,
+      118,   58,   58,   58,   58,   58,   58,   58,   58,   58,
+       58,   58,   58,  255,  119,   58,   58,   58,   58,  255,
+       58,   58,   58,   58,   58,   58,  120,  121,  255,   58,
+       58,  124,  255,  135,  122,   58,   58,  123,  136,  255,
+
+      128,  255,   58,   58,  255,  127,  126,  132,  125,   58,
+       58,   58,   58,  255,  129,  130,  131,   58,   58,  255,
+      137,   58,   58,   58,   58,   58,   58,  255,  133,  134,
+       58,   58,   58,   58,   58,   58,   58,   58,  104,   58,
+       58,  105,   58,   58,   58,   58,  139,  255,  140,  135,
+       58,   58,   58,   58,  136,  138,  255,   58,   58,   58,
+       58,   58,   58,  144,  255,  142,  255,  141,  148,  255,
+      145,  255,  146,  143,  152,  147,  150,  255,  154,  255,
+      149,  151,  158,  155,  255,  111,  111,  108,  156,  153,
+      113,  113,  113,  113,  113,  113,   58,   58,  157,  159,
+
+      160,   58,   58,   58,   58,   58,   58,   58,   58,  255,
+      163,   58,   58,   58,   58,   58,   58,   58,   58,   58,
+       58,   58,   58,  164,   58,   58,   58,   58,   58,   58,
+      165,   58,   58,  255,  168,   58,   58,   58,   58,   58,
+       58,   58,   58,   58,   58,  166,  167,   58,   58,  255,
+      169,   58,   58,   58,   58,  172,   58,   58,  255,  170,
+       58,   58,  171,   58,   58,  255,  175,   58,   58,   58,
+       58,  173,  255,  174,   58,   58,   58,   58,   58,   58,
+       58,   58,   58,   58,   58,   58,   58,   58,  255,  176,
+       58,   58,   58,   58,  177,   58,   58,   58,   58,   58,
+
+       58,   58,   58,   58,   58,   58,   58,  178,  255,  179,
+       58,   58,  255,  186,   58,   58,   58,   58,   58,   58,
+      255,  180,   58,   58,  181,  255,  185,   58,   58,   58,
+       58,   58,   58,   58,   58,   58,   58,   58,   58,   58,
+       58,  255,  182,   58,   58,  189,   58,   58,   58,   58,
+      255,  188,   58,   58,  187,  255,  192,   58,   58,   58,
+       58,  255,  190,  191,   58,   58,  255,   58,   58,   58,
+       58,  194,   58,   58,   58,   58,  193,   58,   58,   58,
+       58,   58,   58,  255,  197,   58,   58,  201,  195,  200,
+       58,   58,  255,  196,   58,   58,   58,   58,   58,   58,
+
+       58,   58,   58,   58,   58,   58,   58,   58,  205,  255,
+      213,  255,  202,   58,   58,  255,  203,  204,  214,   58,
+       58,   58,   58,  209,  206,   58,   58,  255,  215,   58,
+       58,  207,  255,   58,   58,   58,   58,  208,   58,   58,
+      224,  224,   58,   58,  255,  210,  255,  216,   58,   58,
+       58,   58,  220,   58,   58,   58,   58,   58,   58,  217,
+      221,  218,  219,   58,   58,  222,  226,   58,   58,  227,
+       58,   58,   58,   58,  224,  224,  229,  228,  235,  235,
+      255,   58,   58,   58,   58,   58,   58,   58,   58,   58,
+       58,   58,   58,  231,  233,  236,  255,  237,   58,   58,
+
+       58,   58,  255,  230,  235,  235,  235,  235,  255,  232,
+       58,   58,   58,   58,   58,   58,   58,   58,   58,   58,
+      242,  255,  238,   58,   58,   58,   58,  255,  239,  241,
+      255,  247,   58,   58,   58,   58,  255,  246,  255,  240,
+      243,  245,   58,   58,   58,   58,   58,   58,   58,   58,
+       58,   58,  251,  244,   58,   58,   58,   58,  255,  248,
+      255,  252,  253,   58,   58,   58,   58,   58,   58,  255,
+      255,  255,  255,  249,  255,  255,  255,  250,  255,  255,
+      254,   40,   40,   40,   40,   40,   40,   44,   44,   44,
+       44,   44,   44,   51,   51,   51,   51,   51,   51,   57,
+
+       57,   59,   59,   65,   65,  102,  255,  102,  255,  102,
+      103,  255,  103,  103,  103,  103,  106,  255,  106,  106,
+      106,  106,  109,  255,  255,  109,  109,  109,  110,  110,
+      112,  112,  223,  255,  223,  223,  223,  223,  234,  255,
+      234,  234,  234,  234,    9,  255,  255,  255,  255,  255,
+      255,  255,  255,  255,  255,  255,  255,  255,  255,  255,
+      255,  255,  255,  255,  255,  255,  255,  255,  255,  255,
+      255,  255,  255,  255,  255,  255,  255,  255,  255,  255,
+      255,  255,  255,  255,  255,  255,  255,  255,  255,  255,
+      255,  255,  255,  255,  255,  255,  255,  255,  255,  255,
+
+      255,  255
+    } ;
+
+static yyconst short int yy_chk[903] =
+    {   0,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    3,    4,   15,
+       15,   16,   16,    3,    4,    5,    3,    4,    6,    5,
+        5,  212,    6,    6,    7,    7,    7,    7,    7,    7,
+        7,    7,    7,   14,    7,    7,   20,   20,   14,    7,
+        8,    8,    8,    8,    8,    8,    8,    8,    8,  199,
+
+        8,    8,   22,   22,   17,    8,   17,   17,   18,   18,
+       18,   18,   19,   19,   19,   19,   19,   21,   21,   23,
+       23,   24,   24,   25,   25,   26,   26,   21,  198,   18,
+      184,   27,   27,   28,   28,   23,   29,   29,   24,  183,
+       25,   24,   30,   30,   31,   31,   34,   34,   33,   33,
+       26,   26,  162,   18,   32,   32,   28,   35,   35,   26,
+      161,   26,   27,   29,   51,   51,   27,   28,   28,  115,
+       28,   33,   31,   29,   36,   36,   30,   37,   37,   32,
+       31,   32,  114,   34,   56,   33,   38,   38,   55,   32,
+       32,   33,   39,   39,   42,   43,   35,   42,   43,   54,
+
+       32,   36,   37,   52,   52,   57,   57,   37,   37,   58,
+       58,   36,   11,   38,   59,   59,   60,   60,   37,   62,
+       62,   62,   62,   64,   64,   64,   64,   64,    9,   39,
+       63,   63,   63,   63,   63,    0,   63,   63,   63,   63,
+       63,   63,   65,   65,   68,   68,    0,   66,   66,   69,
+       69,   67,   67,   63,   63,   63,   63,   63,   63,   66,
+       67,   70,   70,   71,   71,   72,   72,   73,   73,   74,
+       74,   75,   75,    0,   69,   76,   76,   77,   77,    0,
+       78,   78,   79,   79,   80,   80,   70,   70,    0,   81,
+       81,   72,    0,   82,   71,   83,   83,   71,   82,    0,
+
+       76,  103,   82,   82,  103,   75,   74,   79,   73,   84,
+       84,   85,   85,    0,   76,   77,   78,   86,   86,    0,
+       83,   87,   87,   88,   88,   89,   89,    0,   80,   81,
+       90,   90,   91,   91,   92,   92,   93,   93,  104,   94,
+       94,  104,   95,   95,   97,   97,   87,    0,   87,  135,
+       96,   96,   98,   98,  135,   86,    0,   99,   99,  100,
+      100,  101,  101,   91,    0,   89,  106,   87,   94,  106,
+       92,    0,   92,   90,   96,   92,   95,    0,   97,    0,
+       94,   95,   99,   97,  108,  110,  110,  108,   98,   96,
+      111,  111,  112,  112,  113,  113,  117,  117,   98,  100,
+
+      101,  118,  118,  119,  119,  120,  120,  121,  121,    0,
+      117,  122,  122,  123,  123,  124,  124,  125,  125,  126,
+      126,  127,  127,  118,  128,  128,  129,  129,  130,  130,
+      119,  131,  131,    0,  122,  132,  132,  133,  133,  134,
+      134,  137,  137,  138,  138,  119,  121,  139,  139,    0,
+      123,  140,  140,  141,  141,  127,  142,  142,    0,  123,
+      143,  143,  126,  144,  144,    0,  134,  145,  145,  146,
+      146,  131,    0,  133,  147,  147,  148,  148,  149,  149,
+      150,  150,  151,  151,  152,  152,  153,  153,    0,  142,
+      154,  154,  155,  155,  145,  156,  156,  157,  157,  158,
+
+      158,  159,  159,  160,  160,  164,  164,  148,    0,  153,
+      163,  163,    0,  164,  165,  165,  166,  166,  167,  167,
+        0,  156,  168,  168,  157,    0,  163,  169,  169,  170,
+      170,  171,  171,  172,  172,  173,  173,  174,  174,  175,
+      175,    0,  159,  176,  176,  167,  177,  177,  178,  178,
+        0,  166,  179,  179,  165,    0,  170,  180,  180,  181,
+      181,    0,  168,  169,  182,  182,    0,  185,  185,  186,
+      186,  174,  187,  187,  188,  188,  173,  189,  189,  190,
+      190,  191,  191,    0,  180,  192,  192,  186,  178,  185,
+      193,  193,    0,  179,  194,  194,  195,  195,  196,  196,
+
+      197,  197,  200,  200,  201,  201,  202,  202,  191,    0,
+      200,    0,  187,  203,  203,    0,  188,  189,  201,  204,
+      204,  205,  205,  196,  192,  206,  206,    0,  202,  207,
+      207,  194,    0,  208,  208,  209,  209,  195,  210,  210,
+      211,  211,  215,  215,    0,  197,    0,  203,  213,  213,
+      214,  214,  207,  216,  216,  217,  217,  218,  218,  204,
+      208,  205,  206,  219,  219,  209,  213,  220,  220,  214,
+      221,  221,  222,  222,  223,  223,  216,  215,  224,  224,
+        0,  226,  226,  227,  227,  228,  228,  229,  229,  230,
+      230,  231,  231,  220,  222,  226,    0,  227,  232,  232,
+
+      233,  233,    0,  219,  234,  234,  235,  235,    0,  221,
+      236,  236,  237,  237,  238,  238,  239,  239,  240,  240,
+      237,    0,  228,  241,  241,  242,  242,    0,  229,  236,
+        0,  242,  243,  243,  244,  244,    0,  241,    0,  233,
+      238,  240,  245,  245,  246,  246,  247,  247,  248,  248,
+      249,  249,  246,  239,  250,  250,  251,  251,    0,  243,
+        0,  247,  251,  252,  252,  253,  253,  254,  254,    0,
+        0,    0,    0,  244,    0,    0,    0,  245,    0,    0,
+      253,  256,  256,  256,  256,  256,  256,  257,  257,  257,
+      257,  257,  257,  258,  258,  258,  258,  258,  258,  259,
+
+      259,  260,  260,  261,  261,  262,    0,  262,    0,  262,
+      263,    0,  263,  263,  263,  263,  264,    0,  264,  264,
+      264,  264,  265,    0,    0,  265,  265,  265,  266,  266,
+      267,  267,  268,    0,  268,  268,  268,  268,  269,    0,
+      269,  269,  269,  269,  255,  255,  255,  255,  255,  255,
+      255,  255,  255,  255,  255,  255,  255,  255,  255,  255,
+      255,  255,  255,  255,  255,  255,  255,  255,  255,  255,
+      255,  255,  255,  255,  255,  255,  255,  255,  255,  255,
+      255,  255,  255,  255,  255,  255,  255,  255,  255,  255,
+      255,  255,  255,  255,  255,  255,  255,  255,  255,  255,
+
+      255,  255
+    } ;
+
+static yy_state_type yy_last_accepting_state;
+static char *yy_last_accepting_cpos;
+
+/* The intent behind this definition is that it'll catch
+ * any uses of REJECT which flex missed.
+ */
+#define REJECT reject_used_but_not_detected
+#define yymore() yymore_used_but_not_detected
+#define YY_MORE_ADJ 0
+#define YY_RESTORE_YY_MORE_OFFSET
+char *yytext;
+#line 1 "aicasm_scan.l"
+#define INITIAL 0
+#line 2 "aicasm_scan.l"
+/*
+ * Lexical Analyzer for the Aic7xxx SCSI Host adapter sequencer assembler.
+ *
+ * Copyright (c) 1997, 1998 Justin T. Gibbs.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $Id: //depot/src/aic7xxx/aicasm/aicasm_scan.l#4 $
+ *
+ * $FreeBSD: src/sys/dev/aic7xxx/aicasm/aicasm_scan.l,v 1.13 2000/09/22 22:19:54 gibbs Exp $
+ */
+
+#include <sys/types.h>
+
+#include <limits.h>
+#include <stdio.h>
+#include <string.h>
+#include <sysexits.h>
+#ifdef __linux__
+#include "../queue.h"
+#else
+#include <sys/queue.h>
+#endif
+
+#include "aicasm.h"
+#include "aicasm_symbol.h"
+#include "y.tab.h"
+
+#define MAX_STR_CONST 256
+char string_buf[MAX_STR_CONST];
+char *string_buf_ptr;
+int  parren_count;
+#define COMMENT 1
+
+#define CEXPR 2
+
+#define INCLUDE 3
+
+
+/* Macros after this point can all be overridden by user definitions in
+ * section 1.
+ */
+
+#ifndef YY_SKIP_YYWRAP
+#ifdef __cplusplus
+extern "C" int yywrap YY_PROTO(( void ));
+#else
+extern int yywrap YY_PROTO(( void ));
+#endif
+#endif
+
+#ifndef YY_NO_UNPUT
+static void yyunput YY_PROTO(( int c, char *buf_ptr ));
+#endif
+
+#ifndef yytext_ptr
+static void yy_flex_strncpy YY_PROTO(( char *, yyconst char *, int ));
+#endif
+
+#ifdef YY_NEED_STRLEN
+static int yy_flex_strlen YY_PROTO(( yyconst char * ));
+#endif
+
+#ifndef YY_NO_INPUT
+#ifdef __cplusplus
+static int yyinput YY_PROTO(( void ));
+#else
+static int input YY_PROTO(( void ));
+#endif
+#endif
+
+#if YY_STACK_USED
+static int yy_start_stack_ptr = 0;
+static int yy_start_stack_depth = 0;
+static int *yy_start_stack = 0;
+#ifndef YY_NO_PUSH_STATE
+static void yy_push_state YY_PROTO(( int new_state ));
+#endif
+#ifndef YY_NO_POP_STATE
+static void yy_pop_state YY_PROTO(( void ));
+#endif
+#ifndef YY_NO_TOP_STATE
+static int yy_top_state YY_PROTO(( void ));
+#endif
+
+#else
+#define YY_NO_PUSH_STATE 1
+#define YY_NO_POP_STATE 1
+#define YY_NO_TOP_STATE 1
+#endif
+
+#ifdef YY_MALLOC_DECL
+YY_MALLOC_DECL
+#else
+#if __STDC__
+#ifndef __cplusplus
+#include <stdlib.h>
+#endif
+#else
+/* Just try to get by without declaring the routines.  This will fail
+ * miserably on non-ANSI systems for which sizeof(size_t) != sizeof(int)
+ * or sizeof(void*) != sizeof(int).
+ */
+#endif
+#endif
+
+/* Amount of stuff to slurp up with each read. */
+#ifndef YY_READ_BUF_SIZE
+#define YY_READ_BUF_SIZE 8192
+#endif
+
+/* Copy whatever the last rule matched to the standard output. */
+
+#ifndef ECHO
+/* This used to be an fputs(), but since the string might contain NUL's,
+ * we now use fwrite().
+ */
+#define ECHO (void) fwrite( yytext, yyleng, 1, yyout )
+#endif
+
+/* Gets input and stuffs it into "buf".  number of characters read, or YY_NULL,
+ * is returned in "result".
+ */
+#ifndef YY_INPUT
+#define YY_INPUT(buf,result,max_size) \
+	if ( yy_current_buffer->yy_is_interactive ) \
+		{ \
+		int c = '*', n; \
+		for ( n = 0; n < max_size && \
+			     (c = getc( yyin )) != EOF && c != '\n'; ++n ) \
+			buf[n] = (char) c; \
+		if ( c == '\n' ) \
+			buf[n++] = (char) c; \
+		if ( c == EOF && ferror( yyin ) ) \
+			YY_FATAL_ERROR( "input in flex scanner failed" ); \
+		result = n; \
+		} \
+	else if ( ((result = fread( buf, 1, max_size, yyin )) == 0) \
+		  && ferror( yyin ) ) \
+		YY_FATAL_ERROR( "input in flex scanner failed" );
+#endif
+
+/* No semi-colon after return; correct usage is to write "yyterminate();" -
+ * we don't want an extra ';' after the "return" because that will cause
+ * some compilers to complain about unreachable statements.
+ */
+#ifndef yyterminate
+#define yyterminate() return YY_NULL
+#endif
+
+/* Number of entries by which start-condition stack grows. */
+#ifndef YY_START_STACK_INCR
+#define YY_START_STACK_INCR 25
+#endif
+
+/* Report a fatal error. */
+#ifndef YY_FATAL_ERROR
+#define YY_FATAL_ERROR(msg) yy_fatal_error( msg )
+#endif
+
+/* Default declaration of generated scanner - a define so the user can
+ * easily add parameters.
+ */
+#ifndef YY_DECL
+#define YY_DECL int yylex YY_PROTO(( void ))
+#endif
+
+/* Code executed at the beginning of each rule, after yytext and yyleng
+ * have been set up.
+ */
+#ifndef YY_USER_ACTION
+#define YY_USER_ACTION
+#endif
+
+/* Code executed at the end of each rule. */
+#ifndef YY_BREAK
+#define YY_BREAK break;
+#endif
+
+#define YY_RULE_SETUP \
+	YY_USER_ACTION
+
+YY_DECL
+	{
+	register yy_state_type yy_current_state;
+	register char *yy_cp = NULL, *yy_bp = NULL;
+	register int yy_act;
+
+#line 67 "aicasm_scan.l"
+
+
+	if ( yy_init )
+		{
+		yy_init = 0;
+
+#ifdef YY_USER_INIT
+		YY_USER_INIT;
+#endif
+
+		if ( ! yy_start )
+			yy_start = 1;	/* first start state */
+
+		if ( ! yyin )
+			yyin = stdin;
+
+		if ( ! yyout )
+			yyout = stdout;
+
+		if ( ! yy_current_buffer )
+			yy_current_buffer =
+				yy_create_buffer( yyin, YY_BUF_SIZE );
+
+		yy_load_buffer_state();
+		}
+
+	while ( 1 )		/* loops until end-of-file is reached */
+		{
+		yy_cp = yy_c_buf_p;
+
+		/* Support of yytext. */
+		*yy_cp = yy_hold_char;
+
+		/* yy_bp points to the position in yy_ch_buf of the start of
+		 * the current run.
+		 */
+		yy_bp = yy_cp;
+
+		yy_current_state = yy_start;
+yy_match:
+		do
+			{
+			register YY_CHAR yy_c = yy_ec[YY_SC_TO_UI(*yy_cp)];
+			if ( yy_accept[yy_current_state] )
+				{
+				yy_last_accepting_state = yy_current_state;
+				yy_last_accepting_cpos = yy_cp;
+				}
+			while ( yy_chk[yy_base[yy_current_state] + yy_c] != yy_current_state )
+				{
+				yy_current_state = (int) yy_def[yy_current_state];
+				if ( yy_current_state >= 256 )
+					yy_c = yy_meta[(unsigned int) yy_c];
+				}
+			yy_current_state = yy_nxt[yy_base[yy_current_state] + (unsigned int) yy_c];
+			++yy_cp;
+			}
+		while ( yy_base[yy_current_state] != 845 );
+
+yy_find_action:
+		yy_act = yy_accept[yy_current_state];
+		if ( yy_act == 0 )
+			{ /* have to back up */
+			yy_cp = yy_last_accepting_cpos;
+			yy_current_state = yy_last_accepting_state;
+			yy_act = yy_accept[yy_current_state];
+			}
+
+		YY_DO_BEFORE_ACTION;
+
+
+do_action:	/* This label is used only to access EOF actions. */
+
+
+		switch ( yy_act )
+	{ /* beginning of action switch */
+			case 0: /* must back up */
+			/* undo the effects of YY_DO_BEFORE_ACTION */
+			*yy_cp = yy_hold_char;
+			yy_cp = yy_last_accepting_cpos;
+			yy_current_state = yy_last_accepting_state;
+			goto yy_find_action;
+
+case 1:
+YY_RULE_SETUP
+#line 68 "aicasm_scan.l"
+{ ++yylineno; }
+	YY_BREAK
+case 2:
+YY_RULE_SETUP
+#line 69 "aicasm_scan.l"
+{ BEGIN COMMENT;  /* Enter comment eating state */ }
+	YY_BREAK
+case 3:
+YY_RULE_SETUP
+#line 70 "aicasm_scan.l"
+{ fprintf(stderr, "Warning! Comment within comment."); }
+	YY_BREAK
+case 4:
+YY_RULE_SETUP
+#line 71 "aicasm_scan.l"
+{ ++yylineno; }
+	YY_BREAK
+case 5:
+YY_RULE_SETUP
+#line 72 "aicasm_scan.l"
+;
+	YY_BREAK
+case 6:
+YY_RULE_SETUP
+#line 73 "aicasm_scan.l"
+;
+	YY_BREAK
+case 7:
+YY_RULE_SETUP
+#line 74 "aicasm_scan.l"
+;
+	YY_BREAK
+case 8:
+YY_RULE_SETUP
+#line 75 "aicasm_scan.l"
+{ BEGIN INITIAL; }
+	YY_BREAK
+case 9:
+YY_RULE_SETUP
+#line 76 "aicasm_scan.l"
+{
+				string_buf_ptr = string_buf;
+				parren_count = 1;
+				BEGIN CEXPR;
+				return T_IF;
+			}
+	YY_BREAK
+case 10:
+YY_RULE_SETUP
+#line 82 "aicasm_scan.l"
+{	*string_buf_ptr++ = '('; parren_count++; }
+	YY_BREAK
+case 11:
+YY_RULE_SETUP
+#line 83 "aicasm_scan.l"
+{
+				parren_count--;
+				if (parren_count == 0) {
+					/* All done */
+					BEGIN INITIAL;
+					*string_buf_ptr = '\0';
+					yylval.sym = symtable_get(string_buf);
+					return T_CEXPR;
+				} else {
+					*string_buf_ptr++ = ')';
+				}
+			}
+	YY_BREAK
+case 12:
+YY_RULE_SETUP
+#line 95 "aicasm_scan.l"
+{ ++yylineno; }
+	YY_BREAK
+case 13:
+YY_RULE_SETUP
+#line 96 "aicasm_scan.l"
+{
+				char *yptr = yytext;
+
+				while (*yptr != '\0') {
+					/* Remove duplicate spaces */
+					if (*yptr == '\t')
+						*yptr = ' ';
+					if (*yptr == ' '
+					 && string_buf_ptr != string_buf
+					 && string_buf_ptr[-1] == ' ')
+						yptr++;
+					else 
+						*string_buf_ptr++ = *yptr++;
+				}
+			}
+	YY_BREAK
+case 14:
+YY_RULE_SETUP
+#line 112 "aicasm_scan.l"
+;
+	YY_BREAK
+/* Register/SCB/SRAM definition keywords */
+case 15:
+YY_RULE_SETUP
+#line 115 "aicasm_scan.l"
+{ return T_REGISTER; }
+	YY_BREAK
+case 16:
+YY_RULE_SETUP
+#line 116 "aicasm_scan.l"
+{ yylval.value = FALSE; return T_CONST; }
+	YY_BREAK
+case 17:
+YY_RULE_SETUP
+#line 117 "aicasm_scan.l"
+{ return T_DOWNLOAD; }
+	YY_BREAK
+case 18:
+YY_RULE_SETUP
+#line 118 "aicasm_scan.l"
+{ return T_ADDRESS; }
+	YY_BREAK
+case 19:
+YY_RULE_SETUP
+#line 119 "aicasm_scan.l"
+{ return T_ACCESS_MODE; }
+	YY_BREAK
+case 20:
+YY_RULE_SETUP
+#line 120 "aicasm_scan.l"
+{
+				 if (strcmp(yytext, "RW") == 0)
+					yylval.value = RW;
+				 else if (strcmp(yytext, "RO") == 0)
+					yylval.value = RO;
+				 else
+					yylval.value = WO;
+				 return T_MODE;
+			}
+	YY_BREAK
+case 21:
+YY_RULE_SETUP
+#line 129 "aicasm_scan.l"
+{ return T_BEGIN_CS; }
+	YY_BREAK
+case 22:
+YY_RULE_SETUP
+#line 130 "aicasm_scan.l"
+{ return T_END_CS; }
+	YY_BREAK
+case 23:
+YY_RULE_SETUP
+#line 131 "aicasm_scan.l"
+{ return T_BIT; }
+	YY_BREAK
+case 24:
+YY_RULE_SETUP
+#line 132 "aicasm_scan.l"
+{ return T_MASK; }
+	YY_BREAK
+case 25:
+YY_RULE_SETUP
+#line 133 "aicasm_scan.l"
+{ return T_ALIAS; }
+	YY_BREAK
+case 26:
+YY_RULE_SETUP
+#line 134 "aicasm_scan.l"
+{ return T_SIZE; }
+	YY_BREAK
+case 27:
+YY_RULE_SETUP
+#line 135 "aicasm_scan.l"
+{ return T_SCB; }
+	YY_BREAK
+case 28:
+YY_RULE_SETUP
+#line 136 "aicasm_scan.l"
+{ return T_SRAM; }
+	YY_BREAK
+case 29:
+YY_RULE_SETUP
+#line 137 "aicasm_scan.l"
+{ return T_ACCUM; }
+	YY_BREAK
+case 30:
+YY_RULE_SETUP
+#line 138 "aicasm_scan.l"
+{ return T_ALLONES; }
+	YY_BREAK
+case 31:
+YY_RULE_SETUP
+#line 139 "aicasm_scan.l"
+{ return T_ALLZEROS; }
+	YY_BREAK
+case 32:
+YY_RULE_SETUP
+#line 140 "aicasm_scan.l"
+{ return T_NONE; }
+	YY_BREAK
+case 33:
+YY_RULE_SETUP
+#line 141 "aicasm_scan.l"
+{ return T_SINDEX; }
+	YY_BREAK
+case 34:
+YY_RULE_SETUP
+#line 142 "aicasm_scan.l"
+{ return T_A; }
+	YY_BREAK
+/* Opcodes */
+case 35:
+YY_RULE_SETUP
+#line 145 "aicasm_scan.l"
+{ return T_SHL; }
+	YY_BREAK
+case 36:
+YY_RULE_SETUP
+#line 146 "aicasm_scan.l"
+{ return T_SHR; }
+	YY_BREAK
+case 37:
+YY_RULE_SETUP
+#line 147 "aicasm_scan.l"
+{ return T_ROR; }
+	YY_BREAK
+case 38:
+YY_RULE_SETUP
+#line 148 "aicasm_scan.l"
+{ return T_ROL; }
+	YY_BREAK
+case 39:
+YY_RULE_SETUP
+#line 149 "aicasm_scan.l"
+{ return T_MVI; }
+	YY_BREAK
+case 40:
+YY_RULE_SETUP
+#line 150 "aicasm_scan.l"
+{ return T_MOV; }
+	YY_BREAK
+case 41:
+YY_RULE_SETUP
+#line 151 "aicasm_scan.l"
+{ return T_CLR; }
+	YY_BREAK
+case 42:
+YY_RULE_SETUP
+#line 152 "aicasm_scan.l"
+{ return T_JMP; }
+	YY_BREAK
+case 43:
+YY_RULE_SETUP
+#line 153 "aicasm_scan.l"
+{ return T_JC;	}
+	YY_BREAK
+case 44:
+YY_RULE_SETUP
+#line 154 "aicasm_scan.l"
+{ return T_JNC;	}
+	YY_BREAK
+case 45:
+YY_RULE_SETUP
+#line 155 "aicasm_scan.l"
+{ return T_JE;	}
+	YY_BREAK
+case 46:
+YY_RULE_SETUP
+#line 156 "aicasm_scan.l"
+{ return T_JNE;	}
+	YY_BREAK
+case 47:
+YY_RULE_SETUP
+#line 157 "aicasm_scan.l"
+{ return T_JZ;	}
+	YY_BREAK
+case 48:
+YY_RULE_SETUP
+#line 158 "aicasm_scan.l"
+{ return T_JNZ;	}
+	YY_BREAK
+case 49:
+YY_RULE_SETUP
+#line 159 "aicasm_scan.l"
+{ return T_CALL; }
+	YY_BREAK
+case 50:
+YY_RULE_SETUP
+#line 160 "aicasm_scan.l"
+{ return T_ADD; }
+	YY_BREAK
+case 51:
+YY_RULE_SETUP
+#line 161 "aicasm_scan.l"
+{ return T_ADC; }
+	YY_BREAK
+case 52:
+YY_RULE_SETUP
+#line 162 "aicasm_scan.l"
+{ return T_BMOV; }
+	YY_BREAK
+case 53:
+YY_RULE_SETUP
+#line 163 "aicasm_scan.l"
+{ return T_INC; }
+	YY_BREAK
+case 54:
+YY_RULE_SETUP
+#line 164 "aicasm_scan.l"
+{ return T_DEC; }
+	YY_BREAK
+case 55:
+YY_RULE_SETUP
+#line 165 "aicasm_scan.l"
+{ return T_STC;	}
+	YY_BREAK
+case 56:
+YY_RULE_SETUP
+#line 166 "aicasm_scan.l"
+{ return T_CLC; }
+	YY_BREAK
+case 57:
+YY_RULE_SETUP
+#line 167 "aicasm_scan.l"
+{ return T_CMP;	}
+	YY_BREAK
+case 58:
+YY_RULE_SETUP
+#line 168 "aicasm_scan.l"
+{ return T_NOT;	}
+	YY_BREAK
+case 59:
+YY_RULE_SETUP
+#line 169 "aicasm_scan.l"
+{ return T_XOR;	}
+	YY_BREAK
+case 60:
+YY_RULE_SETUP
+#line 170 "aicasm_scan.l"
+{ return T_TEST;}
+	YY_BREAK
+case 61:
+YY_RULE_SETUP
+#line 171 "aicasm_scan.l"
+{ return T_AND;	}
+	YY_BREAK
+case 62:
+YY_RULE_SETUP
+#line 172 "aicasm_scan.l"
+{ return T_OR;	}
+	YY_BREAK
+case 63:
+YY_RULE_SETUP
+#line 173 "aicasm_scan.l"
+{ return T_RET; }
+	YY_BREAK
+case 64:
+YY_RULE_SETUP
+#line 174 "aicasm_scan.l"
+{ return T_NOP; }
+	YY_BREAK
+case 65:
+YY_RULE_SETUP
+#line 175 "aicasm_scan.l"
+{ return T_ELSE; }
+	YY_BREAK
+/* Allowed Symbols */
+case 66:
+YY_RULE_SETUP
+#line 178 "aicasm_scan.l"
+{ return yytext[0]; }
+	YY_BREAK
+/* Number processing */
+case 67:
+YY_RULE_SETUP
+#line 181 "aicasm_scan.l"
+{
+				yylval.value = strtol(yytext, NULL, 8);
+				return T_NUMBER;
+			}
+	YY_BREAK
+case 68:
+YY_RULE_SETUP
+#line 186 "aicasm_scan.l"
+{
+				yylval.value = strtoul(yytext + 2, NULL, 16);
+				return T_NUMBER;
+			}
+	YY_BREAK
+case 69:
+YY_RULE_SETUP
+#line 191 "aicasm_scan.l"
+{
+				yylval.value = strtol(yytext, NULL, 10);
+				return T_NUMBER;
+			}
+	YY_BREAK
+/* Include Files */
+case 70:
+YY_RULE_SETUP
+#line 197 "aicasm_scan.l"
+{ return T_INCLUDE; BEGIN INCLUDE;}
+	YY_BREAK
+case 71:
+YY_RULE_SETUP
+#line 198 "aicasm_scan.l"
+{ return yytext[0]; }
+	YY_BREAK
+case 72:
+YY_RULE_SETUP
+#line 199 "aicasm_scan.l"
+{ yylval.str = strdup(yytext); return T_PATH; }
+	YY_BREAK
+case 73:
+YY_RULE_SETUP
+#line 200 "aicasm_scan.l"
+{ BEGIN INITIAL; return yytext[0]; }
+	YY_BREAK
+case 74:
+YY_RULE_SETUP
+#line 201 "aicasm_scan.l"
+{ stop("Invalid include line", EX_DATAERR); }
+	YY_BREAK
+/* For parsing C include files with #define foo */
+case 75:
+YY_RULE_SETUP
+#line 204 "aicasm_scan.l"
+{ yylval.value = TRUE; return T_CONST; }
+	YY_BREAK
+/* Throw away macros */
+case 76:
+YY_RULE_SETUP
+#line 206 "aicasm_scan.l"
+;
+	YY_BREAK
+case 77:
+YY_RULE_SETUP
+#line 207 "aicasm_scan.l"
+{ yylval.str = strdup(yytext); return T_PATH; }
+	YY_BREAK
+case 78:
+YY_RULE_SETUP
+#line 209 "aicasm_scan.l"
+{ yylval.sym = symtable_get(yytext);  return T_SYMBOL; }
+	YY_BREAK
+case 79:
+YY_RULE_SETUP
+#line 211 "aicasm_scan.l"
+{ 
+				char buf[255];
+
+				snprintf(buf, sizeof(buf), "Invalid character "
+					 "'%c'", yytext[0]);
+				stop(buf, EX_DATAERR);
+			}
+	YY_BREAK
+case 80:
+YY_RULE_SETUP
+#line 218 "aicasm_scan.l"
+ECHO;
+	YY_BREAK
+case YY_STATE_EOF(INITIAL):
+case YY_STATE_EOF(COMMENT):
+case YY_STATE_EOF(CEXPR):
+case YY_STATE_EOF(INCLUDE):
+	yyterminate();
+
+	case YY_END_OF_BUFFER:
+		{
+		/* Amount of text matched not including the EOB char. */
+		int yy_amount_of_matched_text = (int) (yy_cp - yytext_ptr) - 1;
+
+		/* Undo the effects of YY_DO_BEFORE_ACTION. */
+		*yy_cp = yy_hold_char;
+		YY_RESTORE_YY_MORE_OFFSET
+
+		if ( yy_current_buffer->yy_buffer_status == YY_BUFFER_NEW )
+			{
+			/* We're scanning a new file or input source.  It's
+			 * possible that this happened because the user
+			 * just pointed yyin at a new source and called
+			 * yylex().  If so, then we have to assure
+			 * consistency between yy_current_buffer and our
+			 * globals.  Here is the right place to do so, because
+			 * this is the first action (other than possibly a
+			 * back-up) that will match for the new input source.
+			 */
+			yy_n_chars = yy_current_buffer->yy_n_chars;
+			yy_current_buffer->yy_input_file = yyin;
+			yy_current_buffer->yy_buffer_status = YY_BUFFER_NORMAL;
+			}
+
+		/* Note that here we test for yy_c_buf_p "<=" to the position
+		 * of the first EOB in the buffer, since yy_c_buf_p will
+		 * already have been incremented past the NUL character
+		 * (since all states make transitions on EOB to the
+		 * end-of-buffer state).  Contrast this with the test
+		 * in input().
+		 */
+		if ( yy_c_buf_p <= &yy_current_buffer->yy_ch_buf[yy_n_chars] )
+			{ /* This was really a NUL. */
+			yy_state_type yy_next_state;
+
+			yy_c_buf_p = yytext_ptr + yy_amount_of_matched_text;
+
+			yy_current_state = yy_get_previous_state();
+
+			/* Okay, we're now positioned to make the NUL
+			 * transition.  We couldn't have
+			 * yy_get_previous_state() go ahead and do it
+			 * for us because it doesn't know how to deal
+			 * with the possibility of jamming (and we don't
+			 * want to build jamming into it because then it
+			 * will run more slowly).
+			 */
+
+			yy_next_state = yy_try_NUL_trans( yy_current_state );
+
+			yy_bp = yytext_ptr + YY_MORE_ADJ;
+
+			if ( yy_next_state )
+				{
+				/* Consume the NUL. */
+				yy_cp = ++yy_c_buf_p;
+				yy_current_state = yy_next_state;
+				goto yy_match;
+				}
+
+			else
+				{
+				yy_cp = yy_c_buf_p;
+				goto yy_find_action;
+				}
+			}
+
+		else switch ( yy_get_next_buffer() )
+			{
+			case EOB_ACT_END_OF_FILE:
+				{
+				yy_did_buffer_switch_on_eof = 0;
+
+				if ( yywrap() )
+					{
+					/* Note: because we've taken care in
+					 * yy_get_next_buffer() to have set up
+					 * yytext, we can now set up
+					 * yy_c_buf_p so that if some total
+					 * hoser (like flex itself) wants to
+					 * call the scanner after we return the
+					 * YY_NULL, it'll still work - another
+					 * YY_NULL will get returned.
+					 */
+					yy_c_buf_p = yytext_ptr + YY_MORE_ADJ;
+
+					yy_act = YY_STATE_EOF(YY_START);
+					goto do_action;
+					}
+
+				else
+					{
+					if ( ! yy_did_buffer_switch_on_eof )
+						YY_NEW_FILE;
+					}
+				break;
+				}
+
+			case EOB_ACT_CONTINUE_SCAN:
+				yy_c_buf_p =
+					yytext_ptr + yy_amount_of_matched_text;
+
+				yy_current_state = yy_get_previous_state();
+
+				yy_cp = yy_c_buf_p;
+				yy_bp = yytext_ptr + YY_MORE_ADJ;
+				goto yy_match;
+
+			case EOB_ACT_LAST_MATCH:
+				yy_c_buf_p =
+				&yy_current_buffer->yy_ch_buf[yy_n_chars];
+
+				yy_current_state = yy_get_previous_state();
+
+				yy_cp = yy_c_buf_p;
+				yy_bp = yytext_ptr + YY_MORE_ADJ;
+				goto yy_find_action;
+			}
+		break;
+		}
+
+	default:
+		YY_FATAL_ERROR(
+			"fatal flex scanner internal error--no action found" );
+	} /* end of action switch */
+		} /* end of scanning one token */
+	} /* end of yylex */
+
+
+/* yy_get_next_buffer - try to read in a new buffer
+ *
+ * Returns a code representing an action:
+ *	EOB_ACT_LAST_MATCH -
+ *	EOB_ACT_CONTINUE_SCAN - continue scanning from current position
+ *	EOB_ACT_END_OF_FILE - end of file
+ */
+
+static int yy_get_next_buffer()
+	{
+	register char *dest = yy_current_buffer->yy_ch_buf;
+	register char *source = yytext_ptr;
+	register int number_to_move, i;
+	int ret_val;
+
+	if ( yy_c_buf_p > &yy_current_buffer->yy_ch_buf[yy_n_chars + 1] )
+		YY_FATAL_ERROR(
+		"fatal flex scanner internal error--end of buffer missed" );
+
+	if ( yy_current_buffer->yy_fill_buffer == 0 )
+		{ /* Don't try to fill the buffer, so this is an EOF. */
+		if ( yy_c_buf_p - yytext_ptr - YY_MORE_ADJ == 1 )
+			{
+			/* We matched a single character, the EOB, so
+			 * treat this as a final EOF.
+			 */
+			return EOB_ACT_END_OF_FILE;
+			}
+
+		else
+			{
+			/* We matched some text prior to the EOB, first
+			 * process it.
+			 */
+			return EOB_ACT_LAST_MATCH;
+			}
+		}
+
+	/* Try to read more data. */
+
+	/* First move last chars to start of buffer. */
+	number_to_move = (int) (yy_c_buf_p - yytext_ptr) - 1;
+
+	for ( i = 0; i < number_to_move; ++i )
+		*(dest++) = *(source++);
+
+	if ( yy_current_buffer->yy_buffer_status == YY_BUFFER_EOF_PENDING )
+		/* don't do the read, it's not guaranteed to return an EOF,
+		 * just force an EOF
+		 */
+		yy_current_buffer->yy_n_chars = yy_n_chars = 0;
+
+	else
+		{
+		int num_to_read =
+			yy_current_buffer->yy_buf_size - number_to_move - 1;
+
+		while ( num_to_read <= 0 )
+			{ /* Not enough room in the buffer - grow it. */
+#ifdef YY_USES_REJECT
+			YY_FATAL_ERROR(
+"input buffer overflow, can't enlarge buffer because scanner uses REJECT" );
+#else
+
+			/* just a shorter name for the current buffer */
+			YY_BUFFER_STATE b = yy_current_buffer;
+
+			int yy_c_buf_p_offset =
+				(int) (yy_c_buf_p - b->yy_ch_buf);
+
+			if ( b->yy_is_our_buffer )
+				{
+				int new_size = b->yy_buf_size * 2;
+
+				if ( new_size <= 0 )
+					b->yy_buf_size += b->yy_buf_size / 8;
+				else
+					b->yy_buf_size *= 2;
+
+				b->yy_ch_buf = (char *)
+					/* Include room in for 2 EOB chars. */
+					yy_flex_realloc( (void *) b->yy_ch_buf,
+							 b->yy_buf_size + 2 );
+				}
+			else
+				/* Can't grow it, we don't own it. */
+				b->yy_ch_buf = 0;
+
+			if ( ! b->yy_ch_buf )
+				YY_FATAL_ERROR(
+				"fatal error - scanner input buffer overflow" );
+
+			yy_c_buf_p = &b->yy_ch_buf[yy_c_buf_p_offset];
+
+			num_to_read = yy_current_buffer->yy_buf_size -
+						number_to_move - 1;
+#endif
+			}
+
+		if ( num_to_read > YY_READ_BUF_SIZE )
+			num_to_read = YY_READ_BUF_SIZE;
+
+		/* Read in more data. */
+		YY_INPUT( (&yy_current_buffer->yy_ch_buf[number_to_move]),
+			yy_n_chars, num_to_read );
+
+		yy_current_buffer->yy_n_chars = yy_n_chars;
+		}
+
+	if ( yy_n_chars == 0 )
+		{
+		if ( number_to_move == YY_MORE_ADJ )
+			{
+			ret_val = EOB_ACT_END_OF_FILE;
+			yyrestart( yyin );
+			}
+
+		else
+			{
+			ret_val = EOB_ACT_LAST_MATCH;
+			yy_current_buffer->yy_buffer_status =
+				YY_BUFFER_EOF_PENDING;
+			}
+		}
+
+	else
+		ret_val = EOB_ACT_CONTINUE_SCAN;
+
+	yy_n_chars += number_to_move;
+	yy_current_buffer->yy_ch_buf[yy_n_chars] = YY_END_OF_BUFFER_CHAR;
+	yy_current_buffer->yy_ch_buf[yy_n_chars + 1] = YY_END_OF_BUFFER_CHAR;
+
+	yytext_ptr = &yy_current_buffer->yy_ch_buf[0];
+
+	return ret_val;
+	}
+
+
+/* yy_get_previous_state - get the state just before the EOB char was reached */
+
+static yy_state_type yy_get_previous_state()
+	{
+	register yy_state_type yy_current_state;
+	register char *yy_cp;
+
+	yy_current_state = yy_start;
+
+	for ( yy_cp = yytext_ptr + YY_MORE_ADJ; yy_cp < yy_c_buf_p; ++yy_cp )
+		{
+		register YY_CHAR yy_c = (*yy_cp ? yy_ec[YY_SC_TO_UI(*yy_cp)] : 1);
+		if ( yy_accept[yy_current_state] )
+			{
+			yy_last_accepting_state = yy_current_state;
+			yy_last_accepting_cpos = yy_cp;
+			}
+		while ( yy_chk[yy_base[yy_current_state] + yy_c] != yy_current_state )
+			{
+			yy_current_state = (int) yy_def[yy_current_state];
+			if ( yy_current_state >= 256 )
+				yy_c = yy_meta[(unsigned int) yy_c];
+			}
+		yy_current_state = yy_nxt[yy_base[yy_current_state] + (unsigned int) yy_c];
+		}
+
+	return yy_current_state;
+	}
+
+
+/* yy_try_NUL_trans - try to make a transition on the NUL character
+ *
+ * synopsis
+ *	next_state = yy_try_NUL_trans( current_state );
+ */
+
+#ifdef YY_USE_PROTOS
+static yy_state_type yy_try_NUL_trans( yy_state_type yy_current_state )
+#else
+static yy_state_type yy_try_NUL_trans( yy_current_state )
+yy_state_type yy_current_state;
+#endif
+	{
+	register int yy_is_jam;
+	register char *yy_cp = yy_c_buf_p;
+
+	register YY_CHAR yy_c = 1;
+	if ( yy_accept[yy_current_state] )
+		{
+		yy_last_accepting_state = yy_current_state;
+		yy_last_accepting_cpos = yy_cp;
+		}
+	while ( yy_chk[yy_base[yy_current_state] + yy_c] != yy_current_state )
+		{
+		yy_current_state = (int) yy_def[yy_current_state];
+		if ( yy_current_state >= 256 )
+			yy_c = yy_meta[(unsigned int) yy_c];
+		}
+	yy_current_state = yy_nxt[yy_base[yy_current_state] + (unsigned int) yy_c];
+	yy_is_jam = (yy_current_state == 255);
+
+	return yy_is_jam ? 0 : yy_current_state;
+	}
+
+
+#ifndef YY_NO_UNPUT
+#ifdef YY_USE_PROTOS
+static void yyunput( int c, register char *yy_bp )
+#else
+static void yyunput( c, yy_bp )
+int c;
+register char *yy_bp;
+#endif
+	{
+	register char *yy_cp = yy_c_buf_p;
+
+	/* undo effects of setting up yytext */
+	*yy_cp = yy_hold_char;
+
+	if ( yy_cp < yy_current_buffer->yy_ch_buf + 2 )
+		{ /* need to shift things up to make room */
+		/* +2 for EOB chars. */
+		register int number_to_move = yy_n_chars + 2;
+		register char *dest = &yy_current_buffer->yy_ch_buf[
+					yy_current_buffer->yy_buf_size + 2];
+		register char *source =
+				&yy_current_buffer->yy_ch_buf[number_to_move];
+
+		while ( source > yy_current_buffer->yy_ch_buf )
+			*--dest = *--source;
+
+		yy_cp += (int) (dest - source);
+		yy_bp += (int) (dest - source);
+		yy_current_buffer->yy_n_chars =
+			yy_n_chars = yy_current_buffer->yy_buf_size;
+
+		if ( yy_cp < yy_current_buffer->yy_ch_buf + 2 )
+			YY_FATAL_ERROR( "flex scanner push-back overflow" );
+		}
+
+	*--yy_cp = (char) c;
+
+
+	yytext_ptr = yy_bp;
+	yy_hold_char = *yy_cp;
+	yy_c_buf_p = yy_cp;
+	}
+#endif	/* ifndef YY_NO_UNPUT */
+
+
+#ifdef __cplusplus
+static int yyinput()
+#else
+static int input()
+#endif
+	{
+	int c;
+
+	*yy_c_buf_p = yy_hold_char;
+
+	if ( *yy_c_buf_p == YY_END_OF_BUFFER_CHAR )
+		{
+		/* yy_c_buf_p now points to the character we want to return.
+		 * If this occurs *before* the EOB characters, then it's a
+		 * valid NUL; if not, then we've hit the end of the buffer.
+		 */
+		if ( yy_c_buf_p < &yy_current_buffer->yy_ch_buf[yy_n_chars] )
+			/* This was really a NUL. */
+			*yy_c_buf_p = '\0';
+
+		else
+			{ /* need more input */
+			int offset = yy_c_buf_p - yytext_ptr;
+			++yy_c_buf_p;
+
+			switch ( yy_get_next_buffer() )
+				{
+				case EOB_ACT_LAST_MATCH:
+					/* This happens because yy_g_n_b()
+					 * sees that we've accumulated a
+					 * token and flags that we need to
+					 * try matching the token before
+					 * proceeding.  But for input(),
+					 * there's no matching to consider.
+					 * So convert the EOB_ACT_LAST_MATCH
+					 * to EOB_ACT_END_OF_FILE.
+					 */
+
+					/* Reset buffer status. */
+					yyrestart( yyin );
+
+					/* fall through */
+
+				case EOB_ACT_END_OF_FILE:
+					{
+					if ( yywrap() )
+						return EOF;
+
+					if ( ! yy_did_buffer_switch_on_eof )
+						YY_NEW_FILE;
+#ifdef __cplusplus
+					return yyinput();
+#else
+					return input();
+#endif
+					}
+
+				case EOB_ACT_CONTINUE_SCAN:
+					yy_c_buf_p = yytext_ptr + offset;
+					break;
+				}
+			}
+		}
+
+	c = *(unsigned char *) yy_c_buf_p;	/* cast for 8-bit char's */
+	*yy_c_buf_p = '\0';	/* preserve yytext */
+	yy_hold_char = *++yy_c_buf_p;
+
+
+	return c;
+	}
+
+
+#ifdef YY_USE_PROTOS
+void yyrestart( FILE *input_file )
+#else
+void yyrestart( input_file )
+FILE *input_file;
+#endif
+	{
+	if ( ! yy_current_buffer )
+		yy_current_buffer = yy_create_buffer( yyin, YY_BUF_SIZE );
+
+	yy_init_buffer( yy_current_buffer, input_file );
+	yy_load_buffer_state();
+	}
+
+
+#ifdef YY_USE_PROTOS
+void yy_switch_to_buffer( YY_BUFFER_STATE new_buffer )
+#else
+void yy_switch_to_buffer( new_buffer )
+YY_BUFFER_STATE new_buffer;
+#endif
+	{
+	if ( yy_current_buffer == new_buffer )
+		return;
+
+	if ( yy_current_buffer )
+		{
+		/* Flush out information for old buffer. */
+		*yy_c_buf_p = yy_hold_char;
+		yy_current_buffer->yy_buf_pos = yy_c_buf_p;
+		yy_current_buffer->yy_n_chars = yy_n_chars;
+		}
+
+	yy_current_buffer = new_buffer;
+	yy_load_buffer_state();
+
+	/* We don't actually know whether we did this switch during
+	 * EOF (yywrap()) processing, but the only time this flag
+	 * is looked at is after yywrap() is called, so it's safe
+	 * to go ahead and always set it.
+	 */
+	yy_did_buffer_switch_on_eof = 1;
+	}
+
+
+#ifdef YY_USE_PROTOS
+void yy_load_buffer_state( void )
+#else
+void yy_load_buffer_state()
+#endif
+	{
+	yy_n_chars = yy_current_buffer->yy_n_chars;
+	yytext_ptr = yy_c_buf_p = yy_current_buffer->yy_buf_pos;
+	yyin = yy_current_buffer->yy_input_file;
+	yy_hold_char = *yy_c_buf_p;
+	}
+
+
+#ifdef YY_USE_PROTOS
+YY_BUFFER_STATE yy_create_buffer( FILE *file, int size )
+#else
+YY_BUFFER_STATE yy_create_buffer( file, size )
+FILE *file;
+int size;
+#endif
+	{
+	YY_BUFFER_STATE b;
+
+	b = (YY_BUFFER_STATE) yy_flex_alloc( sizeof( struct yy_buffer_state ) );
+	if ( ! b )
+		YY_FATAL_ERROR( "out of dynamic memory in yy_create_buffer()" );
+
+	b->yy_buf_size = size;
+
+	/* yy_ch_buf has to be 2 characters longer than the size given because
+	 * we need to put in 2 end-of-buffer characters.
+	 */
+	b->yy_ch_buf = (char *) yy_flex_alloc( b->yy_buf_size + 2 );
+	if ( ! b->yy_ch_buf )
+		YY_FATAL_ERROR( "out of dynamic memory in yy_create_buffer()" );
+
+	b->yy_is_our_buffer = 1;
+
+	yy_init_buffer( b, file );
+
+	return b;
+	}
+
+
+#ifdef YY_USE_PROTOS
+void yy_delete_buffer( YY_BUFFER_STATE b )
+#else
+void yy_delete_buffer( b )
+YY_BUFFER_STATE b;
+#endif
+	{
+	if ( ! b )
+		return;
+
+	if ( b == yy_current_buffer )
+		yy_current_buffer = (YY_BUFFER_STATE) 0;
+
+	if ( b->yy_is_our_buffer )
+		yy_flex_free( (void *) b->yy_ch_buf );
+
+	yy_flex_free( (void *) b );
+	}
+
+
+#ifndef YY_ALWAYS_INTERACTIVE
+#ifndef YY_NEVER_INTERACTIVE
+extern int isatty YY_PROTO(( int ));
+#endif
+#endif
+
+#ifdef YY_USE_PROTOS
+void yy_init_buffer( YY_BUFFER_STATE b, FILE *file )
+#else
+void yy_init_buffer( b, file )
+YY_BUFFER_STATE b;
+FILE *file;
+#endif
+
+
+	{
+	yy_flush_buffer( b );
+
+	b->yy_input_file = file;
+	b->yy_fill_buffer = 1;
+
+#if YY_ALWAYS_INTERACTIVE
+	b->yy_is_interactive = 1;
+#else
+#if YY_NEVER_INTERACTIVE
+	b->yy_is_interactive = 0;
+#else
+	b->yy_is_interactive = file ? (isatty( fileno(file) ) > 0) : 0;
+#endif
+#endif
+	}
+
+
+#ifdef YY_USE_PROTOS
+void yy_flush_buffer( YY_BUFFER_STATE b )
+#else
+void yy_flush_buffer( b )
+YY_BUFFER_STATE b;
+#endif
+
+	{
+	if ( ! b )
+		return;
+
+	b->yy_n_chars = 0;
+
+	/* We always need two end-of-buffer characters.  The first causes
+	 * a transition to the end-of-buffer state.  The second causes
+	 * a jam in that state.
+	 */
+	b->yy_ch_buf[0] = YY_END_OF_BUFFER_CHAR;
+	b->yy_ch_buf[1] = YY_END_OF_BUFFER_CHAR;
+
+	b->yy_buf_pos = &b->yy_ch_buf[0];
+
+	b->yy_at_bol = 1;
+	b->yy_buffer_status = YY_BUFFER_NEW;
+
+	if ( b == yy_current_buffer )
+		yy_load_buffer_state();
+	}
+
+
+#ifndef YY_NO_SCAN_BUFFER
+#ifdef YY_USE_PROTOS
+YY_BUFFER_STATE yy_scan_buffer( char *base, yy_size_t size )
+#else
+YY_BUFFER_STATE yy_scan_buffer( base, size )
+char *base;
+yy_size_t size;
+#endif
+	{
+	YY_BUFFER_STATE b;
+
+	if ( size < 2 ||
+	     base[size-2] != YY_END_OF_BUFFER_CHAR ||
+	     base[size-1] != YY_END_OF_BUFFER_CHAR )
+		/* They forgot to leave room for the EOB's. */
+		return 0;
+
+	b = (YY_BUFFER_STATE) yy_flex_alloc( sizeof( struct yy_buffer_state ) );
+	if ( ! b )
+		YY_FATAL_ERROR( "out of dynamic memory in yy_scan_buffer()" );
+
+	b->yy_buf_size = size - 2;	/* "- 2" to take care of EOB's */
+	b->yy_buf_pos = b->yy_ch_buf = base;
+	b->yy_is_our_buffer = 0;
+	b->yy_input_file = 0;
+	b->yy_n_chars = b->yy_buf_size;
+	b->yy_is_interactive = 0;
+	b->yy_at_bol = 1;
+	b->yy_fill_buffer = 0;
+	b->yy_buffer_status = YY_BUFFER_NEW;
+
+	yy_switch_to_buffer( b );
+
+	return b;
+	}
+#endif
+
+
+#ifndef YY_NO_SCAN_STRING
+#ifdef YY_USE_PROTOS
+YY_BUFFER_STATE yy_scan_string( yyconst char *yy_str )
+#else
+YY_BUFFER_STATE yy_scan_string( yy_str )
+yyconst char *yy_str;
+#endif
+	{
+	int len;
+	for ( len = 0; yy_str[len]; ++len )
+		;
+
+	return yy_scan_bytes( yy_str, len );
+	}
+#endif
+
+
+#ifndef YY_NO_SCAN_BYTES
+#ifdef YY_USE_PROTOS
+YY_BUFFER_STATE yy_scan_bytes( yyconst char *bytes, int len )
+#else
+YY_BUFFER_STATE yy_scan_bytes( bytes, len )
+yyconst char *bytes;
+int len;
+#endif
+	{
+	YY_BUFFER_STATE b;
+	char *buf;
+	yy_size_t n;
+	int i;
+
+	/* Get memory for full buffer, including space for trailing EOB's. */
+	n = len + 2;
+	buf = (char *) yy_flex_alloc( n );
+	if ( ! buf )
+		YY_FATAL_ERROR( "out of dynamic memory in yy_scan_bytes()" );
+
+	for ( i = 0; i < len; ++i )
+		buf[i] = bytes[i];
+
+	buf[len] = buf[len+1] = YY_END_OF_BUFFER_CHAR;
+
+	b = yy_scan_buffer( buf, n );
+	if ( ! b )
+		YY_FATAL_ERROR( "bad buffer in yy_scan_bytes()" );
+
+	/* It's okay to grow etc. this buffer, and we should throw it
+	 * away when we're done.
+	 */
+	b->yy_is_our_buffer = 1;
+
+	return b;
+	}
+#endif
+
+
+#ifndef YY_NO_PUSH_STATE
+#ifdef YY_USE_PROTOS
+static void yy_push_state( int new_state )
+#else
+static void yy_push_state( new_state )
+int new_state;
+#endif
+	{
+	if ( yy_start_stack_ptr >= yy_start_stack_depth )
+		{
+		yy_size_t new_size;
+
+		yy_start_stack_depth += YY_START_STACK_INCR;
+		new_size = yy_start_stack_depth * sizeof( int );
+
+		if ( ! yy_start_stack )
+			yy_start_stack = (int *) yy_flex_alloc( new_size );
+
+		else
+			yy_start_stack = (int *) yy_flex_realloc(
+					(void *) yy_start_stack, new_size );
+
+		if ( ! yy_start_stack )
+			YY_FATAL_ERROR(
+			"out of memory expanding start-condition stack" );
+		}
+
+	yy_start_stack[yy_start_stack_ptr++] = YY_START;
+
+	BEGIN(new_state);
+	}
+#endif
+
+
+#ifndef YY_NO_POP_STATE
+static void yy_pop_state()
+	{
+	if ( --yy_start_stack_ptr < 0 )
+		YY_FATAL_ERROR( "start-condition stack underflow" );
+
+	BEGIN(yy_start_stack[yy_start_stack_ptr]);
+	}
+#endif
+
+
+#ifndef YY_NO_TOP_STATE
+static int yy_top_state()
+	{
+	return yy_start_stack[yy_start_stack_ptr - 1];
+	}
+#endif
+
+#ifndef YY_EXIT_FAILURE
+#define YY_EXIT_FAILURE 2
+#endif
+
+#ifdef YY_USE_PROTOS
+static void yy_fatal_error( yyconst char msg[] )
+#else
+static void yy_fatal_error( msg )
+char msg[];
+#endif
+	{
+	(void) fprintf( stderr, "%s\n", msg );
+	exit( YY_EXIT_FAILURE );
+	}
+
+
+
+/* Redefine yyless() so it works in section 3 code. */
+
+#undef yyless
+#define yyless(n) \
+	do \
+		{ \
+		/* Undo effects of setting up yytext. */ \
+		yytext[yyleng] = yy_hold_char; \
+		yy_c_buf_p = yytext + n; \
+		yy_hold_char = *yy_c_buf_p; \
+		*yy_c_buf_p = '\0'; \
+		yyleng = n; \
+		} \
+	while ( 0 )
+
+
+/* Internal utility routines. */
+
+#ifndef yytext_ptr
+#ifdef YY_USE_PROTOS
+static void yy_flex_strncpy( char *s1, yyconst char *s2, int n )
+#else
+static void yy_flex_strncpy( s1, s2, n )
+char *s1;
+yyconst char *s2;
+int n;
+#endif
+	{
+	register int i;
+	for ( i = 0; i < n; ++i )
+		s1[i] = s2[i];
+	}
+#endif
+
+#ifdef YY_NEED_STRLEN
+#ifdef YY_USE_PROTOS
+static int yy_flex_strlen( yyconst char *s )
+#else
+static int yy_flex_strlen( s )
+yyconst char *s;
+#endif
+	{
+	register int n;
+	for ( n = 0; s[n]; ++n )
+		;
+
+	return n;
+	}
+#endif
+
+
+#ifdef YY_USE_PROTOS
+static void *yy_flex_alloc( yy_size_t size )
+#else
+static void *yy_flex_alloc( size )
+yy_size_t size;
+#endif
+	{
+	return (void *) malloc( size );
+	}
+
+#ifdef YY_USE_PROTOS
+static void *yy_flex_realloc( void *ptr, yy_size_t size )
+#else
+static void *yy_flex_realloc( ptr, size )
+void *ptr;
+yy_size_t size;
+#endif
+	{
+	/* The cast to (char *) in the following accommodates both
+	 * implementations that use char* generic pointers, and those
+	 * that use void* generic pointers.  It works with the latter
+	 * because both ANSI C and C++ allow castless assignment from
+	 * any pointer type to void*, and deal with argument conversions
+	 * as though doing an assignment.
+	 */
+	return (void *) realloc( (char *) ptr, size );
+	}
+
+#ifdef YY_USE_PROTOS
+static void yy_flex_free( void *ptr )
+#else
+static void yy_flex_free( ptr )
+void *ptr;
+#endif
+	{
+	free( ptr );
+	}
+
+#if YY_MAIN
+int main()
+	{
+	yylex();
+	return 0;
+	}
+#endif
+#line 218 "aicasm_scan.l"
+
+
+typedef struct include {
+        YY_BUFFER_STATE  buffer;
+        int              lineno;
+        char            *filename;
+	SLIST_ENTRY(include) links;
+}include_t;
+
+SLIST_HEAD(, include) include_stack;
+
+void
+include_file(char *file_name, include_type type)
+{
+	FILE *newfile;
+	include_t *include;
+
+	newfile = NULL;
+	/* Try the current directory first */
+	if (includes_search_curdir != 0 || type == SOURCE_FILE)
+		newfile = fopen(file_name, "r");
+
+	if (newfile == NULL && type != SOURCE_FILE) {
+                path_entry_t include_dir;
+                for (include_dir = search_path.slh_first;
+                     include_dir != NULL;                
+                     include_dir = include_dir->links.sle_next) {
+			char fullname[PATH_MAX];
+
+			if ((include_dir->quoted_includes_only == TRUE)
+			 && (type != QUOTED_INCLUDE))
+				continue;
+
+			snprintf(fullname, sizeof(fullname),
+				 "%s/%s", include_dir->directory, file_name);
+
+			if ((newfile = fopen(fullname, "r")) != NULL)
+				break;
+                }
+        }
+
+	if (newfile == NULL) {
+		perror(file_name);
+		stop("Unable to open input file", EX_SOFTWARE);
+		/* NOTREACHED */
+	}
+
+	if (type != SOURCE_FILE) {
+		include = (include_t *)malloc(sizeof(include_t));
+		if (include == NULL) {
+			stop("Unable to allocate include stack entry",
+			     EX_SOFTWARE);
+			/* NOTREACHED */
+		}
+		include->buffer = YY_CURRENT_BUFFER;
+		include->lineno = yylineno;
+		include->filename = yyfilename;
+		SLIST_INSERT_HEAD(&include_stack, include, links);
+	}
+	yy_switch_to_buffer(yy_create_buffer(newfile, YY_BUF_SIZE));
+	yylineno = 1;
+	yyfilename = strdup(file_name);
+}
+
+int
+yywrap()
+{
+	include_t *include;
+
+	yy_delete_buffer(YY_CURRENT_BUFFER);
+	(void)fclose(yyin);
+	if (yyfilename != NULL)
+		free(yyfilename);
+	yyfilename = NULL;
+	include = include_stack.slh_first;
+	if (include != NULL) {
+		yy_switch_to_buffer(include->buffer);
+		yylineno = include->lineno;
+		yyfilename = include->filename;
+		SLIST_REMOVE_HEAD(&include_stack, links);
+		free(include);
+		return (0);
+	}
+	return (1);
+}
diff -urN linux/drivers/scsi/aic7xxx/aicasm/aicasm_scan.l /tmp/linux/drivers/scsi/aic7xxx/aicasm/aicasm_scan.l
--- linux/drivers/scsi/aic7xxx/aicasm/aicasm_scan.l	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aicasm/aicasm_scan.l	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,302 @@
+%{
+/*
+ * Lexical Analyzer for the Aic7xxx SCSI Host adapter sequencer assembler.
+ *
+ * Copyright (c) 1997, 1998 Justin T. Gibbs.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $Id: //depot/src/aic7xxx/aicasm/aicasm_scan.l#4 $
+ *
+ * $FreeBSD: src/sys/dev/aic7xxx/aicasm/aicasm_scan.l,v 1.13 2000/09/22 22:19:54 gibbs Exp $
+ */
+
+#include <sys/types.h>
+
+#include <limits.h>
+#include <stdio.h>
+#include <string.h>
+#include <sysexits.h>
+#ifdef __linux__
+#include "../queue.h"
+#else
+#include <sys/queue.h>
+#endif
+
+#include "aicasm.h"
+#include "aicasm_symbol.h"
+#include "y.tab.h"
+
+#define MAX_STR_CONST 256
+char string_buf[MAX_STR_CONST];
+char *string_buf_ptr;
+int  parren_count;
+%}
+
+PATH		[-/A-Za-z0-9_.]*[./][-/A-Za-z0-9_.]*
+WORD		[A-Za-z_][-A-Za-z_0-9]*
+SPACE		[ \t]+
+
+%x COMMENT
+%x CEXPR
+%x INCLUDE
+
+%%
+\n			{ ++yylineno; }
+"/*"			{ BEGIN COMMENT;  /* Enter comment eating state */ }
+<COMMENT>"/*"		{ fprintf(stderr, "Warning! Comment within comment."); }
+<COMMENT>\n		{ ++yylineno; }
+<COMMENT>[^*/\n]*	;
+<COMMENT>"*"+[^*/\n]*	;
+<COMMENT>"/"+[^*/\n]*	;
+<COMMENT>"*"+"/"	{ BEGIN INITIAL; }
+if[ \t]*\(		{
+				string_buf_ptr = string_buf;
+				parren_count = 1;
+				BEGIN CEXPR;
+				return T_IF;
+			}
+<CEXPR>\(		{	*string_buf_ptr++ = '('; parren_count++; }
+<CEXPR>\)		{
+				parren_count--;
+				if (parren_count == 0) {
+					/* All done */
+					BEGIN INITIAL;
+					*string_buf_ptr = '\0';
+					yylval.sym = symtable_get(string_buf);
+					return T_CEXPR;
+				} else {
+					*string_buf_ptr++ = ')';
+				}
+			}
+<CEXPR>\n		{ ++yylineno; }
+<CEXPR>[^()\n]+	{
+				char *yptr = yytext;
+
+				while (*yptr != '\0') {
+					/* Remove duplicate spaces */
+					if (*yptr == '\t')
+						*yptr = ' ';
+					if (*yptr == ' '
+					 && string_buf_ptr != string_buf
+					 && string_buf_ptr[-1] == ' ')
+						yptr++;
+					else 
+						*string_buf_ptr++ = *yptr++;
+				}
+			}
+
+{SPACE}			;
+
+	/* Register/SCB/SRAM definition keywords */
+register		{ return T_REGISTER; }
+const			{ yylval.value = FALSE; return T_CONST; }
+download		{ return T_DOWNLOAD; }
+address			{ return T_ADDRESS; }
+access_mode		{ return T_ACCESS_MODE; }
+RW|RO|WO		{
+				 if (strcmp(yytext, "RW") == 0)
+					yylval.value = RW;
+				 else if (strcmp(yytext, "RO") == 0)
+					yylval.value = RO;
+				 else
+					yylval.value = WO;
+				 return T_MODE;
+			}
+BEGIN_CRITICAL		{ return T_BEGIN_CS; }
+END_CRITICAL		{ return T_END_CS; }
+bit			{ return T_BIT; }
+mask			{ return T_MASK; }
+alias			{ return T_ALIAS; }
+size			{ return T_SIZE; }
+scb			{ return T_SCB; }
+scratch_ram		{ return T_SRAM; }
+accumulator		{ return T_ACCUM; }
+allones			{ return T_ALLONES; }
+allzeros		{ return T_ALLZEROS; }
+none			{ return T_NONE; }
+sindex			{ return T_SINDEX; }
+A			{ return T_A; }
+
+	/* Opcodes */
+shl			{ return T_SHL; }
+shr			{ return T_SHR; }
+ror			{ return T_ROR; }
+rol			{ return T_ROL; }
+mvi			{ return T_MVI; }
+mov			{ return T_MOV; }
+clr			{ return T_CLR; }
+jmp			{ return T_JMP; }
+jc			{ return T_JC;	}
+jnc			{ return T_JNC;	}
+je			{ return T_JE;	}
+jne			{ return T_JNE;	}
+jz			{ return T_JZ;	}
+jnz			{ return T_JNZ;	}
+call			{ return T_CALL; }
+add			{ return T_ADD; }
+adc			{ return T_ADC; }
+bmov			{ return T_BMOV; }
+inc			{ return T_INC; }
+dec			{ return T_DEC; }
+stc			{ return T_STC;	}
+clc			{ return T_CLC; }
+cmp			{ return T_CMP;	}
+not			{ return T_NOT;	}
+xor			{ return T_XOR;	}
+test			{ return T_TEST;}
+and			{ return T_AND;	}
+or			{ return T_OR;	}
+ret			{ return T_RET; }
+nop			{ return T_NOP; }
+else			{ return T_ELSE; }
+
+	/* Allowed Symbols */
+[-+,:()~|&."{};<>[\]!]	{ return yytext[0]; }
+
+	/* Number processing */
+0[0-7]*			{
+				yylval.value = strtol(yytext, NULL, 8);
+				return T_NUMBER;
+			}
+
+0[xX][0-9a-fA-F]+	{
+				yylval.value = strtoul(yytext + 2, NULL, 16);
+				return T_NUMBER;
+			}
+
+[1-9][0-9]*		{
+				yylval.value = strtol(yytext, NULL, 10);
+				return T_NUMBER;
+			}
+
+	/* Include Files */
+#include		{ return T_INCLUDE; BEGIN INCLUDE;}
+<INCLUDE>[<>\"]		{ return yytext[0]; }
+<INCLUDE>{PATH}		{ yylval.str = strdup(yytext); return T_PATH; }
+<INCLUDE>;		{ BEGIN INITIAL; return yytext[0]; }
+<INCLUDE>.		{ stop("Invalid include line", EX_DATAERR); }
+
+	/* For parsing C include files with #define foo */
+#define			{ yylval.value = TRUE; return T_CONST; }
+	/* Throw away macros */
+#define[^\n]*[()]+[^\n]* ;
+{PATH}			{ yylval.str = strdup(yytext); return T_PATH; }
+
+{WORD}			{ yylval.sym = symtable_get(yytext);  return T_SYMBOL; }
+
+.			{ 
+				char buf[255];
+
+				snprintf(buf, sizeof(buf), "Invalid character "
+					 "'%c'", yytext[0]);
+				stop(buf, EX_DATAERR);
+			}
+%%
+
+typedef struct include {
+        YY_BUFFER_STATE  buffer;
+        int              lineno;
+        char            *filename;
+	SLIST_ENTRY(include) links;
+}include_t;
+
+SLIST_HEAD(, include) include_stack;
+
+void
+include_file(char *file_name, include_type type)
+{
+	FILE *newfile;
+	include_t *include;
+
+	newfile = NULL;
+	/* Try the current directory first */
+	if (includes_search_curdir != 0 || type == SOURCE_FILE)
+		newfile = fopen(file_name, "r");
+
+	if (newfile == NULL && type != SOURCE_FILE) {
+                path_entry_t include_dir;
+                for (include_dir = search_path.slh_first;
+                     include_dir != NULL;                
+                     include_dir = include_dir->links.sle_next) {
+			char fullname[PATH_MAX];
+
+			if ((include_dir->quoted_includes_only == TRUE)
+			 && (type != QUOTED_INCLUDE))
+				continue;
+
+			snprintf(fullname, sizeof(fullname),
+				 "%s/%s", include_dir->directory, file_name);
+
+			if ((newfile = fopen(fullname, "r")) != NULL)
+				break;
+                }
+        }
+
+	if (newfile == NULL) {
+		perror(file_name);
+		stop("Unable to open input file", EX_SOFTWARE);
+		/* NOTREACHED */
+	}
+
+	if (type != SOURCE_FILE) {
+		include = (include_t *)malloc(sizeof(include_t));
+		if (include == NULL) {
+			stop("Unable to allocate include stack entry",
+			     EX_SOFTWARE);
+			/* NOTREACHED */
+		}
+		include->buffer = YY_CURRENT_BUFFER;
+		include->lineno = yylineno;
+		include->filename = yyfilename;
+		SLIST_INSERT_HEAD(&include_stack, include, links);
+	}
+	yy_switch_to_buffer(yy_create_buffer(newfile, YY_BUF_SIZE));
+	yylineno = 1;
+	yyfilename = strdup(file_name);
+}
+
+int
+yywrap()
+{
+	include_t *include;
+
+	yy_delete_buffer(YY_CURRENT_BUFFER);
+	(void)fclose(yyin);
+	if (yyfilename != NULL)
+		free(yyfilename);
+	yyfilename = NULL;
+	include = include_stack.slh_first;
+	if (include != NULL) {
+		yy_switch_to_buffer(include->buffer);
+		yylineno = include->lineno;
+		yyfilename = include->filename;
+		SLIST_REMOVE_HEAD(&include_stack, links);
+		free(include);
+		return (0);
+	}
+	return (1);
+}
diff -urN linux/drivers/scsi/aic7xxx/aicasm/aicasm_symbol.c /tmp/linux/drivers/scsi/aic7xxx/aicasm/aicasm_symbol.c
--- linux/drivers/scsi/aic7xxx/aicasm/aicasm_symbol.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aicasm/aicasm_symbol.c	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,468 @@
+/*
+ * Aic7xxx SCSI host adapter firmware asssembler symbol table implementation
+ *
+ * Copyright (c) 1997 Justin T. Gibbs.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $Id: //depot/src/aic7xxx/aicasm/aicasm_symbol.c#4 $
+ *
+ * $FreeBSD: src/sys/dev/aic7xxx/aicasm/aicasm_symbol.c,v 1.11 2000/09/22 22:19:54 gibbs Exp $
+ */
+
+#include <sys/types.h>
+
+#ifdef __linux__
+#include <db1/db.h>
+#else
+#include <db.h>
+#endif
+#include <fcntl.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <sysexits.h>
+
+#include "aicasm_symbol.h"
+#include "aicasm.h"
+
+static DB *symtable;
+
+symbol_t *
+symbol_create(char *name)
+{
+	symbol_t *new_symbol;
+
+	new_symbol = (symbol_t *)malloc(sizeof(symbol_t));
+	if (new_symbol == NULL) {
+		perror("Unable to create new symbol");
+		exit(EX_SOFTWARE);
+	}
+	memset(new_symbol, 0, sizeof(*new_symbol));
+	new_symbol->name = strdup(name);
+	new_symbol->type = UNINITIALIZED;
+	return (new_symbol);
+}
+
+void
+symbol_delete(symbol_t *symbol)
+{
+	if (symtable != NULL) {
+		DBT	 key;
+
+		key.data = symbol->name;
+		key.size = strlen(symbol->name);
+		symtable->del(symtable, &key, /*flags*/0);
+	}
+	switch(symbol->type) {
+	case SCBLOC:
+	case SRAMLOC:
+	case REGISTER:
+		if (symbol->info.rinfo != NULL)
+			free(symbol->info.rinfo);
+		break;
+	case ALIAS:
+		if (symbol->info.ainfo != NULL)
+			free(symbol->info.ainfo);
+		break;
+	case MASK:
+	case BIT:
+		if (symbol->info.minfo != NULL) {
+			symlist_free(&symbol->info.minfo->symrefs);
+			free(symbol->info.minfo);
+		}
+		break;
+	case DOWNLOAD_CONST:
+	case CONST:
+		if (symbol->info.cinfo != NULL)
+			free(symbol->info.cinfo);
+		break;
+	case LABEL:
+		if (symbol->info.linfo != NULL)
+			free(symbol->info.linfo);
+		break;
+	case UNINITIALIZED:
+	default:
+		break;
+	}
+	free(symbol->name);
+	free(symbol);
+}
+
+void
+symtable_open()
+{
+	symtable = dbopen(/*filename*/NULL,
+			  O_CREAT | O_NONBLOCK | O_RDWR, /*mode*/0, DB_HASH,
+			  /*openinfo*/NULL);
+
+	if (symtable == NULL) {
+		perror("Symbol table creation failed");
+		exit(EX_SOFTWARE);
+		/* NOTREACHED */
+	}
+}
+
+void
+symtable_close()
+{
+	if (symtable != NULL) {
+		DBT	 key;
+		DBT	 data;
+
+		while (symtable->seq(symtable, &key, &data, R_FIRST) == 0) {
+			symbol_t *stored_ptr;
+
+			memcpy(&stored_ptr, data.data, sizeof(stored_ptr));
+			symbol_delete(stored_ptr);
+		}
+		symtable->close(symtable);
+	}
+}
+
+/*
+ * The semantics of get is to return an uninitialized symbol entry
+ * if a lookup fails.
+ */
+symbol_t *
+symtable_get(char *name)
+{
+	symbol_t *stored_ptr;
+	DBT	  key;
+	DBT	  data;
+	int	  retval;
+
+	key.data = (void *)name;
+	key.size = strlen(name);
+
+	if ((retval = symtable->get(symtable, &key, &data, /*flags*/0)) != 0) {
+		if (retval == -1) {
+			perror("Symbol table get operation failed");
+			exit(EX_SOFTWARE);
+			/* NOTREACHED */
+		} else if (retval == 1) {
+			/* Symbol wasn't found, so create a new one */
+			symbol_t *new_symbol;
+
+			new_symbol = symbol_create(name);
+			data.data = &new_symbol;
+			data.size = sizeof(new_symbol);
+			if (symtable->put(symtable, &key, &data,
+					  /*flags*/0) !=0) {
+				perror("Symtable put failed");
+				exit(EX_SOFTWARE);
+			}
+			return (new_symbol);
+		} else {
+			perror("Unexpected return value from db get routine");
+			exit(EX_SOFTWARE);
+			/* NOTREACHED */
+		}
+	}
+	memcpy(&stored_ptr, data.data, sizeof(stored_ptr));
+	return (stored_ptr);
+}
+
+symbol_node_t *
+symlist_search(symlist_t *symlist, char *symname)
+{
+	symbol_node_t *curnode;
+
+	curnode = symlist->slh_first;
+	while(curnode != NULL) {
+		if (strcmp(symname, curnode->symbol->name) == 0)
+			break;
+		curnode = curnode->links.sle_next;
+	}
+	return (curnode);
+}
+
+void
+symlist_add(symlist_t *symlist, symbol_t *symbol, int how)
+{
+	symbol_node_t *newnode;
+
+	newnode = (symbol_node_t *)malloc(sizeof(symbol_node_t));
+	if (newnode == NULL) {
+		stop("symlist_add: Unable to malloc symbol_node", EX_SOFTWARE);
+		/* NOTREACHED */
+	}
+	newnode->symbol = symbol;
+	if (how == SYMLIST_SORT) {
+		symbol_node_t *curnode;
+		int mask;
+
+		mask = FALSE;
+		switch(symbol->type) {
+		case REGISTER:
+		case SCBLOC:
+		case SRAMLOC:
+			break;
+		case BIT:
+		case MASK:
+			mask = TRUE;
+			break;
+		default:
+			stop("symlist_add: Invalid symbol type for sorting",
+			     EX_SOFTWARE);
+			/* NOTREACHED */
+		}
+
+		curnode = symlist->slh_first;
+		if (curnode == NULL
+		 || (mask && (curnode->symbol->info.minfo->mask >
+		              newnode->symbol->info.minfo->mask))
+		 || (!mask && (curnode->symbol->info.rinfo->address >
+		               newnode->symbol->info.rinfo->address))) {
+			SLIST_INSERT_HEAD(symlist, newnode, links);
+			return;
+		}
+
+		while (1) {
+			if (curnode->links.sle_next == NULL) {
+				SLIST_INSERT_AFTER(curnode, newnode,
+						   links);
+				break;
+			} else {
+				symbol_t *cursymbol;
+
+				cursymbol = curnode->links.sle_next->symbol;
+				if ((mask && (cursymbol->info.minfo->mask >
+				              symbol->info.minfo->mask))
+				 || (!mask &&(cursymbol->info.rinfo->address >
+				              symbol->info.rinfo->address))){
+					SLIST_INSERT_AFTER(curnode, newnode,
+							   links);
+					break;
+				}
+			}
+			curnode = curnode->links.sle_next;
+		}
+	} else {
+		SLIST_INSERT_HEAD(symlist, newnode, links);
+	}
+}
+
+void
+symlist_free(symlist_t *symlist)
+{
+	symbol_node_t *node1, *node2;
+
+	node1 = symlist->slh_first;
+	while (node1 != NULL) {
+		node2 = node1->links.sle_next;
+		free(node1);
+		node1 = node2;
+	}
+	SLIST_INIT(symlist);
+}
+
+void
+symlist_merge(symlist_t *symlist_dest, symlist_t *symlist_src1,
+	      symlist_t *symlist_src2)
+{
+	symbol_node_t *node;
+
+	*symlist_dest = *symlist_src1;
+	while((node = symlist_src2->slh_first) != NULL) {
+		SLIST_REMOVE_HEAD(symlist_src2, links);
+		SLIST_INSERT_HEAD(symlist_dest, node, links);
+	}
+
+	/* These are now empty */
+	SLIST_INIT(symlist_src1);
+	SLIST_INIT(symlist_src2);
+}
+
+void
+symtable_dump(FILE *ofile)
+{
+	/*
+	 * Sort the registers by address with a simple insertion sort.
+	 * Put bitmasks next to the first register that defines them.
+	 * Put constants at the end.
+	 */
+	symlist_t registers;
+	symlist_t masks;
+	symlist_t constants;
+	symlist_t download_constants;
+	symlist_t aliases;
+
+	SLIST_INIT(&registers);
+	SLIST_INIT(&masks);
+	SLIST_INIT(&constants);
+	SLIST_INIT(&download_constants);
+	SLIST_INIT(&aliases);
+
+	if (symtable != NULL) {
+		DBT	 key;
+		DBT	 data;
+		int	 flag = R_FIRST;
+
+		while (symtable->seq(symtable, &key, &data, flag) == 0) {
+			symbol_t *cursym;
+
+			memcpy(&cursym, data.data, sizeof(cursym));
+			switch(cursym->type) {
+			case REGISTER:
+			case SCBLOC:
+			case SRAMLOC:
+				symlist_add(&registers, cursym, SYMLIST_SORT);
+				break;
+			case MASK:
+			case BIT:
+				symlist_add(&masks, cursym, SYMLIST_SORT);
+				break;
+			case CONST:
+				if (cursym->info.cinfo->define == FALSE) {
+					symlist_add(&constants, cursym,
+						    SYMLIST_INSERT_HEAD);
+				}
+				break;
+			case DOWNLOAD_CONST:
+				symlist_add(&download_constants, cursym,
+					    SYMLIST_INSERT_HEAD);
+				break;
+			case ALIAS:
+				symlist_add(&aliases, cursym,
+					    SYMLIST_INSERT_HEAD);
+				break;
+			default:
+				break;
+			}
+			flag = R_NEXT;
+		}
+
+		/* Put in the masks and bits */
+		while (masks.slh_first != NULL) {
+			symbol_node_t *curnode;
+			symbol_node_t *regnode;
+			char *regname;
+
+			curnode = masks.slh_first;
+			SLIST_REMOVE_HEAD(&masks, links);
+
+			regnode =
+			    curnode->symbol->info.minfo->symrefs.slh_first;
+			regname = regnode->symbol->name;
+			regnode = symlist_search(&registers, regname);
+			SLIST_INSERT_AFTER(regnode, curnode, links);
+		}
+
+		/* Add the aliases */
+		while (aliases.slh_first != NULL) {
+			symbol_node_t *curnode;
+			symbol_node_t *regnode;
+			char *regname;
+
+			curnode = aliases.slh_first;
+			SLIST_REMOVE_HEAD(&aliases, links);
+
+			regname = curnode->symbol->info.ainfo->parent->name;
+			regnode = symlist_search(&registers, regname);
+			SLIST_INSERT_AFTER(regnode, curnode, links);
+		}
+
+		/* Output what we have */
+		fprintf(ofile,
+"/*
+  * DO NOT EDIT - This file is automatically generated.
+  */\n");
+		while (registers.slh_first != NULL) {
+			symbol_node_t *curnode;
+			u_int8_t value;
+			char *tab_str;
+			char *tab_str2;
+
+			curnode = registers.slh_first;
+			SLIST_REMOVE_HEAD(&registers, links);
+			switch(curnode->symbol->type) {
+			case REGISTER:
+			case SCBLOC:
+			case SRAMLOC:
+				fprintf(ofile, "\n");
+				value = curnode->symbol->info.rinfo->address;
+				tab_str = "\t";
+				tab_str2 = "\t\t";
+				break;
+			case ALIAS:
+			{
+				symbol_t *parent;
+
+				parent = curnode->symbol->info.ainfo->parent;
+				value = parent->info.rinfo->address;
+				tab_str = "\t";
+				tab_str2 = "\t\t";
+				break;
+			}
+			case MASK:
+			case BIT:
+				value = curnode->symbol->info.minfo->mask;
+				tab_str = "\t\t";
+				tab_str2 = "\t";
+				break;
+			default:
+				value = 0; /* Quiet compiler */
+				tab_str = NULL;
+				tab_str2 = NULL;
+				stop("symtable_dump: Invalid symbol type "
+				     "encountered", EX_SOFTWARE);
+				break;
+			}
+			fprintf(ofile, "#define%s%-16s%s0x%02x\n",
+				tab_str, curnode->symbol->name, tab_str2,
+				value);
+			free(curnode);
+		}
+		fprintf(ofile, "\n\n");
+
+		while (constants.slh_first != NULL) {
+			symbol_node_t *curnode;
+
+			curnode = constants.slh_first;
+			SLIST_REMOVE_HEAD(&constants, links);
+			fprintf(ofile, "#define\t%-8s\t0x%02x\n",
+				curnode->symbol->name,
+				curnode->symbol->info.cinfo->value);
+			free(curnode);
+		}
+
+		
+		fprintf(ofile, "\n\n/* Downloaded Constant Definitions */\n");
+
+		while (download_constants.slh_first != NULL) {
+			symbol_node_t *curnode;
+
+			curnode = download_constants.slh_first;
+			SLIST_REMOVE_HEAD(&download_constants, links);
+			fprintf(ofile, "#define\t%-8s\t0x%02x\n",
+				curnode->symbol->name,
+				curnode->symbol->info.cinfo->value);
+			free(curnode);
+		}
+	}
+}
+
diff -urN linux/drivers/scsi/aic7xxx/aicasm/aicasm_symbol.h /tmp/linux/drivers/scsi/aic7xxx/aicasm/aicasm_symbol.h
--- linux/drivers/scsi/aic7xxx/aicasm/aicasm_symbol.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aicasm/aicasm_symbol.h	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,177 @@
+/*
+ * Aic7xxx SCSI host adapter firmware asssembler symbol table definitions
+ *
+ * Copyright (c) 1997 Justin T. Gibbs.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $Id: //depot/src/aic7xxx/aicasm/aicasm_symbol.h#4 $
+ *
+ * $FreeBSD: src/sys/dev/aic7xxx/aicasm/aicasm_symbol.h,v 1.11 2000/09/22 22:19:55 gibbs Exp $
+ */
+
+#ifdef __linux__
+#include "../queue.h"
+#else
+#include <sys/queue.h>
+#endif
+
+typedef enum {
+	UNINITIALIZED,
+	REGISTER,
+	ALIAS,
+	SCBLOC,
+	SRAMLOC,
+	MASK,
+	BIT,
+	CONST,
+	DOWNLOAD_CONST,
+	LABEL,
+	CONDITIONAL
+}symtype;
+
+typedef enum {
+	RO = 0x01,
+	WO = 0x02,
+	RW = 0x03
+}amode_t;
+
+struct reg_info {
+	u_int8_t address;
+	int	 size;
+	amode_t	 mode;
+	u_int8_t valid_bitmask;
+	int	 typecheck_masks;
+};
+
+typedef SLIST_HEAD(symlist, symbol_node) symlist_t;
+
+struct mask_info {
+	symlist_t symrefs;
+	u_int8_t mask;
+};
+
+struct const_info {
+	u_int8_t value;
+	int	 define;
+};
+
+struct alias_info {
+	struct symbol *parent;
+};
+
+struct label_info {
+	int	address;
+};
+
+struct cond_info {
+	int	func_num;
+};
+
+typedef struct expression_info {
+        symlist_t       referenced_syms;
+        int             value;
+} expression_t;
+
+typedef struct symbol {
+	char	*name;
+	symtype	type;
+	union	{
+		struct reg_info *rinfo;
+		struct mask_info *minfo;
+		struct const_info *cinfo;
+		struct alias_info *ainfo;
+		struct label_info *linfo;
+		struct cond_info *condinfo;
+	}info;
+} symbol_t;
+
+typedef struct symbol_ref {
+	symbol_t *symbol;
+	int	 offset;
+} symbol_ref_t;
+
+typedef struct symbol_node {
+	SLIST_ENTRY(symbol_node) links;
+	symbol_t *symbol;
+} symbol_node_t;
+
+typedef struct critical_section {
+	TAILQ_ENTRY(critical_section) links;
+	int begin_addr;
+	int end_addr;
+} critical_section_t;
+
+typedef enum {
+	SCOPE_ROOT,
+	SCOPE_IF,
+	SCOPE_ELSE_IF,
+	SCOPE_ELSE
+} scope_type;
+
+typedef struct patch_info {
+	int skip_patch;
+	int skip_instr;
+} patch_info_t;
+
+typedef struct scope {
+	SLIST_ENTRY(scope) scope_stack_links;
+	TAILQ_ENTRY(scope) scope_links;
+	TAILQ_HEAD(, scope) inner_scope;
+	scope_type type;
+	int inner_scope_patches;
+	int begin_addr;
+        int end_addr;
+	patch_info_t patches[2];
+	int func_num;
+} scope_t;
+
+TAILQ_HEAD(cs_tailq, critical_section);
+SLIST_HEAD(scope_list, scope);
+TAILQ_HEAD(scope_tailq, scope);
+
+void	symbol_delete __P((symbol_t *symbol));
+
+void	symtable_open __P((void));
+
+void	symtable_close __P((void));
+
+symbol_t *
+	symtable_get __P((char *name));
+
+symbol_node_t *
+	symlist_search __P((symlist_t *symlist, char *symname));
+
+void
+	symlist_add __P((symlist_t *symlist, symbol_t *symbol, int how));
+#define SYMLIST_INSERT_HEAD	0x00
+#define SYMLIST_SORT		0x01
+
+void	symlist_free __P((symlist_t *symlist));
+
+void	symlist_merge __P((symlist_t *symlist_dest, symlist_t *symlist_src1,
+			   symlist_t *symlist_src2));
+void	symtable_dump __P((FILE *ofile));
diff -urN linux/drivers/scsi/aic7xxx/aicasm/y.tab.h /tmp/linux/drivers/scsi/aic7xxx/aicasm/y.tab.h
--- linux/drivers/scsi/aic7xxx/aicasm/y.tab.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/aicasm/y.tab.h	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,70 @@
+#define T_REGISTER 257
+#define T_CONST 258
+#define T_DOWNLOAD 259
+#define T_SCB 260
+#define T_SRAM 261
+#define T_ALIAS 262
+#define T_SIZE 263
+#define T_ADDRESS 264
+#define T_ACCESS_MODE 265
+#define T_MODE 266
+#define T_BEGIN_CS 267
+#define T_END_CS 268
+#define T_BIT 269
+#define T_MASK 270
+#define T_NUMBER 271
+#define T_PATH 272
+#define T_CEXPR 273
+#define T_EOF 274
+#define T_INCLUDE 275
+#define T_SHR 276
+#define T_SHL 277
+#define T_ROR 278
+#define T_ROL 279
+#define T_MVI 280
+#define T_MOV 281
+#define T_CLR 282
+#define T_BMOV 283
+#define T_JMP 284
+#define T_JC 285
+#define T_JNC 286
+#define T_JE 287
+#define T_JNE 288
+#define T_JNZ 289
+#define T_JZ 290
+#define T_CALL 291
+#define T_ADD 292
+#define T_ADC 293
+#define T_INC 294
+#define T_DEC 295
+#define T_STC 296
+#define T_CLC 297
+#define T_CMP 298
+#define T_NOT 299
+#define T_XOR 300
+#define T_TEST 301
+#define T_AND 302
+#define T_OR 303
+#define T_RET 304
+#define T_NOP 305
+#define T_ACCUM 306
+#define T_ALLONES 307
+#define T_ALLZEROS 308
+#define T_NONE 309
+#define T_SINDEX 310
+#define T_A 311
+#define T_SYMBOL 312
+#define T_NL 313
+#define T_IF 314
+#define T_ELSE 315
+#define T_ELSE_IF 316
+#define T_ENDIF 317
+#define UMINUS 318
+typedef union {
+	int		value;
+	char		*str;
+	symbol_t	*sym;
+	symbol_ref_t	sym_ref;
+	expression_t	expression;
+} YYSTYPE;
+extern YYSTYPE yylval;
diff -urN linux/drivers/scsi/aic7xxx/cam.h /tmp/linux/drivers/scsi/aic7xxx/cam.h
--- linux/drivers/scsi/aic7xxx/cam.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/cam.h	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,142 @@
+/*
+ * Data structures and definitions for the CAM system.
+ *
+ * Copyright (c) 1997 Justin T. Gibbs.
+ * Copyright (c) 2000 Adaptec Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU Public License ("GPL").
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $Id: //depot/src/linux/drivers/scsi/aic7xxx/cam.h#9 $
+ */
+
+#ifndef _AIC7XXX_CAM_H
+#define _AIC7XXX_CAM_H 1
+
+/* Provide a mapping from CAM constructs to Linux SCSI constructs */
+
+#define	CAM_BUS_WILDCARD ((u_int)~0)
+#define	CAM_TARGET_WILDCARD ((u_int)~0)
+#define	CAM_LUN_WILDCARD ((u_int)~0)
+
+/* CAM Status field values */
+typedef enum {
+	/* CCB request is in progress */
+	CAM_REQ_INPROG		= 0x3F, /* Some value unused by Linux */
+	/* CCB request completed without error */
+	CAM_REQ_CMP		= DID_OK,
+	/* CCB request aborted by the host */
+	CAM_REQ_ABORTED		= DID_ABORT,
+	/* Unable to abort CCB request */
+	CAM_UA_ABORT		= DID_ERROR,
+	/* CCB request completed with an error */
+	CAM_REQ_CMP_ERR		= DID_ERROR,
+	/* CAM subsytem is busy */
+	CAM_BUSY		= DID_BUS_BUSY,
+	/* CCB request was invalid */
+	CAM_REQ_INVALID		= DID_BAD_TARGET,
+	/* Supplied Path ID is invalid */
+	CAM_PATH_INVALID	= DID_BAD_TARGET,
+	/* Target Selection Timeout */
+	CAM_SEL_TIMEOUT		= DID_NO_CONNECT,
+	/* Command timeout */
+	CAM_CMD_TIMEOUT		= DID_ERROR, /*
+					      * Should never occur in Linux
+					      * as the upper level code
+					      * handles all timeout processing.
+					      */
+	/* SCSI error, look at error code in CCB */
+	CAM_SCSI_STATUS_ERROR	= DID_OK, /* Linux looks at status byte */
+	/* SCSI Bus Reset Sent/Received */
+	CAM_SCSI_BUS_RESET	= DID_RESET,
+	/* Uncorrectable parity error occurred */
+	CAM_UNCOR_PARITY	= DID_PARITY,
+	/* Autosense: request sense cmd fail */
+	CAM_AUTOSENSE_FAIL	= DID_ERROR,
+	/* No HBA Detected Error */
+	CAM_NO_HBA		= DID_ERROR,
+	/* Data Overrun error */
+	CAM_DATA_RUN_ERR	= DID_ERROR,
+	/* Unexpected Bus Free */
+	CAM_UNEXP_BUSFREE	= DID_ERROR,
+	/* CCB length supplied is inadequate */
+	CAM_CCB_LEN_ERR		= DID_ERROR,
+	/* Unable to provide requested capability */
+	CAM_PROVIDE_FAIL	= DID_ERROR,
+	/* A SCSI BDR msg was sent to target */
+	CAM_BDR_SENT		= DID_RESET,
+	/* CCB request terminated by the host */
+	CAM_REQ_TERMIO		= DID_ERROR,
+	/* Unrecoverable Host Bus Adapter Error */
+	CAM_UNREC_HBA_ERROR	= DID_ERROR,
+	/* The request was too large for this host */
+	CAM_REQ_TOO_BIG		= DID_ERROR,
+	/*
+	 * This request should be requeued to preserve
+	 * transaction ordering.  This typically occurs
+	 * when the SIM recognizes an error that should
+	 * freeze the queue and must place additional
+	 * requests for the target at the sim level
+	 * back into the XPT queue.
+	 */
+	CAM_REQUEUE_REQ		= DID_BUS_BUSY,
+	/*
+	 * The transaction has made it through our queue routine.
+	 */
+	CAM_SIM_QUEUED		= 0x200,
+
+	CAM_STATUS_MASK		= 0x3F
+} cam_status;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,3,0)
+#define	SCSI_DATA_READ	1
+#define	SCSI_DATA_WRITE 2
+#define SCSI_DATA_NONE  3
+#endif
+
+/*
+ * Definitions for the asynchronous callback CCB fields.
+ */
+typedef enum {
+	AC_GETDEV_CHANGED	= 0x800,/* Getdev info might have changed */
+	AC_INQ_CHANGED		= 0x400,/* Inquiry info might have changed */
+	AC_TRANSFER_NEG		= 0x200,/* New transfer settings in effect */
+	AC_LOST_DEVICE		= 0x100,/* A device went away */
+	AC_FOUND_DEVICE		= 0x080,/* A new device was found */
+	AC_PATH_DEREGISTERED	= 0x040,/* A path has de-registered */
+	AC_PATH_REGISTERED	= 0x020,/* A new path has been registered */
+	AC_SENT_BDR		= 0x010,/* A BDR message was sent to target */
+	AC_SCSI_AEN		= 0x008,/* A SCSI AEN has been received */
+	AC_UNSOL_RESEL		= 0x002,/* Unsolicited reselection occurred */
+	AC_BUS_RESET		= 0x001 /* A SCSI bus reset occurred */
+} ac_code;
+
+typedef enum {
+	CAM_DIR_IN		= SCSI_DATA_READ,
+	CAM_DIR_OUT		= SCSI_DATA_WRITE,
+	CAM_DIR_NONE		= SCSI_DATA_NONE
+} ccb_flags;
+
+#endif /* _AIC7XXX_CAM_H */
diff -urN linux/drivers/scsi/aic7xxx/queue.h /tmp/linux/drivers/scsi/aic7xxx/queue.h
--- linux/drivers/scsi/aic7xxx/queue.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/drivers/scsi/aic7xxx/queue.h	Fri Feb  2 19:06:14 2001
@@ -0,0 +1,501 @@
+/*
+ * Copyright (c) 1991, 1993
+ *	The Regents of the University of California.  All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ *	@(#)queue.h	8.5 (Berkeley) 8/20/94
+ * $FreeBSD: src/sys/sys/queue.h,v 1.38 2000/05/26 02:06:56 jake Exp $
+ */
+
+#ifndef _SYS_QUEUE_H_
+#define	_SYS_QUEUE_H_
+
+/*
+ * This file defines five types of data structures: singly-linked lists,
+ * singly-linked tail queues, lists, tail queues, and circular queues.
+ *
+ * A singly-linked list is headed by a single forward pointer. The elements
+ * are singly linked for minimum space and pointer manipulation overhead at
+ * the expense of O(n) removal for arbitrary elements. New elements can be
+ * added to the list after an existing element or at the head of the list.
+ * Elements being removed from the head of the list should use the explicit
+ * macro for this purpose for optimum efficiency. A singly-linked list may
+ * only be traversed in the forward direction.  Singly-linked lists are ideal
+ * for applications with large datasets and few or no removals or for
+ * implementing a LIFO queue.
+ *
+ * A singly-linked tail queue is headed by a pair of pointers, one to the
+ * head of the list and the other to the tail of the list. The elements are
+ * singly linked for minimum space and pointer manipulation overhead at the
+ * expense of O(n) removal for arbitrary elements. New elements can be added
+ * to the list after an existing element, at the head of the list, or at the
+ * end of the list. Elements being removed from the head of the tail queue
+ * should use the explicit macro for this purpose for optimum efficiency.
+ * A singly-linked tail queue may only be traversed in the forward direction.
+ * Singly-linked tail queues are ideal for applications with large datasets
+ * and few or no removals or for implementing a FIFO queue.
+ *
+ * A list is headed by a single forward pointer (or an array of forward
+ * pointers for a hash table header). The elements are doubly linked
+ * so that an arbitrary element can be removed without a need to
+ * traverse the list. New elements can be added to the list before
+ * or after an existing element or at the head of the list. A list
+ * may only be traversed in the forward direction.
+ *
+ * A tail queue is headed by a pair of pointers, one to the head of the
+ * list and the other to the tail of the list. The elements are doubly
+ * linked so that an arbitrary element can be removed without a need to
+ * traverse the list. New elements can be added to the list before or
+ * after an existing element, at the head of the list, or at the end of
+ * the list. A tail queue may be traversed in either direction.
+ *
+ * A circle queue is headed by a pair of pointers, one to the head of the
+ * list and the other to the tail of the list. The elements are doubly
+ * linked so that an arbitrary element can be removed without a need to
+ * traverse the list. New elements can be added to the list before or after
+ * an existing element, at the head of the list, or at the end of the list.
+ * A circle queue may be traversed in either direction, but has a more
+ * complex end of list detection.
+ *
+ * For details on the use of these macros, see the queue(3) manual page.
+ *
+ *
+ *			SLIST	LIST	STAILQ	TAILQ	CIRCLEQ
+ * _HEAD		+	+	+	+	+
+ * _HEAD_INITIALIZER	+	+	+	+	+
+ * _ENTRY		+	+	+	+	+
+ * _INIT		+	+	+	+	+
+ * _EMPTY		+	+	+	+	+
+ * _FIRST		+	+	+	+	+
+ * _NEXT		+	+	+	+	+
+ * _PREV		-	-	-	+	+
+ * _LAST		-	-	+	+	+
+ * _FOREACH		+	+	+	+	+
+ * _FOREACH_REVERSE	-	-	-	+	+
+ * _INSERT_HEAD		+	+	+	+	+
+ * _INSERT_BEFORE	-	+	-	+	+
+ * _INSERT_AFTER	+	+	+	+	+
+ * _INSERT_TAIL		-	-	+	+	+
+ * _REMOVE_HEAD		+	-	+	-	-
+ * _REMOVE		+	+	+	+	+
+ *
+ */
+
+/*
+ * Singly-linked List declarations.
+ */
+#define	SLIST_HEAD(name, type)						\
+struct name {								\
+	struct type *slh_first;	/* first element */			\
+}
+
+#define	SLIST_HEAD_INITIALIZER(head)					\
+	{ NULL }
+ 
+#define	SLIST_ENTRY(type)						\
+struct {								\
+	struct type *sle_next;	/* next element */			\
+}
+ 
+/*
+ * Singly-linked List functions.
+ */
+#define	SLIST_EMPTY(head)	((head)->slh_first == NULL)
+
+#define	SLIST_FIRST(head)	((head)->slh_first)
+
+#define	SLIST_FOREACH(var, head, field)					\
+	for ((var) = SLIST_FIRST((head));				\
+	    (var);							\
+	    (var) = SLIST_NEXT((var), field))
+
+#define	SLIST_INIT(head) do {						\
+	SLIST_FIRST((head)) = NULL;					\
+} while (0)
+
+#define	SLIST_INSERT_AFTER(slistelm, elm, field) do {			\
+	SLIST_NEXT((elm), field) = SLIST_NEXT((slistelm), field);	\
+	SLIST_NEXT((slistelm), field) = (elm);				\
+} while (0)
+
+#define	SLIST_INSERT_HEAD(head, elm, field) do {			\
+	SLIST_NEXT((elm), field) = SLIST_FIRST((head));			\
+	SLIST_FIRST((head)) = (elm);					\
+} while (0)
+
+#define	SLIST_NEXT(elm, field)	((elm)->field.sle_next)
+
+#define	SLIST_REMOVE(head, elm, type, field) do {			\
+	if (SLIST_FIRST((head)) == (elm)) {				\
+		SLIST_REMOVE_HEAD((head), field);			\
+	}								\
+	else {								\
+		struct type *curelm = SLIST_FIRST((head));		\
+		while (SLIST_NEXT(curelm, field) != (elm))		\
+			curelm = SLIST_NEXT(curelm, field);		\
+		SLIST_NEXT(curelm, field) =				\
+		    SLIST_NEXT(SLIST_NEXT(curelm, field), field);	\
+	}								\
+} while (0)
+
+#define	SLIST_REMOVE_HEAD(head, field) do {				\
+	SLIST_FIRST((head)) = SLIST_NEXT(SLIST_FIRST((head)), field);	\
+} while (0)
+
+/*
+ * Singly-linked Tail queue declarations.
+ */
+#define	STAILQ_HEAD(name, type)						\
+struct name {								\
+	struct type *stqh_first;/* first element */			\
+	struct type **stqh_last;/* addr of last next element */		\
+}
+
+#define	STAILQ_HEAD_INITIALIZER(head)					\
+	{ NULL, &(head).stqh_first }
+
+#define	STAILQ_ENTRY(type)						\
+struct {								\
+	struct type *stqe_next;	/* next element */			\
+}
+
+/*
+ * Singly-linked Tail queue functions.
+ */
+#define	STAILQ_EMPTY(head)	((head)->stqh_first == NULL)
+
+#define	STAILQ_FIRST(head)	((head)->stqh_first)
+
+#define	STAILQ_FOREACH(var, head, field)				\
+	for((var) = STAILQ_FIRST((head));				\
+	   (var);							\
+	   (var) = STAILQ_NEXT((var), field))
+
+#define	STAILQ_INIT(head) do {						\
+	STAILQ_FIRST((head)) = NULL;					\
+	(head)->stqh_last = &STAILQ_FIRST((head));			\
+} while (0)
+
+#define	STAILQ_INSERT_AFTER(head, tqelm, elm, field) do {		\
+	if ((STAILQ_NEXT((elm), field) = STAILQ_NEXT((tqelm), field)) == NULL)\
+		(head)->stqh_last = &STAILQ_NEXT((elm), field);		\
+	STAILQ_NEXT((tqelm), field) = (elm);				\
+} while (0)
+
+#define	STAILQ_INSERT_HEAD(head, elm, field) do {			\
+	if ((STAILQ_NEXT((elm), field) = STAILQ_FIRST((head))) == NULL)	\
+		(head)->stqh_last = &STAILQ_NEXT((elm), field);		\
+	STAILQ_FIRST((head)) = (elm);					\
+} while (0)
+
+#define	STAILQ_INSERT_TAIL(head, elm, field) do {			\
+	STAILQ_NEXT((elm), field) = NULL;				\
+	STAILQ_LAST((head)) = (elm);					\
+	(head)->stqh_last = &STAILQ_NEXT((elm), field);			\
+} while (0)
+
+#define	STAILQ_LAST(head)	(*(head)->stqh_last)
+
+#define	STAILQ_NEXT(elm, field)	((elm)->field.stqe_next)
+
+#define	STAILQ_REMOVE(head, elm, type, field) do {			\
+	if (STAILQ_FIRST((head)) == (elm)) {				\
+		STAILQ_REMOVE_HEAD(head, field);			\
+	}								\
+	else {								\
+		struct type *curelm = STAILQ_FIRST((head));		\
+		while (STAILQ_NEXT(curelm, field) != (elm))		\
+			curelm = STAILQ_NEXT(curelm, field);		\
+		if ((STAILQ_NEXT(curelm, field) =			\
+		     STAILQ_NEXT(STAILQ_NEXT(curelm, field), field)) == NULL)\
+			(head)->stqh_last = &STAILQ_NEXT((curelm), field);\
+	}								\
+} while (0)
+
+#define	STAILQ_REMOVE_HEAD(head, field) do {				\
+	if ((STAILQ_FIRST((head)) =					\
+	     STAILQ_NEXT(STAILQ_FIRST((head)), field)) == NULL)		\
+		(head)->stqh_last = &STAILQ_FIRST((head));		\
+} while (0)
+
+#define	STAILQ_REMOVE_HEAD_UNTIL(head, elm, field) do {			\
+	if ((STAILQ_FIRST((head)) = STAILQ_NEXT((elm), field)) == NULL)	\
+		(head)->stqh_last = &STAILQ_FIRST((head));		\
+} while (0)
+
+/*
+ * List declarations.
+ */
+#define	LIST_HEAD(name, type)						\
+struct name {								\
+	struct type *lh_first;	/* first element */			\
+}
+
+#define	LIST_HEAD_INITIALIZER(head)					\
+	{ NULL }
+
+#define	LIST_ENTRY(type)						\
+struct {								\
+	struct type *le_next;	/* next element */			\
+	struct type **le_prev;	/* address of previous next element */	\
+}
+
+/*
+ * List functions.
+ */
+
+#define	LIST_EMPTY(head)	((head)->lh_first == NULL)
+
+#define	LIST_FIRST(head)	((head)->lh_first)
+
+#define	LIST_FOREACH(var, head, field)					\
+	for ((var) = LIST_FIRST((head));				\
+	    (var);							\
+	    (var) = LIST_NEXT((var), field))
+
+#define	LIST_INIT(head) do {						\
+	LIST_FIRST((head)) = NULL;					\
+} while (0)
+
+#define	LIST_INSERT_AFTER(listelm, elm, field) do {			\
+	if ((LIST_NEXT((elm), field) = LIST_NEXT((listelm), field)) != NULL)\
+		LIST_NEXT((listelm), field)->field.le_prev =		\
+		    &LIST_NEXT((elm), field);				\
+	LIST_NEXT((listelm), field) = (elm);				\
+	(elm)->field.le_prev = &LIST_NEXT((listelm), field);		\
+} while (0)
+
+#define	LIST_INSERT_BEFORE(listelm, elm, field) do {			\
+	(elm)->field.le_prev = (listelm)->field.le_prev;		\
+	LIST_NEXT((elm), field) = (listelm);				\
+	*(listelm)->field.le_prev = (elm);				\
+	(listelm)->field.le_prev = &LIST_NEXT((elm), field);		\
+} while (0)
+
+#define	LIST_INSERT_HEAD(head, elm, field) do {				\
+	if ((LIST_NEXT((elm), field) = LIST_FIRST((head))) != NULL)	\
+		LIST_FIRST((head))->field.le_prev = &LIST_NEXT((elm), field);\
+	LIST_FIRST((head)) = (elm);					\
+	(elm)->field.le_prev = &LIST_FIRST((head));			\
+} while (0)
+
+#define	LIST_NEXT(elm, field)	((elm)->field.le_next)
+
+#define	LIST_REMOVE(elm, field) do {					\
+	if (LIST_NEXT((elm), field) != NULL)				\
+		LIST_NEXT((elm), field)->field.le_prev = 		\
+		    (elm)->field.le_prev;				\
+	*(elm)->field.le_prev = LIST_NEXT((elm), field);		\
+} while (0)
+
+/*
+ * Tail queue declarations.
+ */
+#define	TAILQ_HEAD(name, type)						\
+struct name {								\
+	struct type *tqh_first;	/* first element */			\
+	struct type **tqh_last;	/* addr of last next element */		\
+}
+
+#define	TAILQ_HEAD_INITIALIZER(head)					\
+	{ NULL, &(head).tqh_first }
+
+#define	TAILQ_ENTRY(type)						\
+struct {								\
+	struct type *tqe_next;	/* next element */			\
+	struct type **tqe_prev;	/* address of previous next element */	\
+}
+
+/*
+ * Tail queue functions.
+ */
+#define	TAILQ_EMPTY(head)	((head)->tqh_first == NULL)
+
+#define	TAILQ_FIRST(head)	((head)->tqh_first)
+
+#define	TAILQ_FOREACH(var, head, field)					\
+	for ((var) = TAILQ_FIRST((head));				\
+	    (var);							\
+	    (var) = TAILQ_NEXT((var), field))
+
+#define	TAILQ_FOREACH_REVERSE(var, head, headname, field)		\
+	for ((var) = TAILQ_LAST((head), headname);			\
+	    (var);							\
+	    (var) = TAILQ_PREV((var), headname, field))
+
+#define	TAILQ_INIT(head) do {						\
+	TAILQ_FIRST((head)) = NULL;					\
+	(head)->tqh_last = &TAILQ_FIRST((head));			\
+} while (0)
+
+#define	TAILQ_INSERT_AFTER(head, listelm, elm, field) do {		\
+	if ((TAILQ_NEXT((elm), field) = TAILQ_NEXT((listelm), field)) != NULL)\
+		TAILQ_NEXT((elm), field)->field.tqe_prev = 		\
+		    &TAILQ_NEXT((elm), field);				\
+	else								\
+		(head)->tqh_last = &TAILQ_NEXT((elm), field);		\
+	TAILQ_NEXT((listelm), field) = (elm);				\
+	(elm)->field.tqe_prev = &TAILQ_NEXT((listelm), field);		\
+} while (0)
+
+#define	TAILQ_INSERT_BEFORE(listelm, elm, field) do {			\
+	(elm)->field.tqe_prev = (listelm)->field.tqe_prev;		\
+	TAILQ_NEXT((elm), field) = (listelm);				\
+	*(listelm)->field.tqe_prev = (elm);				\
+	(listelm)->field.tqe_prev = &TAILQ_NEXT((elm), field);		\
+} while (0)
+
+#define	TAILQ_INSERT_HEAD(head, elm, field) do {			\
+	if ((TAILQ_NEXT((elm), field) = TAILQ_FIRST((head))) != NULL)	\
+		TAILQ_FIRST((head))->field.tqe_prev =			\
+		    &TAILQ_NEXT((elm), field);				\
+	else								\
+		(head)->tqh_last = &TAILQ_NEXT((elm), field);		\
+	TAILQ_FIRST((head)) = (elm);					\
+	(elm)->field.tqe_prev = &TAILQ_FIRST((head));			\
+} while (0)
+
+#define	TAILQ_INSERT_TAIL(head, elm, field) do {			\
+	TAILQ_NEXT((elm), field) = NULL;				\
+	(elm)->field.tqe_prev = (head)->tqh_last;			\
+	*(head)->tqh_last = (elm);					\
+	(head)->tqh_last = &TAILQ_NEXT((elm), field);			\
+} while (0)
+
+#define	TAILQ_LAST(head, headname)					\
+	(*(((struct headname *)((head)->tqh_last))->tqh_last))
+
+#define	TAILQ_NEXT(elm, field) ((elm)->field.tqe_next)
+
+#define	TAILQ_PREV(elm, headname, field)				\
+	(*(((struct headname *)((elm)->field.tqe_prev))->tqh_last))
+
+#define	TAILQ_REMOVE(head, elm, field) do {				\
+	if ((TAILQ_NEXT((elm), field)) != NULL)				\
+		TAILQ_NEXT((elm), field)->field.tqe_prev = 		\
+		    (elm)->field.tqe_prev;				\
+	else								\
+		(head)->tqh_last = (elm)->field.tqe_prev;		\
+	*(elm)->field.tqe_prev = TAILQ_NEXT((elm), field);		\
+} while (0)
+
+/*
+ * Circular queue declarations.
+ */
+#define	CIRCLEQ_HEAD(name, type)					\
+struct name {								\
+	struct type *cqh_first;		/* first element */		\
+	struct type *cqh_last;		/* last element */		\
+}
+
+#define	CIRCLEQ_HEAD_INITIALIZER(head)					\
+	{ (void *)&(head), (void *)&(head) }
+
+#define	CIRCLEQ_ENTRY(type)						\
+struct {								\
+	struct type *cqe_next;		/* next element */		\
+	struct type *cqe_prev;		/* previous element */		\
+}
+
+/*
+ * Circular queue functions.
+ */
+#define	CIRCLEQ_EMPTY(head)	((head)->cqh_first == (void *)(head))
+
+#define	CIRCLEQ_FIRST(head)	((head)->cqh_first)
+
+#define	CIRCLEQ_FOREACH(var, head, field)				\
+	for ((var) = CIRCLEQ_FIRST((head));				\
+	    (var) != (void *)(head);					\
+	    (var) = CIRCLEQ_NEXT((var), field))
+
+#define	CIRCLEQ_FOREACH_REVERSE(var, head, field)			\
+	for ((var) = CIRCLEQ_LAST((head));				\
+	    (var) != (void *)(head);					\
+	    (var) = CIRCLEQ_PREV((var), field))
+
+#define	CIRCLEQ_INIT(head) do {						\
+	CIRCLEQ_FIRST((head)) = (void *)(head);				\
+	CIRCLEQ_LAST((head)) = (void *)(head);				\
+} while (0)
+
+#define	CIRCLEQ_INSERT_AFTER(head, listelm, elm, field) do {		\
+	CIRCLEQ_NEXT((elm), field) = CIRCLEQ_NEXT((listelm), field);	\
+	CIRCLEQ_PREV((elm), field) = (listelm);				\
+	if (CIRCLEQ_NEXT((listelm), field) == (void *)(head))		\
+		CIRCLEQ_LAST((head)) = (elm);				\
+	else								\
+		CIRCLEQ_PREV(CIRCLEQ_NEXT((listelm), field), field) = (elm);\
+	CIRCLEQ_NEXT((listelm), field) = (elm);				\
+} while (0)
+
+#define	CIRCLEQ_INSERT_BEFORE(head, listelm, elm, field) do {		\
+	CIRCLEQ_NEXT((elm), field) = (listelm);				\
+	CIRCLEQ_PREV((elm), field) = CIRCLEQ_PREV((listelm), field);	\
+	if (CIRCLEQ_PREV((listelm), field) == (void *)(head))		\
+		CIRCLEQ_FIRST((head)) = (elm);				\
+	else								\
+		CIRCLEQ_NEXT(CIRCLEQ_PREV((listelm), field), field) = (elm);\
+	CIRCLEQ_PREV((listelm), field) = (elm);				\
+} while (0)
+
+#define	CIRCLEQ_INSERT_HEAD(head, elm, field) do {			\
+	CIRCLEQ_NEXT((elm), field) = CIRCLEQ_FIRST((head));		\
+	CIRCLEQ_PREV((elm), field) = (void *)(head);			\
+	if (CIRCLEQ_LAST((head)) == (void *)(head))			\
+		CIRCLEQ_LAST((head)) = (elm);				\
+	else								\
+		CIRCLEQ_PREV(CIRCLEQ_FIRST((head)), field) = (elm);	\
+	CIRCLEQ_FIRST((head)) = (elm);					\
+} while (0)
+
+#define	CIRCLEQ_INSERT_TAIL(head, elm, field) do {			\
+	CIRCLEQ_NEXT((elm), field) = (void *)(head);			\
+	CIRCLEQ_PREV((elm), field) = CIRCLEQ_LAST((head));		\
+	if (CIRCLEQ_FIRST((head)) == (void *)(head))			\
+		CIRCLEQ_FIRST((head)) = (elm);				\
+	else								\
+		CIRCLEQ_NEXT(CIRCLEQ_LAST((head)), field) = (elm);	\
+	CIRCLEQ_LAST((head)) = (elm);					\
+} while (0)
+
+#define	CIRCLEQ_LAST(head)	((head)->cqh_last)
+
+#define	CIRCLEQ_NEXT(elm,field)	((elm)->field.cqe_next)
+
+#define	CIRCLEQ_PREV(elm,field)	((elm)->field.cqe_prev)
+
+#define	CIRCLEQ_REMOVE(head, elm, field) do {				\
+	if (CIRCLEQ_NEXT((elm), field) == (void *)(head))		\
+		CIRCLEQ_LAST((head)) = CIRCLEQ_PREV((elm), field);	\
+	else								\
+		CIRCLEQ_PREV(CIRCLEQ_NEXT((elm), field), field) =	\
+		    CIRCLEQ_PREV((elm), field);				\
+	if (CIRCLEQ_PREV((elm), field) == (void *)(head))		\
+		CIRCLEQ_FIRST((head)) = CIRCLEQ_NEXT((elm), field);	\
+	else								\
+		CIRCLEQ_NEXT(CIRCLEQ_PREV((elm), field), field) =	\
+		    CIRCLEQ_NEXT((elm), field);				\
+} while (0)
+
+#endif /* !_SYS_QUEUE_H_ */
diff -urN linux/drivers/scsi/aic7xxx/scsi_message.h /tmp/linux/drivers/scsi/aic7xxx/scsi_message.h
--- linux/drivers/scsi/aic7xxx/scsi_message.h	Wed Jun  9 17:59:34 1999
+++ /tmp/linux/drivers/scsi/aic7xxx/scsi_message.h	Fri Feb  2 19:06:14 2001
@@ -1,34 +1,52 @@
+/*
+ * This file is in the public domain.
+ * $FreeBSD: src/sys/cam/scsi/scsi_message.h,v 1.2 2000/05/01 20:21:29 peter Exp $
+ */
+
 /* Messages (1 byte) */		     /* I/T (M)andatory or (O)ptional */
 #define MSG_CMDCOMPLETE		0x00 /* M/M */
+#define MSG_TASK_COMPLETE	0x00 /* M/M */ /* SPI3 Terminology */
 #define MSG_EXTENDED		0x01 /* O/O */
 #define MSG_SAVEDATAPOINTER	0x02 /* O/O */
 #define MSG_RESTOREPOINTERS	0x03 /* O/O */
 #define MSG_DISCONNECT		0x04 /* O/O */
 #define MSG_INITIATOR_DET_ERR	0x05 /* M/M */
 #define MSG_ABORT		0x06 /* O/M */
+#define MSG_ABORT_TASK_SET	0x06 /* O/M */ /* SPI3 Terminology */
 #define MSG_MESSAGE_REJECT	0x07 /* M/M */
 #define MSG_NOOP		0x08 /* M/M */
 #define MSG_PARITY_ERROR	0x09 /* M/M */
 #define MSG_LINK_CMD_COMPLETE	0x0a /* O/O */
 #define MSG_LINK_CMD_COMPLETEF	0x0b /* O/O */
 #define MSG_BUS_DEV_RESET	0x0c /* O/M */
+#define MSG_TARGET_RESET	0x0c /* O/M */ /* SPI3 Terminology */
 #define MSG_ABORT_TAG		0x0d /* O/O */
+#define MSG_ABORT_TASK		0x0d /* O/O */ /* SPI3 Terminology */
 #define MSG_CLEAR_QUEUE		0x0e /* O/O */
-#define MSG_INIT_RECOVERY	0x0f /* O/O */
-#define MSG_REL_RECOVERY	0x10 /* O/O */
-#define MSG_TERM_IO_PROC	0x11 /* O/O */
+#define MSG_CLEAR_TASK_SET	0x0e /* O/O */ /* SPI3 Terminology */
+#define MSG_INIT_RECOVERY	0x0f /* O/O */ /* Deprecated in SPI3 */
+#define MSG_REL_RECOVERY	0x10 /* O/O */ /* Deprecated in SPI3 */
+#define MSG_TERM_IO_PROC	0x11 /* O/O */ /* Deprecated in SPI3 */
+#define MSG_CLEAR_ACA		0x16 /* O/O */ /* SPI3 */
+#define MSG_LOGICAL_UNIT_RESET	0x17 /* O/O */ /* SPI3 */
+#define MSG_QAS_REQUEST		0x55 /* O/O */ /* SPI3 */
 
 /* Messages (2 byte) */
 #define MSG_SIMPLE_Q_TAG	0x20 /* O/O */
+#define MSG_SIMPLE_TASK		0x20 /* O/O */ /* SPI3 Terminology */
 #define MSG_HEAD_OF_Q_TAG	0x21 /* O/O */
+#define MSG_HEAD_OF_QUEUE_TASK	0x21 /* O/O */ /* SPI3 Terminology */
 #define MSG_ORDERED_Q_TAG	0x22 /* O/O */
+#define MSG_ORDERED_TASK	0x22 /* O/O */ /* SPI3 Terminology */
 #define MSG_IGN_WIDE_RESIDUE	0x23 /* O/O */
+#define MSG_ACA_TASK		0x24 /* 0/0 */ /* SPI3 */
 
 /* Identify message */		     /* M/M */	
 #define MSG_IDENTIFYFLAG	0x80 
 #define MSG_IDENTIFY_DISCFLAG	0x40 
 #define MSG_IDENTIFY(lun, disc)	(((disc) ? 0xc0 : MSG_IDENTIFYFLAG) | (lun))
 #define MSG_ISIDENTIFY(m)	((m) & MSG_IDENTIFYFLAG)
+#define MSG_IDENTIFY_LUNMASK	0x03F 
 
 /* Extended messages (opcode and length) */
 #define MSG_EXT_SDTR		0x01
@@ -38,12 +56,10 @@
 #define MSG_EXT_WDTR_LEN	0x02
 #define MSG_EXT_WDTR_BUS_8_BIT	0x00
 #define MSG_EXT_WDTR_BUS_16_BIT	0x01
-#define MSG_EXT_WDTR_BUS_32_BIT	0x02
+#define MSG_EXT_WDTR_BUS_32_BIT	0x02 /* Deprecated in SPI3 */
 
-#define MSG_EXT_PPR     0x04
-#define MSG_EXT_PPR_LEN	0x06
-#define MSG_EXT_PPR_OPTION_ST 0x00
-#define MSG_EXT_PPR_OPTION_DT_CRC 0x02
-#define MSG_EXT_PPR_OPTION_DT_UNITS 0x03
-#define MSG_EXT_PPR_OPTION_DT_CRC_QUICK 0x04
-#define MSG_EXT_PPR_OPTION_DT_UNITS_QUICK 0x05
+#define MSG_EXT_PPR		0x04 /* SPI3 */
+#define MSG_EXT_PPR_LEN		0x06
+#define MSG_EXT_PPR_QAS_REQ	0x04
+#define MSG_EXT_PPR_DT_REQ	0x02
+#define MSG_EXT_PPR_IU_REQ	0x01
diff -urN linux/drivers/scsi/aic7xxx/sequencer.h /tmp/linux/drivers/scsi/aic7xxx/sequencer.h
--- linux/drivers/scsi/aic7xxx/sequencer.h	Sun Nov  8 14:20:14 1998
+++ /tmp/linux/drivers/scsi/aic7xxx/sequencer.h	Wed Dec 31 17:00:00 1969
@@ -1,135 +0,0 @@
-/*
- * Instruction formats for the sequencer program downloaded to
- * Aic7xxx SCSI host adapters
- *
- * Copyright (c) 1997, 1998 Justin T. Gibbs.
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions, and the following disclaimer,
- *    without modification, immediately at the beginning of the file.
- * 2. The name of the author may not be used to endorse or promote products
- *    derived from this software without specific prior written permission.
- *
- * Where this Software is combined with software released under the terms of 
- * the GNU Public License ("GPL") and the terms of the GPL would require the 
- * combined work to also be released under the terms of the GPL, the terms
- * and conditions of this License will apply in addition to those of the
- * GPL with the exception of any terms or conditions of this License that
- * conflict with, or are expressly prohibited by, the GPL.
- *
- * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
- * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
- * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
- * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
- * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
- * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
- * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
- * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
- * SUCH DAMAGE.
- *
- *      $Id: sequencer.h,v 1.3 1997/09/27 19:37:31 gibbs Exp $
- */
-
-#ifdef __LITTLE_ENDIAN_BITFIELD
-struct ins_format1 {
-	unsigned int
-			immediate	: 8,
-			source		: 9,
-			destination	: 9,
-			ret		: 1,
-			opcode		: 4,
-			parity		: 1;
-};
-
-struct ins_format2 {
-	unsigned int
-			shift_control	: 8,
-			source		: 9,
-			destination	: 9,
-			ret		: 1,
-			opcode		: 4,
-			parity		: 1;
-};
-
-struct ins_format3 {
-	unsigned int
-			immediate	: 8,
-			source		: 9,
-			address		: 10,
-			opcode		: 4,
-			parity		: 1;
-};
-#elif defined(__BIG_ENDIAN_BITFIELD)
-struct ins_format1 {
-	unsigned int
-			parity		: 1,
-			opcode		: 4,
-			ret		: 1,
-			destination	: 9,
-			source		: 9,
-			immediate	: 8;
-};
-
-struct ins_format2 {
-	unsigned int
-			parity		: 1,
-			opcode		: 4,
-			ret		: 1,
-			destination	: 9,
-			source		: 9,
-			shift_control	: 8;
-};
-
-struct ins_format3 {
-	unsigned int
-			parity		: 1,
-			opcode		: 4,
-			address		: 10,
-			source		: 9,
-			immediate	: 8;
-};
-#endif
-
-union ins_formats {
-		struct ins_format1 format1;
-		struct ins_format2 format2;
-		struct ins_format3 format3;
-		unsigned char	   bytes[4];
-		unsigned int	   integer;
-};
-struct instruction {
-	union	ins_formats format;
-	unsigned int	srcline;
-	struct symbol *patch_label;
-  struct {
-    struct instruction *stqe_next;
-  } links;
-};
-
-#define	AIC_OP_OR	0x0
-#define	AIC_OP_AND	0x1
-#define AIC_OP_XOR	0x2
-#define	AIC_OP_ADD	0x3
-#define	AIC_OP_ADC	0x4
-#define	AIC_OP_ROL	0x5
-#define	AIC_OP_BMOV	0x6
-
-#define	AIC_OP_JMP	0x8
-#define AIC_OP_JC	0x9
-#define AIC_OP_JNC	0xa
-#define AIC_OP_CALL	0xb
-#define	AIC_OP_JNE	0xc
-#define	AIC_OP_JNZ	0xd
-#define	AIC_OP_JE	0xe
-#define	AIC_OP_JZ	0xf
-
-/* Pseudo Ops */
-#define	AIC_OP_SHL	0x10
-#define	AIC_OP_SHR	0x20
-#define	AIC_OP_ROR	0x30
diff -urN linux/drivers/scsi/aic7xxx.c /tmp/linux/drivers/scsi/aic7xxx.c
--- linux/drivers/scsi/aic7xxx.c	Mon Sep  4 11:39:21 2000
+++ /tmp/linux/drivers/scsi/aic7xxx.c	Wed Dec 31 17:00:00 1969
@@ -1,12652 +0,0 @@
-/*+M*************************************************************************
- * Adaptec AIC7xxx device driver for Linux.
- *
- * Copyright (c) 1994 John Aycock
- *   The University of Calgary Department of Computer Science.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2, or (at your option)
- * any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; see the file COPYING.  If not, write to
- * the Free Software Foundation, 675 Mass Ave, Cambridge, MA 02139, USA.
- *
- * Sources include the Adaptec 1740 driver (aha1740.c), the Ultrastor 24F
- * driver (ultrastor.c), various Linux kernel source, the Adaptec EISA
- * config file (!adp7771.cfg), the Adaptec AHA-2740A Series User's Guide,
- * the Linux Kernel Hacker's Guide, Writing a SCSI Device Driver for Linux,
- * the Adaptec 1542 driver (aha1542.c), the Adaptec EISA overlay file
- * (adp7770.ovl), the Adaptec AHA-2740 Series Technical Reference Manual,
- * the Adaptec AIC-7770 Data Book, the ANSI SCSI specification, the
- * ANSI SCSI-2 specification (draft 10c), ...
- *
- * --------------------------------------------------------------------------
- *
- *  Modifications by Daniel M. Eischen (deischen@iworks.InterWorks.org):
- *
- *  Substantially modified to include support for wide and twin bus
- *  adapters, DMAing of SCBs, tagged queueing, IRQ sharing, bug fixes,
- *  SCB paging, and other rework of the code.
- *
- *  Parts of this driver were also based on the FreeBSD driver by
- *  Justin T. Gibbs.  His copyright follows:
- *
- * --------------------------------------------------------------------------  
- * Copyright (c) 1994-1997 Justin Gibbs.
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions, and the following disclaimer,
- *    without modification, immediately at the beginning of the file.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. The name of the author may not be used to endorse or promote products
- *    derived from this software without specific prior written permission.
- *
- * Where this Software is combined with software released under the terms of 
- * the GNU Public License ("GPL") and the terms of the GPL would require the 
- * combined work to also be released under the terms of the GPL, the terms
- * and conditions of this License will apply in addition to those of the
- * GPL with the exception of any terms or conditions of this License that
- * conflict with, or are expressly prohibited by, the GPL.
- *
- * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
- * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
- * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
- * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
- * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
- * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
- * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
- * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
- * SUCH DAMAGE.
- *
- *      $Id: aic7xxx.c,v 1.119 1997/06/27 19:39:18 gibbs Exp $
- *---------------------------------------------------------------------------
- *
- *  Thanks also go to (in alphabetical order) the following:
- *
- *    Rory Bolt     - Sequencer bug fixes
- *    Jay Estabrook - Initial DEC Alpha support
- *    Doug Ledford  - Much needed abort/reset bug fixes
- *    Kai Makisara  - DMAing of SCBs
- *
- *  A Boot time option was also added for not resetting the scsi bus.
- *
- *    Form:  aic7xxx=extended
- *           aic7xxx=no_reset
- *           aic7xxx=ultra
- *           aic7xxx=irq_trigger:[0,1]  # 0 edge, 1 level
- *           aic7xxx=verbose
- *
- *  Daniel M. Eischen, deischen@iworks.InterWorks.org, 1/23/97
- *
- *  $Id: aic7xxx.c,v 4.1 1997/06/12 08:23:42 deang Exp $
- *-M*************************************************************************/
-
-/*+M**************************************************************************
- *
- * Further driver modifications made by Doug Ledford <dledford@redhat.com>
- *
- * Copyright (c) 1997-1999 Doug Ledford
- *
- * These changes are released under the same licensing terms as the FreeBSD
- * driver written by Justin Gibbs.  Please see his Copyright notice above
- * for the exact terms and conditions covering my changes as well as the
- * warranty statement.
- *
- * Modifications made to the aic7xxx.c,v 4.1 driver from Dan Eischen include
- * but are not limited to:
- *
- *  1: Import of the latest FreeBSD sequencer code for this driver
- *  2: Modification of kernel code to accomodate different sequencer semantics
- *  3: Extensive changes throughout kernel portion of driver to improve
- *     abort/reset processing and error hanndling
- *  4: Other work contributed by various people on the Internet
- *  5: Changes to printk information and verbosity selection code
- *  6: General reliability related changes, especially in IRQ management
- *  7: Modifications to the default probe/attach order for supported cards
- *  8: SMP friendliness has been improved
- *
- * Overall, this driver represents a significant departure from the official
- * aic7xxx driver released by Dan Eischen in two ways.  First, in the code
- * itself.  A diff between the two version of the driver is now a several
- * thousand line diff.  Second, in approach to solving the same problem.  The
- * problem is importing the FreeBSD aic7xxx driver code to linux can be a
- * difficult and time consuming process, that also can be error prone.  Dan
- * Eischen's official driver uses the approach that the linux and FreeBSD
- * drivers should be as identical as possible.  To that end, his next version
- * of this driver will be using a mid-layer code library that he is developing
- * to moderate communications between the linux mid-level SCSI code and the
- * low level FreeBSD driver.  He intends to be able to essentially drop the
- * FreeBSD driver into the linux kernel with only a few minor tweaks to some
- * include files and the like and get things working, making for fast easy
- * imports of the FreeBSD code into linux.
- *
- * I disagree with Dan's approach.  Not that I don't think his way of doing
- * things would be nice, easy to maintain, and create a more uniform driver
- * between FreeBSD and Linux.  I have no objection to those issues.  My
- * disagreement is on the needed functionality.  There simply are certain
- * things that are done differently in FreeBSD than linux that will cause
- * problems for this driver regardless of any middle ware Dan implements.
- * The biggest example of this at the moment is interrupt semantics.  Linux
- * doesn't provide the same protection techniques as FreeBSD does, nor can
- * they be easily implemented in any middle ware code since they would truly
- * belong in the kernel proper and would effect all drivers.  For the time
- * being, I see issues such as these as major stumbling blocks to the 
- * reliability of code based upon such middle ware.  Therefore, I choose to
- * use a different approach to importing the FreeBSD code that doesn't
- * involve any middle ware type code.  My approach is to import the sequencer
- * code from FreeBSD wholesale.  Then, to only make changes in the kernel
- * portion of the driver as they are needed for the new sequencer semantics.
- * In this way, the portion of the driver that speaks to the rest of the
- * linux kernel is fairly static and can be changed/modified to solve
- * any problems one might encounter without concern for the FreeBSD driver.
- *
- * Note: If time and experience should prove me wrong that the middle ware
- * code Dan writes is reliable in its operation, then I'll retract my above
- * statements.  But, for those that don't know, I'm from Missouri (in the US)
- * and our state motto is "The Show-Me State".  Well, before I will put
- * faith into it, you'll have to show me that it works :)
- *
- *_M*************************************************************************/
-
-/*
- * The next three defines are user configurable.  These should be the only
- * defines a user might need to get in here and change.  There are other
- * defines buried deeper in the code, but those really shouldn't need touched
- * under normal conditions.
- */
-
-/*
- * AIC7XXX_STRICT_PCI_SETUP
- *   Should we assume the PCI config options on our controllers are set with
- *   sane and proper values, or should we be anal about our PCI config
- *   registers and force them to what we want?  The main advantage to
- *   defining this option is on non-Intel hardware where the BIOS may not
- *   have been run to set things up, or if you have one of the BIOSless
- *   Adaptec controllers, such as a 2910, that don't get set up by the
- *   BIOS.  However, keep in mind that we really do set the most important
- *   items in the driver regardless of this setting, this only controls some
- *   of the more esoteric PCI options on these cards.  In that sense, I
- *   would default to leaving this off.  However, if people wish to try
- *   things both ways, that would also help me to know if there are some
- *   machines where it works one way but not another.
- *
- *   -- July 7, 17:09
- *     OK...I need this on my machine for testing, so the default is to
- *     leave it defined.
- *
- *   -- July 7, 18:49
- *     I needed it for testing, but it didn't make any difference, so back
- *     off she goes.
- *
- *   -- July 16, 23:04
- *     I turned it back on to try and compensate for the 2.1.x PCI code
- *     which no longer relies solely on the BIOS and now tries to set
- *     things itself.
- */
-
-#define AIC7XXX_STRICT_PCI_SETUP
-
-/*
- * AIC7XXX_VERBOSE_DEBUGGING
- *   This option enables a lot of extra printk();s in the code, surrounded
- *   by if (aic7xxx_verbose ...) statements.  Executing all of those if
- *   statements and the extra checks can get to where it actually does have
- *   an impact on CPU usage and such, as well as code size.  Disabling this
- *   define will keep some of those from becoming part of the code.
- *
- *   NOTE:  Currently, this option has no real effect, I will be adding the
- *   various #ifdef's in the code later when I've decided a section is
- *   complete and no longer needs debugging.  OK...a lot of things are now
- *   surrounded by this define, so turning this off does have an impact.
- */
- 
-/*
- * #define AIC7XXX_VERBOSE_DEBUGGING
- */
- 
-#if defined(MODULE) || defined(PCMCIA)
-#include <linux/module.h>
-#endif
-
-#if defined(PCMCIA)
-#  undef MODULE
-#endif
-
-#include <stdarg.h>
-#include <asm/io.h>
-#include <asm/irq.h>
-#include <asm/byteorder.h>
-#include <linux/version.h>
-#include <linux/string.h>
-#include <linux/errno.h>
-#include <linux/kernel.h>
-#include <linux/ioport.h>
-#include <linux/delay.h>
-#include <linux/sched.h>
-#include <linux/pci.h>
-#include <linux/proc_fs.h>
-#include <linux/blk.h>
-#include <linux/tqueue.h>
-#include <linux/tasks.h>
-#include "sd.h"
-#include "scsi.h"
-#include "hosts.h"
-#include "aic7xxx.h"
-
-#include "aic7xxx/sequencer.h"
-#include "aic7xxx/scsi_message.h"
-#include "aic7xxx_reg.h"
-#include <scsi/scsicam.h>
-
-#include <linux/stat.h>
-#include <linux/malloc.h>        /* for kmalloc() */
-
-#include <linux/config.h>        /* for CONFIG_PCI */
-
-/*
- * To generate the correct addresses for the controller to issue
- * on the bus.  Originally added for DEC Alpha support.
- */
-#define VIRT_TO_BUS(a) (unsigned int)virt_to_bus((void *)(a))
-
-struct proc_dir_entry proc_scsi_aic7xxx = {
-    PROC_SCSI_AIC7XXX, 7, "aic7xxx",
-    S_IFDIR | S_IRUGO | S_IXUGO, 2,
-    0, 0, 0, NULL, NULL, NULL, NULL, NULL, NULL, NULL
-};
-
-#define AIC7XXX_C_VERSION  "5.1.31"
-
-#define NUMBER(arr)     (sizeof(arr) / sizeof(arr[0]))
-#define MIN(a,b)        (((a) < (b)) ? (a) : (b))
-#define MAX(a,b)        (((a) > (b)) ? (a) : (b))
-#define ALL_TARGETS -1
-#define ALL_CHANNELS -1
-#define ALL_LUNS -1
-#define MAX_TARGETS  16
-#define MAX_LUNS     8
-#ifndef TRUE
-#  define TRUE 1
-#endif
-#ifndef FALSE
-#  define FALSE 0
-#endif
-
-#ifndef KERNEL_VERSION
-#  define KERNEL_VERSION(x,y,z) (((x)<<16)+((y)<<8)+(z))
-#endif
-
-/*
- * We need the bios32.h file if we are kernel version 2.1.92 or less.  The
- * full set of pci_* changes wasn't in place until 2.1.93
- */
-
-#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,1,92)
-#  if defined(__sparc_v9__) || defined(__powerpc__)
-#    error "PPC and Sparc platforms are only support under 2.1.92 and above"
-#  endif
-#  include <linux/bios32.h>
-#endif
-
-#if defined(__powerpc__)
-#  define MMAPIO
-#  ifdef mb
-#    undef mb
-#  endif
-#  define mb() \
-     __asm__ __volatile__("eieio" ::: "memory")
-#elif defined(__i386__)
-#  define MMAPIO
-#  ifdef mb
-#    undef mb
-#  endif
-#  define mb() \
-     __asm__ __volatile__("lock ; addl $0,0(%%esp)": : :"memory")
-#elif defined(__alpha__)
-#  ifdef mb
-#    undef mb
-#  endif
-#  define mb() \
-     __asm__ __volatile__("mb": : :"memory")
-#endif
-
-#if LINUX_VERSION_CODE > KERNEL_VERSION(2,1,0)
-#  include <asm/spinlock.h>
-#  include <linux/smp.h>
-#  define cpuid smp_processor_id()
-#  if LINUX_VERSION_CODE < KERNEL_VERSION(2,1,95)
-#    define DRIVER_LOCK_INIT \
-       spin_lock_init(&p->spin_lock);
-#    define DRIVER_LOCK \
-       if(!p->cpu_lock_count[cpuid]) { \
-         spin_lock_irqsave(&p->spin_lock, cpu_flags); \
-         p->cpu_lock_count[cpuid]++; \
-       } else { \
-         p->cpu_lock_count[cpuid]++; \
-       }
-#    define DRIVER_UNLOCK \
-       if(--p->cpu_lock_count[cpuid] == 0) \
-         spin_unlock_irqrestore(&p->spin_lock, cpu_flags);
-#  else
-#    define DRIVER_LOCK_INIT
-#    define DRIVER_LOCK
-#    define DRIVER_UNLOCK
-#  endif
-#else
-#  define cpuid 0
-#  define DRIVER_LOCK_INIT
-#  define DRIVER_LOCK \
-       save_flags(cpu_flags); \
-       cli();
-#  define DRIVER_UNLOCK \
-       restore_flags(cpu_flags);
-#  define le32_to_cpu(x) (x)
-#  define cpu_to_le32(x) (x)
-#endif
-
-/*
- * You can try raising me if tagged queueing is enabled, or lowering
- * me if you only have 4 SCBs.
- */
-#ifdef CONFIG_AIC7XXX_CMDS_PER_DEVICE
-#define AIC7XXX_CMDS_PER_DEVICE CONFIG_AIC7XXX_CMDS_PER_DEVICE
-#else
-#define AIC7XXX_CMDS_PER_DEVICE 8
-#endif
-
-/* Set this to the delay in seconds after SCSI bus reset. */
-#ifdef CONFIG_AIC7XXX_RESET_DELAY
-#define AIC7XXX_RESET_DELAY CONFIG_AIC7XXX_RESET_DELAY
-#else
-#define AIC7XXX_RESET_DELAY 5
-#endif
-
-/*
- * Control collection of SCSI transfer statistics for the /proc filesystem.
- *
- * NOTE: Do NOT enable this when running on kernels version 1.2.x and below.
- * NOTE: This does affect performance since it has to maintain statistics.
- */
-#ifdef CONFIG_AIC7XXX_PROC_STATS
-#define AIC7XXX_PROC_STATS
-#endif
-
-/*
- * NOTE: Uncommenting the define below no longer has any effect, the
- *       tagged queue value array is always active now.  I've added
- *       a setup option to set this particular array and I'm hoping
- *       insmod will be smart enough to set it properly as well.  It's
- *       by use of this array that a person can enable tagged queueing.
- *       The DEFAULT_TAG_COMMANDS define has been changed to disable
- *       tagged queueing by default, so if your devices can handle tagged
- *       queueing you will need to add a line to their lilo.conf file like:
- *       append="aic7xxx=verbose,tag_info:{{32,32,32,32},{32,32,32,32}}"
- *       which will result in the first four devices on the first two
- *       controllers being set to a tagged queue depth of 32.
- *
- * Set this for defining the number of tagged commands on a device
- * by device, and controller by controller basis.  The first set
- * of tagged commands will be used for the first detected aic7xxx
- * controller, the second set will be used for the second detected
- * aic7xxx controller, and so on.  These values will *only* be used
- * for targets that are tagged queueing capable; these values will
- * be ignored in all other cases.  The tag_commands is an array of
- * 16 to allow for wide and twin adapters.  Twin adapters will use
- * indexes 0-7 for channel 0, and indexes 8-15 for channel 1.
- *
- * *** Determining commands per LUN ***
- * 
- * When AIC7XXX_CMDS_PER_DEVICE is not defined, the driver will use its
- * own algorithm to determine the commands/LUN.  If SCB paging is
- * enabled, which is always now, the default is 8 commands per lun
- * that indicates it supports tagged queueing.  All non-tagged devices
- * use an internal queue depth of 3, with no more than one of those
- * three commands active at one time.
- */
-/* #define AIC7XXX_TAGGED_QUEUEING_BY_DEVICE */
-
-typedef struct
-{
-  unsigned char tag_commands[16];   /* Allow for wide/twin adapters. */
-} adapter_tag_info_t;
-
-/*
- * Make a define that will tell the driver not to use tagged queueing
- * by default.
- */
-#ifdef CONFIG_AIC7XXX_TCQ_ON_BY_DEFAULT
-#define DEFAULT_TAG_COMMANDS {0, 0, 0, 0, 0, 0, 0, 0,\
-                              0, 0, 0, 0, 0, 0, 0, 0}
-#else
-#define DEFAULT_TAG_COMMANDS {255, 255, 255, 255, 255, 255, 255, 255,\
-                              255, 255, 255, 255, 255, 255, 255, 255}
-#endif
-
-/*
- * Modify this as you see fit for your system.  By setting tag_commands
- * to 0, the driver will use it's own algorithm for determining the
- * number of commands to use (see above).  When 255, the driver will
- * not enable tagged queueing for that particular device.  When positive
- * (> 0) and (< 255) the values in the array are used for the queue_depth.
- * Note that the maximum value for an entry is 254, but you're insane if
- * you try to use that many commands on one device.
- *
- * In this example, the first line will disable tagged queueing for all
- * the devices on the first probed aic7xxx adapter.
- *
- * The second line enables tagged queueing with 4 commands/LUN for IDs
- * (1, 2-11, 13-15), disables tagged queueing for ID 12, and tells the
- * driver to use its own algorithm for ID 1.
- *
- * The third line is the same as the first line.
- *
- * The fourth line disables tagged queueing for devices 0 and 3.  It
- * enables tagged queueing for the other IDs, with 16 commands/LUN
- * for IDs 1 and 4, 127 commands/LUN for ID 8, and 4 commands/LUN for
- * IDs 2, 5-7, and 9-15.
- */
-
-/*
- * NOTE: The below structure is for reference only, the actual structure
- *       to modify in order to change things is located around line
- *       number 1305
-adapter_tag_info_t aic7xxx_tag_info[] =
-{
-  {DEFAULT_TAG_COMMANDS},
-  {{4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 255, 4, 4, 4}},
-  {DEFAULT_TAG_COMMANDS},
-  {{255, 16, 4, 255, 16, 4, 4, 4, 127, 4, 4, 4, 4, 4, 4, 4}}
-};
-*/
-
-static adapter_tag_info_t aic7xxx_tag_info[] =
-{
-  {DEFAULT_TAG_COMMANDS},
-  {DEFAULT_TAG_COMMANDS},
-  {DEFAULT_TAG_COMMANDS},
-  {DEFAULT_TAG_COMMANDS},
-  {DEFAULT_TAG_COMMANDS},
-  {DEFAULT_TAG_COMMANDS},
-  {DEFAULT_TAG_COMMANDS},
-  {DEFAULT_TAG_COMMANDS},
-  {DEFAULT_TAG_COMMANDS},
-  {DEFAULT_TAG_COMMANDS},
-  {DEFAULT_TAG_COMMANDS},
-  {DEFAULT_TAG_COMMANDS},
-  {DEFAULT_TAG_COMMANDS},
-  {DEFAULT_TAG_COMMANDS},
-  {DEFAULT_TAG_COMMANDS},
-  {DEFAULT_TAG_COMMANDS}
-};
-
-
-/*
- * Define an array of board names that can be indexed by aha_type.
- * Don't forget to change this when changing the types!
- */
-static const char *board_names[] = {
-  "AIC-7xxx Unknown",                                   /* AIC_NONE */
-  "Adaptec AIC-7810 Hardware RAID Controller",          /* AIC_7810 */
-  "Adaptec AIC-7770 SCSI host adapter",                 /* AIC_7770 */
-  "Adaptec AHA-274X SCSI host adapter",                 /* AIC_7771 */
-  "Adaptec AHA-284X SCSI host adapter",                 /* AIC_284x */
-  "Adaptec AIC-7850 SCSI host adapter",                 /* AIC_7850 */
-  "Adaptec AIC-7855 SCSI host adapter",                 /* AIC_7855 */
-  "Adaptec AIC-7860 Ultra SCSI host adapter",           /* AIC_7860 */
-  "Adaptec AHA-2940A Ultra SCSI host adapter",          /* AIC_7861 */
-  "Adaptec AIC-7870 SCSI host adapter",                 /* AIC_7870 */
-  "Adaptec AHA-294X SCSI host adapter",                 /* AIC_7871 */
-  "Adaptec AHA-394X SCSI host adapter",                 /* AIC_7872 */
-  "Adaptec AHA-398X SCSI host adapter",                 /* AIC_7873 */
-  "Adaptec AHA-2944 SCSI host adapter",                 /* AIC_7874 */
-  "Adaptec AIC-7880 Ultra SCSI host adapter",           /* AIC_7880 */
-  "Adaptec AHA-294X Ultra SCSI host adapter",           /* AIC_7881 */
-  "Adaptec AHA-394X Ultra SCSI host adapter",           /* AIC_7882 */
-  "Adaptec AHA-398X Ultra SCSI host adapter",           /* AIC_7883 */
-  "Adaptec AHA-2944 Ultra SCSI host adapter",           /* AIC_7884 */
-  "Adaptec AHA-2940UW Pro Ultra SCSI host adapter",     /* AIC_7887 */
-  "Adaptec AIC-7895 Ultra SCSI host adapter",           /* AIC_7895 */
-  "Adaptec AIC-7890/1 Ultra2 SCSI host adapter",        /* AIC_7890 */
-  "Adaptec AHA-293X Ultra2 SCSI host adapter",          /* AIC_7890 */
-  "Adaptec AHA-294X Ultra2 SCSI host adapter",          /* AIC_7890 */
-  "Adaptec AIC-7896/7 Ultra2 SCSI host adapter",        /* AIC_7896 */
-  "Adaptec AHA-394X Ultra2 SCSI host adapter",          /* AIC_7897 */
-  "Adaptec AHA-395X Ultra2 SCSI host adapter",          /* AIC_7897 */
-  "Adaptec PCMCIA SCSI controller",                     /* card bus stuff */
-  "Adaptec AIC-7892 Ultra 160/m SCSI host adapter",     /* AIC_7892 */
-  "Adaptec AIC-7899 Ultra 160/m SCSI host adapter",     /* AIC_7899 */
-};
-
-/*
- * There should be a specific return value for this in scsi.h, but
- * it seems that most drivers ignore it.
- */
-#define DID_UNDERFLOW   DID_ERROR
-
-/*
- *  What we want to do is have the higher level scsi driver requeue
- *  the command to us. There is no specific driver status for this
- *  condition, but the higher level scsi driver will requeue the
- *  command on a DID_BUS_BUSY error.
- *
- *  Upon further inspection and testing, it seems that DID_BUS_BUSY
- *  will *always* retry the command.  We can get into an infinite loop
- *  if this happens when we really want some sort of counter that
- *  will automatically abort/reset the command after so many retries.
- *  Using DID_ERROR will do just that.  (Made by a suggestion by
- *  Doug Ledford 8/1/96)
- */
-#define DID_RETRY_COMMAND DID_ERROR
-
-#define HSCSIID        0x07
-#define SCSI_RESET     0x040
-
-/*
- * EISA/VL-bus stuff
- */
-#define MINSLOT                1
-#define MAXSLOT                15
-#define SLOTBASE(x)        ((x) << 12)
-#define BASE_TO_SLOT(x) ((x) >> 12)
-
-/*
- * Standard EISA Host ID regs  (Offset from slot base)
- */
-#define AHC_HID0              0x80   /* 0,1: msb of ID2, 2-7: ID1      */
-#define AHC_HID1              0x81   /* 0-4: ID3, 5-7: LSB ID2         */
-#define AHC_HID2              0x82   /* product                        */
-#define AHC_HID3              0x83   /* firmware revision              */
-
-/*
- * AIC-7770 I/O range to reserve for a card
- */
-#define MINREG                0xC00
-#define MAXREG                0xCFF
-
-#define INTDEF                0x5C      /* Interrupt Definition Register */
-
-/*
- * AIC-78X0 PCI registers
- */
-#define        CLASS_PROGIF_REVID        0x08
-#define                DEVREVID        0x000000FFul
-#define                PROGINFC        0x0000FF00ul
-#define                SUBCLASS        0x00FF0000ul
-#define                BASECLASS        0xFF000000ul
-
-#define        CSIZE_LATTIME                0x0C
-#define                CACHESIZE        0x0000003Ful        /* only 5 bits */
-#define                LATTIME                0x0000FF00ul
-
-#define        DEVCONFIG                0x40
-#define                SCBSIZE32        0x00010000ul        /* aic789X only */
-#define                MPORTMODE        0x00000400ul        /* aic7870 only */
-#define                RAMPSM           0x00000200ul        /* aic7870 only */
-#define                RAMPSM_ULTRA2    0x00000004
-#define                VOLSENSE         0x00000100ul
-#define                SCBRAMSEL        0x00000080ul
-#define                SCBRAMSEL_ULTRA2 0x00000008
-#define                MRDCEN           0x00000040ul
-#define                EXTSCBTIME       0x00000020ul        /* aic7870 only */
-#define                EXTSCBPEN        0x00000010ul        /* aic7870 only */
-#define                BERREN           0x00000008ul
-#define                DACEN            0x00000004ul
-#define                STPWLEVEL        0x00000002ul
-#define                DIFACTNEGEN      0x00000001ul        /* aic7870 only */
-
-#define        SCAMCTL                  0x1a                /* Ultra2 only  */
-#define        CCSCBBADDR               0xf0                /* aic7895/6/7  */
-
-/*
- * Define the different types of SEEPROMs on aic7xxx adapters
- * and make it also represent the address size used in accessing
- * its registers.  The 93C46 chips have 1024 bits organized into
- * 64 16-bit words, while the 93C56 chips have 2048 bits organized
- * into 128 16-bit words.  The C46 chips use 6 bits to address
- * each word, while the C56 and C66 (4096 bits) use 8 bits to
- * address each word.
- */
-typedef enum {C46 = 6, C56_66 = 8} seeprom_chip_type;
-
-/*
- *
- * Define the format of the SEEPROM registers (16 bits).
- *
- */
-struct seeprom_config {
-
-/*
- * SCSI ID Configuration Flags
- */
-#define CFXFER                0x0007      /* synchronous transfer rate */
-#define CFSYNCH               0x0008      /* enable synchronous transfer */
-#define CFDISC                0x0010      /* enable disconnection */
-#define CFWIDEB               0x0020      /* wide bus device (wide card) */
-#define CFSYNCHISULTRA        0x0040      /* CFSYNC is an ultra offset */
-#define CFNEWULTRAFORMAT      0x0080      /* Use the Ultra2 SEEPROM format */
-#define CFSTART               0x0100      /* send start unit SCSI command */
-#define CFINCBIOS             0x0200      /* include in BIOS scan */
-#define CFRNFOUND             0x0400      /* report even if not found */
-#define CFMULTILUN            0x0800      /* probe mult luns in BIOS scan */
-#define CFWBCACHEYES          0x4000      /* Enable W-Behind Cache on drive */
-#define CFWBCACHENC           0xc000      /* Don't change W-Behind Cache */
-/* UNUSED                0x3000 */
-  unsigned short device_flags[16];        /* words 0-15 */
-
-/*
- * BIOS Control Bits
- */
-#define CFSUPREM        0x0001  /* support all removable drives */
-#define CFSUPREMB       0x0002  /* support removable drives for boot only */
-#define CFBIOSEN        0x0004  /* BIOS enabled */
-/* UNUSED                0x0008 */
-#define CFSM2DRV        0x0010  /* support more than two drives */
-#define CF284XEXTEND    0x0020  /* extended translation (284x cards) */
-/* UNUSED                0x0040 */
-#define CFEXTEND        0x0080  /* extended translation enabled */
-/* UNUSED                0xFF00 */
-  unsigned short bios_control;  /* word 16 */
-
-/*
- * Host Adapter Control Bits
- */
-#define CFAUTOTERM      0x0001  /* Perform Auto termination */
-#define CFULTRAEN       0x0002  /* Ultra SCSI speed enable (Ultra cards) */
-#define CF284XSELTO     0x0003  /* Selection timeout (284x cards) */
-#define CF284XFIFO      0x000C  /* FIFO Threshold (284x cards) */
-#define CFSTERM         0x0004  /* SCSI low byte termination */
-#define CFWSTERM        0x0008  /* SCSI high byte termination (wide card) */
-#define CFSPARITY       0x0010  /* SCSI parity */
-#define CF284XSTERM     0x0020  /* SCSI low byte termination (284x cards) */
-#define CFRESETB        0x0040  /* reset SCSI bus at boot */
-#define CFBPRIMARY      0x0100  /* Channel B primary on 7895 chipsets */
-#define CFSEAUTOTERM    0x0400  /* aic7890 Perform SE Auto Term */
-#define CFLVDSTERM      0x0800  /* aic7890 LVD Termination */
-/* UNUSED                0xF280 */
-  unsigned short adapter_control;        /* word 17 */
-
-/*
- * Bus Release, Host Adapter ID
- */
-#define CFSCSIID        0x000F                /* host adapter SCSI ID */
-/* UNUSED                0x00F0 */
-#define CFBRTIME        0xFF00                /* bus release time */
-  unsigned short brtime_id;                /* word 18 */
-
-/*
- * Maximum targets
- */
-#define CFMAXTARG        0x00FF        /* maximum targets */
-/* UNUSED                0xFF00 */
-  unsigned short max_targets;                /* word 19 */
-
-  unsigned short res_1[11];                /* words 20-30 */
-  unsigned short checksum;                /* word 31 */
-};
-
-#define SELBUS_MASK                0x0a
-#define         SELNARROW        0x00
-#define         SELBUSB                0x08
-#define SINGLE_BUS                0x00
-
-#define SCB_TARGET(scb)         \
-       (((scb)->hscb->target_channel_lun & TID) >> 4)
-#define SCB_LUN(scb)            \
-       ((scb)->hscb->target_channel_lun & LID)
-#define SCB_IS_SCSIBUS_B(scb)   \
-       (((scb)->hscb->target_channel_lun & SELBUSB) != 0)
-
-/*
- * If an error occurs during a data transfer phase, run the command
- * to completion - it's easier that way - making a note of the error
- * condition in this location. This then will modify a DID_OK status
- * into an appropriate error for the higher-level SCSI code.
- */
-#define aic7xxx_error(cmd)        ((cmd)->SCp.Status)
-
-/*
- * Keep track of the targets returned status.
- */
-#define aic7xxx_status(cmd)        ((cmd)->SCp.sent_command)
-
-/*
- * The position of the SCSI commands scb within the scb array.
- */
-#define aic7xxx_position(cmd)        ((cmd)->SCp.have_data_in)
-
-/*
- * So we can keep track of our host structs
- */
-static struct aic7xxx_host *first_aic7xxx = NULL;
-
-/*
- * As of Linux 2.1, the mid-level SCSI code uses virtual addresses
- * in the scatter-gather lists.  We need to convert the virtual
- * addresses to physical addresses.
- */
-struct hw_scatterlist {
-  unsigned int address;
-  unsigned int length;
-};
-
-/*
- * Maximum number of SG segments these cards can support.
- */
-#define        AIC7XXX_MAX_SG 128
-
-/*
- * The maximum number of SCBs we could have for ANY type
- * of card. DON'T FORGET TO CHANGE THE SCB MASK IN THE
- * SEQUENCER CODE IF THIS IS MODIFIED!
- */
-#define AIC7XXX_MAXSCB        255
-
-
-struct aic7xxx_hwscb {
-/* ------------    Begin hardware supported fields    ---------------- */
-/* 0*/  unsigned char control;
-/* 1*/  unsigned char target_channel_lun;       /* 4/1/3 bits */
-/* 2*/  unsigned char target_status;
-/* 3*/  unsigned char SG_segment_count;
-/* 4*/  unsigned int  SG_list_pointer;
-/* 8*/  unsigned char residual_SG_segment_count;
-/* 9*/  unsigned char residual_data_count[3];
-/*12*/  unsigned int  data_pointer;
-/*16*/  unsigned int  data_count;
-/*20*/  unsigned int  SCSI_cmd_pointer;
-/*24*/  unsigned char SCSI_cmd_length;
-/*25*/  unsigned char tag;          /* Index into our kernel SCB array.
-                                     * Also used as the tag for tagged I/O
-                                     */
-#define SCB_PIO_TRANSFER_SIZE  26   /* amount we need to upload/download
-                                     * via PIO to initialize a transaction.
-                                     */
-/*26*/  unsigned char next;         /* Used to thread SCBs awaiting selection
-                                     * or disconnected down in the sequencer.
-                                     */
-/*27*/  unsigned char prev;
-/*28*/  unsigned int pad;           /*
-                                     * Unused by the kernel, but we require
-                                     * the padding so that the array of
-                                     * hardware SCBs is alligned on 32 byte
-                                     * boundaries so the sequencer can index
-                                     */
-};
-
-typedef enum {
-        SCB_FREE                = 0x0000,
-        SCB_WAITINGQ            = 0x0002,
-        SCB_ACTIVE              = 0x0004,
-        SCB_SENSE               = 0x0008,
-        SCB_ABORT               = 0x0010,
-        SCB_DEVICE_RESET        = 0x0020,
-        SCB_RESET               = 0x0040,
-        SCB_RECOVERY_SCB        = 0x0080,
-        SCB_MSGOUT_PPR          = 0x0100,
-        SCB_MSGOUT_SENT         = 0x0200,
-        SCB_MSGOUT_SDTR         = 0x0400,
-        SCB_MSGOUT_WDTR         = 0x0800,
-        SCB_MSGOUT_BITS         = SCB_MSGOUT_PPR |
-                                  SCB_MSGOUT_SENT | 
-                                  SCB_MSGOUT_SDTR |
-                                  SCB_MSGOUT_WDTR,
-        SCB_QUEUED_ABORT        = 0x1000,
-        SCB_QUEUED_FOR_DONE     = 0x2000,
-        SCB_WAS_BUSY            = 0x4000
-} scb_flag_type;
-
-typedef enum {
-        AHC_FNONE                 = 0x00000000,
-        AHC_PAGESCBS              = 0x00000001,
-        AHC_CHANNEL_B_PRIMARY     = 0x00000002,
-        AHC_USEDEFAULTS           = 0x00000004,
-        AHC_INDIRECT_PAGING       = 0x00000008,
-        AHC_CHNLB                 = 0x00000020,
-        AHC_CHNLC                 = 0x00000040,
-        AHC_EXTEND_TRANS_A        = 0x00000100,
-        AHC_EXTEND_TRANS_B        = 0x00000200,
-        AHC_TERM_ENB_A            = 0x00000400,
-        AHC_TERM_ENB_SE_LOW       = 0x00000400,
-        AHC_TERM_ENB_B            = 0x00000800,
-        AHC_TERM_ENB_SE_HIGH      = 0x00000800,
-        AHC_HANDLING_REQINITS     = 0x00001000,
-        AHC_TARGETMODE            = 0x00002000,
-        AHC_NEWEEPROM_FMT         = 0x00004000,
- /*
-  *  Here ends the FreeBSD defined flags and here begins the linux defined
-  *  flags.  NOTE: I did not preserve the old flag name during this change
-  *  specifically to force me to evaluate what flags were being used properly
-  *  and what flags weren't.  This way, I could clean up the flag usage on
-  *  a use by use basis.  Doug Ledford
-  */
-        AHC_MOTHERBOARD           = 0x00020000,
-        AHC_NO_STPWEN             = 0x00040000,
-        AHC_RESET_DELAY           = 0x00080000,
-        AHC_A_SCANNED             = 0x00100000,
-        AHC_B_SCANNED             = 0x00200000,
-        AHC_MULTI_CHANNEL         = 0x00400000,
-        AHC_BIOS_ENABLED          = 0x00800000,
-        AHC_SEEPROM_FOUND         = 0x01000000,
-        AHC_TERM_ENB_LVD          = 0x02000000,
-        AHC_ABORT_PENDING         = 0x04000000,
-        AHC_RESET_PENDING         = 0x08000000,
-#define AHC_IN_ISR_BIT              28
-        AHC_IN_ISR                = 0x10000000,
-        AHC_IN_ABORT              = 0x20000000,
-        AHC_IN_RESET              = 0x40000000,
-        AHC_EXTERNAL_SRAM         = 0x80000000
-} ahc_flag_type;
-
-typedef enum {
-  AHC_NONE             = 0x0000,
-  AHC_CHIPID_MASK      = 0x00ff,
-  AHC_AIC7770          = 0x0001,
-  AHC_AIC7850          = 0x0002,
-  AHC_AIC7860          = 0x0003,
-  AHC_AIC7870          = 0x0004,
-  AHC_AIC7880          = 0x0005,
-  AHC_AIC7890          = 0x0006,
-  AHC_AIC7895          = 0x0007,
-  AHC_AIC7896          = 0x0008,
-  AHC_AIC7892          = 0x0009,
-  AHC_AIC7899          = 0x000a,
-  AHC_VL               = 0x0100,
-  AHC_EISA             = 0x0200,
-  AHC_PCI              = 0x0400,
-} ahc_chip;
-
-typedef enum {
-  AHC_FENONE           = 0x0000,
-  AHC_ULTRA            = 0x0001,
-  AHC_ULTRA2           = 0x0002,
-  AHC_WIDE             = 0x0004,
-  AHC_TWIN             = 0x0008,
-  AHC_MORE_SRAM        = 0x0010,
-  AHC_CMD_CHAN         = 0x0020,
-  AHC_QUEUE_REGS       = 0x0040,
-  AHC_SG_PRELOAD       = 0x0080,
-  AHC_SPIOCAP          = 0x0100,
-  AHC_ULTRA3           = 0x0200,
-  AHC_NEW_AUTOTERM     = 0x0400,
-  AHC_AIC7770_FE       = AHC_FENONE,
-  AHC_AIC7850_FE       = AHC_SPIOCAP,
-  AHC_AIC7860_FE       = AHC_ULTRA|AHC_SPIOCAP,
-  AHC_AIC7870_FE       = AHC_FENONE,
-  AHC_AIC7880_FE       = AHC_ULTRA,
-  AHC_AIC7890_FE       = AHC_MORE_SRAM|AHC_CMD_CHAN|AHC_ULTRA2|
-                         AHC_QUEUE_REGS|AHC_SG_PRELOAD|AHC_NEW_AUTOTERM,
-  AHC_AIC7895_FE       = AHC_MORE_SRAM|AHC_CMD_CHAN|AHC_ULTRA,
-  AHC_AIC7896_FE       = AHC_AIC7890_FE,
-  AHC_AIC7892_FE       = AHC_AIC7890_FE|AHC_ULTRA3,
-  AHC_AIC7899_FE       = AHC_AIC7890_FE|AHC_ULTRA3,
-} ahc_feature;
-
-struct aic7xxx_scb {
-        struct aic7xxx_hwscb  *hscb;          /* corresponding hardware scb */
-        Scsi_Cmnd             *cmd;              /* Scsi_Cmnd for this scb */
-        struct aic7xxx_scb    *q_next;        /* next scb in queue */
-        volatile scb_flag_type flags;         /* current state of scb */
-        struct hw_scatterlist *sg_list;       /* SG list in adapter format */
-        unsigned char          tag_action;
-        unsigned char          sg_count;
-        unsigned char          sense_cmd[6];  /*
-                                               * Allocate 6 characters for
-                                               * sense command.
-                                               */
-        unsigned int           sg_length; /* We init this during buildscb so we
-                                           * don't have to calculate anything
-                                           * during underflow/overflow/stat code
-                                           */
-        void                  *kmalloc_ptr;
-};
-
-/*
- * Define a linked list of SCBs.
- */
-typedef struct {
-  struct aic7xxx_scb *head;
-  struct aic7xxx_scb *tail;
-} scb_queue_type;
-
-static struct {
-  unsigned char errno;
-  const char *errmesg;
-} hard_error[] = {
-  { ILLHADDR,  "Illegal Host Access" },
-  { ILLSADDR,  "Illegal Sequencer Address referenced" },
-  { ILLOPCODE, "Illegal Opcode in sequencer program" },
-  { SQPARERR,  "Sequencer Ram Parity Error" },
-  { DPARERR,   "Data-Path Ram Parity Error" },
-  { MPARERR,   "Scratch Ram/SCB Array Ram Parity Error" },
-  { PCIERRSTAT,"PCI Error detected" },
-  { CIOPARERR, "CIOBUS Parity Error" }
-};
-
-static unsigned char
-generic_sense[] = { REQUEST_SENSE, 0, 0, 0, 255, 0 };
-
-typedef struct {
-  scb_queue_type free_scbs;        /*
-                                    * SCBs assigned to free slot on
-                                    * card (no paging required)
-                                    */
-  struct aic7xxx_scb   *scb_array[AIC7XXX_MAXSCB];
-  struct aic7xxx_hwscb *hscbs;
-  unsigned char  numscbs;          /* current number of scbs */
-  unsigned char  maxhscbs;         /* hardware scbs */
-  unsigned char  maxscbs;          /* max scbs including pageable scbs */
-  void          *hscb_kmalloc_ptr;
-} scb_data_type;
-
-struct target_cmd {
-  unsigned char mesg_bytes[4];
-  unsigned char command[28];
-};
-
-#define AHC_TRANS_CUR    0x0001
-#define AHC_TRANS_ACTIVE 0x0002
-#define AHC_TRANS_GOAL   0x0004
-#define AHC_TRANS_USER   0x0008
-#define AHC_TRANS_QUITE  0x0010
-typedef struct {
-  unsigned char cur_width;
-  unsigned char goal_width;
-  unsigned char cur_period;
-  unsigned char goal_period;
-  unsigned char cur_offset;
-  unsigned char goal_offset;
-  unsigned char cur_options;
-  unsigned char goal_options;
-  unsigned char user_width;
-  unsigned char user_period;
-  unsigned char user_offset;
-  unsigned char user_options;
-} transinfo_type;
-
-/*
- * Define a structure used for each host adapter.  Note, in order to avoid
- * problems with architectures I can't test on (because I don't have one,
- * such as the Alpha based systems) which happen to give faults for
- * non-aligned memory accesses, care was taken to align this structure
- * in a way that gauranteed all accesses larger than 8 bits were aligned
- * on the appropriate boundary.  It's also organized to try and be more
- * cache line efficient.  Be careful when changing this lest you might hurt
- * overall performance and bring down the wrath of the masses.
- */
-struct aic7xxx_host {
-  /*
-   *  This is the first 64 bytes in the host struct
-   */
-
-  /*
-   * We are grouping things here....first, items that get either read or
-   * written with nearly every interrupt
-   */
-  volatile ahc_flag_type   flags;
-  ahc_feature              features;         /* chip features */
-  unsigned long            base;             /* card base address */
-  volatile unsigned char  *maddr;            /* memory mapped address */
-  unsigned long            isr_count;        /* Interrupt count */
-  unsigned long            spurious_int;
-  scb_data_type           *scb_data;
-  volatile unsigned short  needdv;
-  volatile unsigned short  needppr;
-  volatile unsigned short  needsdtr;
-  volatile unsigned short  needwdtr;
-  volatile unsigned short  dtr_pending;
-  struct aic7xxx_cmd_queue {
-    Scsi_Cmnd *head;
-    Scsi_Cmnd *tail;
-  } completeq;
-
-  /*
-   * Things read/written on nearly every entry into aic7xxx_queue()
-   */
-  volatile scb_queue_type  waiting_scbs;
-  unsigned short           discenable;       /* Targets allowed to disconnect */
-  unsigned short           tagenable;        /* Targets using tagged I/O */
-  unsigned short           orderedtag;       /* Ordered Q tags allowed */
-  unsigned char            unpause;          /* unpause value for HCNTRL */
-  unsigned char            pause;            /* pause value for HCNTRL */
-  volatile unsigned char   qoutfifonext;
-  volatile unsigned char   activescbs;       /* active scbs */
-  volatile unsigned char   max_activescbs;
-  volatile unsigned char   qinfifonext;
-
-#define  DEVICE_PRESENT                 0x01
-#define  BUS_DEVICE_RESET_PENDING       0x02
-#define  DEVICE_RESET_DELAY             0x04
-#define  DEVICE_PRINT_DTR               0x08
-#define  DEVICE_PARITY_ERROR            0x10
-#define  DEVICE_WAS_BUSY                0x20
-#define  DEVICE_SCSI_3                  0x40
-#define  DEVICE_SCANNED                 0x80
-  volatile unsigned char   dev_flags[MAX_TARGETS];
-  volatile unsigned char   dev_active_cmds[MAX_TARGETS];
-  volatile unsigned char   dev_temp_queue_depth[MAX_TARGETS];
-  unsigned char            dev_commands_sent[MAX_TARGETS];
-
-  unsigned int             dev_timer_active; /* Which devs have a timer set */
-  struct timer_list        dev_timer;
-  unsigned long            dev_expires[MAX_TARGETS];
-
-#if LINUX_VERSION_CODE > KERNEL_VERSION(2,1,0)
-  spinlock_t               spin_lock;
-  volatile unsigned char   cpu_lock_count[NR_CPUS];
-#endif
-
-
-  Scsi_Cmnd               *dev_dtr_cmnd[MAX_TARGETS];
-
-  unsigned int             dev_checksum[MAX_TARGETS];
-  
-  unsigned char            dev_last_queue_full[MAX_TARGETS];
-  unsigned char            dev_last_queue_full_count[MAX_TARGETS];
-  unsigned char            dev_max_queue_depth[MAX_TARGETS];
-
-  volatile scb_queue_type  delayed_scbs[MAX_TARGETS];
-
-
-  unsigned char            msg_buf[13];      /* The message for the target */
-  unsigned char            msg_type;
-#define MSG_TYPE_NONE              0x00
-#define MSG_TYPE_INITIATOR_MSGOUT  0x01
-#define MSG_TYPE_INITIATOR_MSGIN   0x02
-  unsigned char            msg_len;          /* Length of message */
-  unsigned char            msg_index;        /* Index into msg_buf array */
-  transinfo_type           transinfo[MAX_TARGETS];
-
-
-  /*
-   * We put the less frequently used host structure items after the more
-   * frequently used items to try and ease the burden on the cache subsystem.
-   * These entries are not *commonly* accessed, whereas the preceding entries
-   * are accessed very often.  The only exceptions are the qinfifo, qoutfifo,
-   * and untagged_scbs array.  But, they are often accessed only once and each
-   * access into these arrays is likely to blow a cache line, so they are put
-   * down here so we can minimize the number of cache lines required to hold
-   * the preceeding entries.
-   */
-
-  volatile unsigned char   untagged_scbs[256];
-  volatile unsigned char   qoutfifo[256];
-  volatile unsigned char   qinfifo[256];
-  unsigned int             irq;              /* IRQ for this adapter */
-  int                      instance;         /* aic7xxx instance number */
-  int                      scsi_id;          /* host adapter SCSI ID */
-  int                      scsi_id_b;        /* channel B for twin adapters */
-  unsigned int             bios_address;
-  int                      board_name_index;
-  unsigned short           needppr_copy;     /* default config */
-  unsigned short           needsdtr_copy;    /* default config */
-  unsigned short           needwdtr_copy;    /* default config */
-  unsigned short           ultraenb;         /* Ultra mode target list */
-  unsigned short           bios_control;     /* bios control - SEEPROM */
-  unsigned short           adapter_control;  /* adapter control - SEEPROM */
-#if LINUX_VERSION_CODE > KERNEL_VERSION(2,1,92)
-  struct pci_dev          *pdev;
-#endif
-  unsigned char            pci_bus;
-  unsigned char            pci_device_fn;
-  struct seeprom_config    sc;
-  unsigned short           sc_type;
-  unsigned short           sc_size;
-  struct aic7xxx_host     *next;             /* allow for multiple IRQs */
-  struct Scsi_Host        *host;             /* pointer to scsi host */
-  int                      host_no;          /* SCSI host number */
-  unsigned long            mbase;            /* I/O memory address */
-  ahc_chip                 chip;             /* chip type */
-
-  /*
-   * Statistics Kept:
-   *
-   * Total Xfers (count for each command that has a data xfer),
-   * broken down further by reads && writes.
-   *
-   * Binned sizes, writes && reads:
-   *    < 512, 512, 1-2K, 2-4K, 4-8K, 8-16K, 16-32K, 32-64K, 64K-128K, > 128K
-   *
-   * Total amounts read/written above 512 bytes (amts under ignored)
-   *
-   * NOTE: Enabling this feature is likely to cause a noticeable performance
-   * decrease as the accesses into the stats structures blows apart multiple
-   * cache lines and is CPU time consuming.
-   *
-   * NOTE: Since it doesn't really buy us much, but consumes *tons* of RAM
-   * and blows apart all sorts of cache lines, I modified this so that we
-   * no longer look at the LUN.  All LUNs now go into the same bin on each
-   * device for stats purposes.
-   */
-  struct aic7xxx_xferstats {
-    long w_total;                          /* total writes */
-    long r_total;                          /* total reads */
-#ifdef AIC7XXX_PROC_STATS
-    long w_bins[8];                       /* binned write */
-    long r_bins[8];                       /* binned reads */
-#endif /* AIC7XXX_PROC_STATS */
-  } stats[MAX_TARGETS];                    /* [(channel << 3)|target] */
-
-#if 0
-  struct target_cmd       *targetcmds;
-  unsigned int             num_targetcmds;
-#endif
-
-};
-
-/*
- * Valid SCSIRATE values. (p. 3-17)
- * Provides a mapping of transfer periods in ns/4 to the proper value to
- * stick in the SCSIRATE reg to use that transfer rate.
- */
-#define AHC_SYNCRATE_ULTRA3 0
-#define AHC_SYNCRATE_ULTRA2 1
-#define AHC_SYNCRATE_ULTRA  3
-#define AHC_SYNCRATE_FAST   6
-#define AHC_SYNCRATE_CRC 0x40
-#define AHC_SYNCRATE_SE  0x10
-static struct aic7xxx_syncrate {
-  /* Rates in Ultra mode have bit 8 of sxfr set */
-#define                ULTRA_SXFR 0x100
-  int sxfr_ultra2;
-  int sxfr;
-  unsigned char period;
-  const char *rate[2];
-} aic7xxx_syncrates[] = {
-  { 0x42,  0x000,   9,  {"80.0", "160.0"} },
-  { 0x13,  0x000,  10,  {"40.0", "80.0"} },
-  { 0x14,  0x000,  11,  {"33.0", "66.6"} },
-  { 0x15,  0x100,  12,  {"20.0", "40.0"} },
-  { 0x16,  0x110,  15,  {"16.0", "32.0"} },
-  { 0x17,  0x120,  18,  {"13.4", "26.8"} },
-  { 0x18,  0x000,  25,  {"10.0", "20.0"} },
-  { 0x19,  0x010,  31,  {"8.0",  "16.0"} },
-  { 0x1a,  0x020,  37,  {"6.67", "13.3"} },
-  { 0x1b,  0x030,  43,  {"5.7",  "11.4"} },
-  { 0x10,  0x040,  50,  {"5.0",  "10.0"} },
-  { 0x00,  0x050,  56,  {"4.4",  "8.8" } },
-  { 0x00,  0x060,  62,  {"4.0",  "8.0" } },
-  { 0x00,  0x070,  68,  {"3.6",  "7.2" } },
-  { 0x00,  0x000,  0,   {NULL, NULL}   },
-};
-
-#define CTL_OF_SCB(scb) (((scb->hscb)->target_channel_lun >> 3) & 0x1),  \
-                        (((scb->hscb)->target_channel_lun >> 4) & 0xf), \
-                        ((scb->hscb)->target_channel_lun & 0x07)
-
-#define CTL_OF_CMD(cmd) ((cmd->channel) & 0x01),  \
-                        ((cmd->target) & 0x0f), \
-                        ((cmd->lun) & 0x07)
-
-#define TARGET_INDEX(cmd)  ((cmd)->target | ((cmd)->channel << 3))
-
-/*
- * A nice little define to make doing our printks a little easier
- */
-
-#define WARN_LEAD KERN_WARNING "(scsi%d:%d:%d:%d) "
-#define INFO_LEAD KERN_INFO "(scsi%d:%d:%d:%d) "
-
-/*
- * XXX - these options apply unilaterally to _all_ 274x/284x/294x
- *       cards in the system.  This should be fixed.  Exceptions to this
- *       rule are noted in the comments.
- */
-
-
-/*
- * Skip the scsi bus reset.  Non 0 make us skip the reset at startup.  This
- * has no effect on any later resets that might occur due to things like
- * SCSI bus timeouts.
- */
-static unsigned int aic7xxx_no_reset = 0;
-/*
- * Certain PCI motherboards will scan PCI devices from highest to lowest,
- * others scan from lowest to highest, and they tend to do all kinds of
- * strange things when they come into contact with PCI bridge chips.  The
- * net result of all this is that the PCI card that is actually used to boot
- * the machine is very hard to detect.  Most motherboards go from lowest
- * PCI slot number to highest, and the first SCSI controller found is the
- * one you boot from.  The only exceptions to this are when a controller
- * has its BIOS disabled.  So, we by default sort all of our SCSI controllers
- * from lowest PCI slot number to highest PCI slot number.  We also force
- * all controllers with their BIOS disabled to the end of the list.  This
- * works on *almost* all computers.  Where it doesn't work, we have this
- * option.  Setting this option to non-0 will reverse the order of the sort
- * to highest first, then lowest, but will still leave cards with their BIOS
- * disabled at the very end.  That should fix everyone up unless there are
- * really strange cirumstances.
- */
-static int aic7xxx_reverse_scan = 0;
-/*
- * Should we force EXTENDED translation on a controller.
- *     0 == Use whatever is in the SEEPROM or default to off
- *     1 == Use whatever is in the SEEPROM or default to on
- */
-static unsigned int aic7xxx_extended = 0;
-/*
- * The IRQ trigger method used on EISA controllers. Does not effect PCI cards.
- *   -1 = Use detected settings.
- *    0 = Force Edge triggered mode.
- *    1 = Force Level triggered mode.
- */
-static int aic7xxx_irq_trigger = -1;
-/*
- * This variable is used to override the termination settings on a controller.
- * This should not be used under normal conditions.  However, in the case
- * that a controller does not have a readable SEEPROM (so that we can't
- * read the SEEPROM settings directly) and that a controller has a buggered
- * version of the cable detection logic, this can be used to force the 
- * correct termination.  It is preferable to use the manual termination
- * settings in the BIOS if possible, but some motherboard controllers store
- * those settings in a format we can't read.  In other cases, auto term
- * should also work, but the chipset was put together with no auto term
- * logic (common on motherboard controllers).  In those cases, we have
- * 32 bits here to work with.  That's good for 8 controllers/channels.  The
- * bits are organized as 4 bits per channel, with scsi0 getting the lowest
- * 4 bits in the int.  A 1 in a bit position indicates the termination setting
- * that corresponds to that bit should be enabled, a 0 is disabled.
- * It looks something like this:
- *
- *    0x0f =  1111-Single Ended Low Byte Termination on/off
- *            ||\-Single Ended High Byte Termination on/off
- *            |\-LVD Low Byte Termination on/off
- *            \-LVD High Byte Termination on/off
- *
- * For non-Ultra2 controllers, the upper 2 bits are not important.  So, to
- * enable both high byte and low byte termination on scsi0, I would need to
- * make sure that the override_term variable was set to 0x03 (bits 0011).
- * To make sure that all termination is enabled on an Ultra2 controller at
- * scsi2 and only high byte termination on scsi1 and high and low byte
- * termination on scsi0, I would set override_term=0xf23 (bits 1111 0010 0011)
- *
- * For the most part, users should never have to use this, that's why I
- * left it fairly cryptic instead of easy to understand.  If you need it,
- * most likely someone will be telling you what your's needs to be set to.
- */
-static int aic7xxx_override_term = -1;
-/*
- * Certain motherboard chipset controllers tend to screw
- * up the polarity of the term enable output pin.  Use this variable
- * to force the correct polarity for your system.  This is a bitfield variable
- * similar to the previous one, but this one has one bit per channel instead
- * of four.
- *    0 = Force the setting to active low.
- *    1 = Force setting to active high.
- * Most Adaptec cards are active high, several motherboards are active low.
- * To force a 2940 card at SCSI 0 to active high and a motherboard 7895
- * controller at scsi1 and scsi2 to active low, and a 2910 card at scsi3
- * to active high, you would need to set stpwlev=0x9 (bits 1001).
- *
- * People shouldn't need to use this, but if you are experiencing lots of
- * SCSI timeout problems, this may help.  There is one sure way to test what
- * this option needs to be.  Using a boot floppy to boot the system, configure
- * your system to enable all SCSI termination (in the Adaptec SCSI BIOS) and
- * if needed then also pass a value to override_term to make sure that the
- * driver is enabling SCSI termination, then set this variable to either 0
- * or 1.  When the driver boots, make sure there are *NO* SCSI cables
- * connected to your controller.  If it finds and inits the controller
- * without problem, then the setting you passed to stpwlev was correct.  If
- * the driver goes into a reset loop and hangs the system, then you need the
- * other setting for this variable.  If neither setting lets the machine
- * boot then you have definite termination problems that may not be fixable.
- */
-static int aic7xxx_stpwlev = -1;
-/*
- * Set this to non-0 in order to force the driver to panic the kernel
- * and print out debugging info on a SCSI abort or reset cycle.
- */
-static int aic7xxx_panic_on_abort = 0;
-/*
- * PCI bus parity checking of the Adaptec controllers.  This is somewhat
- * dubious at best.  To my knowledge, this option has never actually
- * solved a PCI parity problem, but on certain machines with broken PCI
- * chipset configurations, it can generate tons of false error messages.
- * It's included in the driver for completeness.
- *   0 = Shut off PCI parity check
- *  -1 = Normal polarity pci parity checking
- *   1 = reverse polarity pci parity checking
- *
- * NOTE: you can't actually pass -1 on the lilo prompt.  So, to set this
- * variable to -1 you would actually want to simply pass the variable
- * name without a number.  That will invert the 0 which will result in
- * -1.
- */
-static int aic7xxx_pci_parity = 0;
-/*
- * Set this to any non-0 value to cause us to dump the contents of all
- * the card's registers in a hex dump format tailored to each model of
- * controller.
- * 
- * NOTE: THE CONTROLLER IS LEFT IN AN UNUSEABLE STATE BY THIS OPTION.
- *       YOU CANNOT BOOT UP WITH THIS OPTION, IT IS FOR DEBUGGING PURPOSES
- *       ONLY
- */
-static int aic7xxx_dump_card = 0;
-/*
- * Set this to a non-0 value to make us dump out the 32 bit instruction
- * registers on the card after completing the sequencer download.  This
- * allows the actual sequencer download to be verified.  It is possible
- * to use this option and still boot up and run your system.  This is
- * only intended for debugging purposes.
- */
-static int aic7xxx_dump_sequencer = 0;
-/*
- * Certain newer motherboards have put new PCI based devices into the
- * IO spaces that used to typically be occupied by VLB or EISA cards.
- * This overlap can cause these newer motherboards to lock up when scanned
- * for older EISA and VLB devices.  Setting this option to non-0 will
- * cause the driver to skip scanning for any VLB or EISA controllers and
- * only support the PCI controllers.  NOTE: this means that if the kernel
- * os compiled with PCI support disabled, then setting this to non-0
- * would result in never finding any devices :)
- */
-static int aic7xxx_no_probe = 0;
-/*
- * On some machines, enabling the external SCB RAM isn't reliable yet.  I
- * haven't had time to make test patches for things like changing the
- * timing mode on that external RAM either.  Some of those changes may
- * fix the problem.  Until then though, we default to external SCB RAM
- * off and give a command line option to enable it.
- */
-static int aic7xxx_scbram = 0;
-/*
- * So that we can set how long each device is given as a selection timeout.
- * The table of values goes like this:
- *   0 - 256ms
- *   1 - 128ms
- *   2 - 64ms
- *   3 - 32ms
- * We default to 64ms because it's fast.  Some old SCSI-I devices need a
- * longer time.  The final value has to be left shifted by 3, hence 0x10
- * is the final value.
- */
-static int aic7xxx_seltime = 0x10;
-/*
- * So that insmod can find the variable and make it point to something
- */
-#ifdef MODULE
-static char * aic7xxx = NULL;
-#if LINUX_VERSION_CODE > KERNEL_VERSION(2,1,18)
-MODULE_PARM(aic7xxx, "s");
-#endif
-
-/*
- * Just in case someone uses commas to separate items on the insmod
- * command line, we define a dummy buffer here to avoid having insmod
- * write wild stuff into our code segment
- */
-static char dummy_buffer[60] = "Please don't trounce on me insmod!!\n";
-
-#endif
-
-#define VERBOSE_NORMAL         0x0000
-#define VERBOSE_NEGOTIATION    0x0001
-#define VERBOSE_SEQINT         0x0002
-#define VERBOSE_SCSIINT        0x0004
-#define VERBOSE_PROBE          0x0008
-#define VERBOSE_PROBE2         0x0010
-#define VERBOSE_NEGOTIATION2   0x0020
-#define VERBOSE_MINOR_ERROR    0x0040
-#define VERBOSE_TRACING        0x0080
-#define VERBOSE_ABORT          0x0f00
-#define VERBOSE_ABORT_MID      0x0100
-#define VERBOSE_ABORT_FIND     0x0200
-#define VERBOSE_ABORT_PROCESS  0x0400
-#define VERBOSE_ABORT_RETURN   0x0800
-#define VERBOSE_RESET          0xf000
-#define VERBOSE_RESET_MID      0x1000
-#define VERBOSE_RESET_FIND     0x2000
-#define VERBOSE_RESET_PROCESS  0x4000
-#define VERBOSE_RESET_RETURN   0x8000
-static int aic7xxx_verbose = VERBOSE_NORMAL | VERBOSE_NEGOTIATION |
-           VERBOSE_PROBE;                     /* verbose messages */
-
-
-/****************************************************************************
- *
- * We're going to start putting in function declarations so that order of
- * functions is no longer important.  As needed, they are added here.
- *
- ***************************************************************************/
-
-static void aic7xxx_panic_abort(struct aic7xxx_host *p, Scsi_Cmnd *cmd);
-static void aic7xxx_print_card(struct aic7xxx_host *p);
-static void aic7xxx_print_scratch_ram(struct aic7xxx_host *p);
-static void aic7xxx_print_sequencer(struct aic7xxx_host *p, int downloaded);
-#ifdef AIC7XXX_VERBOSE_DEBUGGING
-static void aic7xxx_check_scbs(struct aic7xxx_host *p, char *buffer);
-#endif
-
-/****************************************************************************
- *
- * These functions are now used.  They happen to be wrapped in useless
- * inb/outb port read/writes around the real reads and writes because it
- * seems that certain very fast CPUs have a problem dealing with us when
- * going at full speed.
- *
- ***************************************************************************/
-
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,1,0)
-static inline int
-time_after_eq(unsigned long a, unsigned long b)
-{
-  return((long)((a) - (b)) >= 0L);
-}
-
-static inline int
-timer_pending(struct timer_list *timer)
-{
-  return( timer->prev != NULL );
-}
-
-#define PCI_DEVICE_ID_ADAPTEC_1480A 0x6075
-
-#endif
-
-static inline unsigned char
-aic_inb(struct aic7xxx_host *p, long port)
-{
-#ifdef MMAPIO
-  unsigned char x;
-  if(p->maddr)
-  {
-    x = readb(p->maddr + port);
-  }
-  else
-  {
-    x = inb(p->base + port);
-  }
-  return(x);
-#else
-  return(inb(p->base + port));
-#endif
-}
-
-static inline void
-aic_outb(struct aic7xxx_host *p, unsigned char val, long port)
-{
-#ifdef MMAPIO
-  if(p->maddr)
-  {
-    writeb(val, p->maddr + port);
-    mb(); /* locked operation in order to force ordering */
-    readb(p->maddr + HCNTRL); /* dummy read to flush the write */
-  }
-  else
-  {
-    outb(val, p->base + port);
-    mb(); /* locked operation in order to force ordering */
-  }
-#else
-  outb(val, p->base + port);
-  mb(); /* locked operation in order to force ordering */
-#endif
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_setup
- *
- * Description:
- *   Handle Linux boot parameters. This routine allows for assigning a value
- *   to a parameter with a ':' between the parameter and the value.
- *   ie. aic7xxx=unpause:0x0A,extended
- *-F*************************************************************************/
-void
-aic7xxx_setup(char *s, int *dummy)
-{
-  int   i, n;
-  char *p;
-  char *end;
-
-  static struct {
-    const char *name;
-    unsigned int *flag;
-  } options[] = {
-    { "extended",    &aic7xxx_extended },
-    { "no_reset",    &aic7xxx_no_reset },
-    { "irq_trigger", &aic7xxx_irq_trigger },
-    { "verbose",     &aic7xxx_verbose },
-    { "reverse_scan",&aic7xxx_reverse_scan },
-    { "override_term", &aic7xxx_override_term },
-    { "stpwlev", &aic7xxx_stpwlev },
-    { "no_probe", &aic7xxx_no_probe },
-    { "panic_on_abort", &aic7xxx_panic_on_abort },
-    { "pci_parity", &aic7xxx_pci_parity },
-    { "dump_card", &aic7xxx_dump_card },
-    { "dump_sequencer", &aic7xxx_dump_sequencer },
-    { "scbram", &aic7xxx_scbram },
-    { "seltime", &aic7xxx_seltime },
-    { "tag_info",    NULL }
-  };
-
-  end = strchr(s, '\0');
-
-  for (p = strtok(s, ",."); p; p = strtok(NULL, ",."))
-  {
-    for (i = 0; i < NUMBER(options); i++)
-    {
-      n = strlen(options[i].name);
-      if (!strncmp(options[i].name, p, n))
-      {
-        if (!strncmp(p, "tag_info", n))
-        {
-          if (p[n] == ':')
-          {
-            char *base;
-            char *tok, *tok_end, *tok_end2;
-            char tok_list[] = { '.', ',', '{', '}', '\0' };
-            int i, instance = -1, device = -1;
-            unsigned char done = FALSE;
-
-            base = p;
-            tok = base + n + 1;  /* Forward us just past the ':' */
-            tok_end = strchr(tok, '\0');
-            if (tok_end < end)
-              *tok_end = ',';
-            while(!done)
-            {
-              switch(*tok)
-              {
-                case '{':
-                  if (instance == -1)
-                    instance = 0;
-                  else if (device == -1)
-                    device = 0;
-                  tok++;
-                  break;
-                case '}':
-                  if (device != -1)
-                    device = -1;
-                  else if (instance != -1)
-                    instance = -1;
-                  tok++;
-                  break;
-                case ',':
-                case '.':
-                  if (instance == -1)
-                    done = TRUE;
-                  else if (device >= 0)
-                    device++;
-                  else if (instance >= 0)
-                    instance++;
-                  if ( (device >= MAX_TARGETS) || 
-                       (instance >= NUMBER(aic7xxx_tag_info)) )
-                    done = TRUE;
-                  tok++;
-                  if (!done)
-                  {
-                    base = tok;
-                  }
-                  break;
-                case '\0':
-                  done = TRUE;
-                  break;
-                default:
-                  done = TRUE;
-                  tok_end = strchr(tok, '\0');
-                  for(i=0; tok_list[i]; i++)
-                  {
-                    tok_end2 = strchr(tok, tok_list[i]);
-                    if ( (tok_end2) && (tok_end2 < tok_end) )
-                    {
-                      tok_end = tok_end2;
-                      done = FALSE;
-                    }
-                  }
-                  if ( (instance >= 0) && (device >= 0) &&
-                       (instance < NUMBER(aic7xxx_tag_info)) &&
-                       (device < MAX_TARGETS) )
-                    aic7xxx_tag_info[instance].tag_commands[device] =
-                      simple_strtoul(tok, NULL, 0) & 0xff;
-                  tok = tok_end;
-                  break;
-              }
-            }
-            while((p != base) && (p != NULL))
-              p = strtok(NULL, ",.");
-          }
-        }
-        else if (p[n] == ':')
-        {
-          *(options[i].flag) = simple_strtoul(p + n + 1, NULL, 0);
-          if(!strncmp(p, "seltime", n))
-          {
-            *(options[i].flag) = (*(options[i].flag) % 4) << 3;
-          }
-        }
-        else if (!strncmp(p, "verbose", n))
-        {
-          *(options[i].flag) = 0xff09;
-        }
-        else
-        {
-          *(options[i].flag) = ~(*(options[i].flag));
-          if(!strncmp(p, "seltime", n))
-          {
-            *(options[i].flag) = (*(options[i].flag) % 4) << 3;
-          }
-        }
-      }
-    }
-  }
-}
-
-/*+F*************************************************************************
- * Function:
- *   pause_sequencer
- *
- * Description:
- *   Pause the sequencer and wait for it to actually stop - this
- *   is important since the sequencer can disable pausing for critical
- *   sections.
- *-F*************************************************************************/
-static inline void
-pause_sequencer(struct aic7xxx_host *p)
-{
-  aic_outb(p, p->pause, HCNTRL);
-  while ((aic_inb(p, HCNTRL) & PAUSE) == 0)
-  {
-    ;
-  }
-  if(p->features & AHC_ULTRA2)
-  {
-    aic_inb(p, CCSCBCTL);
-  }
-}
-
-/*+F*************************************************************************
- * Function:
- *   unpause_sequencer
- *
- * Description:
- *   Unpause the sequencer. Unremarkable, yet done often enough to
- *   warrant an easy way to do it.
- *-F*************************************************************************/
-static inline void
-unpause_sequencer(struct aic7xxx_host *p, int unpause_always)
-{
-  if (unpause_always ||
-      ( !(aic_inb(p, INTSTAT) & (SCSIINT | SEQINT | BRKADRINT)) &&
-        !(p->flags & AHC_HANDLING_REQINITS) ) )
-  {
-    aic_outb(p, p->unpause, HCNTRL);
-  }
-}
-
-/*+F*************************************************************************
- * Function:
- *   restart_sequencer
- *
- * Description:
- *   Restart the sequencer program from address zero.  This assumes
- *   that the sequencer is already paused.
- *-F*************************************************************************/
-static inline void
-restart_sequencer(struct aic7xxx_host *p)
-{
-  aic_outb(p, 0, SEQADDR0);
-  aic_outb(p, 0, SEQADDR1);
-  aic_outb(p, FASTMODE, SEQCTL);
-}
-
-/*
- * We include the aic7xxx_seq.c file here so that the other defines have
- * already been made, and so that it comes before the code that actually
- * downloads the instructions (since we don't typically use function
- * prototype, our code has to be ordered that way, it's a left-over from
- * the original driver days.....I should fix it some time DL).
- */
-#include "aic7xxx_seq.c"
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_check_patch
- *
- * Description:
- *   See if the next patch to download should be downloaded.
- *-F*************************************************************************/
-static int
-aic7xxx_check_patch(struct aic7xxx_host *p,
-  struct sequencer_patch **start_patch, int start_instr, int *skip_addr)
-{
-  struct sequencer_patch *cur_patch;
-  struct sequencer_patch *last_patch;
-  int num_patches;
-
-  num_patches = sizeof(sequencer_patches)/sizeof(struct sequencer_patch);
-  last_patch = &sequencer_patches[num_patches];
-  cur_patch = *start_patch;
-
-  while ((cur_patch < last_patch) && (start_instr == cur_patch->begin))
-  {
-    if (cur_patch->patch_func(p) == 0)
-    {
-      /*
-       * Start rejecting code.
-       */
-      *skip_addr = start_instr + cur_patch->skip_instr;
-      cur_patch += cur_patch->skip_patch;
-    }
-    else
-    {
-      /*
-       * Found an OK patch.  Advance the patch pointer to the next patch
-       * and wait for our instruction pointer to get here.
-       */
-      cur_patch++;
-    }
-  }
-
-  *start_patch = cur_patch;
-  if (start_instr < *skip_addr)
-    /*
-     * Still skipping
-     */
-    return (0);
-  return(1);
-}
-
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_download_instr
- *
- * Description:
- *   Find the next patch to download.
- *-F*************************************************************************/
-static void
-aic7xxx_download_instr(struct aic7xxx_host *p, int instrptr,
-  unsigned char *dconsts)
-{
-  union ins_formats instr;
-  struct ins_format1 *fmt1_ins;
-  struct ins_format3 *fmt3_ins;
-  unsigned char opcode;
-
-  instr = *(union ins_formats*) &seqprog[instrptr * 4];
-
-  instr.integer = le32_to_cpu(instr.integer);
-  
-  fmt1_ins = &instr.format1;
-  fmt3_ins = NULL;
-
-  /* Pull the opcode */
-  opcode = instr.format1.opcode;
-  switch (opcode)
-  {
-    case AIC_OP_JMP:
-    case AIC_OP_JC:
-    case AIC_OP_JNC:
-    case AIC_OP_CALL:
-    case AIC_OP_JNE:
-    case AIC_OP_JNZ:
-    case AIC_OP_JE:
-    case AIC_OP_JZ:
-    {
-      struct sequencer_patch *cur_patch;
-      int address_offset;
-      unsigned int address;
-      int skip_addr;
-      int i;
-
-      fmt3_ins = &instr.format3;
-      address_offset = 0;
-      address = fmt3_ins->address;
-      cur_patch = sequencer_patches;
-      skip_addr = 0;
-
-      for (i = 0; i < address;)
-      {
-        aic7xxx_check_patch(p, &cur_patch, i, &skip_addr);
-        if (skip_addr > i)
-        {
-          int end_addr;
-
-          end_addr = MIN(address, skip_addr);
-          address_offset += end_addr - i;
-          i = skip_addr;
-        }
-        else
-        {
-          i++;
-        }
-      }
-      address -= address_offset;
-      fmt3_ins->address = address;
-      /* Fall Through to the next code section */
-    }
-    case AIC_OP_OR:
-    case AIC_OP_AND:
-    case AIC_OP_XOR:
-    case AIC_OP_ADD:
-    case AIC_OP_ADC:
-    case AIC_OP_BMOV:
-      if (fmt1_ins->parity != 0)
-      {
-        fmt1_ins->immediate = dconsts[fmt1_ins->immediate];
-      }
-      fmt1_ins->parity = 0;
-      /* Fall Through to the next code section */
-    case AIC_OP_ROL:
-      if ((p->features & AHC_ULTRA2) != 0)
-      {
-        int i, count;
-
-        /* Calculate odd parity for the instruction */
-        for ( i=0, count=0; i < 31; i++)
-        {
-          unsigned int mask;
-
-          mask = 0x01 << i;
-          if ((instr.integer & mask) != 0)
-            count++;
-        }
-        if (!(count & 0x01))
-          instr.format1.parity = 1;
-      }
-      else
-      {
-        if (fmt3_ins != NULL)
-        {
-          instr.integer =  fmt3_ins->immediate |
-                          (fmt3_ins->source << 8) |
-                          (fmt3_ins->address << 16) |
-                          (fmt3_ins->opcode << 25);
-        }
-        else
-        {
-          instr.integer =  fmt1_ins->immediate |
-                          (fmt1_ins->source << 8) |
-                          (fmt1_ins->destination << 16) |
-                          (fmt1_ins->ret << 24) |
-                          (fmt1_ins->opcode << 25);
-        }
-      }
-      aic_outb(p, (instr.integer & 0xff), SEQRAM);
-      aic_outb(p, ((instr.integer >> 8) & 0xff), SEQRAM);
-      aic_outb(p, ((instr.integer >> 16) & 0xff), SEQRAM);
-      aic_outb(p, ((instr.integer >> 24) & 0xff), SEQRAM);
-      udelay(10);
-      break;
-
-    default:
-      panic("aic7xxx: Unknown opcode encountered in sequencer program.");
-      break;
-  }
-}
-
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_loadseq
- *
- * Description:
- *   Load the sequencer code into the controller memory.
- *-F*************************************************************************/
-static void
-aic7xxx_loadseq(struct aic7xxx_host *p)
-{
-  struct sequencer_patch *cur_patch;
-  int i;
-  int downloaded;
-  int skip_addr;
-  unsigned char download_consts[4] = {0, 0, 0, 0};
-
-  if (aic7xxx_verbose & VERBOSE_PROBE)
-  {
-    printk(KERN_INFO "(scsi%d) Downloading sequencer code...", p->host_no);
-  }
-#if 0
-  download_consts[TMODE_NUMCMDS] = p->num_targetcmds;
-#endif
-  download_consts[TMODE_NUMCMDS] = 0;
-  cur_patch = &sequencer_patches[0];
-  downloaded = 0;
-  skip_addr = 0;
-
-  aic_outb(p, PERRORDIS|LOADRAM|FAILDIS|FASTMODE, SEQCTL);
-  aic_outb(p, 0, SEQADDR0);
-  aic_outb(p, 0, SEQADDR1);
-
-  for (i = 0; i < sizeof(seqprog) / 4;  i++)
-  {
-    if (aic7xxx_check_patch(p, &cur_patch, i, &skip_addr) == 0)
-    {
-      /* Skip this instruction for this configuration. */
-      continue;
-    }
-    aic7xxx_download_instr(p, i, &download_consts[0]);
-    downloaded++;
-  }
-
-  aic_outb(p, 0, SEQADDR0);
-  aic_outb(p, 0, SEQADDR1);
-  aic_outb(p, FASTMODE | FAILDIS, SEQCTL);
-  unpause_sequencer(p, TRUE);
-  mdelay(1);
-  pause_sequencer(p);
-  aic_outb(p, FASTMODE, SEQCTL);
-  if (aic7xxx_verbose & VERBOSE_PROBE)
-  {
-    printk(" %d instructions downloaded\n", downloaded);
-  }
-  if (aic7xxx_dump_sequencer)
-    aic7xxx_print_sequencer(p, downloaded);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_print_sequencer
- *
- * Description:
- *   Print the contents of the sequencer memory to the screen.
- *-F*************************************************************************/
-static void
-aic7xxx_print_sequencer(struct aic7xxx_host *p, int downloaded)
-{
-  int i, k, temp;
-  
-  aic_outb(p, PERRORDIS|LOADRAM|FAILDIS|FASTMODE, SEQCTL);
-  aic_outb(p, 0, SEQADDR0);
-  aic_outb(p, 0, SEQADDR1);
-
-  k = 0;
-  for (i=0; i < downloaded; i++)
-  {
-    if ( k == 0 )
-      printk("%03x: ", i);
-    temp = aic_inb(p, SEQRAM);
-    temp |= (aic_inb(p, SEQRAM) << 8);
-    temp |= (aic_inb(p, SEQRAM) << 16);
-    temp |= (aic_inb(p, SEQRAM) << 24);
-    printk("%08x", temp);
-    if ( ++k == 8 )
-    {
-      printk("\n");
-      k = 0;
-    }
-    else
-      printk(" ");
-  }
-  aic_outb(p, 0, SEQADDR0);
-  aic_outb(p, 0, SEQADDR1);
-  aic_outb(p, FASTMODE | FAILDIS, SEQCTL);
-  unpause_sequencer(p, TRUE);
-  mdelay(1);
-  pause_sequencer(p);
-  aic_outb(p, FASTMODE, SEQCTL);
-  printk("\n");
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_info
- *
- * Description:
- *   Return a string describing the driver.
- *-F*************************************************************************/
-const char *
-aic7xxx_info(struct Scsi_Host *dooh)
-{
-  static char buffer[256];
-  char *bp;
-  struct aic7xxx_host *p;
-
-  bp = &buffer[0];
-  p = (struct aic7xxx_host *)dooh->hostdata;
-  memset(bp, 0, sizeof(buffer));
-  strcpy(bp, "Adaptec AHA274x/284x/294x (EISA/VLB/PCI-Fast SCSI) ");
-  strcat(bp, AIC7XXX_C_VERSION);
-  strcat(bp, "/");
-  strcat(bp, AIC7XXX_H_VERSION);
-  strcat(bp, "\n");
-  strcat(bp, "       <");
-  strcat(bp, board_names[p->board_name_index]);
-  strcat(bp, ">");
-
-  return(bp);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_find_syncrate
- *
- * Description:
- *   Look up the valid period to SCSIRATE conversion in our table
- *-F*************************************************************************/
-static struct aic7xxx_syncrate *
-aic7xxx_find_syncrate(struct aic7xxx_host *p, unsigned int *period,
-  unsigned int maxsync, unsigned char *options)
-{
-  struct aic7xxx_syncrate *syncrate;
-  int done = FALSE;
-
-  switch(*options)
-  {
-    case MSG_EXT_PPR_OPTION_DT_CRC:
-    case MSG_EXT_PPR_OPTION_DT_UNITS:
-      if(!(p->features & AHC_ULTRA3))
-      {
-        *options = 0;
-        maxsync = MAX(maxsync, AHC_SYNCRATE_ULTRA2);
-      }
-      break;
-    case MSG_EXT_PPR_OPTION_DT_CRC_QUICK:
-    case MSG_EXT_PPR_OPTION_DT_UNITS_QUICK:
-      if(!(p->features & AHC_ULTRA3))
-      {
-        *options = 0;
-        maxsync = MAX(maxsync, AHC_SYNCRATE_ULTRA2);
-      }
-      else
-      {
-        /*
-         * we don't support the Quick Arbitration variants of dual edge
-         * clocking.  As it turns out, we want to send back the
-         * same basic option, but without the QA attribute.
-         * We know that we are responding because we would never set
-         * these options ourself, we would only respond to them.
-         */
-        switch(*options)
-        {
-          case MSG_EXT_PPR_OPTION_DT_CRC_QUICK:
-            *options = MSG_EXT_PPR_OPTION_DT_CRC;
-            break;
-          case MSG_EXT_PPR_OPTION_DT_UNITS_QUICK:
-            *options = MSG_EXT_PPR_OPTION_DT_UNITS;
-            break;
-        }
-      }
-      break;
-    default:
-      *options = 0;
-      maxsync = MAX(maxsync, AHC_SYNCRATE_ULTRA2);
-      break;
-  }
-  syncrate = &aic7xxx_syncrates[maxsync];
-  while ( (syncrate->rate[0] != NULL) &&
-         (!(p->features & AHC_ULTRA2) || syncrate->sxfr_ultra2) )
-  {
-    if (*period <= syncrate->period) 
-    {
-      switch(*options)
-      {
-        case MSG_EXT_PPR_OPTION_DT_CRC:
-        case MSG_EXT_PPR_OPTION_DT_UNITS:
-          if(!(syncrate->sxfr_ultra2 & AHC_SYNCRATE_CRC))
-          {
-            done = TRUE;
-            /*
-             * oops, we went too low for the CRC/DualEdge signalling, so
-             * clear the options byte
-             */
-            *options = 0;
-            /*
-             * We'll be sending a reply to this packet to set the options
-             * properly, so unilaterally set the period as well.
-             */
-            *period = syncrate->period;
-          }
-          else
-          {
-            done = TRUE;
-            if(syncrate == &aic7xxx_syncrates[maxsync])
-            {
-              *period = syncrate->period;
-            }
-          }
-          break;
-        default:
-          if(!(syncrate->sxfr_ultra2 & AHC_SYNCRATE_CRC))
-          {
-            done = TRUE;
-            if(syncrate == &aic7xxx_syncrates[maxsync])
-            {
-              *period = syncrate->period;
-            }
-          }
-          break;
-      }
-      if(done)
-      {
-        break;
-      }
-    }
-    syncrate++;
-  }
-  if ( (*period == 0) || (syncrate->rate[0] == NULL) ||
-       ((p->features & AHC_ULTRA2) && (syncrate->sxfr_ultra2 == 0)) )
-  {
-    /*
-     * Use async transfers for this target
-     */
-    *options = 0;
-    *period = 255;
-    syncrate = NULL;
-  }
-  return (syncrate);
-}
-
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_find_period
- *
- * Description:
- *   Look up the valid SCSIRATE to period conversion in our table
- *-F*************************************************************************/
-static unsigned int
-aic7xxx_find_period(struct aic7xxx_host *p, unsigned int scsirate,
-  unsigned int maxsync)
-{
-  struct aic7xxx_syncrate *syncrate;
-
-  if (p->features & AHC_ULTRA2)
-  {
-    scsirate &= SXFR_ULTRA2;
-  }
-  else
-  {
-    scsirate &= SXFR;
-  }
-
-  syncrate = &aic7xxx_syncrates[maxsync];
-  while (syncrate->rate[0] != NULL)
-  {
-    if (p->features & AHC_ULTRA2)
-    {
-      if (syncrate->sxfr_ultra2 == 0)
-        break;
-      else if (scsirate == syncrate->sxfr_ultra2)
-        return (syncrate->period);
-      else if (scsirate == (syncrate->sxfr_ultra2 & ~AHC_SYNCRATE_CRC))
-        return (syncrate->period);
-    }
-    else if (scsirate == (syncrate->sxfr & ~ULTRA_SXFR))
-    {
-      return (syncrate->period);
-    }
-    syncrate++;
-  }
-  return (0); /* async */
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_validate_offset
- *
- * Description:
- *   Set a valid offset value for a particular card in use and transfer
- *   settings in use.
- *-F*************************************************************************/
-static void
-aic7xxx_validate_offset(struct aic7xxx_host *p,
-  struct aic7xxx_syncrate *syncrate, unsigned int *offset, int wide)
-{
-  unsigned int maxoffset;
-
-  /* Limit offset to what the card (and device) can do */
-  if (syncrate == NULL)
-  {
-    maxoffset = 0;
-  }
-  else if (p->features & AHC_ULTRA2)
-  {
-    maxoffset = MAX_OFFSET_ULTRA2;
-  }
-  else
-  {
-    if (wide)
-      maxoffset = MAX_OFFSET_16BIT;
-    else
-      maxoffset = MAX_OFFSET_8BIT;
-  }
-  *offset = MIN(*offset, maxoffset);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_set_syncrate
- *
- * Description:
- *   Set the actual syncrate down in the card and in our host structs
- *-F*************************************************************************/
-static void
-aic7xxx_set_syncrate(struct aic7xxx_host *p, struct aic7xxx_syncrate *syncrate,
-    int target, int channel, unsigned int period, unsigned int offset,
-    unsigned char options, unsigned int type)
-{
-  unsigned char tindex;
-  unsigned short target_mask;
-  unsigned char lun, old_options;
-  unsigned int old_period, old_offset;
-
-  tindex = target | (channel << 3);
-  target_mask = 0x01 << tindex;
-  lun = aic_inb(p, SCB_TCL) & 0x07;
-
-  if (syncrate == NULL)
-  {
-    period = 0;
-    offset = 0;
-  }
-
-  old_period = p->transinfo[tindex].cur_period;
-  old_offset = p->transinfo[tindex].cur_offset;
-  old_options = p->transinfo[tindex].cur_options;
-
-  
-  if (type & AHC_TRANS_CUR)
-  {
-    unsigned int scsirate;
-
-    scsirate = aic_inb(p, TARG_SCSIRATE + tindex);
-    if (p->features & AHC_ULTRA2)
-    {
-      scsirate &= ~SXFR_ULTRA2;
-      if (syncrate != NULL)
-      {
-        switch(options)
-        {
-          case MSG_EXT_PPR_OPTION_DT_UNITS:
-            /*
-             * mask off the CRC bit in the xfer settings
-             */
-            scsirate |= (syncrate->sxfr_ultra2 & ~AHC_SYNCRATE_CRC);
-            break;
-          default:
-            scsirate |= syncrate->sxfr_ultra2;
-            break;
-        }
-      }
-      if (type & AHC_TRANS_ACTIVE)
-      {
-        aic_outb(p, offset, SCSIOFFSET);
-      }
-      aic_outb(p, offset, TARG_OFFSET + tindex);
-    }
-    else /* Not an Ultra2 controller */
-    {
-      scsirate &= ~(SXFR|SOFS);
-      p->ultraenb &= ~target_mask;
-      if (syncrate != NULL)
-      {
-        if (syncrate->sxfr & ULTRA_SXFR)
-        {
-          p->ultraenb |= target_mask;
-        }
-        scsirate |= (syncrate->sxfr & SXFR);
-        scsirate |= (offset & SOFS);
-      }
-      if (type & AHC_TRANS_ACTIVE)
-      {
-        unsigned char sxfrctl0;
-
-        sxfrctl0 = aic_inb(p, SXFRCTL0);
-        sxfrctl0 &= ~FAST20;
-        if (p->ultraenb & target_mask)
-          sxfrctl0 |= FAST20;
-        aic_outb(p, sxfrctl0, SXFRCTL0);
-      }
-      aic_outb(p, p->ultraenb & 0xff, ULTRA_ENB);
-      aic_outb(p, (p->ultraenb >> 8) & 0xff, ULTRA_ENB + 1 );
-    }
-    if (type & AHC_TRANS_ACTIVE)
-    {
-      aic_outb(p, scsirate, SCSIRATE);
-    }
-    aic_outb(p, scsirate, TARG_SCSIRATE + tindex);
-    p->transinfo[tindex].cur_period = period;
-    p->transinfo[tindex].cur_offset = offset;
-    p->transinfo[tindex].cur_options = options;
-    if ( !(type & AHC_TRANS_QUITE) &&
-         (aic7xxx_verbose & VERBOSE_NEGOTIATION) &&
-         (p->dev_flags[tindex] & DEVICE_PRINT_DTR) )
-    {
-      if (offset)
-      {
-        int rate_mod = (scsirate & WIDEXFER) ? 1 : 0;
-      
-        printk(INFO_LEAD "Synchronous at %s Mbyte/sec, "
-               "offset %d.\n", p->host_no, channel, target, lun,
-               syncrate->rate[rate_mod], offset);
-      }
-      else
-      {
-        printk(INFO_LEAD "Using asynchronous transfers.\n",
-               p->host_no, channel, target, lun);
-      }
-      p->dev_flags[tindex] &= ~DEVICE_PRINT_DTR;
-    }
-  }
-
-  if (type & AHC_TRANS_GOAL)
-  {
-    p->transinfo[tindex].goal_period = period;
-    p->transinfo[tindex].goal_offset = offset;
-    p->transinfo[tindex].goal_options = options;
-  }
-
-  if (type & AHC_TRANS_USER)
-  {
-    p->transinfo[tindex].user_period = period;
-    p->transinfo[tindex].user_offset = offset;
-    p->transinfo[tindex].user_options = options;
-  }
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_set_width
- *
- * Description:
- *   Set the actual width down in the card and in our host structs
- *-F*************************************************************************/
-static void
-aic7xxx_set_width(struct aic7xxx_host *p, int target, int channel, int lun,
-    unsigned int width, unsigned int type)
-{
-  unsigned char tindex;
-  unsigned short target_mask;
-  unsigned int old_width;
-
-  tindex = target | (channel << 3);
-  target_mask = 1 << tindex;
-  
-  old_width = p->transinfo[tindex].cur_width;
-
-  if (type & AHC_TRANS_CUR) 
-  {
-    unsigned char scsirate;
-
-    scsirate = aic_inb(p, TARG_SCSIRATE + tindex);
-
-    scsirate &= ~WIDEXFER;
-    if (width == MSG_EXT_WDTR_BUS_16_BIT)
-      scsirate |= WIDEXFER;
-
-    aic_outb(p, scsirate, TARG_SCSIRATE + tindex);
-
-    if (type & AHC_TRANS_ACTIVE)
-      aic_outb(p, scsirate, SCSIRATE);
-
-    p->transinfo[tindex].cur_width = width;
-
-    if ( !(type & AHC_TRANS_QUITE) &&
-          (aic7xxx_verbose & VERBOSE_NEGOTIATION2) && 
-          (p->dev_flags[tindex] & DEVICE_PRINT_DTR) )
-    {
-      printk(INFO_LEAD "Using %s transfers\n", p->host_no, channel, target,
-        lun, (scsirate & WIDEXFER) ? "Wide(16bit)" : "Narrow(8bit)" );
-    }
-  }
-
-  if (type & AHC_TRANS_GOAL)
-    p->transinfo[tindex].goal_width = width;
-  if (type & AHC_TRANS_USER)
-    p->transinfo[tindex].user_width = width;
-
-  if (p->transinfo[tindex].goal_offset)
-  {
-    if (p->features & AHC_ULTRA2)
-    {
-      p->transinfo[tindex].goal_offset = MAX_OFFSET_ULTRA2;
-    }
-    else if (width == MSG_EXT_WDTR_BUS_16_BIT)
-    {
-      p->transinfo[tindex].goal_offset = MAX_OFFSET_16BIT;
-    }
-    else
-    {
-      p->transinfo[tindex].goal_offset = MAX_OFFSET_8BIT;
-    }
-  }
-}
-      
-/*+F*************************************************************************
- * Function:
- *   scbq_init
- *
- * Description:
- *   SCB queue initialization.
- *
- *-F*************************************************************************/
-static void
-scbq_init(volatile scb_queue_type *queue)
-{
-  queue->head = NULL;
-  queue->tail = NULL;
-}
-
-/*+F*************************************************************************
- * Function:
- *   scbq_insert_head
- *
- * Description:
- *   Add an SCB to the head of the list.
- *
- *-F*************************************************************************/
-static inline void
-scbq_insert_head(volatile scb_queue_type *queue, struct aic7xxx_scb *scb)
-{
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,1,95)
-  unsigned long cpu_flags;
-#endif
-
-  DRIVER_LOCK
-  scb->q_next = queue->head;
-  queue->head = scb;
-  if (queue->tail == NULL)       /* If list was empty, update tail. */
-    queue->tail = queue->head;
-  DRIVER_UNLOCK
-}
-
-/*+F*************************************************************************
- * Function:
- *   scbq_remove_head
- *
- * Description:
- *   Remove an SCB from the head of the list.
- *
- *-F*************************************************************************/
-static inline struct aic7xxx_scb *
-scbq_remove_head(volatile scb_queue_type *queue)
-{
-  struct aic7xxx_scb * scbp;
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,1,95)
-  unsigned long cpu_flags;
-#endif
-
-  DRIVER_LOCK
-  scbp = queue->head;
-  if (queue->head != NULL)
-    queue->head = queue->head->q_next;
-  if (queue->head == NULL)       /* If list is now empty, update tail. */
-    queue->tail = NULL;
-  DRIVER_UNLOCK
-  return(scbp);
-}
-
-/*+F*************************************************************************
- * Function:
- *   scbq_remove
- *
- * Description:
- *   Removes an SCB from the list.
- *
- *-F*************************************************************************/
-static inline void
-scbq_remove(volatile scb_queue_type *queue, struct aic7xxx_scb *scb)
-{
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,1,95)
-  unsigned long cpu_flags;
-#endif
-
-  DRIVER_LOCK
-  if (queue->head == scb)
-  {
-    /* At beginning of queue, remove from head. */
-    scbq_remove_head(queue);
-  }
-  else
-  {
-    struct aic7xxx_scb *curscb = queue->head;
-
-    /*
-     * Search until the next scb is the one we're looking for, or
-     * we run out of queue.
-     */
-    while ((curscb != NULL) && (curscb->q_next != scb))
-    {
-      curscb = curscb->q_next;
-    }
-    if (curscb != NULL)
-    {
-      /* Found it. */
-      curscb->q_next = scb->q_next;
-      if (scb->q_next == NULL)
-      {
-        /* Update the tail when removing the tail. */
-        queue->tail = curscb;
-      }
-    }
-  }
-  DRIVER_UNLOCK
-}
-
-/*+F*************************************************************************
- * Function:
- *   scbq_insert_tail
- *
- * Description:
- *   Add an SCB at the tail of the list.
- *
- *-F*************************************************************************/
-static inline void
-scbq_insert_tail(volatile scb_queue_type *queue, struct aic7xxx_scb *scb)
-{
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,1,95)
-  unsigned long cpu_flags;
-#endif
-
-  DRIVER_LOCK
-  scb->q_next = NULL;
-  if (queue->tail != NULL)       /* Add the scb at the end of the list. */
-    queue->tail->q_next = scb;
-  queue->tail = scb;             /* Update the tail. */
-  if (queue->head == NULL)       /* If list was empty, update head. */
-    queue->head = queue->tail;
-  DRIVER_UNLOCK
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_match_scb
- *
- * Description:
- *   Checks to see if an scb matches the target/channel as specified.
- *   If target is ALL_TARGETS (-1), then we're looking for any device
- *   on the specified channel; this happens when a channel is going
- *   to be reset and all devices on that channel must be aborted.
- *-F*************************************************************************/
-static int
-aic7xxx_match_scb(struct aic7xxx_host *p, struct aic7xxx_scb *scb,
-    int target, int channel, int lun, unsigned char tag)
-{
-  int targ = (scb->hscb->target_channel_lun >> 4) & 0x0F;
-  int chan = (scb->hscb->target_channel_lun >> 3) & 0x01;
-  int slun = scb->hscb->target_channel_lun & 0x07;
-  int match;
-
-  match = ((chan == channel) || (channel == ALL_CHANNELS));
-  if (match != 0)
-    match = ((targ == target) || (target == ALL_TARGETS));
-  if (match != 0)
-    match = ((lun == slun) || (lun == ALL_LUNS));
-  if (match != 0)
-    match = ((tag == scb->hscb->tag) || (tag == SCB_LIST_NULL));
-
-  return (match);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_add_curscb_to_free_list
- *
- * Description:
- *   Adds the current scb (in SCBPTR) to the list of free SCBs.
- *-F*************************************************************************/
-static void
-aic7xxx_add_curscb_to_free_list(struct aic7xxx_host *p)
-{
-  /*
-   * Invalidate the tag so that aic7xxx_find_scb doesn't think
-   * it's active
-   */
-  aic_outb(p, SCB_LIST_NULL, SCB_TAG);
-  aic_outb(p, 0, SCB_CONTROL);
-
-  aic_outb(p, aic_inb(p, FREE_SCBH), SCB_NEXT);
-  aic_outb(p, aic_inb(p, SCBPTR), FREE_SCBH);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_rem_scb_from_disc_list
- *
- * Description:
- *   Removes the current SCB from the disconnected list and adds it
- *   to the free list.
- *-F*************************************************************************/
-static unsigned char
-aic7xxx_rem_scb_from_disc_list(struct aic7xxx_host *p, unsigned char scbptr,
-                               unsigned char prev)
-{
-  unsigned char next;
-
-  aic_outb(p, scbptr, SCBPTR);
-  next = aic_inb(p, SCB_NEXT);
-  aic7xxx_add_curscb_to_free_list(p);
-
-  if (prev != SCB_LIST_NULL)
-  {
-    aic_outb(p, prev, SCBPTR);
-    aic_outb(p, next, SCB_NEXT);
-  }
-  else
-  {
-    aic_outb(p, next, DISCONNECTED_SCBH);
-  }
-
-  return next;
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_busy_target
- *
- * Description:
- *   Set the specified target busy.
- *-F*************************************************************************/
-static inline void
-aic7xxx_busy_target(struct aic7xxx_host *p, struct aic7xxx_scb *scb)
-{
-  p->untagged_scbs[scb->hscb->target_channel_lun] = scb->hscb->tag;
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_index_busy_target
- *
- * Description:
- *   Returns the index of the busy target, and optionally sets the
- *   target inactive.
- *-F*************************************************************************/
-static inline unsigned char
-aic7xxx_index_busy_target(struct aic7xxx_host *p, unsigned char tcl,
-    int unbusy)
-{
-  unsigned char busy_scbid;
-
-  busy_scbid = p->untagged_scbs[tcl];
-  if (unbusy)
-  {
-    p->untagged_scbs[tcl] = SCB_LIST_NULL;
-  }
-  return (busy_scbid);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_find_scb
- *
- * Description:
- *   Look through the SCB array of the card and attempt to find the
- *   hardware SCB that corresponds to the passed in SCB.  Return
- *   SCB_LIST_NULL if unsuccessful.  This routine assumes that the
- *   card is already paused.
- *-F*************************************************************************/
-static unsigned char
-aic7xxx_find_scb(struct aic7xxx_host *p, struct aic7xxx_scb *scb)
-{
-  unsigned char saved_scbptr;
-  unsigned char curindex;
-
-  saved_scbptr = aic_inb(p, SCBPTR);
-  curindex = 0;
-  for (curindex = 0; curindex < p->scb_data->maxhscbs; curindex++)
-  {
-    aic_outb(p, curindex, SCBPTR);
-    if (aic_inb(p, SCB_TAG) == scb->hscb->tag)
-    {
-      break;
-    }
-  }
-  aic_outb(p, saved_scbptr, SCBPTR);
-  if (curindex >= p->scb_data->maxhscbs)
-  {
-    curindex = SCB_LIST_NULL;
-  }
-
-  return (curindex);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_allocate_scb
- *
- * Description:
- *   Get an SCB from the free list or by allocating a new one.
- *-F*************************************************************************/
-static int
-aic7xxx_allocate_scb(struct aic7xxx_host *p)
-{
-  struct aic7xxx_scb   *scbp = NULL;
-  int scb_size = sizeof(struct aic7xxx_scb) +
-                 sizeof (struct hw_scatterlist) * AIC7XXX_MAX_SG;
-  int i;
-  int step = PAGE_SIZE / 1024;
-  unsigned long scb_count = 0;
-  struct hw_scatterlist *hsgp;
-  struct aic7xxx_scb *scb_ap;
-  unsigned long temp;
-
-
-  if (p->scb_data->numscbs < p->scb_data->maxscbs)
-  {
-    /*
-     * Calculate the optimal number of SCBs to allocate.
-     *
-     * NOTE: This formula works because the sizeof(sg_array) is always
-     * 1024.  Therefore, scb_size * i would always be > PAGE_SIZE *
-     * (i/step).  The (i-1) allows the left hand side of the equation
-     * to grow into the right hand side to a point of near perfect
-     * efficiency since scb_size * (i -1) is growing slightly faster
-     * than the right hand side.  If the number of SG array elements
-     * is changed, this function may not be near so efficient any more.
-     */
-    for ( i=step;; i *= 2 )
-    {
-      if ( (scb_size * (i-1)) >= ( (PAGE_SIZE * (i/step)) - 64 ) )
-      {
-        i /= 2;
-        break;
-      }
-    }
-    scb_count = MIN( (i-1), p->scb_data->maxscbs - p->scb_data->numscbs);
-    scb_ap = (struct aic7xxx_scb *)kmalloc(scb_size * scb_count, GFP_ATOMIC);
-    if (scb_ap != NULL)
-    {
-#ifdef AIC7XXX_VERBOSE_DEBUGGING
-      if (aic7xxx_verbose > 0xffff)
-      {
-        if (p->scb_data->numscbs == 0)
-          printk(INFO_LEAD "Allocating initial %ld SCB structures.\n",
-            p->host_no, -1, -1, -1, scb_count);
-        else
-          printk(INFO_LEAD "Allocating %ld additional SCB structures.\n",
-            p->host_no, -1, -1, -1, scb_count);
-      }
-#endif
-      memset(scb_ap, 0, scb_count * scb_size);
-      temp = (unsigned long) &scb_ap[scb_count];
-      temp += 1023;
-      temp &= ~1023;
-      hsgp = (struct hw_scatterlist *)temp;
-      for (i=0; i < scb_count; i++)
-      {
-        scbp = &scb_ap[i];
-        scbp->hscb = &p->scb_data->hscbs[p->scb_data->numscbs];
-        scbp->sg_list = &hsgp[i * AIC7XXX_MAX_SG];
-        memset(scbp->hscb, 0, sizeof(struct aic7xxx_hwscb));
-        scbp->hscb->tag = p->scb_data->numscbs;
-        /*
-         * Place in the scb array; never is removed
-         */
-        p->scb_data->scb_array[p->scb_data->numscbs++] = scbp;
-        scbq_insert_tail(&p->scb_data->free_scbs, scbp);
-      }
-      scbp->kmalloc_ptr = scb_ap;
-    }
-    else
-    {
-      return(0);
-    }
-  }
-  return(scb_count);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_queue_cmd_complete
- *
- * Description:
- *   Due to race conditions present in the SCSI subsystem, it is easier
- *   to queue completed commands, then call scsi_done() on them when
- *   we're finished.  This function queues the completed commands.
- *-F*************************************************************************/
-static void
-aic7xxx_queue_cmd_complete(struct aic7xxx_host *p, Scsi_Cmnd *cmd)
-{
-  cmd->host_scribble = (char *)p->completeq.head;
-  p->completeq.head = cmd;
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_done_cmds_complete
- *
- * Description:
- *   Process the completed command queue.
- *-F*************************************************************************/
-static void
-aic7xxx_done_cmds_complete(struct aic7xxx_host *p)
-{
-  Scsi_Cmnd *cmd;
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,1,95)
-  unsigned int cpu_flags = 0;
-#endif
-  
-  DRIVER_LOCK
-  while (p->completeq.head != NULL)
-  {
-    cmd = p->completeq.head;
-    p->completeq.head = (Scsi_Cmnd *)cmd->host_scribble;
-    cmd->host_scribble = NULL;
-    cmd->scsi_done(cmd);
-  }
-  DRIVER_UNLOCK
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_free_scb
- *
- * Description:
- *   Free the scb and insert into the free scb list.
- *-F*************************************************************************/
-static void
-aic7xxx_free_scb(struct aic7xxx_host *p, struct aic7xxx_scb *scb)
-{
-
-  scb->flags = SCB_FREE;
-  scb->cmd = NULL;
-  scb->sg_count = 0;
-  scb->sg_length = 0;
-  scb->tag_action = 0;
-  scb->hscb->control = 0;
-  scb->hscb->target_status = 0;
-  scb->hscb->target_channel_lun = SCB_LIST_NULL;
-
-  scbq_insert_head(&p->scb_data->free_scbs, scb);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_done
- *
- * Description:
- *   Calls the higher level scsi done function and frees the scb.
- *-F*************************************************************************/
-static void
-aic7xxx_done(struct aic7xxx_host *p, struct aic7xxx_scb *scb)
-{
-  Scsi_Cmnd *cmd = scb->cmd;
-  int tindex = TARGET_INDEX(cmd);
-  struct aic7xxx_scb *scbp;
-  unsigned char queue_depth;
-
-  if (scb->flags & SCB_RECOVERY_SCB)
-  {
-    p->flags &= ~AHC_ABORT_PENDING;
-  }
-  if (scb->flags & SCB_RESET)
-  {
-      cmd->result = (DID_RESET << 16) | (cmd->result & 0xffff);
-  }
-  else if (scb->flags & SCB_ABORT)
-  {
-      cmd->result = (DID_RESET << 16) | (cmd->result & 0xffff);
-  }
-  else if (!(p->dev_flags[tindex] & DEVICE_SCANNED))
-  {
-    if ( (cmd->cmnd[0] == INQUIRY) && (cmd->result == DID_OK) )
-    {
-      char *buffer;
-      
-      p->dev_flags[tindex] |= DEVICE_PRESENT;
-      if(cmd->use_sg)
-      {
-        struct scatterlist *sg;
-
-        sg = (struct scatterlist *)cmd->request_buffer;
-        buffer = (char *)sg[0].address;
-      }
-      else
-      {
-        buffer = (char *)cmd->request_buffer;
-      }
-#define WIDE_INQUIRY_BITS 0x60
-#define SYNC_INQUIRY_BITS 0x10
-#define SCSI_VERSION_BITS 0x07
-#define SCSI_DT_BIT       0x04
-      if ( (buffer[7] & WIDE_INQUIRY_BITS) &&
-           (p->features & AHC_WIDE) )
-      {
-        p->needwdtr |= (1<<tindex);
-        p->needwdtr_copy |= (1<<tindex);
-        p->transinfo[tindex].goal_width = p->transinfo[tindex].user_width;
-      }
-      else
-      {
-        p->needwdtr &= ~(1<<tindex);
-        p->needwdtr_copy &= ~(1<<tindex);
-        pause_sequencer(p);
-        aic7xxx_set_width(p, cmd->target, cmd->channel, cmd->lun,
-                          MSG_EXT_WDTR_BUS_8_BIT, (AHC_TRANS_ACTIVE |
-                                                   AHC_TRANS_GOAL |
-                                                   AHC_TRANS_CUR) );
-        unpause_sequencer(p, FALSE);
-      }
-      if ( (buffer[7] & SYNC_INQUIRY_BITS) &&
-            p->transinfo[tindex].user_offset )
-      {
-        p->transinfo[tindex].goal_period = p->transinfo[tindex].user_period;
-        p->transinfo[tindex].goal_options = p->transinfo[tindex].user_options;
-        if (p->features & AHC_ULTRA2)
-          p->transinfo[tindex].goal_offset = MAX_OFFSET_ULTRA2;
-        else if (p->transinfo[tindex].goal_width == MSG_EXT_WDTR_BUS_16_BIT)
-          p->transinfo[tindex].goal_offset = MAX_OFFSET_16BIT;
-        else
-          p->transinfo[tindex].goal_offset = MAX_OFFSET_8BIT;
-        if ( (((buffer[2] & SCSI_VERSION_BITS) == 3) ||
-               (buffer[56] & SCSI_DT_BIT) ||
-	       (p->dev_flags[tindex] & DEVICE_SCSI_3) ) &&
-               (p->transinfo[tindex].user_period <= 9) &&
-               (p->transinfo[tindex].user_options) )
-        {
-          p->needppr |= (1<<tindex);
-          p->needppr_copy |= (1<<tindex);
-          p->needsdtr &= ~(1<<tindex);
-          p->needsdtr_copy &= ~(1<<tindex);
-          p->needwdtr &= ~(1<<tindex);
-          p->needwdtr_copy &= ~(1<<tindex);
-          p->dev_flags[tindex] |= DEVICE_SCSI_3;
-        }
-        else
-        {
-          p->needsdtr |= (1<<tindex);
-          p->needsdtr_copy |= (1<<tindex);
-          p->transinfo[tindex].goal_period = 
-            MAX(10, p->transinfo[tindex].goal_period);
-          p->transinfo[tindex].goal_options = 0;
-        }
-      }
-      else
-      {
-        p->needsdtr &= ~(1<<tindex);
-        p->needsdtr_copy &= ~(1<<tindex);
-        p->transinfo[tindex].goal_period = 255;
-        p->transinfo[tindex].goal_offset = 0;
-        p->transinfo[tindex].goal_options = 0;
-      }
-      /*
-       * This is needed to work around a sequencer bug for now.  Regardless
-       * of the controller in use, if we have a Quantum drive, we need to
-       * limit the speed to 80MByte/sec.  As soon as I get a fixed version
-       * of the sequencer, this code will get yanked.
-       */
-      if(!strncmp(buffer + 8, "QUANTUM", 7) &&
-         p->transinfo[tindex].goal_options )
-      {
-        p->transinfo[tindex].goal_period = 
-          MAX(p->transinfo[tindex].goal_period, 10);
-        p->transinfo[tindex].goal_options = 0;
-        p->needppr &= ~(1<<tindex);
-        p->needppr_copy &= ~(1<<tindex);
-        p->needsdtr |= (1<<tindex);
-        p->needsdtr_copy |= (1<<tindex);
-        p->needwdtr |= (1<<tindex);
-        p->needwdtr_copy |= (1<<tindex);
-      }
-      /*
-       * Get the INQUIRY checksum.  We use this on Ultra 160/m
-       * and older devices both.  It allows us to drop speed on any bus type
-       * while at the same time giving us the needed domain validation for
-       * Ultra 160/m
-       *
-       * Note: We only get the checksum and set the SCANNED bit if this is
-       * one of our dtr commands.  If we don't do this, then we end up
-       * getting bad checksum results on the mid-level SCSI code's INQUIRY
-       * commands.
-       */
-      if(p->dev_dtr_cmnd[tindex] == cmd) {
-        unsigned int checksum = 0;
-        int *ibuffer;
-        int i=0;
-
-        ibuffer = (int *)buffer;
-        for( i = 0; i < (cmd->request_bufflen >> 2); i++)
-        {
-          checksum += ibuffer[i];
-        }
-        p->dev_checksum[tindex] = checksum;
-        p->dev_flags[tindex] |= DEVICE_SCANNED;
-        p->dev_flags[tindex] |= DEVICE_PRINT_DTR;
-      }
-#undef WIDE_INQUIRY_BITS
-#undef SYNC_INQUIRY_BITS
-#undef SCSI_VERSION_BITS
-#undef SCSI_DT_BIT
-    }
-  }
-  else if ((scb->flags & SCB_MSGOUT_BITS) != 0)
-  {
-    unsigned short mask;
-    int message_error = FALSE;
-
-    mask = 0x01 << tindex;
- 
-    /*
-     * Check to see if we get an invalid message or a message error
-     * after failing to negotiate a wide or sync transfer message.
-     */
-    if ((scb->flags & SCB_SENSE) && 
-          ((scb->cmd->sense_buffer[12] == 0x43) ||  /* INVALID_MESSAGE */
-          (scb->cmd->sense_buffer[12] == 0x49))) /* MESSAGE_ERROR  */
-    {
-      message_error = TRUE;
-    }
-
-    if (scb->flags & SCB_MSGOUT_WDTR)
-    {
-      p->dtr_pending &= ~mask;
-      if (message_error)
-      {
-        if ( (aic7xxx_verbose & VERBOSE_NEGOTIATION2) &&
-             (p->dev_flags[tindex] & DEVICE_PRINT_DTR) )
-        {
-          printk(INFO_LEAD "Device failed to complete Wide Negotiation "
-            "processing and\n", p->host_no, CTL_OF_SCB(scb));
-          printk(INFO_LEAD "returned a sense error code for invalid message, "
-            "disabling future\n", p->host_no, CTL_OF_SCB(scb));
-          printk(INFO_LEAD "Wide negotiation to this device.\n", p->host_no,
-            CTL_OF_SCB(scb));
-        }
-        p->needwdtr &= ~mask;
-        p->needwdtr_copy &= ~mask;
-      }
-    }
-    if (scb->flags & SCB_MSGOUT_SDTR)
-    {
-      p->dtr_pending &= ~mask;
-      if (message_error)
-      {
-        if ( (aic7xxx_verbose & VERBOSE_NEGOTIATION2) &&
-             (p->dev_flags[tindex] & DEVICE_PRINT_DTR) )
-        {
-          printk(INFO_LEAD "Device failed to complete Sync Negotiation "
-            "processing and\n", p->host_no, CTL_OF_SCB(scb));
-          printk(INFO_LEAD "returned a sense error code for invalid message, "
-            "disabling future\n", p->host_no, CTL_OF_SCB(scb));
-          printk(INFO_LEAD "Sync negotiation to this device.\n", p->host_no,
-            CTL_OF_SCB(scb));
-          p->dev_flags[tindex] &= ~DEVICE_PRINT_DTR;
-        }
-        p->needsdtr &= ~mask;
-        p->needsdtr_copy &= ~mask;
-      }
-    }
-    if (scb->flags & SCB_MSGOUT_PPR)
-    {
-      p->dtr_pending &= ~mask;
-      if(message_error)
-      {
-        if ( (aic7xxx_verbose & VERBOSE_NEGOTIATION2) &&
-             (p->dev_flags[tindex] & DEVICE_PRINT_DTR) )
-        {
-          printk(INFO_LEAD "Device failed to complete Parallel Protocol "
-            "Request processing and\n", p->host_no, CTL_OF_SCB(scb));
-          printk(INFO_LEAD "returned a sense error code for invalid message, "
-            "disabling future\n", p->host_no, CTL_OF_SCB(scb));
-          printk(INFO_LEAD "Parallel Protocol Request negotiation to this "
-            "device.\n", p->host_no, CTL_OF_SCB(scb));
-        }
-        /*
-         * Disable PPR negotiation and revert back to WDTR and SDTR setup
-         */
-        p->needppr &= ~mask;
-        p->needppr_copy &= ~mask;
-        p->needsdtr |= mask;
-        p->needsdtr_copy |= mask;
-        p->needwdtr |= mask;
-        p->needwdtr_copy |= mask;
-      }
-    }
-  }
-  queue_depth = p->dev_temp_queue_depth[tindex];
-  if (queue_depth >= p->dev_active_cmds[tindex])
-  {
-    scbp = scbq_remove_head(&p->delayed_scbs[tindex]);
-    if (scbp)
-    {
-      if (queue_depth == 1)
-      {
-        /*
-         * Give extra preference to untagged devices, such as CD-R devices
-         * This makes it more likely that a drive *won't* stuff up while
-         * waiting on data at a critical time, such as CD-R writing and
-         * audio CD ripping operations.  Should also benefit tape drives.
-         */
-        scbq_insert_head(&p->waiting_scbs, scbp);
-      }
-      else
-      {
-        scbq_insert_tail(&p->waiting_scbs, scbp);
-      }
-#ifdef AIC7XXX_VERBOSE_DEBUGGING
-      if (aic7xxx_verbose > 0xffff)
-        printk(INFO_LEAD "Moving SCB from delayed to waiting queue.\n",
-               p->host_no, CTL_OF_SCB(scbp));
-#endif
-      if (queue_depth > p->dev_active_cmds[tindex])
-      {
-        scbp = scbq_remove_head(&p->delayed_scbs[tindex]);
-        if (scbp)
-          scbq_insert_tail(&p->waiting_scbs, scbp);
-      }
-    }
-  }
-  if ( !(scb->tag_action) && (p->tagenable & (1<<tindex)) )
-  {
-    p->dev_temp_queue_depth[tindex] = p->dev_max_queue_depth[tindex];
-  }
-  p->dev_active_cmds[tindex]--;
-  p->activescbs--;
-
-  {
-    int actual;
-
-    /*
-     * XXX: we should actually know how much actually transferred
-     * XXX: for each command, but apparently that's too difficult.
-     * 
-     * We set a lower limit of 512 bytes on the transfer length.  We
-     * ignore anything less than this because we don't have a real
-     * reason to count it.  Read/Writes to tapes are usually about 20K
-     * and disks are a minimum of 512 bytes unless you want to count
-     * non-read/write commands (such as TEST_UNIT_READY) which we don't
-     */
-    actual = scb->sg_length;
-    if ((actual >= 512) && (((cmd->result >> 16) & 0xf) == DID_OK))
-    {
-      struct aic7xxx_xferstats *sp;
-#ifdef AIC7XXX_PROC_STATS
-      long *ptr;
-      int x;
-#endif /* AIC7XXX_PROC_STATS */
-
-      sp = &p->stats[TARGET_INDEX(cmd)];
-
-      /*
-       * For block devices, cmd->request.cmd is always == either READ or
-       * WRITE.  For character devices, this isn't always set properly, so
-       * we check data_cmnd[0].  This catches the conditions for st.c, but
-       * I'm still not sure if request.cmd is valid for sg devices.
-       */
-      if ( (cmd->request.cmd == WRITE) || (cmd->data_cmnd[0] == WRITE_6) ||
-           (cmd->data_cmnd[0] == WRITE_FILEMARKS) )
-      {
-        sp->w_total++;
-#ifdef AIC7XXX_VERBOSE_DEBUGGING
-        if ( (sp->w_total > 16) && (aic7xxx_verbose > 0xffff) )
-          aic7xxx_verbose &= 0xffff;
-#endif
-#ifdef AIC7XXX_PROC_STATS
-        ptr = sp->w_bins;
-#endif /* AIC7XXX_PROC_STATS */
-      }
-      else
-      {
-        sp->r_total++;
-#ifdef AIC7XXX_VERBOSE_DEBUGGING
-        if ( (sp->r_total > 16) && (aic7xxx_verbose > 0xffff) )
-          aic7xxx_verbose &= 0xffff;
-#endif
-#ifdef AIC7XXX_PROC_STATS
-        ptr = sp->r_bins;
-#endif /* AIC7XXX_PROC_STATS */
-      }
-#ifdef AIC7XXX_PROC_STATS
-      x = -11;
-      while(actual)
-      {
-        actual >>= 1;
-        x++;
-      }
-      if (x < 0)
-      {
-        ptr[0]++;
-      }
-      else if (x > 7)
-      {
-        ptr[7]++;
-      }
-      else
-      {
-        ptr[x]++;
-      }
-#endif /* AIC7XXX_PROC_STATS */
-    }
-  }
-  aic7xxx_free_scb(p, scb);
-  aic7xxx_queue_cmd_complete(p, cmd);
-
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_run_done_queue
- *
- * Description:
- *   Calls the aic7xxx_done() for the Scsi_Cmnd of each scb in the
- *   aborted list, and adds each scb to the free list.  If complete
- *   is TRUE, we also process the commands complete list.
- *-F*************************************************************************/
-static void
-aic7xxx_run_done_queue(struct aic7xxx_host *p, /*complete*/ int complete)
-{
-  struct aic7xxx_scb *scb;
-  int i, found = 0;
-
-  for (i = 0; i < p->scb_data->numscbs; i++)
-  {
-    scb = p->scb_data->scb_array[i];
-    if (scb->flags & SCB_QUEUED_FOR_DONE)
-    {
-      if (aic7xxx_verbose & (VERBOSE_ABORT_PROCESS | VERBOSE_RESET_PROCESS))
-        printk(INFO_LEAD "Aborting scb %d\n",
-             p->host_no, CTL_OF_SCB(scb), scb->hscb->tag);
-      found++;
-      aic7xxx_done(p, scb);
-    }
-  }
-  if (aic7xxx_verbose & (VERBOSE_ABORT_RETURN | VERBOSE_RESET_RETURN))
-  {
-    printk(INFO_LEAD "%d commands found and queued for "
-        "completion.\n", p->host_no, -1, -1, -1, found);
-  }
-  if (complete)
-  {
-    aic7xxx_done_cmds_complete(p);
-  }
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_abort_waiting_scb
- *
- * Description:
- *   Manipulate the waiting for selection list and return the
- *   scb that follows the one that we remove.
- *-F*************************************************************************/
-static unsigned char
-aic7xxx_abort_waiting_scb(struct aic7xxx_host *p, struct aic7xxx_scb *scb,
-    unsigned char scbpos, unsigned char prev)
-{
-  unsigned char curscb, next;
-
-  /*
-   * Select the SCB we want to abort and pull the next pointer out of it.
-   */
-  curscb = aic_inb(p, SCBPTR);
-  aic_outb(p, scbpos, SCBPTR);
-  next = aic_inb(p, SCB_NEXT);
-
-  aic7xxx_add_curscb_to_free_list(p);
-
-  /*
-   * Update the waiting list
-   */
-  if (prev == SCB_LIST_NULL)
-  {
-    /*
-     * First in the list
-     */
-    aic_outb(p, next, WAITING_SCBH);
-  }
-  else
-  {
-    /*
-     * Select the scb that pointed to us and update its next pointer.
-     */
-    aic_outb(p, prev, SCBPTR);
-    aic_outb(p, next, SCB_NEXT);
-  }
-  /*
-   * Point us back at the original scb position and inform the SCSI
-   * system that the command has been aborted.
-   */
-  aic_outb(p, curscb, SCBPTR);
-  return (next);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_search_qinfifo
- *
- * Description:
- *   Search the queue-in FIFO for matching SCBs and conditionally
- *   requeue.  Returns the number of matching SCBs.
- *-F*************************************************************************/
-static int
-aic7xxx_search_qinfifo(struct aic7xxx_host *p, int target, int channel,
-    int lun, unsigned char tag, int flags, int requeue,
-    volatile scb_queue_type *queue)
-{
-  int      found;
-  unsigned char qinpos, qintail;
-  struct aic7xxx_scb *scbp;
-
-  found = 0;
-  qinpos = aic_inb(p, QINPOS);
-  qintail = p->qinfifonext;
-
-  p->qinfifonext = qinpos;
-
-  while (qinpos != qintail)
-  {
-    scbp = p->scb_data->scb_array[p->qinfifo[qinpos++]];
-    if (aic7xxx_match_scb(p, scbp, target, channel, lun, tag))
-    {
-       /*
-        * We found an scb that needs to be removed.
-        */
-       if (requeue && (queue != NULL))
-       {
-         if (scbp->flags & SCB_WAITINGQ)
-         {
-           scbq_remove(queue, scbp);
-           scbq_remove(&p->waiting_scbs, scbp);
-           scbq_remove(&p->delayed_scbs[TARGET_INDEX(scbp->cmd)], scbp);
-           p->dev_active_cmds[TARGET_INDEX(scbp->cmd)]++;
-           p->activescbs++;
-         }
-         scbq_insert_tail(queue, scbp);
-         p->dev_active_cmds[TARGET_INDEX(scbp->cmd)]--;
-         p->activescbs--;
-         scbp->flags |= SCB_WAITINGQ;
-         if ( !(scbp->tag_action & TAG_ENB) )
-         {
-           aic7xxx_index_busy_target(p, scbp->hscb->target_channel_lun,
-             TRUE);
-         }
-       }
-       else if (requeue)
-       {
-         p->qinfifo[p->qinfifonext++] = scbp->hscb->tag;
-       }
-       else
-       {
-        /*
-         * Preserve any SCB_RECOVERY_SCB flags on this scb then set the
-         * flags we were called with, presumeably so aic7xxx_run_done_queue
-         * can find this scb
-         */
-         scbp->flags = flags | (scbp->flags & SCB_RECOVERY_SCB);
-         if (aic7xxx_index_busy_target(p, scbp->hscb->target_channel_lun,
-                                       FALSE) == scbp->hscb->tag)
-         {
-           aic7xxx_index_busy_target(p, scbp->hscb->target_channel_lun,
-             TRUE);
-         }
-       }
-       found++;
-    }
-    else
-    {
-      p->qinfifo[p->qinfifonext++] = scbp->hscb->tag;
-    }
-  }
-  /*
-   * Now that we've done the work, clear out any left over commands in the
-   * qinfifo and update the KERNEL_QINPOS down on the card.
-   *
-   *  NOTE: This routine expect the sequencer to already be paused when
-   *        it is run....make sure it's that way!
-   */
-  qinpos = p->qinfifonext;
-  while(qinpos != qintail)
-  {
-    p->qinfifo[qinpos++] = SCB_LIST_NULL;
-  }
-  if (p->features & AHC_QUEUE_REGS)
-    aic_outb(p, p->qinfifonext, HNSCB_QOFF);
-  else
-    aic_outb(p, p->qinfifonext, KERNEL_QINPOS);
-
-  return (found);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_scb_on_qoutfifo
- *
- * Description:
- *   Is the scb that was passed to us currently on the qoutfifo?
- *-F*************************************************************************/
-static int
-aic7xxx_scb_on_qoutfifo(struct aic7xxx_host *p, struct aic7xxx_scb *scb)
-{
-  int i=0;
-
-  while(p->qoutfifo[(p->qoutfifonext + i) & 0xff ] != SCB_LIST_NULL)
-  {
-    if(p->qoutfifo[(p->qoutfifonext + i) & 0xff ] == scb->hscb->tag)
-      return TRUE;
-    else
-      i++;
-  }
-  return FALSE;
-}
-
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_reset_device
- *
- * Description:
- *   The device at the given target/channel has been reset.  Abort
- *   all active and queued scbs for that target/channel.  This function
- *   need not worry about linked next pointers because if was a MSG_ABORT_TAG
- *   then we had a tagged command (no linked next), if it was MSG_ABORT or
- *   MSG_BUS_DEV_RESET then the device won't know about any commands any more
- *   and no busy commands will exist, and if it was a bus reset, then nothing
- *   knows about any linked next commands any more.  In all cases, we don't
- *   need to worry about the linked next or busy scb, we just need to clear
- *   them.
- *-F*************************************************************************/
-static void
-aic7xxx_reset_device(struct aic7xxx_host *p, int target, int channel,
-                     int lun, unsigned char tag)
-{
-  struct aic7xxx_scb *scbp;
-  unsigned char active_scb, tcl;
-  int i = 0, j, init_lists = FALSE;
-
-  /*
-   * Restore this when we're done
-   */
-  active_scb = aic_inb(p, SCBPTR);
-
-  if (aic7xxx_verbose & (VERBOSE_RESET_PROCESS | VERBOSE_ABORT_PROCESS))
-    printk(INFO_LEAD "Reset device, active_scb %d\n",
-         p->host_no, channel, target, lun, active_scb);
-  /*
-   * Deal with the busy target and linked next issues.
-   */
-  {
-    int min_target, max_target;
-    struct aic7xxx_scb *scbp, *prev_scbp;
-
-    /* Make all targets 'relative' to bus A. */
-    if (target == ALL_TARGETS)
-    {
-      switch (channel)
-      {
-        case 0:
-                 min_target = 0;
-                 max_target = (p->features & AHC_WIDE) ? 15 : 7;
-                 break;
-        case 1:
-                 min_target = 8;
-                 max_target = 15;
-                 break;
-        case ALL_CHANNELS:
-        default:
-                 min_target = 0;
-                 max_target = (p->features & (AHC_TWIN|AHC_WIDE)) ? 15 : 7;
-                 break;
-      }
-    }
-    else
-    { 
-      min_target = target | (channel << 3);
-      max_target = min_target;
-    }
-
-
-    for (i = min_target; i <= max_target; i++)
-    {
-      if ( i == p->scsi_id )
-      {
-        continue;
-      }
-      if (aic7xxx_verbose & (VERBOSE_ABORT_PROCESS | VERBOSE_RESET_PROCESS))
-        printk(INFO_LEAD "Cleaning up status information "
-          "and delayed_scbs.\n", p->host_no, channel, i, lun);
-      p->dev_flags[i] &= ~(BUS_DEVICE_RESET_PENDING | DEVICE_PARITY_ERROR);
-      if ( tag == SCB_LIST_NULL )
-      {
-        p->dev_flags[i] |= DEVICE_PRINT_DTR | DEVICE_RESET_DELAY;
-        p->dev_expires[i] = jiffies + (4 * HZ);
-        p->dev_timer_active |= (0x01 << i);
-        p->dev_last_queue_full_count[i] = 0;
-        p->dev_last_queue_full[i] = 0;
-        p->dev_temp_queue_depth[i] =
-          p->dev_max_queue_depth[i];
-      }
-      for(j=0; j<MAX_LUNS; j++)
-      {
-        if (channel == 1)
-          tcl = ((i << 4) & 0x70) | (channel << 3) | j;
-        else
-          tcl = (i << 4) | (channel << 3) | j;
-        if ( (aic7xxx_index_busy_target(p, tcl, FALSE) == tag) ||
-             (tag == SCB_LIST_NULL) )
-          aic7xxx_index_busy_target(p, tcl, /* unbusy */ TRUE);
-      }
-      j = 0; 
-      prev_scbp = NULL; 
-      scbp = p->delayed_scbs[i].head;
-      while ( (scbp != NULL) && (j++ <= (p->scb_data->numscbs + 1)) )
-      {
-        prev_scbp = scbp;
-        scbp = scbp->q_next;
-        if ( prev_scbp == scbp )
-        {
-          if (aic7xxx_verbose & (VERBOSE_ABORT | VERBOSE_RESET))
-            printk(WARN_LEAD "Yikes!! scb->q_next == scb "
-              "in the delayed_scbs queue!\n", p->host_no, channel, i, lun);
-          scbp = NULL;
-          prev_scbp->q_next = NULL;
-          p->delayed_scbs[i].tail = prev_scbp;
-        }
-        if (aic7xxx_match_scb(p, prev_scbp, target, channel, lun, tag))
-        {
-          scbq_remove(&p->delayed_scbs[i], prev_scbp);
-          if (prev_scbp->flags & SCB_WAITINGQ)
-          {
-            p->dev_active_cmds[i]++;
-            p->activescbs++;
-          }
-          prev_scbp->flags &= ~(SCB_ACTIVE | SCB_WAITINGQ);
-          prev_scbp->flags |= SCB_RESET | SCB_QUEUED_FOR_DONE;
-        }
-      }
-      if ( j > (p->scb_data->maxscbs + 1) )
-      {
-        if (aic7xxx_verbose & (VERBOSE_ABORT | VERBOSE_RESET))
-          printk(WARN_LEAD "Yikes!! There's a loop in the "
-            "delayed_scbs queue!\n", p->host_no, channel, i, lun);
-        scbq_init(&p->delayed_scbs[i]);
-      }
-      if ( !(p->dev_timer_active & (0x01 << MAX_TARGETS)) ||
-            time_after_eq(p->dev_timer.expires, p->dev_expires[i]) )
-      {
-        del_timer(&p->dev_timer);
-        p->dev_timer.expires = p->dev_expires[i];
-        add_timer(&p->dev_timer);
-        p->dev_timer_active |= (0x01 << MAX_TARGETS);
-      }
-    }
-  }
-
-  if (aic7xxx_verbose & (VERBOSE_ABORT_PROCESS | VERBOSE_RESET_PROCESS))
-    printk(INFO_LEAD "Cleaning QINFIFO.\n", p->host_no, channel, target, lun );
-  aic7xxx_search_qinfifo(p, target, channel, lun, tag,
-      SCB_RESET | SCB_QUEUED_FOR_DONE, /* requeue */ FALSE, NULL);
-
-/*
- *  Search the waiting_scbs queue for matches, this catches any SCB_QUEUED
- *  ABORT/RESET commands.
- */
-  if (aic7xxx_verbose & (VERBOSE_ABORT_PROCESS | VERBOSE_RESET_PROCESS))
-    printk(INFO_LEAD "Cleaning waiting_scbs.\n", p->host_no, channel,
-      target, lun );
-  {
-    struct aic7xxx_scb *scbp, *prev_scbp;
-
-    j = 0; 
-    prev_scbp = NULL; 
-    scbp = p->waiting_scbs.head;
-    while ( (scbp != NULL) && (j++ <= (p->scb_data->numscbs + 1)) )
-    {
-      prev_scbp = scbp;
-      scbp = scbp->q_next;
-      if ( prev_scbp == scbp )
-      {
-        if (aic7xxx_verbose & (VERBOSE_ABORT | VERBOSE_RESET))
-          printk(WARN_LEAD "Yikes!! scb->q_next == scb "
-            "in the waiting_scbs queue!\n", p->host_no, CTL_OF_SCB(scbp));
-        scbp = NULL;
-        prev_scbp->q_next = NULL;
-        p->waiting_scbs.tail = prev_scbp;
-      }
-      if (aic7xxx_match_scb(p, prev_scbp, target, channel, lun, tag))
-      {
-        scbq_remove(&p->waiting_scbs, prev_scbp);
-        if (prev_scbp->flags & SCB_WAITINGQ)
-        {
-          p->dev_active_cmds[TARGET_INDEX(prev_scbp->cmd)]++;
-          p->activescbs++;
-        }
-        prev_scbp->flags &= ~(SCB_ACTIVE | SCB_WAITINGQ);
-        prev_scbp->flags |= SCB_RESET | SCB_QUEUED_FOR_DONE;
-      }
-    }
-    if ( j > (p->scb_data->maxscbs + 1) )
-    {
-      if (aic7xxx_verbose & (VERBOSE_ABORT | VERBOSE_RESET))
-        printk(WARN_LEAD "Yikes!! There's a loop in the "
-          "waiting_scbs queue!\n", p->host_no, channel, target, lun);
-      scbq_init(&p->waiting_scbs);
-    }
-  }
-
-
-  /*
-   * Search waiting for selection list.
-   */
-  if (aic7xxx_verbose & (VERBOSE_ABORT_PROCESS | VERBOSE_RESET_PROCESS))
-    printk(INFO_LEAD "Cleaning waiting for selection "
-      "list.\n", p->host_no, channel, target, lun);
-  {
-    unsigned char next, prev, scb_index;
-
-    next = aic_inb(p, WAITING_SCBH);  /* Start at head of list. */
-    prev = SCB_LIST_NULL;
-    j = 0;
-    while ( (next != SCB_LIST_NULL) && (j++ <= (p->scb_data->maxscbs + 1)) )
-    {
-      aic_outb(p, next, SCBPTR);
-      scb_index = aic_inb(p, SCB_TAG);
-      if (scb_index >= p->scb_data->numscbs)
-      {
-       /*
-        * No aic7xxx_verbose check here.....we want to see this since it
-        * means either the kernel driver or the sequencer screwed things up
-        */
-        printk(WARN_LEAD "Waiting List inconsistency; SCB index=%d, "
-          "numscbs=%d\n", p->host_no, channel, target, lun, scb_index,
-          p->scb_data->numscbs);
-        next = aic_inb(p, SCB_NEXT);
-        aic7xxx_add_curscb_to_free_list(p);
-      }
-      else
-      {
-        scbp = p->scb_data->scb_array[scb_index];
-        if (aic7xxx_match_scb(p, scbp, target, channel, lun, tag))
-        {
-          next = aic7xxx_abort_waiting_scb(p, scbp, next, prev);
-          if (scbp->flags & SCB_WAITINGQ)
-          {
-            p->dev_active_cmds[TARGET_INDEX(scbp->cmd)]++;
-            p->activescbs++;
-          }
-          scbp->flags &= ~(SCB_ACTIVE | SCB_WAITINGQ);
-          scbp->flags |= SCB_RESET | SCB_QUEUED_FOR_DONE;
-          if (prev == SCB_LIST_NULL)
-          {
-            /*
-             * This is either the first scb on the waiting list, or we
-             * have already yanked the first and haven't left any behind.
-             * Either way, we need to turn off the selection hardware if
-             * it isn't already off.
-             */
-            aic_outb(p, aic_inb(p, SCSISEQ) & ~ENSELO, SCSISEQ);
-            aic_outb(p, CLRSELTIMEO, CLRSINT1);
-          }
-        }
-        else
-        {
-          prev = next;
-          next = aic_inb(p, SCB_NEXT);
-        }
-      }
-    }
-    if ( j > (p->scb_data->maxscbs + 1) )
-    {
-      printk(WARN_LEAD "Yikes!!  There is a loop in the waiting for "
-        "selection list!\n", p->host_no, channel, target, lun);
-      init_lists = TRUE;
-    }
-  }
-
-  /*
-   * Go through disconnected list and remove any entries we have queued
-   * for completion, zeroing their control byte too.
-   */
-  if (aic7xxx_verbose & (VERBOSE_ABORT_PROCESS | VERBOSE_RESET_PROCESS))
-    printk(INFO_LEAD "Cleaning disconnected scbs "
-      "list.\n", p->host_no, channel, target, lun);
-  if (p->flags & AHC_PAGESCBS)
-  {
-    unsigned char next, prev, scb_index;
-
-    next = aic_inb(p, DISCONNECTED_SCBH);
-    prev = SCB_LIST_NULL;
-    j = 0;
-    while ( (next != SCB_LIST_NULL) && (j++ <= (p->scb_data->maxscbs + 1)) )
-    {
-      aic_outb(p, next, SCBPTR);
-      scb_index = aic_inb(p, SCB_TAG);
-      if (scb_index > p->scb_data->numscbs)
-      {
-        printk(WARN_LEAD "Disconnected List inconsistency; SCB index=%d, "
-          "numscbs=%d\n", p->host_no, channel, target, lun, scb_index,
-          p->scb_data->numscbs);
-        next = aic7xxx_rem_scb_from_disc_list(p, next, prev);
-      }
-      else
-      {
-        scbp = p->scb_data->scb_array[scb_index];
-        if (aic7xxx_match_scb(p, scbp, target, channel, lun, tag))
-        {
-          next = aic7xxx_rem_scb_from_disc_list(p, next, prev);
-          if (scbp->flags & SCB_WAITINGQ)
-          {
-            p->dev_active_cmds[TARGET_INDEX(scbp->cmd)]++;
-            p->activescbs++;
-          }
-          scbp->flags &= ~(SCB_ACTIVE | SCB_WAITINGQ);
-          scbp->flags |= SCB_RESET | SCB_QUEUED_FOR_DONE;
-          scbp->hscb->control = 0;
-        }
-        else
-        {
-          prev = next;
-          next = aic_inb(p, SCB_NEXT);
-        }
-      }
-    }
-    if ( j > (p->scb_data->maxscbs + 1) )
-    {
-      printk(WARN_LEAD "Yikes!!  There is a loop in the disconnected list!\n",
-        p->host_no, channel, target, lun);
-      init_lists = TRUE;
-    }
-  }
-
-  /*
-   * Walk the free list making sure no entries on the free list have
-   * a valid SCB_TAG value or SCB_CONTROL byte.
-   */
-  if (p->flags & AHC_PAGESCBS)
-  {
-    unsigned char next;
-
-    j = 0;
-    next = aic_inb(p, FREE_SCBH);
-    if ( (next >= p->scb_data->maxhscbs) && (next != SCB_LIST_NULL) )
-    {
-      printk(WARN_LEAD "Bogus FREE_SCBH!.\n", p->host_no, channel,
-        target, lun);
-      init_lists = TRUE;
-      next = SCB_LIST_NULL;
-    }
-    while ( (next != SCB_LIST_NULL) && (j++ <= (p->scb_data->maxscbs + 1)) )
-    {
-      aic_outb(p, next, SCBPTR);
-      if (aic_inb(p, SCB_TAG) < p->scb_data->numscbs)
-      {
-        printk(WARN_LEAD "Free list inconsistency!.\n", p->host_no, channel,
-          target, lun);
-        init_lists = TRUE;
-        next = SCB_LIST_NULL;
-      }
-      else
-      {
-        aic_outb(p, SCB_LIST_NULL, SCB_TAG);
-        aic_outb(p, 0, SCB_CONTROL);
-        next = aic_inb(p, SCB_NEXT);
-      }
-    }
-    if ( j > (p->scb_data->maxscbs + 1) )
-    {
-      printk(WARN_LEAD "Yikes!!  There is a loop in the free list!\n",
-        p->host_no, channel, target, lun);
-      init_lists = TRUE;
-    }
-  }
-
-  /*
-   * Go through the hardware SCB array looking for commands that
-   * were active but not on any list.
-   */
-  if (init_lists)
-  {
-    aic_outb(p, SCB_LIST_NULL, FREE_SCBH);
-    aic_outb(p, SCB_LIST_NULL, WAITING_SCBH);
-    aic_outb(p, SCB_LIST_NULL, DISCONNECTED_SCBH);
-  }
-  for (i = p->scb_data->maxhscbs - 1; i >= 0; i--)
-  {
-    unsigned char scbid;
-
-    aic_outb(p, i, SCBPTR);
-    if (init_lists)
-    {
-      aic_outb(p, SCB_LIST_NULL, SCB_TAG);
-      aic_outb(p, SCB_LIST_NULL, SCB_NEXT);
-      aic_outb(p, 0, SCB_CONTROL);
-      aic7xxx_add_curscb_to_free_list(p);
-    }
-    else
-    {
-      scbid = aic_inb(p, SCB_TAG);
-      if (scbid < p->scb_data->numscbs)
-      {
-        scbp = p->scb_data->scb_array[scbid];
-        if (aic7xxx_match_scb(p, scbp, target, channel, lun, tag))
-        {
-          aic_outb(p, 0, SCB_CONTROL);
-          aic_outb(p, SCB_LIST_NULL, SCB_TAG);
-          aic7xxx_add_curscb_to_free_list(p);
-        }
-      }
-    }
-  }
-
-  /*
-   * Go through the entire SCB array now and look for commands for
-   * for this target that are stillactive.  These are other (most likely
-   * tagged) commands that were disconnected when the reset occurred.
-   * Any commands we find here we know this about, it wasn't on any queue,
-   * it wasn't in the qinfifo, it wasn't in the disconnected or waiting
-   * lists, so it really must have been a paged out SCB.  In that case,
-   * we shouldn't need to bother with updating any counters, just mark
-   * the correct flags and go on.
-   */
-  for (i = 0; i < p->scb_data->numscbs; i++)
-  {
-    scbp = p->scb_data->scb_array[i];
-    if ((scbp->flags & SCB_ACTIVE) &&
-        aic7xxx_match_scb(p, scbp, target, channel, lun, tag) &&
-        !aic7xxx_scb_on_qoutfifo(p, scbp))
-    {
-      if (scbp->flags & SCB_WAITINGQ)
-      {
-        scbq_remove(&p->waiting_scbs, scbp);
-        scbq_remove(&p->delayed_scbs[TARGET_INDEX(scbp->cmd)], scbp);
-        p->dev_active_cmds[TARGET_INDEX(scbp->cmd)]++;
-        p->activescbs++;
-      }
-      scbp->flags |= SCB_RESET | SCB_QUEUED_FOR_DONE;
-      scbp->flags &= ~(SCB_ACTIVE | SCB_WAITINGQ);
-    }
-  }
-
-  aic_outb(p, active_scb, SCBPTR);
-}
-
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_clear_intstat
- *
- * Description:
- *   Clears the interrupt status.
- *-F*************************************************************************/
-static void
-aic7xxx_clear_intstat(struct aic7xxx_host *p)
-{
-  /* Clear any interrupt conditions this may have caused. */
-  aic_outb(p, CLRSELDO | CLRSELDI | CLRSELINGO, CLRSINT0);
-  aic_outb(p, CLRSELTIMEO | CLRATNO | CLRSCSIRSTI | CLRBUSFREE | CLRSCSIPERR |
-       CLRPHASECHG | CLRREQINIT, CLRSINT1);
-  aic_outb(p, CLRSCSIINT | CLRSEQINT | CLRBRKADRINT | CLRPARERR, CLRINT);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_reset_current_bus
- *
- * Description:
- *   Reset the current SCSI bus.
- *-F*************************************************************************/
-static void
-aic7xxx_reset_current_bus(struct aic7xxx_host *p)
-{
-
-  /* Disable reset interrupts. */
-  aic_outb(p, aic_inb(p, SIMODE1) & ~ENSCSIRST, SIMODE1);
-
-  /* Turn off the bus' current operations, after all, we shouldn't have any
-   * valid commands left to cause a RSELI and SELO once we've tossed the
-   * bus away with this reset, so we might as well shut down the sequencer
-   * until the bus is restarted as oppossed to saving the current settings
-   * and restoring them (which makes no sense to me). */
-
-  /* Turn on the bus reset. */
-  aic_outb(p, aic_inb(p, SCSISEQ) | SCSIRSTO, SCSISEQ);
-  while ( (aic_inb(p, SCSISEQ) & SCSIRSTO) == 0)
-    mdelay(5);
-
-  /*
-   * Some of the new Ultra2 chipsets need a longer delay after a chip
-   * reset than just the init setup creates, so we have to delay here
-   * before we go into a reset in order to make the chips happy.
-   */
-  if (p->features & AHC_ULTRA2)
-    mdelay(250);
-  else
-    mdelay(50);
-
-  /* Turn off the bus reset. */
-  aic_outb(p, 0, SCSISEQ);
-  mdelay(10);
-
-  aic7xxx_clear_intstat(p);
-  /* Re-enable reset interrupts. */
-  aic_outb(p, aic_inb(p, SIMODE1) | ENSCSIRST, SIMODE1);
-
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_reset_channel
- *
- * Description:
- *   Reset the channel.
- *-F*************************************************************************/
-static void
-aic7xxx_reset_channel(struct aic7xxx_host *p, int channel, int initiate_reset)
-{
-  unsigned long offset_min, offset_max;
-  unsigned char sblkctl;
-  int cur_channel;
-
-  if (aic7xxx_verbose & VERBOSE_RESET_PROCESS)
-    printk(INFO_LEAD "Reset channel called, %s initiate reset.\n",
-      p->host_no, channel, -1, -1, (initiate_reset==TRUE) ? "will" : "won't" );
-
-
-  if (channel == 1)
-  {
-    p->needsdtr |= (p->needsdtr_copy & 0xFF00);
-    p->dtr_pending &= 0x00FF;
-    offset_min = 8;
-    offset_max = 16;
-  }
-  else
-  {
-    if (p->features & AHC_TWIN)
-    {
-      /* Channel A */
-      p->needsdtr |= (p->needsdtr_copy & 0x00FF);
-      p->dtr_pending &= 0xFF00;
-      offset_min = 0;
-      offset_max = 8;
-    }
-    else
-    {
-      p->needppr = p->needppr_copy;
-      p->needsdtr = p->needsdtr_copy;
-      p->needwdtr = p->needwdtr_copy;
-      p->dtr_pending = 0x0;
-      offset_min = 0;
-      if (p->features & AHC_WIDE)
-      {
-        offset_max = 16;
-      }
-      else
-      {
-        offset_max = 8;
-      }
-    }
-  }
-
-  while (offset_min < offset_max)
-  {
-    /*
-     * Revert to async/narrow transfers until we renegotiate.
-     */
-    aic_outb(p, 0, TARG_SCSIRATE + offset_min);
-    if (p->features & AHC_ULTRA2)
-    {
-      aic_outb(p, 0, TARG_OFFSET + offset_min);
-    }
-    offset_min++;
-  }
-
-  /*
-   * Reset the bus and unpause/restart the controller
-   */
-  sblkctl = aic_inb(p, SBLKCTL);
-  if ( (p->chip & AHC_CHIPID_MASK) == AHC_AIC7770 )
-    cur_channel = (sblkctl & SELBUSB) >> 3;
-  else
-    cur_channel = 0;
-  if ( (cur_channel != channel) && (p->features & AHC_TWIN) )
-  {
-    /*
-     * Case 1: Command for another bus is active
-     */
-    if (aic7xxx_verbose & VERBOSE_RESET_PROCESS)
-      printk(INFO_LEAD "Stealthily resetting idle channel.\n", p->host_no,
-        channel, -1, -1);
-    /*
-     * Stealthily reset the other bus without upsetting the current bus.
-     */
-    aic_outb(p, sblkctl ^ SELBUSB, SBLKCTL);
-    aic_outb(p, aic_inb(p, SIMODE1) & ~ENBUSFREE, SIMODE1);
-    if (initiate_reset)
-    {
-      aic7xxx_reset_current_bus(p);
-    }
-    aic_outb(p, aic_inb(p, SCSISEQ) & (ENSELI|ENRSELI|ENAUTOATNP), SCSISEQ);
-    aic7xxx_clear_intstat(p);
-    aic_outb(p, sblkctl, SBLKCTL);
-  }
-  else
-  {
-    /*
-     * Case 2: A command from this bus is active or we're idle.
-     */
-    if (aic7xxx_verbose & VERBOSE_RESET_PROCESS)
-      printk(INFO_LEAD "Resetting currently active channel.\n", p->host_no,
-        channel, -1, -1);
-    aic_outb(p, aic_inb(p, SIMODE1) & ~(ENBUSFREE|ENREQINIT),
-      SIMODE1);
-    p->flags &= ~AHC_HANDLING_REQINITS;
-    p->msg_type = MSG_TYPE_NONE;
-    p->msg_len = 0;
-    if (initiate_reset)
-    {
-      aic7xxx_reset_current_bus(p);
-    }
-    aic_outb(p, aic_inb(p, SCSISEQ) & (ENSELI|ENRSELI|ENAUTOATNP), SCSISEQ);
-    aic7xxx_clear_intstat(p);
-  }
-  if (aic7xxx_verbose & VERBOSE_RESET_RETURN)
-    printk(INFO_LEAD "Channel reset\n", p->host_no, channel, -1, -1);
-  /*
-   * Clean up all the state information for the pending transactions
-   * on this bus.
-   */
-  aic7xxx_reset_device(p, ALL_TARGETS, channel, ALL_LUNS, SCB_LIST_NULL);
-
-  if ( !(p->features & AHC_TWIN) )
-  {
-    restart_sequencer(p);
-  }
-
-  return;
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_run_waiting_queues
- *
- * Description:
- *   Scan the awaiting_scbs queue downloading and starting as many
- *   scbs as we can.
- *-F*************************************************************************/
-static void
-aic7xxx_run_waiting_queues(struct aic7xxx_host *p)
-{
-  struct aic7xxx_scb *scb;
-  int tindex;
-  int sent;
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,1,95)
-  unsigned long cpu_flags = 0;
-#endif
-
-
-  if (p->waiting_scbs.head == NULL)
-    return;
-
-  sent = 0;
-
-  /*
-   * First handle SCBs that are waiting but have been assigned a slot.
-   */
-  DRIVER_LOCK
-  while ((scb = scbq_remove_head(&p->waiting_scbs)) != NULL)
-  {
-    tindex = TARGET_INDEX(scb->cmd);
-    if ( !scb->tag_action && (p->tagenable & (1<<tindex)) )
-    {
-      p->dev_temp_queue_depth[tindex] = 1;
-    }
-    if ( (p->dev_active_cmds[tindex] >=
-          p->dev_temp_queue_depth[tindex]) ||
-         (p->dev_flags[tindex] & (DEVICE_RESET_DELAY|DEVICE_WAS_BUSY)) ||
-         (p->flags & AHC_RESET_DELAY) )
-    {
-      scbq_insert_tail(&p->delayed_scbs[tindex], scb);
-    }
-    else
-    {
-        scb->flags &= ~SCB_WAITINGQ;
-        p->dev_active_cmds[tindex]++;
-        p->activescbs++;
-        if ( !(scb->tag_action) )
-        {
-          aic7xxx_busy_target(p, scb);
-        }
-        p->qinfifo[p->qinfifonext++] = scb->hscb->tag;
-        sent++;
-    }
-  }
-  if (sent)
-  {
-    if (p->features & AHC_QUEUE_REGS)
-      aic_outb(p, p->qinfifonext, HNSCB_QOFF);
-    else
-    {
-      pause_sequencer(p);
-      aic_outb(p, p->qinfifonext, KERNEL_QINPOS);
-      unpause_sequencer(p, FALSE);
-    }
-    if (p->activescbs > p->max_activescbs)
-      p->max_activescbs = p->activescbs;
-  }
-  DRIVER_UNLOCK
-}
-
-#ifdef CONFIG_PCI
-
-#define  DPE 0x80
-#define  SSE 0x40
-#define  RMA 0x20
-#define  RTA 0x10
-#define  STA 0x08
-#define  DPR 0x01
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_pci_intr
- *
- * Description:
- *   Check the scsi card for PCI errors and clear the interrupt
- *
- *   NOTE: If you don't have this function and a 2940 card encounters
- *         a PCI error condition, the machine will end up locked as the
- *         interrupt handler gets slammed with non-stop PCI error interrupts
- *-F*************************************************************************/
-static void
-aic7xxx_pci_intr(struct aic7xxx_host *p)
-{
-  unsigned char status1;
-
-#if LINUX_VERSION_CODE > KERNEL_VERSION(2,1,92)
-  pci_read_config_byte(p->pdev, PCI_STATUS + 1, &status1);
-#else
-  pcibios_read_config_byte(p->pci_bus, p->pci_device_fn,
-                           PCI_STATUS + 1, &status1);
-#endif
-
-  if ( (status1 & DPE) && (aic7xxx_verbose & VERBOSE_MINOR_ERROR) )
-    printk(WARN_LEAD "Data Parity Error during PCI address or PCI write"
-      "phase.\n", p->host_no, -1, -1, -1);
-  if ( (status1 & SSE) && (aic7xxx_verbose & VERBOSE_MINOR_ERROR) )
-    printk(WARN_LEAD "Signal System Error Detected\n", p->host_no,
-      -1, -1, -1);
-  if ( (status1 & RMA) && (aic7xxx_verbose & VERBOSE_MINOR_ERROR) )
-    printk(WARN_LEAD "Received a PCI Master Abort\n", p->host_no,
-      -1, -1, -1);
-  if ( (status1 & RTA) && (aic7xxx_verbose & VERBOSE_MINOR_ERROR) )
-    printk(WARN_LEAD "Received a PCI Target Abort\n", p->host_no,
-      -1, -1, -1);
-  if ( (status1 & STA) && (aic7xxx_verbose & VERBOSE_MINOR_ERROR) )
-    printk(WARN_LEAD "Signaled a PCI Target Abort\n", p->host_no,
-      -1, -1, -1);
-  if ( (status1 & DPR) && (aic7xxx_verbose & VERBOSE_MINOR_ERROR) )
-    printk(WARN_LEAD "Data Parity Error has been reported via PCI pin "
-      "PERR#\n", p->host_no, -1, -1, -1);
-  
-#if LINUX_VERSION_CODE > KERNEL_VERSION(2,1,92)
-  pci_write_config_byte(p->pdev, PCI_STATUS + 1, status1);
-#else
-  pcibios_write_config_byte(p->pci_bus, p->pci_device_fn,
-                            PCI_STATUS + 1, status1);
-#endif
-  if (status1 & (DPR|RMA|RTA))
-    aic_outb(p,  CLRPARERR, CLRINT);
-
-  if ( (aic7xxx_panic_on_abort) && (p->spurious_int > 500) )
-    aic7xxx_panic_abort(p, NULL);
-
-}
-#endif /* CONFIG_PCI */
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_timer
- *
- * Description:
- *   Take expired extries off of delayed queues and place on waiting queue
- *   then run waiting queue to start commands.
- ***************************************************************************/
-static void
-aic7xxx_timer(struct aic7xxx_host *p)
-{
-  int i, j;
-  unsigned long cpu_flags = 0;
-  struct aic7xxx_scb *scb;
-
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,1,95)   
-  DRIVER_LOCK
-#else
-  spin_lock_irqsave(&io_request_lock, cpu_flags);
-#endif
-  p->dev_timer_active &= ~(0x01 << MAX_TARGETS);
-  if ( (p->dev_timer_active & (0x01 << p->scsi_id)) &&
-       time_after_eq(jiffies, p->dev_expires[p->scsi_id]) )
-  {
-    p->flags &= ~AHC_RESET_DELAY;
-    p->dev_timer_active &= ~(0x01 << p->scsi_id);
-  }
-  for(i=0; i<MAX_TARGETS; i++)
-  {
-    if ( (i != p->scsi_id) &&
-         (p->dev_timer_active & (0x01 << i)) &&
-         time_after_eq(jiffies, p->dev_expires[i]) )
-    {
-      p->dev_timer_active &= ~(0x01 << i);
-      p->dev_flags[i] &= ~(DEVICE_RESET_DELAY|DEVICE_WAS_BUSY);
-      p->dev_temp_queue_depth[i] =  p->dev_max_queue_depth[i];
-      j = 0;
-      while ( ((scb = scbq_remove_head(&p->delayed_scbs[i])) != NULL) &&
-              (j++ < p->scb_data->numscbs) )
-      {
-        scbq_insert_tail(&p->waiting_scbs, scb);
-      }
-      if (j == p->scb_data->numscbs)
-      {
-        printk(INFO_LEAD "timer: Yikes, loop in delayed_scbs list.\n",
-          p->host_no, 0, i, -1);
-        scbq_init(&p->delayed_scbs[i]);
-        scbq_init(&p->waiting_scbs);
-        /*
-         * Well, things are screwed now, wait for a reset to clean the junk
-         * out.
-         */
-      }
-    }
-    else if ( p->dev_timer_active & (0x01 << i) )
-    {
-      if ( p->dev_timer_active & (0x01 << MAX_TARGETS) )
-      {
-        if ( time_after_eq(p->dev_timer.expires, p->dev_expires[i]) )
-        {
-          p->dev_timer.expires = p->dev_expires[i];
-        }
-      }
-      else
-      {
-        p->dev_timer.expires = p->dev_expires[i];
-        p->dev_timer_active |= (0x01 << MAX_TARGETS);
-      }
-    }
-  }
-  if ( p->dev_timer_active & (0x01 << MAX_TARGETS) )
-  {
-    add_timer(&p->dev_timer);
-  }
-
-  aic7xxx_run_waiting_queues(p);
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,1,95)   
-  DRIVER_UNLOCK
-#else
-  spin_unlock_irqrestore(&io_request_lock, cpu_flags);
-#endif
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_construct_ppr
- *
- * Description:
- *   Build up a Parallel Protocol Request message for use with SCSI-3
- *   devices.
- *-F*************************************************************************/
-static void
-aic7xxx_construct_ppr(struct aic7xxx_host *p, struct aic7xxx_scb *scb)
-{
-  int tindex = TARGET_INDEX(scb->cmd);
-
-  p->msg_buf[p->msg_index++] = MSG_EXTENDED;
-  p->msg_buf[p->msg_index++] = MSG_EXT_PPR_LEN;
-  p->msg_buf[p->msg_index++] = MSG_EXT_PPR;
-  p->msg_buf[p->msg_index++] = p->transinfo[tindex].goal_period;
-  p->msg_buf[p->msg_index++] = 0;
-  p->msg_buf[p->msg_index++] = p->transinfo[tindex].goal_offset;
-  p->msg_buf[p->msg_index++] = p->transinfo[tindex].goal_width;
-  p->msg_buf[p->msg_index++] = p->transinfo[tindex].goal_options;
-  p->msg_len += 8;
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_construct_sdtr
- *
- * Description:
- *   Constucts a synchronous data transfer message in the message
- *   buffer on the sequencer.
- *-F*************************************************************************/
-static void
-aic7xxx_construct_sdtr(struct aic7xxx_host *p, unsigned char period,
-        unsigned char offset)
-{
-  p->msg_buf[p->msg_index++] = MSG_EXTENDED;
-  p->msg_buf[p->msg_index++] = MSG_EXT_SDTR_LEN;
-  p->msg_buf[p->msg_index++] = MSG_EXT_SDTR;
-  p->msg_buf[p->msg_index++] = period;
-  p->msg_buf[p->msg_index++] = offset;
-  p->msg_len += 5;
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_construct_wdtr
- *
- * Description:
- *   Constucts a wide data transfer message in the message buffer
- *   on the sequencer.
- *-F*************************************************************************/
-static void
-aic7xxx_construct_wdtr(struct aic7xxx_host *p, unsigned char bus_width)
-{
-  p->msg_buf[p->msg_index++] = MSG_EXTENDED;
-  p->msg_buf[p->msg_index++] = MSG_EXT_WDTR_LEN;
-  p->msg_buf[p->msg_index++] = MSG_EXT_WDTR;
-  p->msg_buf[p->msg_index++] = bus_width;
-  p->msg_len += 4;
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_calc_residual
- *
- * Description:
- *   Calculate the residual data not yet transferred.
- *-F*************************************************************************/
-static void
-aic7xxx_calculate_residual (struct aic7xxx_host *p, struct aic7xxx_scb *scb)
-{
-  struct aic7xxx_hwscb *hscb;
-  Scsi_Cmnd *cmd;
-  int actual, i;
-
-  cmd = scb->cmd;
-  hscb = scb->hscb;
-
-  /*
-   *  Don't destroy valid residual information with
-   *  residual coming from a check sense operation.
-   */
-  if (((scb->hscb->control & DISCONNECTED) == 0) &&
-      (scb->flags & SCB_SENSE) == 0)
-  {
-    /*
-     *  We had an underflow. At this time, there's only
-     *  one other driver that bothers to check for this,
-     *  and cmd->underflow seems to be set rather half-
-     *  heartedly in the higher-level SCSI code.
-     */
-    actual = scb->sg_length;
-    for (i=1; i < hscb->residual_SG_segment_count; i++)
-    {
-      actual -= scb->sg_list[scb->sg_count - i].length;
-    }
-    actual -= (hscb->residual_data_count[2] << 16) |
-              (hscb->residual_data_count[1] <<  8) |
-              hscb->residual_data_count[0];
-
-    if (actual < cmd->underflow)
-    {
-      if (aic7xxx_verbose & VERBOSE_MINOR_ERROR)
-        printk(INFO_LEAD "Underflow - Wanted %u, %s %u, residual SG "
-          "count %d.\n", p->host_no, CTL_OF_SCB(scb), cmd->underflow,
-          (cmd->request.cmd == WRITE) ? "wrote" : "read", actual,
-          hscb->residual_SG_segment_count);
-      aic7xxx_error(cmd) = DID_RETRY_COMMAND;
-      aic7xxx_status(cmd) = hscb->target_status;
-    }
-  }
-
-  /*
-   * Clean out the residual information in the SCB for the
-   * next consumer.
-   */
-  hscb->residual_data_count[2] = 0;
-  hscb->residual_data_count[1] = 0;
-  hscb->residual_data_count[0] = 0;
-  hscb->residual_SG_segment_count = 0;
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_handle_device_reset
- *
- * Description:
- *   Interrupt handler for sequencer interrupts (SEQINT).
- *-F*************************************************************************/
-static void
-aic7xxx_handle_device_reset(struct aic7xxx_host *p, int target, int channel)
-{
-  unsigned short targ_mask;
-  unsigned char tindex = target;
-
-  tindex |= ((channel & 0x01) << 3);
-
-  targ_mask = (0x01 << tindex);
-  /*
-   * Go back to async/narrow transfers and renegotiate.
-   */
-  p->needppr |= (p->needppr_copy & targ_mask);
-  p->needsdtr |= (p->needsdtr_copy & targ_mask);
-  p->needwdtr |= (p->needwdtr_copy & targ_mask);
-  p->dtr_pending &= ~targ_mask;
-  aic_outb(p, 0, TARG_SCSIRATE + tindex);
-  if (p->features & AHC_ULTRA2)
-    aic_outb(p, 0, TARG_OFFSET + tindex);
-  aic7xxx_reset_device(p, target, channel, ALL_LUNS, SCB_LIST_NULL);
-  if (aic7xxx_verbose & VERBOSE_RESET_PROCESS)
-    printk(INFO_LEAD "Bus Device Reset delivered.\n", p->host_no, channel,
-      target, -1);
-  aic7xxx_run_done_queue(p, /*complete*/ TRUE);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_handle_seqint
- *
- * Description:
- *   Interrupt handler for sequencer interrupts (SEQINT).
- *-F*************************************************************************/
-static void
-aic7xxx_handle_seqint(struct aic7xxx_host *p, unsigned char intstat)
-{
-  struct aic7xxx_scb *scb;
-  unsigned short target_mask;
-  unsigned char target, lun, tindex;
-  unsigned char queue_flag = FALSE;
-  char channel;
-
-  target = ((aic_inb(p, SAVED_TCL) >> 4) & 0x0f);
-  if ( (p->chip & AHC_CHIPID_MASK) == AHC_AIC7770 )
-    channel = (aic_inb(p, SBLKCTL) & SELBUSB) >> 3;
-  else
-    channel = 0;
-  tindex = target + (channel << 3);
-  lun = aic_inb(p, SAVED_TCL) & 0x07;
-  target_mask = (0x01 << tindex);
-
-  /*
-   * Go ahead and clear the SEQINT now, that avoids any interrupt race
-   * conditions later on in case we enable some other interrupt.
-   */
-  aic_outb(p, CLRSEQINT, CLRINT);
-  switch (intstat & SEQINT_MASK)
-  {
-    case NO_MATCH:
-      {
-        aic_outb(p, aic_inb(p, SCSISEQ) & (ENSELI|ENRSELI|ENAUTOATNP),
-                 SCSISEQ);
-        printk(WARN_LEAD "No active SCB for reconnecting target - Issuing "
-               "BUS DEVICE RESET.\n", p->host_no, channel, target, lun);
-        printk(WARN_LEAD "      SAVED_TCL=0x%x, ARG_1=0x%x, SEQADDR=0x%x\n",
-               p->host_no, channel, target, lun,
-               aic_inb(p, SAVED_TCL), aic_inb(p, ARG_1),
-               (aic_inb(p, SEQADDR1) << 8) | aic_inb(p, SEQADDR0));
-        if (aic7xxx_panic_on_abort)
-          aic7xxx_panic_abort(p, NULL);
-      }
-      break;
-
-    case SEND_REJECT:
-      {
-        if (aic7xxx_verbose & VERBOSE_MINOR_ERROR)
-          printk(INFO_LEAD "Rejecting unknown message (0x%x) received from "
-            "target, SEQ_FLAGS=0x%x\n", p->host_no, channel, target, lun,
-            aic_inb(p, ACCUM), aic_inb(p, SEQ_FLAGS));
-      }
-      break;
-
-    case NO_IDENT:
-      {
-        /*
-         * The reconnecting target either did not send an identify
-         * message, or did, but we didn't find an SCB to match and
-         * before it could respond to our ATN/abort, it hit a dataphase.
-         * The only safe thing to do is to blow it away with a bus
-         * reset.
-         */
-        if (aic7xxx_verbose & (VERBOSE_SEQINT | VERBOSE_RESET_MID))
-          printk(INFO_LEAD "Target did not send an IDENTIFY message; "
-            "LASTPHASE 0x%x, SAVED_TCL 0x%x\n", p->host_no, channel, target,
-            lun, aic_inb(p, LASTPHASE), aic_inb(p, SAVED_TCL));
-
-        aic7xxx_reset_channel(p, channel, /*initiate reset*/ TRUE);
-        aic7xxx_run_done_queue(p, TRUE);
-
-      }
-      break;
-
-    case BAD_PHASE:
-      if (aic_inb(p, LASTPHASE) == P_BUSFREE)
-      {
-        if (aic7xxx_verbose & VERBOSE_SEQINT)
-          printk(INFO_LEAD "Missed busfree.\n", p->host_no, channel,
-            target, lun);
-        restart_sequencer(p);
-      }
-      else
-      {
-        if (aic7xxx_verbose & VERBOSE_SEQINT)
-          printk(INFO_LEAD "Unknown scsi bus phase, continuing\n", p->host_no,
-            channel, target, lun);
-      }
-      break;
-
-    case EXTENDED_MSG:
-      {
-        p->msg_type = MSG_TYPE_INITIATOR_MSGIN;
-        p->msg_len = 0;
-        p->msg_index = 0;
-
-#ifdef AIC7XXX_VERBOSE_DEBUGGING
-        if (aic7xxx_verbose > 0xffff)
-          printk(INFO_LEAD "Enabling REQINITs for MSG_IN\n", p->host_no,
-                 channel, target, lun);
-#endif
-
-       /*      
-        * To actually receive the message, simply turn on
-        * REQINIT interrupts and let our interrupt handler
-        * do the rest (REQINIT should already be true).
-        */
-        p->flags |= AHC_HANDLING_REQINITS;
-        aic_outb(p, aic_inb(p, SIMODE1) | ENREQINIT, SIMODE1);
-
-       /*
-        * We don't want the sequencer unpaused yet so we return early
-        */
-        return;
-      }
-
-    case REJECT_MSG:
-      {
-        /*
-         * What we care about here is if we had an outstanding SDTR
-         * or WDTR message for this target. If we did, this is a
-         * signal that the target is refusing negotiation.
-         */
-        unsigned char scb_index;
-        unsigned char last_msg;
-
-        scb_index = aic_inb(p, SCB_TAG);
-        scb = p->scb_data->scb_array[scb_index];
-        last_msg = aic_inb(p, LAST_MSG);
-
-        if ( (last_msg == MSG_IDENTIFYFLAG) &&
-             (scb->tag_action) &&
-            !(scb->flags & SCB_MSGOUT_BITS) )
-        {
-          if (scb->tag_action == MSG_ORDERED_Q_TAG)
-          {
-            /*
-             * OK...the device seems able to accept tagged commands, but
-             * not ordered tag commands, only simple tag commands.  So, we
-             * disable ordered tag commands and go on with life just like
-             * normal.
-             */
-            p->orderedtag &= ~target_mask;
-            scb->tag_action = MSG_SIMPLE_Q_TAG;
-            scb->hscb->control &= ~SCB_TAG_TYPE;
-            scb->hscb->control |= MSG_SIMPLE_Q_TAG;
-            aic_outb(p, scb->hscb->control, SCB_CONTROL);
-            /*
-             * OK..we set the tag type to simple tag command, now we re-assert
-             * ATNO and hope this will take us into the identify phase again
-             * so we can resend the tag type and info to the device.
-             */
-            aic_outb(p, MSG_IDENTIFYFLAG, MSG_OUT);
-            aic_outb(p, aic_inb(p, SCSISIGI) | ATNO, SCSISIGO);
-          }
-          else if (scb->tag_action == MSG_SIMPLE_Q_TAG)
-          {
-            unsigned char i, reset = 0;
-            struct aic7xxx_scb *scbp;
-            int old_verbose;
-            /*
-             * Hmmmm....the device is flaking out on tagged commands.  The
-             * bad thing is that we already have tagged commands enabled in
-             * the device struct in the mid level code.  We also have a queue
-             * set according to the tagged queue depth.  Gonna have to live
-             * with it by controlling our queue depth internally and making
-             * sure we don't set the tagged command flag any more.
-             */
-            p->tagenable &= ~target_mask;
-            p->orderedtag &= ~target_mask;
-            p->dev_max_queue_depth[tindex] =
-               p->dev_temp_queue_depth[tindex] = 1;
-            /*
-             * We set this command up as a bus device reset.  However, we have
-             * to clear the tag type as it's causing us problems.  We shouldnt
-             * have to worry about any other commands being active, since if
-             * the device is refusing tagged commands, this should be the
-             * first tagged command sent to the device, however, we do have
-             * to worry about any other tagged commands that may already be
-             * in the qinfifo.  The easiest way to do this, is to issue a BDR,
-             * send all the commands back to the mid level code, then let them
-             * come back and get rebuilt as untagged commands.
-             */
-            scb->tag_action = 0;
-            scb->hscb->control &= ~(TAG_ENB | SCB_TAG_TYPE);
-            aic_outb(p,  scb->hscb->control, SCB_CONTROL);
-
-            old_verbose = aic7xxx_verbose;
-            aic7xxx_verbose &= ~(VERBOSE_RESET|VERBOSE_ABORT);
-            for (i=0; i!=p->scb_data->numscbs; i++)
-            {
-              scbp = p->scb_data->scb_array[i];
-              if ((scbp->flags & SCB_ACTIVE) && (scbp != scb))
-              {
-                if (aic7xxx_match_scb(p, scbp, target, channel, lun, i))
-                {
-                  aic7xxx_reset_device(p, target, channel, lun, i);
-                  reset++;
-                }
-                aic7xxx_run_done_queue(p, TRUE);
-              }
-            }
-            aic7xxx_verbose = old_verbose;
-            /*
-             * Wait until after the for loop to set the busy index since
-             * aic7xxx_reset_device will clear the busy index during its
-             * operation.
-             */
-            aic7xxx_busy_target(p, scb);
-            printk(INFO_LEAD "Device is refusing tagged commands, using "
-              "untagged I/O.\n", p->host_no, channel, target, lun);
-            aic_outb(p, MSG_IDENTIFYFLAG, MSG_OUT);
-            aic_outb(p, aic_inb(p, SCSISIGI) | ATNO, SCSISIGO);
-          }
-        }
-        else if (scb->flags & SCB_MSGOUT_PPR)
-        {
-          /*
-           * As per the draft specs, any device capable of supporting any of
-           * the option values other than 0 are not allowed to reject the
-           * PPR message.  Instead, they must negotiate out what they do
-           * support instead of rejecting our offering or else they cause
-           * a parity error during msg_out phase to signal that they don't
-           * like our settings.
-           */
-          p->needppr &= ~target_mask;
-          p->needppr_copy &= ~target_mask;
-          aic7xxx_set_width(p, target, channel, lun, MSG_EXT_WDTR_BUS_8_BIT,
-            (AHC_TRANS_ACTIVE|AHC_TRANS_CUR|AHC_TRANS_QUITE));
-          aic7xxx_set_syncrate(p, NULL, target, channel, 0, 0, 0,
-                               AHC_TRANS_ACTIVE|AHC_TRANS_CUR|AHC_TRANS_QUITE);
-          p->transinfo[tindex].goal_options = 0;
-          p->dtr_pending &= ~target_mask;
-          scb->flags &= ~SCB_MSGOUT_BITS;
-          if(aic7xxx_verbose & VERBOSE_NEGOTIATION2)
-          {
-            printk(INFO_LEAD "Device is rejecting PPR messages, falling "
-              "back.\n", p->host_no, channel, target, lun);
-          }
-          if ( p->transinfo[tindex].goal_width )
-          {
-            p->needwdtr |= target_mask;
-            p->needwdtr_copy |= target_mask;
-            p->dtr_pending |= target_mask;
-            scb->flags |= SCB_MSGOUT_WDTR;
-          }
-          if ( p->transinfo[tindex].goal_offset )
-          {
-            p->needsdtr |= target_mask;
-            p->needsdtr_copy |= target_mask;
-            if( !(p->dtr_pending & target_mask) )
-            {
-              p->dtr_pending |= target_mask;
-              scb->flags |= SCB_MSGOUT_SDTR;
-            }
-          }
-          if ( p->dtr_pending & target_mask )
-          {
-            aic_outb(p, HOST_MSG, MSG_OUT);
-            aic_outb(p, aic_inb(p, SCSISIGI) | ATNO, SCSISIGO);
-          }
-        }
-        else if (scb->flags & SCB_MSGOUT_WDTR)
-        {
-          /*
-           * note 8bit xfers and clear flag
-           */
-          p->needwdtr &= ~target_mask;
-          p->needwdtr_copy &= ~target_mask;
-          p->dtr_pending &= ~target_mask;
-          scb->flags &= ~SCB_MSGOUT_BITS;
-          aic7xxx_set_width(p, target, channel, lun, MSG_EXT_WDTR_BUS_8_BIT,
-            (AHC_TRANS_ACTIVE|AHC_TRANS_GOAL|AHC_TRANS_CUR));
-          aic7xxx_set_syncrate(p, NULL, target, channel, 0, 0, 0,
-                               AHC_TRANS_ACTIVE|AHC_TRANS_CUR|AHC_TRANS_QUITE);
-          if(aic7xxx_verbose & VERBOSE_NEGOTIATION2)
-          {
-            printk(INFO_LEAD "Device is rejecting WDTR messages, using "
-              "narrow transfers.\n", p->host_no, channel, target, lun);
-          }
-          p->needsdtr |= (p->needsdtr_copy & target_mask);
-        }
-        else if (scb->flags & SCB_MSGOUT_SDTR)
-        {
-         /*
-          * note asynch xfers and clear flag
-          */
-          p->needsdtr &= ~target_mask;
-          p->needsdtr_copy &= ~target_mask;
-          p->dtr_pending &= ~target_mask;
-          scb->flags &= ~SCB_MSGOUT_SDTR;
-          aic7xxx_set_syncrate(p, NULL, target, channel, 0, 0, 0,
-            (AHC_TRANS_CUR|AHC_TRANS_ACTIVE|AHC_TRANS_GOAL));
-          if(aic7xxx_verbose & VERBOSE_NEGOTIATION2)
-          {
-            printk(INFO_LEAD "Device is rejecting SDTR messages, using "
-              "async transfers.\n", p->host_no, channel, target, lun);
-          }
-        }
-        else if (aic7xxx_verbose & VERBOSE_SEQINT)
-        {
-          /*
-           * Otherwise, we ignore it.
-           */
-          printk(INFO_LEAD "Received MESSAGE_REJECT for unknown cause.  "
-            "Ignoring.\n", p->host_no, channel, target, lun);
-        }
-      }
-      break;
-
-    case BAD_STATUS:
-      {
-        unsigned char scb_index;
-        struct aic7xxx_hwscb *hscb;
-        Scsi_Cmnd *cmd;
-
-        /* The sequencer will notify us when a command has an error that
-         * would be of interest to the kernel.  This allows us to leave
-         * the sequencer running in the common case of command completes
-         * without error.  The sequencer will have DMA'd the SCB back
-         * up to us, so we can reference the drivers SCB array.
-         *
-         * Set the default return value to 0 indicating not to send
-         * sense.  The sense code will change this if needed and this
-         * reduces code duplication.
-         */
-        aic_outb(p, 0, RETURN_1);
-        scb_index = aic_inb(p, SCB_TAG);
-        if (scb_index > p->scb_data->numscbs)
-        {
-          printk(WARN_LEAD "Invalid SCB during SEQINT 0x%02x, SCB_TAG %d.\n",
-            p->host_no, channel, target, lun, intstat, scb_index);
-          break;
-        }
-        scb = p->scb_data->scb_array[scb_index];
-        hscb = scb->hscb;
-
-        if (!(scb->flags & SCB_ACTIVE) || (scb->cmd == NULL))
-        {
-          printk(WARN_LEAD "Invalid SCB during SEQINT 0x%x, scb %d, flags 0x%x,"
-            " cmd 0x%lx.\n", p->host_no, channel, target, lun, intstat,
-            scb_index, scb->flags, (unsigned long) scb->cmd);
-        }
-        else
-        {
-          cmd = scb->cmd;
-          hscb->target_status = aic_inb(p, SCB_TARGET_STATUS);
-          aic7xxx_status(cmd) = hscb->target_status;
-
-          cmd->result = hscb->target_status;
-
-          switch (status_byte(hscb->target_status))
-          {
-            case GOOD:
-              if (aic7xxx_verbose & VERBOSE_SEQINT)
-                printk(INFO_LEAD "Interrupted for status of GOOD???\n",
-                  p->host_no, CTL_OF_SCB(scb));
-              break;
-
-            case COMMAND_TERMINATED:
-            case CHECK_CONDITION:
-              if ( !(scb->flags & SCB_SENSE) )
-              {
-
-                /*
-                   * Send a sense command to the requesting target.
-                 * XXX - revisit this and get rid of the memcopys.
-                   */
-                memcpy(&scb->sense_cmd[0], &generic_sense[0],
-                       sizeof(generic_sense));
-
-                scb->sense_cmd[1] = (cmd->lun << 5);
-                scb->sense_cmd[4] = sizeof(cmd->sense_buffer);
-
-                scb->sg_list[0].address = 
-                  cpu_to_le32(VIRT_TO_BUS(&cmd->sense_buffer[0]));
-                scb->sg_list[0].length = 
-                  cpu_to_le32(sizeof(cmd->sense_buffer));
-
-                /*
-                 * XXX - We should allow disconnection, but can't as it
-                 * might allow overlapped tagged commands.
-                 */
-                  /* hscb->control &= DISCENB; */
-                hscb->control = 0;
-                hscb->target_status = 0;
-                hscb->SG_list_pointer = 
-                  cpu_to_le32(VIRT_TO_BUS(&scb->sg_list[0]));
-                hscb->data_pointer = scb->sg_list[0].address;
-                hscb->data_count = scb->sg_list[0].length;
-                hscb->SCSI_cmd_pointer = 
-                  cpu_to_le32(VIRT_TO_BUS(&scb->sense_cmd[0]));
-                hscb->SCSI_cmd_length = COMMAND_SIZE(scb->sense_cmd[0]);
-                hscb->residual_SG_segment_count = 0;
-                hscb->residual_data_count[0] = 0;
-                hscb->residual_data_count[1] = 0;
-                hscb->residual_data_count[2] = 0;
-
-                scb->sg_count = hscb->SG_segment_count = 1;
-                scb->sg_length = sizeof(cmd->sense_buffer);
-                scb->tag_action = 0;
-                scb->flags |= SCB_SENSE;
-                /*
-                 * Ensure the target is busy since this will be an
-                 * an untagged request.
-                 */
-#ifdef AIC7XXX_VERBOSE_DEBUGGING
-                if (aic7xxx_verbose & VERBOSE_NEGOTIATION2)
-                {
-                  if (scb->flags & SCB_MSGOUT_BITS)
-                    printk(INFO_LEAD "Requesting SENSE with %s\n", p->host_no,
-                           CTL_OF_SCB(scb), (scb->flags & SCB_MSGOUT_SDTR) ?
-                           "SDTR" : "WDTR");
-                  else
-                    printk(INFO_LEAD "Requesting SENSE, no MSG\n", p->host_no,
-                           CTL_OF_SCB(scb));
-                }
-#endif
-                aic7xxx_busy_target(p, scb);
-                aic_outb(p, SEND_SENSE, RETURN_1);
-                aic7xxx_error(cmd) = DID_OK;
-                break;
-              }  /* first time sense, no errors */
-              aic7xxx_error(cmd) = DID_ERROR;
-              scb->flags &= ~SCB_SENSE;
-              break;
-
-            case QUEUE_FULL:
-              queue_flag = TRUE;    /* Mark that this is a QUEUE_FULL and */
-            case BUSY:              /* drop through to here */
-            {
-              struct aic7xxx_scb *next_scbp, *prev_scbp;
-              unsigned char active_hscb, next_hscb, prev_hscb, scb_index;
-              /*
-               * We have to look three places for queued commands:
-               *  1: QINFIFO
-               *  2: p->waiting_scbs queue
-               *  3: WAITING_SCBS list on card (for commands that are started
-               *     but haven't yet made it to the device)
-               */
-              aic7xxx_search_qinfifo(p, target, channel, lun,
-                SCB_LIST_NULL, 0, TRUE,
-                &p->delayed_scbs[tindex]);
-              next_scbp = p->waiting_scbs.head;
-              while ( next_scbp != NULL )
-              {
-                prev_scbp = next_scbp;
-                next_scbp = next_scbp->q_next;
-                if ( aic7xxx_match_scb(p, prev_scbp, target, channel, lun,
-                     SCB_LIST_NULL) )
-                {
-                  scbq_remove(&p->waiting_scbs, prev_scbp);
-                  scbq_insert_tail(&p->delayed_scbs[tindex],
-                    prev_scbp);
-                }
-              }
-              next_scbp = NULL;
-              active_hscb = aic_inb(p, SCBPTR);
-              prev_hscb = next_hscb = scb_index = SCB_LIST_NULL;
-              next_hscb = aic_inb(p, WAITING_SCBH);
-              while (next_hscb != SCB_LIST_NULL)
-              {
-                aic_outb(p, next_hscb, SCBPTR);
-                scb_index = aic_inb(p, SCB_TAG);
-                if (scb_index < p->scb_data->numscbs)
-                {
-                  next_scbp = p->scb_data->scb_array[scb_index];
-                  if (aic7xxx_match_scb(p, next_scbp, target, channel, lun,
-                      SCB_LIST_NULL) )
-                  {
-                    if (next_scbp->flags & SCB_WAITINGQ)
-                    {
-                      p->dev_active_cmds[tindex]++;
-                      p->activescbs--;
-                      scbq_remove(&p->delayed_scbs[tindex], next_scbp);
-                      scbq_remove(&p->waiting_scbs, next_scbp);
-                    }
-                    scbq_insert_head(&p->delayed_scbs[tindex],
-                      next_scbp);
-                    next_scbp->flags |= SCB_WAITINGQ;
-                    p->dev_active_cmds[tindex]--;
-                    p->activescbs--;
-                    next_hscb = aic_inb(p, SCB_NEXT);
-                    aic_outb(p, 0, SCB_CONTROL);
-                    aic_outb(p, SCB_LIST_NULL, SCB_TAG);
-                    aic7xxx_add_curscb_to_free_list(p);
-                    if (prev_hscb == SCB_LIST_NULL)
-                    {
-                      /* We were first on the list,
-                       * so we kill the selection
-                       * hardware.  Let the sequencer
-                       * re-init the hardware itself
-                       */
-                      aic_outb(p, aic_inb(p, SCSISEQ) & ~ENSELO, SCSISEQ);
-                      aic_outb(p, CLRSELTIMEO, CLRSINT1);
-                      aic_outb(p, next_hscb, WAITING_SCBH);
-                    }
-                    else
-                    {
-                      aic_outb(p, prev_hscb, SCBPTR);
-                      aic_outb(p, next_hscb, SCB_NEXT);
-                    }
-                  }
-                  else
-                  {
-                    prev_hscb = next_hscb;
-                    next_hscb = aic_inb(p, SCB_NEXT);
-                  }
-                } /* scb_index >= p->scb_data->numscbs */
-              }
-              aic_outb(p, active_hscb, SCBPTR);
-              if (scb->flags & SCB_WAITINGQ)
-              {
-                scbq_remove(&p->delayed_scbs[tindex], scb);
-                scbq_remove(&p->waiting_scbs, scb);
-                p->dev_active_cmds[tindex]++;
-                p->activescbs++;
-              }
-              scbq_insert_head(&p->delayed_scbs[tindex], scb);
-              p->dev_active_cmds[tindex]--;
-              p->activescbs--;
-              scb->flags |= SCB_WAITINGQ | SCB_WAS_BUSY;
-                  
-              if ( !(p->dev_timer_active & (0x01 << tindex)) ) 
-              {
-                p->dev_timer_active |= (0x01 << tindex);
-                if ( p->dev_active_cmds[tindex] )
-                {
-                  p->dev_expires[tindex] = jiffies + HZ;
-                }
-                else
-                {
-                  p->dev_expires[tindex] = jiffies + (HZ / 10);
-                }
-                if ( !(p->dev_timer_active & (0x01 << MAX_TARGETS)) )
-                {
-                  p->dev_timer.expires = p->dev_expires[tindex];
-                  p->dev_timer_active |= (0x01 << MAX_TARGETS);
-                  add_timer(&p->dev_timer);
-                }
-                else if ( time_after_eq(p->dev_timer.expires,
-                                        p->dev_expires[tindex]) )
-                {
-                  del_timer(&p->dev_timer);
-                  p->dev_timer.expires = p->dev_expires[tindex];
-                  add_timer(&p->dev_timer);
-                }
-              }
-#ifdef AIC7XXX_VERBOSE_DEBUGGING
-              if( (aic7xxx_verbose & VERBOSE_MINOR_ERROR) ||
-                  (aic7xxx_verbose > 0xffff) )
-              {
-                if (queue_flag)
-                  printk(INFO_LEAD "Queue full received; queue depth %d, "
-                    "active %d\n", p->host_no, CTL_OF_SCB(scb),
-                    p->dev_max_queue_depth[tindex],
-                    p->dev_active_cmds[tindex]);
-                else
-                  printk(INFO_LEAD "Target busy\n", p->host_no, CTL_OF_SCB(scb));
-
-              }
-#endif
-              if (queue_flag)
-              {
-                if ( p->dev_last_queue_full[tindex] !=
-                     p->dev_active_cmds[tindex] )
-                {
-                  p->dev_last_queue_full[tindex] = 
-                      p->dev_active_cmds[tindex];
-                  p->dev_last_queue_full_count[tindex] = 0;
-                }
-                else
-                {
-                  p->dev_last_queue_full_count[tindex]++;
-                }
-                if ( (p->dev_last_queue_full_count[tindex] > 14) &&
-                     (p->dev_active_cmds[tindex] > 4) )
-                {
-                  if (aic7xxx_verbose & VERBOSE_NEGOTIATION2)
-                    printk(INFO_LEAD "Queue depth reduced to %d\n", p->host_no,
-                      CTL_OF_SCB(scb), p->dev_active_cmds[tindex]);
-                  p->dev_max_queue_depth[tindex] = 
-                      p->dev_active_cmds[tindex];
-                  p->dev_last_queue_full[tindex] = 0;
-                  p->dev_last_queue_full_count[tindex] = 0;
-                  p->dev_temp_queue_depth[tindex] = 
-                    p->dev_active_cmds[tindex];
-                }
-                else if (p->dev_active_cmds[tindex] == 0)
-                {
-                  if (aic7xxx_verbose & VERBOSE_NEGOTIATION)
-                  {
-                    printk(INFO_LEAD "QUEUE_FULL status received with 0 "
-                           "commands active.\n", p->host_no, CTL_OF_SCB(scb));
-                    printk(INFO_LEAD "Tagged Command Queueing disabled\n",
-                           p->host_no, CTL_OF_SCB(scb));
-                  }
-                  p->dev_max_queue_depth[tindex] = 1;
-                  p->dev_temp_queue_depth[tindex] = 1;
-                  scb->tag_action = 0;
-                  scb->hscb->control &= ~(MSG_ORDERED_Q_TAG|MSG_SIMPLE_Q_TAG);
-                }
-                else
-                {
-                  p->dev_flags[tindex] |= DEVICE_WAS_BUSY;
-                  p->dev_temp_queue_depth[tindex] = 
-                    p->dev_active_cmds[tindex];
-                }
-              }
-              break;
-            }
-            
-            default:
-              if (aic7xxx_verbose & VERBOSE_SEQINT)
-                printk(INFO_LEAD "Unexpected target status 0x%x.\n", p->host_no,
-                     CTL_OF_SCB(scb), scb->hscb->target_status);
-              if (!aic7xxx_error(cmd))
-              {
-                aic7xxx_error(cmd) = DID_RETRY_COMMAND;
-              }
-              break;
-          }  /* end switch */
-        }  /* end else of */
-      }
-      break;
-
-    case AWAITING_MSG:
-      {
-        unsigned char scb_index, msg_out;
-
-        scb_index = aic_inb(p, SCB_TAG);
-        msg_out = aic_inb(p, MSG_OUT);
-        scb = p->scb_data->scb_array[scb_index];
-        p->msg_index = p->msg_len = 0;
-        /*
-         * This SCB had a MK_MESSAGE set in its control byte informing
-         * the sequencer that we wanted to send a special message to
-         * this target.
-         */
-
-        if ( !(scb->flags & SCB_DEVICE_RESET) &&
-              (msg_out == MSG_IDENTIFYFLAG) &&
-              (scb->hscb->control & TAG_ENB) )
-        {
-          p->msg_buf[p->msg_index++] = scb->tag_action;
-          p->msg_buf[p->msg_index++] = scb->hscb->tag;
-          p->msg_len += 2;
-        }
-
-        if (scb->flags & SCB_DEVICE_RESET)
-        {
-          p->msg_buf[p->msg_index++] = MSG_BUS_DEV_RESET;
-          p->msg_len++;
-          if (aic7xxx_verbose & VERBOSE_RESET_PROCESS)
-            printk(INFO_LEAD "Bus device reset mailed.\n",
-                 p->host_no, CTL_OF_SCB(scb));
-        }
-        else if (scb->flags & SCB_ABORT)
-        {
-          if (scb->tag_action)
-          {
-            p->msg_buf[p->msg_index++] = MSG_ABORT_TAG;
-          }
-          else
-          {
-            p->msg_buf[p->msg_index++] = MSG_ABORT;
-          }
-          p->msg_len++;
-          if (aic7xxx_verbose & VERBOSE_ABORT_PROCESS)
-            printk(INFO_LEAD "Abort message mailed.\n", p->host_no,
-              CTL_OF_SCB(scb));
-        }
-        else if (scb->flags & SCB_MSGOUT_PPR)
-        {
-          if (aic7xxx_verbose & VERBOSE_NEGOTIATION2)
-          {
-            printk(INFO_LEAD "Sending PPR (%d/%d/%d/%d) message.\n",
-                   p->host_no, CTL_OF_SCB(scb),
-                   p->transinfo[tindex].goal_period,
-                   p->transinfo[tindex].goal_offset,
-                   p->transinfo[tindex].goal_width,
-                   p->transinfo[tindex].goal_options);
-          }
-          aic7xxx_construct_ppr(p, scb);
-        }
-        else if (scb->flags & SCB_MSGOUT_WDTR)
-        {
-          if (aic7xxx_verbose & VERBOSE_NEGOTIATION2)
-          {
-            printk(INFO_LEAD "Sending WDTR message.\n", p->host_no,
-                   CTL_OF_SCB(scb));
-          }
-          aic7xxx_construct_wdtr(p, p->transinfo[tindex].goal_width);
-        }
-        else if (scb->flags & SCB_MSGOUT_SDTR)
-        {
-          unsigned int max_sync, period;
-          unsigned char options = 0;
-          /*
-           * Now that the device is selected, use the bits in SBLKCTL and
-           * SSTAT2 to determine the max sync rate for this device.
-           */
-          if (p->features & AHC_ULTRA2)
-          {
-            if ( (aic_inb(p, SBLKCTL) & ENAB40) &&
-                !(aic_inb(p, SSTAT2) & EXP_ACTIVE) )
-            {
-              max_sync = AHC_SYNCRATE_ULTRA2;
-            }
-            else
-            {
-              max_sync = AHC_SYNCRATE_ULTRA;
-            }
-          }
-          else if (p->features & AHC_ULTRA)
-          {
-            max_sync = AHC_SYNCRATE_ULTRA;
-          }
-          else
-          {
-            max_sync = AHC_SYNCRATE_FAST;
-          }
-          period = p->transinfo[tindex].goal_period;
-          aic7xxx_find_syncrate(p, &period, max_sync, &options);
-          if (aic7xxx_verbose & VERBOSE_NEGOTIATION2)
-          {
-            printk(INFO_LEAD "Sending SDTR %d/%d message.\n", p->host_no,
-                   CTL_OF_SCB(scb), period,
-                   p->transinfo[tindex].goal_offset);
-          }
-          aic7xxx_construct_sdtr(p, period,
-            p->transinfo[tindex].goal_offset);
-        }
-        else 
-        {
-          sti();
-          panic("aic7xxx: AWAITING_MSG for an SCB that does "
-                "not have a waiting message.\n");
-        }
-        /*
-         * We've set everything up to send our message, now to actually do
-         * so we need to enable reqinit interrupts and let the interrupt
-         * handler do the rest.  We don't want to unpause the sequencer yet
-         * though so we'll return early.  We also have to make sure that
-         * we clear the SEQINT *BEFORE* we set the REQINIT handler active
-         * or else it's possible on VLB cards to loose the first REQINIT
-         * interrupt.  Edge triggered EISA cards could also loose this
-         * interrupt, although PCI and level triggered cards should not
-         * have this problem since they continually interrupt the kernel
-         * until we take care of the situation.
-         */
-        scb->flags |= SCB_MSGOUT_SENT;
-        p->msg_index = 0;
-        p->msg_type = MSG_TYPE_INITIATOR_MSGOUT;
-        p->flags |= AHC_HANDLING_REQINITS;
-        aic_outb(p, aic_inb(p, SIMODE1) | ENREQINIT, SIMODE1);
-        return;
-      }
-      break;
-
-    case DATA_OVERRUN:
-      {
-        unsigned char scb_index = aic_inb(p, SCB_TAG);
-        unsigned char lastphase = aic_inb(p, LASTPHASE);
-        unsigned int i;
-
-        scb = (p->scb_data->scb_array[scb_index]);
-        /*
-         * XXX - What do we really want to do on an overrun?  The
-         *       mid-level SCSI code should handle this, but for now,
-         *       we'll just indicate that the command should retried.
-         *    If we retrieved sense info on this target, then the 
-         *    base SENSE info should have been saved prior to the
-         *    overrun error.  In that case, we return DID_OK and let
-         *    the mid level code pick up on the sense info.  Otherwise
-         *    we return DID_ERROR so the command will get retried.
-         */
-        if ( !(scb->flags & SCB_SENSE) )
-        {
-          printk(WARN_LEAD "Data overrun detected in %s phase, tag %d;\n",
-            p->host_no, CTL_OF_SCB(scb), 
-            (lastphase == P_DATAIN) ? "Data-In" : "Data-Out", scb->hscb->tag);
-          printk(KERN_WARNING "  %s seen Data Phase. Length=%d, NumSGs=%d.\n",
-            (aic_inb(p, SEQ_FLAGS) & DPHASE) ? "Have" : "Haven't",
-            scb->sg_length, scb->sg_count);
-          for (i = 0; i < scb->sg_count; i++)
-          {
-            printk(KERN_WARNING "     sg[%d] - Addr 0x%x : Length %d\n",
-                 i, 
-                 le32_to_cpu(scb->sg_list[i].address),
-                 le32_to_cpu(scb->sg_list[i].length) );
-          }
-          aic7xxx_error(scb->cmd) = DID_ERROR;
-        }
-        else
-          printk(INFO_LEAD "Data Overrun during SEND_SENSE operation.\n",
-            p->host_no, CTL_OF_SCB(scb));
-      }
-      break;
-
-    case WIDE_RESIDUE:
-      {
-        unsigned char resid_sgcnt, index;
-        unsigned char scb_index = aic_inb(p, SCB_TAG);
-        unsigned int cur_addr, resid_dcnt;
-        unsigned int native_addr, native_length;
-        int i;
-
-        if(scb_index > p->scb_data->numscbs)
-        {
-          printk(WARN_LEAD "invalid scb_index during WIDE_RESIDUE.\n",
-            p->host_no, -1, -1, -1);
-          /*
-           * XXX: Add error handling here
-           */
-          break;
-        }
-        scb = p->scb_data->scb_array[scb_index];
-        if(!(scb->flags & SCB_ACTIVE) || (scb->cmd == NULL))
-        {
-          printk(WARN_LEAD "invalid scb during WIDE_RESIDUE flags:0x%x "
-                 "scb->cmd:0x%x\n", p->host_no, CTL_OF_SCB(scb),
-                 scb->flags, (unsigned int)scb->cmd);
-          break;
-        }
-
-        /*
-         * We have a valid scb to use on this WIDE_RESIDUE message, so
-         * we need to walk the sg list looking for this particular sg
-         * segment, then see if we happen to be at the very beginning of
-         * the segment.  If we are, then we have to back things up to
-         * the previous segment.  If not, then we simply need to remove
-         * one byte from this segments address and add one to the byte
-         * count.
-         */
-        cur_addr = aic_inb(p, SHADDR) | (aic_inb(p, SHADDR + 1) << 8) |
-          (aic_inb(p, SHADDR + 2) << 16) | (aic_inb(p, SHADDR + 3) << 24);
-        resid_sgcnt = aic_inb(p, SCB_RESID_SGCNT);
-        resid_dcnt = aic_inb(p, SCB_RESID_DCNT) |
-          (aic_inb(p, SCB_RESID_DCNT + 1) << 8) |
-          (aic_inb(p, SCB_RESID_DCNT + 2) << 16);
-        index = scb->sg_count - (resid_sgcnt + 1);
-        native_addr = le32_to_cpu(scb->sg_list[index].address);
-        native_length = le32_to_cpu(scb->sg_list[index].length);
-        /*
-         * Make sure this is a valid sg_seg for the given pointer
-         */
-        if(cur_addr < native_addr ||
-           cur_addr > (native_addr + native_length + 1))
-        {
-          printk(WARN_LEAD "invalid cur_addr:0x%x during WIDE_RESIDUE\n",
-                 p->host_no, CTL_OF_SCB(scb), cur_addr);
-          if(index > 0)
-            printk(WARN_LEAD "  sg_address[-1]:0x%x sg_length[-1]:%d\n",
-                   p->host_no, CTL_OF_SCB(scb),
-                   le32_to_cpu(scb->sg_list[index - 1].address),
-                   le32_to_cpu(scb->sg_list[index - 1].length));
-          printk(WARN_LEAD "  sg_address:0x%x sg_length:%d\n",
-                 p->host_no, CTL_OF_SCB(scb),
-                 native_addr, native_length);
-          if(resid_sgcnt > 1)
-            printk(WARN_LEAD "  sg_address[1]:0x%x sg_length[1]:%d\n",
-                   p->host_no, CTL_OF_SCB(scb),
-                   le32_to_cpu(scb->sg_list[index + 1].address),
-                   le32_to_cpu(scb->sg_list[index + 1].length));
-          printk(WARN_LEAD "  cur_address:0x%x resid_dcnt:0x%06x\n",
-                 p->host_no, CTL_OF_SCB(scb),
-                 cur_addr, resid_dcnt);
-          break;
-        }
-
-        if( (resid_sgcnt == 0) &&
-            ((resid_dcnt == 0) || (resid_dcnt == 0xffffff)))
-        {
-          /*
-           * We are at the end of the transfer and this is about a byte
-           * we ignored already (because the sequencer knew this was
-           * the last segment and set the adapter to ignore any wide
-           * residue bytes that might come through, which is only done
-           * on the last scatter gather segment of transfers).
-           */
-          break;
-        }
-        else if(cur_addr == native_addr)
-        {
-          /*
-           * If our current address matches the sg_seg->address then we
-           * have to back up the sg array to the previous segment and set
-           * it up to have only one byte of transfer left to go.
-           */
-          if(index == 0)
-          {
-            printk(WARN_LEAD "bogus WIDE_RESIDUE message, no data has been "
-                   "transferred.\n", p->host_no, CTL_OF_SCB(scb));
-            break;
-          }
-          resid_sgcnt++;
-          index--;
-          cur_addr = le32_to_cpu(scb->sg_list[index].address) + 
-            le32_to_cpu(scb->sg_list[index].length) - 1;
-          native_addr = aic_inb(p, SG_NEXT) | (aic_inb(p, SG_NEXT + 1) << 8)
-            | (aic_inb(p, SG_NEXT + 2) << 16) | (aic_inb(p, SG_NEXT + 3) << 24);
-          native_addr -= SG_SIZEOF;
-          aic_outb(p, resid_sgcnt, SG_COUNT);
-          aic_outb(p, resid_sgcnt, SCB_RESID_SGCNT);
-          aic_outb(p, native_addr & 0xff, SG_NEXT);
-          aic_outb(p, (native_addr >> 8) & 0xff, SG_NEXT + 1);
-          aic_outb(p, (native_addr >> 16) & 0xff, SG_NEXT + 2);
-          aic_outb(p, (native_addr >> 24) & 0xff, SG_NEXT + 3);
-          aic_outb(p, 1, SCB_RESID_DCNT); 
-          aic_outb(p, 0, SCB_RESID_DCNT + 1); 
-          aic_outb(p, 0, SCB_RESID_DCNT + 2); 
-          aic_outb(p, 1, HCNT); 
-          aic_outb(p, 0, HCNT + 1); 
-          aic_outb(p, 0, HCNT + 2); 
-          aic_outb(p, cur_addr & 0xff, HADDR);
-          aic_outb(p, (cur_addr >> 8) & 0xff, HADDR + 1);
-          aic_outb(p, (cur_addr >> 16) & 0xff, HADDR + 2);
-          aic_outb(p, (cur_addr >> 24) & 0xff, HADDR + 3);
-        }
-        else
-        {
-          /*
-           * Back the data pointer up by one and add one to the remaining
-           * byte count.  Then store that in the HCNT and HADDR registers.
-           */
-          cur_addr--;
-          resid_dcnt++;
-          aic_outb(p, resid_dcnt & 0xff, SCB_RESID_DCNT); 
-          aic_outb(p, (resid_dcnt >> 8) & 0xff, SCB_RESID_DCNT + 1); 
-          aic_outb(p, (resid_dcnt >> 16) & 0xff, SCB_RESID_DCNT + 2); 
-          aic_outb(p, resid_dcnt & 0xff, HCNT); 
-          aic_outb(p, (resid_dcnt >> 8) & 0xff, HCNT + 1); 
-          aic_outb(p, (resid_dcnt >> 16) & 0xff, HCNT + 2); 
-          aic_outb(p, cur_addr & 0xff, HADDR);
-          aic_outb(p, (cur_addr >> 8) & 0xff, HADDR + 1);
-          aic_outb(p, (cur_addr >> 16) & 0xff, HADDR + 2);
-          aic_outb(p, (cur_addr >> 24) & 0xff, HADDR + 3);
-        }
-        /*
-         * The sequencer actually wants to find the new address and byte
-         * count in the SHCNT and SHADDR register sets.  These registers
-         * are a shadow of the regular HCNT and HADDR registers.  On the
-         * Ultra2 controllers, these registers are read only and the way
-         * we have to set their values is to put the values we want into
-         * the HCNT and HADDR registers and then output PRELOADEN into
-         * the DFCNTRL register which causes the card to latch the current
-         * values in the HADDR and HCNT registers and drop it through to
-         * the shadow registers.  On older cards we copy them directly
-         * across by hand.
-         */
-        if(p->features & AHC_ULTRA2)
-        {
-          aic_outb(p, aic_inb(p, DMAPARAMS), DFCNTRL);
-          i=0;
-          udelay(1);
-          while(((aic_inb(p, SSTAT0) & SDONE) != 0) && (i++ < 1000))
-          {
-            udelay(1);
-          }
-          aic_outb(p, aic_inb(p, DMAPARAMS) & ~(SCSIEN|HDMAEN), DFCNTRL);
-          i=0;
-          udelay(1);
-          while(((aic_inb(p, DFCNTRL) & (SCSIEN|HDMAEN)) != 0) && (i++ < 1000))
-          {
-            udelay(1);
-          }
-        }
-        else
-        {
-          aic_outb(p, resid_dcnt & 0xff, STCNT); 
-          aic_outb(p, (resid_dcnt >> 8) & 0xff, STCNT + 1); 
-          aic_outb(p, (resid_dcnt >> 16) & 0xff, STCNT + 2); 
-          aic_outb(p, cur_addr & 0xff, SHADDR);
-          aic_outb(p, (cur_addr >> 8) & 0xff, SHADDR + 1);
-          aic_outb(p, (cur_addr >> 16) & 0xff, SHADDR + 2);
-          aic_outb(p, (cur_addr >> 24) & 0xff, SHADDR + 3);
-        }
-      }
-      break;
-
-        
-#if AIC7XXX_NOT_YET 
-    case TRACEPOINT:
-      {
-        printk(INFO_LEAD "Tracepoint #1 reached.\n", p->host_no,
-               channel, target, lun);
-      }
-      break;
-
-    case TRACEPOINT2:
-      {
-        printk(INFO_LEAD "Tracepoint #2 reached.\n", p->host_no,
-               channel, target, lun);
-      }
-      break;
-
-    /* XXX Fill these in later */
-    case MSG_BUFFER_BUSY:
-      printk("aic7xxx: Message buffer busy.\n");
-      break;
-    case MSGIN_PHASEMIS:
-      printk("aic7xxx: Message-in phasemis.\n");
-      break;
-#endif
-
-    default:                   /* unknown */
-      printk(WARN_LEAD "Unknown SEQINT, INTSTAT 0x%x, SCSISIGI 0x%x.\n",
-             p->host_no, channel, target, lun, intstat,
-             aic_inb(p, SCSISIGI));
-      break;
-  }
-
-  /*
-   * Clear the sequencer interrupt and unpause the sequencer.
-   */
-  unpause_sequencer(p, /* unpause always */ TRUE);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_parse_msg
- *
- * Description:
- *   Parses incoming messages into actions on behalf of
- *   aic7xxx_handle_reqinit
- *_F*************************************************************************/
-static int
-aic7xxx_parse_msg(struct aic7xxx_host *p, struct aic7xxx_scb *scb)
-{
-  int reject, reply, done;
-  unsigned char target_scsirate, tindex;
-  unsigned short target_mask;
-  unsigned char target, channel, lun;
-
-  target = scb->cmd->target;
-  channel = scb->cmd->channel;
-  lun = scb->cmd->lun;
-  reply = reject = done = FALSE;
-  tindex = TARGET_INDEX(scb->cmd);
-  target_scsirate = aic_inb(p, TARG_SCSIRATE + tindex);
-  target_mask = (0x01 << tindex);
-
-  /*
-   * Parse as much of the message as is availible,
-   * rejecting it if we don't support it.  When
-   * the entire message is availible and has been
-   * handled, return TRUE indicating that we have
-   * parsed an entire message.
-   */
-
-  if (p->msg_buf[0] != MSG_EXTENDED)
-  {
-    reject = TRUE;
-  }
-
-  /*
-   * Just accept the length byte outright and perform
-   * more checking once we know the message type.
-   */
-
-  if ( !reject && (p->msg_len > 2) )
-  {
-    switch(p->msg_buf[2])
-    {
-      case MSG_EXT_SDTR:
-      {
-        unsigned int period, offset;
-        unsigned char maxsync, saved_offset, options;
-        struct aic7xxx_syncrate *syncrate;
-        
-        if (p->msg_buf[1] != MSG_EXT_SDTR_LEN)
-        {
-          reject = TRUE;
-          break;
-        }
-
-        if (p->msg_len < (MSG_EXT_SDTR_LEN + 2))
-        {
-          break;
-        }
-
-        period = p->msg_buf[3];
-        saved_offset = offset = p->msg_buf[4];
-        options = 0;
-
-        /*
-         * Even if we are an Ultra3 card, don't allow Ultra3 sync rates when
-         * using the SDTR messages.  We need the PPR messages to enable the
-         * higher speeds that include things like Dual Edge clocking.
-         */
-        if (p->features & AHC_ULTRA2)
-        {
-          if ( (aic_inb(p, SBLKCTL) & ENAB40) &&
-               !(aic_inb(p, SSTAT2) & EXP_ACTIVE) )
-          {
-            maxsync = AHC_SYNCRATE_ULTRA2;
-          }
-          else
-          {
-            maxsync = AHC_SYNCRATE_ULTRA;
-          }
-        }
-        else if (p->features & AHC_ULTRA)
-        {
-          maxsync = AHC_SYNCRATE_ULTRA;
-        }
-        else
-        {
-          maxsync = AHC_SYNCRATE_FAST;
-        }
-        /*
-         * We might have a device that is starting negotiation with us
-         * before we can start up negotiation with it....be prepared to
-         * have a device ask for a higher speed then we want to give it
-         * in that case
-         */
-        if ( (scb->flags & (SCB_MSGOUT_SENT|SCB_MSGOUT_SDTR)) !=
-             (SCB_MSGOUT_SENT|SCB_MSGOUT_SDTR) )
-        {
-          if (!(p->dev_flags[tindex] & DEVICE_SCANNED) &&
-              !(p->needsdtr_copy & target_mask) &&
-               (p->transinfo[tindex].user_offset) )
-          {
-            /*
-             * Not only is the device starting this up, but it also hasn't
-             * been scanned yet, so this would likely be our TUR or our
-             * INQUIRY command at scan time, so we need to use the
-             * settings from the SEEPROM if they existed.  Of course, even
-             * if we didn't find a SEEPROM, we stuffed default values into
-             * the user settings anyway, so use those in all cases.
-             */
-            p->transinfo[tindex].goal_period =
-              p->transinfo[tindex].user_period;
-            if(p->features & AHC_ULTRA2)
-            {
-              p->transinfo[tindex].goal_offset = MAX_OFFSET_ULTRA2;
-            }
-            else if (p->transinfo[tindex].cur_width)
-            {
-              p->transinfo[tindex].goal_offset = MAX_OFFSET_16BIT;
-            }
-            else
-            {
-              p->transinfo[tindex].goal_offset = MAX_OFFSET_8BIT;
-            }
-            p->needsdtr_copy |= target_mask;
-          }
-          if (aic7xxx_verbose & VERBOSE_NEGOTIATION2)
-          {
-            printk(INFO_LEAD "Received pre-emptive SDTR message from "
-                   "target.\n", p->host_no, CTL_OF_SCB(scb));
-          }
-          if ( !p->transinfo[tindex].goal_offset )
-            period = 255;
-          if ( p->transinfo[tindex].goal_period > period )
-            period = p->transinfo[tindex].goal_period;
-        }
-  
-        syncrate = aic7xxx_find_syncrate(p, &period, maxsync, &options);
-        aic7xxx_validate_offset(p, syncrate, &offset,
-                                target_scsirate & WIDEXFER);
-        aic7xxx_set_syncrate(p, syncrate, target, channel, period,
-                             offset, options, AHC_TRANS_ACTIVE|AHC_TRANS_CUR);
-
-        /*
-         * Did we drop to async?  Or are we sending a reply?  If we are,
-         * then we have to make sure that the reply value reflects the proper
-         * settings so we need to set the goal values according to what
-         * we need to send.
-         */
-        if ( (offset != saved_offset) ||
-             ((scb->flags & (SCB_MSGOUT_SENT|SCB_MSGOUT_SDTR)) !=
-              (SCB_MSGOUT_SENT|SCB_MSGOUT_SDTR) ) )
-        {
-          aic7xxx_set_syncrate(p, syncrate, target, channel, period, offset,
-                               options, AHC_TRANS_GOAL|AHC_TRANS_QUITE);
-        }
-        
-        /*
-         * Did we start this, if not, or if we went to low and had to
-         * go async, then send an SDTR back to the target
-         */
-        p->needsdtr &= ~target_mask;
-        p->dtr_pending &= ~target_mask;
-        if ( ((scb->flags & (SCB_MSGOUT_SENT|SCB_MSGOUT_SDTR)) !=
-              (SCB_MSGOUT_SENT|SCB_MSGOUT_SDTR)) ||
-             (offset != saved_offset) )
-        {
-          reply = TRUE;
-          p->dtr_pending |= target_mask;
-          scb->flags &= ~SCB_MSGOUT_BITS;
-          scb->flags |= SCB_MSGOUT_SDTR;
-          aic_outb(p, HOST_MSG, MSG_OUT);
-          aic_outb(p, aic_inb(p, SCSISIGO) | ATNO, SCSISIGO);
-        }
-        done = TRUE;
-        break;
-      }
-      case MSG_EXT_WDTR:
-      {
-        unsigned char bus_width;
-          
-        if (p->msg_buf[1] != MSG_EXT_WDTR_LEN)
-        {
-          reject = TRUE;
-          break;
-        }
-
-        if (p->msg_len < (MSG_EXT_WDTR_LEN + 2))
-        {
-          break;
-        }
-
-        bus_width = p->msg_buf[3];
-        if ( (scb->flags & (SCB_MSGOUT_SENT|SCB_MSGOUT_WDTR)) ==
-             (SCB_MSGOUT_SENT|SCB_MSGOUT_WDTR) )
-        {
-          switch(bus_width)
-          {
-            default:
-            {
-              reject = TRUE;
-              if ( (aic7xxx_verbose & VERBOSE_NEGOTIATION2) &&
-                   ((p->dev_flags[tindex] & DEVICE_PRINT_DTR) ||
-                    (aic7xxx_verbose > 0xffff)) )
-              {
-                printk(INFO_LEAD "Requesting %d bit transfers, rejecting.\n",
-                  p->host_no, CTL_OF_SCB(scb), 8 * (0x01 << bus_width));
-              }
-            } /* We fall through on purpose */
-            case MSG_EXT_WDTR_BUS_8_BIT:
-            {
-              bus_width = MSG_EXT_WDTR_BUS_8_BIT;
-              p->needwdtr_copy &= ~target_mask;
-              break;
-            }
-            case MSG_EXT_WDTR_BUS_16_BIT:
-            {
-              break;
-            }
-          }
-          p->dtr_pending &= ~target_mask;
-          p->needwdtr &= ~target_mask;
-        }
-        else
-        {
-          if ( !(p->dev_flags[tindex] & DEVICE_SCANNED) )
-          {
-            /* 
-             * Well, we now know the WDTR and SYNC caps of this device since
-             * it contacted us first, mark it as such and copy the user stuff
-             * over to the goal stuff.
-             */
-            p->transinfo[tindex].goal_period =
-              p->transinfo[tindex].user_period;
-            if(p->transinfo[tindex].user_offset)
-            {
-              if(p->features & AHC_ULTRA2)
-              {
-                p->transinfo[tindex].goal_offset = MAX_OFFSET_ULTRA2;
-              }
-              else if( p->transinfo[tindex].user_width &&
-                       (bus_width == MSG_EXT_WDTR_BUS_16_BIT) &&
-                       p->features & AHC_WIDE )
-              {
-                p->transinfo[tindex].goal_offset = MAX_OFFSET_16BIT;
-              }
-              else
-              {
-                p->transinfo[tindex].goal_offset = MAX_OFFSET_8BIT;
-              }
-            }
-            p->transinfo[tindex].goal_width =
-              p->transinfo[tindex].user_width;
-            p->needwdtr_copy |= target_mask;
-            p->needsdtr_copy |= target_mask;
-          }
-          if (aic7xxx_verbose & VERBOSE_NEGOTIATION2)
-          {
-            printk(INFO_LEAD "Received pre-emptive WDTR message from "
-                   "target.\n", p->host_no, CTL_OF_SCB(scb));
-          }
-          switch(bus_width)
-          {
-            default:
-            {
-              if ( (p->features & AHC_WIDE) &&
-                   (p->transinfo[tindex].goal_width ==
-                    MSG_EXT_WDTR_BUS_16_BIT) )
-              {
-                bus_width = MSG_EXT_WDTR_BUS_16_BIT;
-                break;
-              }
-            } /* Fall through if we aren't a wide card */
-            case MSG_EXT_WDTR_BUS_8_BIT:
-            {
-              p->needwdtr_copy &= ~target_mask;
-              bus_width = MSG_EXT_WDTR_BUS_8_BIT;
-              aic7xxx_set_width(p, target, channel, lun, bus_width,
-                                AHC_TRANS_GOAL|AHC_TRANS_QUITE);
-              break;
-            }
-          }
-          reply = TRUE;
-          scb->flags &= ~SCB_MSGOUT_BITS;
-          scb->flags |= SCB_MSGOUT_WDTR;
-          p->needwdtr &= ~target_mask;
-          p->dtr_pending |= target_mask;
-          aic_outb(p, HOST_MSG, MSG_OUT);
-          aic_outb(p, aic_inb(p, SCSISIGO) | ATNO, SCSISIGO);
-        }
-        aic7xxx_set_width(p, target, channel, lun, bus_width,
-                          AHC_TRANS_ACTIVE|AHC_TRANS_CUR);
-        
-        /*
-         * By virtue of the SCSI spec, a WDTR message negates any existing
-         * SDTR negotiations.  So, even if needsdtr isn't marked for this
-         * device, we still have to do a new SDTR message if the device
-         * supports SDTR at all.  Therefore, we check needsdtr_copy instead
-         * of needstr.
-         */
-        aic7xxx_set_syncrate(p, NULL, target, channel, 0, 0, 0,
-                             AHC_TRANS_ACTIVE|AHC_TRANS_CUR|AHC_TRANS_QUITE);
-        p->needsdtr |= (p->needsdtr_copy & target_mask);
-        done = TRUE;
-        break;
-      }
-      case MSG_EXT_PPR:
-      {
-        unsigned char bus_width, trans_options, new_trans_options;
-        unsigned int period, offset;
-        unsigned char maxsync, saved_offset;
-        struct aic7xxx_syncrate *syncrate;
-        
-        if (p->msg_buf[1] != MSG_EXT_PPR_LEN)
-        {
-          reject = TRUE;
-          break;
-        }
-
-        if (p->msg_len < (MSG_EXT_PPR_LEN + 2))
-        {
-          break;
-        }
-
-        period = p->msg_buf[3];
-        offset = saved_offset = p->msg_buf[5];
-        bus_width = p->msg_buf[6];
-        trans_options = new_trans_options = p->msg_buf[7] & 0xf;
-
-        if(aic7xxx_verbose & VERBOSE_NEGOTIATION2)
-        {
-          printk(INFO_LEAD "Parsing PPR message (%d/%d/%d/%d)\n",
-                 p->host_no, CTL_OF_SCB(scb), period, offset, bus_width,
-                 trans_options);
-        }
-
-        if ( (aic_inb(p, SBLKCTL) & ENAB40) &&
-            !(aic_inb(p, SSTAT2) & EXP_ACTIVE) )
-        {
-          if(p->features & AHC_ULTRA3)
-          {
-            maxsync = AHC_SYNCRATE_ULTRA3;
-          }
-          else
-          {
-            maxsync = AHC_SYNCRATE_ULTRA2;
-          }
-        }
-        else
-        {
-          maxsync = AHC_SYNCRATE_ULTRA;
-        }
-        /*
-         * We might have a device that is starting negotiation with us
-         * before we can start up negotiation with it....be prepared to
-         * have a device ask for a higher speed then we want to give it
-         * in that case
-         */
-        if ( (scb->flags & (SCB_MSGOUT_SENT|SCB_MSGOUT_PPR)) !=
-             (SCB_MSGOUT_SENT|SCB_MSGOUT_PPR) )
-        {
-          reply = TRUE;
-          scb->flags &= ~SCB_MSGOUT_BITS;
-          scb->flags |= SCB_MSGOUT_PPR;
-	  p->dev_flags[tindex] |= DEVICE_SCSI_3;
-          if (!(p->dev_flags[tindex] & DEVICE_SCANNED))
-          {
-            /*
-             * Not only is the device starting this up, but it also hasn't
-             * been scanned yet, so this would likely be our TUR or our
-             * INQUIRY command at scan time, so we need to use the
-             * settings from the SEEPROM if they existed.  Of course, even
-             * if we didn't find a SEEPROM, we stuffed default values into
-             * the user settings anyway, so use those in all cases.
-             */
-            p->transinfo[tindex].goal_period =
-              p->transinfo[tindex].user_period;
-            if(p->transinfo[tindex].user_offset)
-            {
-              if(p->features & AHC_ULTRA2)
-              {
-                p->transinfo[tindex].goal_offset = MAX_OFFSET_ULTRA2;
-              }
-              else if( p->transinfo[tindex].user_width &&
-                       (bus_width == MSG_EXT_WDTR_BUS_16_BIT) &&
-                       p->features & AHC_WIDE )
-              {
-                p->transinfo[tindex].goal_offset = MAX_OFFSET_16BIT;
-              }
-              else
-              {
-                p->transinfo[tindex].goal_offset = MAX_OFFSET_8BIT;
-              }
-            }
-            p->transinfo[tindex].goal_width =
-              p->transinfo[tindex].user_width;
-            p->transinfo[tindex].goal_options =
-              p->transinfo[tindex].user_options;
-          }
-          if (aic7xxx_verbose & VERBOSE_NEGOTIATION2)
-          {
-            printk(INFO_LEAD "Received pre-emptive PPR message from "
-                   "target.\n", p->host_no, CTL_OF_SCB(scb));
-          }
-          if ( !p->transinfo[tindex].goal_offset )
-            period = 255;
-          if ( p->transinfo[tindex].goal_period > period )
-            period = p->transinfo[tindex].goal_period;
-          if ( p->transinfo[tindex].goal_options == 0 )
-            new_trans_options = 0;
-          switch(bus_width)
-          {
-            default:
-            {
-              if ( (p->features & AHC_WIDE) &&
-                   (p->transinfo[tindex].goal_width ==
-                    MSG_EXT_WDTR_BUS_16_BIT) )
-              {
-                bus_width = MSG_EXT_WDTR_BUS_16_BIT;
-                break;
-              }
-            } /* Fall through if we aren't a wide card */
-            case MSG_EXT_WDTR_BUS_8_BIT:
-            {
-              p->needwdtr_copy &= ~target_mask;
-              bus_width = MSG_EXT_WDTR_BUS_8_BIT;
-              aic7xxx_set_width(p, target, channel, lun, bus_width,
-                                AHC_TRANS_GOAL|AHC_TRANS_QUITE);
-              break;
-            }
-          }
-          if ( (p->transinfo[tindex].goal_period > 9) ||
-               (p->transinfo[tindex].goal_options == 0) )
-          {
-            scb->flags &= ~SCB_MSGOUT_BITS;
-            reject = TRUE;
-            reply = FALSE;
-            p->needppr &= ~(1 << tindex);
-            p->needppr_copy &= ~(1 << tindex);
-            if ( p->transinfo[tindex].goal_offset )
-            {
-              p->needsdtr |= (1 << tindex);
-              p->needsdtr_copy |= (1 << tindex);
-            }
-            if ( p->transinfo[tindex].goal_width )
-            {
-              p->needwdtr |= (1 << tindex);
-              p->needwdtr_copy |= (1 << tindex);
-            }
-          }
-        }
-        else
-        {
-          switch(bus_width)
-          {
-            default:
-            {
-              reject = TRUE;
-              if ( (aic7xxx_verbose & VERBOSE_NEGOTIATION2) &&
-                   ((p->dev_flags[tindex] & DEVICE_PRINT_DTR) ||
-                    (aic7xxx_verbose > 0xffff)) )
-              {
-                printk(INFO_LEAD "Requesting %d bit transfers, rejecting.\n",
-                  p->host_no, CTL_OF_SCB(scb), 8 * (0x01 << bus_width));
-              }
-            } /* We fall through on purpose */
-            case MSG_EXT_WDTR_BUS_8_BIT:
-            {
-              /*
-               * According to the spec, if we aren't wide, we also can't be
-               * Dual Edge so clear the options byte
-               */
-              new_trans_options = 0;
-              bus_width = MSG_EXT_WDTR_BUS_8_BIT;
-              break;
-            }
-            case MSG_EXT_WDTR_BUS_16_BIT:
-            {
-              break;
-            }
-          }
-        }
-
-        if ( !reject )
-        {
-          aic7xxx_set_width(p, target, channel, lun, bus_width,
-                            AHC_TRANS_ACTIVE|AHC_TRANS_CUR);
-          syncrate = aic7xxx_find_syncrate(p, &period, maxsync,
-                                           &new_trans_options);
-          aic7xxx_validate_offset(p, syncrate, &offset, bus_width);
-          aic7xxx_set_syncrate(p, syncrate, target, channel, period,
-                               offset, new_trans_options,
-                               AHC_TRANS_ACTIVE|AHC_TRANS_CUR);
-        }
-
-        p->dtr_pending &= ~target_mask;
-        p->needppr &= ~target_mask;
-        if(reply)
-        {
-          p->dtr_pending |= target_mask;
-          scb->flags &= ~SCB_MSGOUT_BITS;
-          scb->flags |= SCB_MSGOUT_PPR;
-          aic_outb(p, HOST_MSG, MSG_OUT);
-          aic_outb(p, aic_inb(p, SCSISIGO) | ATNO, SCSISIGO);
-        }
-        done = TRUE;
-        break;
-      }
-      default:
-      {
-        reject = TRUE;
-        break;
-      }
-    } /* end of switch(p->msg_type) */
-  } /* end of if (!reject && (p->msg_len > 2)) */
-
-  if (!reply && reject)
-  {
-    aic_outb(p, MSG_MESSAGE_REJECT, MSG_OUT);
-    aic_outb(p, aic_inb(p, SCSISIGO) | ATNO, SCSISIGO);
-    done = TRUE;
-  }
-  return(done);
-}
-
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_handle_reqinit
- *
- * Description:
- *   Interrupt handler for REQINIT interrupts (used to transfer messages to
- *    and from devices).
- *_F*************************************************************************/
-static void
-aic7xxx_handle_reqinit(struct aic7xxx_host *p, struct aic7xxx_scb *scb)
-{
-  unsigned char lastbyte;
-  unsigned char phasemis;
-  int done = FALSE;
-
-  switch(p->msg_type)
-  {
-    case MSG_TYPE_INITIATOR_MSGOUT:
-      {
-        if (p->msg_len == 0)
-          panic("aic7xxx: REQINIT with no active message!\n");
-
-        lastbyte = (p->msg_index == (p->msg_len - 1));
-        phasemis = ( aic_inb(p, SCSISIGI) & PHASE_MASK) != P_MESGOUT;
-
-        if (lastbyte || phasemis)
-        {
-          /* Time to end the message */
-          p->msg_len = 0;
-          p->msg_type = MSG_TYPE_NONE;
-          /*
-           * NOTE-TO-MYSELF: If you clear the REQINIT after you
-           * disable REQINITs, then cases of REJECT_MSG stop working
-           * and hang the bus
-           */
-          aic_outb(p, aic_inb(p, SIMODE1) & ~ENREQINIT, SIMODE1);
-          aic_outb(p, CLRSCSIINT, CLRINT);
-          p->flags &= ~AHC_HANDLING_REQINITS;
-
-          if (phasemis == 0)
-          {
-            aic_outb(p, p->msg_buf[p->msg_index], SINDEX);
-            aic_outb(p, 0, RETURN_1);
-#ifdef AIC7XXX_VERBOSE_DEBUGGING
-            if (aic7xxx_verbose > 0xffff)
-              printk(INFO_LEAD "Completed sending of REQINIT message.\n",
-                     p->host_no, CTL_OF_SCB(scb));
-#endif
-          }
-          else
-          {
-            aic_outb(p, MSGOUT_PHASEMIS, RETURN_1);
-#ifdef AIC7XXX_VERBOSE_DEBUGGING
-            if (aic7xxx_verbose > 0xffff)
-              printk(INFO_LEAD "PHASEMIS while sending REQINIT message.\n",
-                     p->host_no, CTL_OF_SCB(scb));
-#endif
-          }
-          unpause_sequencer(p, TRUE);
-        }
-        else
-        {
-          /*
-           * Present the byte on the bus (clearing REQINIT) but don't
-           * unpause the sequencer.
-           */
-          aic_outb(p, CLRREQINIT, CLRSINT1);
-          aic_outb(p, CLRSCSIINT, CLRINT);
-          aic_outb(p,  p->msg_buf[p->msg_index++], SCSIDATL);
-        }
-        break;
-      }
-    case MSG_TYPE_INITIATOR_MSGIN:
-      {
-        phasemis = ( aic_inb(p, SCSISIGI) & PHASE_MASK ) != P_MESGIN;
-
-        if (phasemis == 0)
-        {
-          p->msg_len++;
-          /* Pull the byte in without acking it */
-          p->msg_buf[p->msg_index] = aic_inb(p, SCSIBUSL);
-          done = aic7xxx_parse_msg(p, scb);
-          /* Ack the byte */
-          aic_outb(p, CLRREQINIT, CLRSINT1);
-          aic_outb(p, CLRSCSIINT, CLRINT);
-          aic_inb(p, SCSIDATL);
-          p->msg_index++;
-        }
-        if (phasemis || done)
-        {
-#ifdef AIC7XXX_VERBOSE_DEBUGGING
-          if (aic7xxx_verbose > 0xffff)
-          {
-            if (phasemis)
-              printk(INFO_LEAD "PHASEMIS while receiving REQINIT message.\n",
-                     p->host_no, CTL_OF_SCB(scb));
-            else
-              printk(INFO_LEAD "Completed receipt of REQINIT message.\n",
-                     p->host_no, CTL_OF_SCB(scb));
-          }
-#endif
-          /* Time to end our message session */
-          p->msg_len = 0;
-          p->msg_type = MSG_TYPE_NONE;
-          aic_outb(p, aic_inb(p, SIMODE1) & ~ENREQINIT, SIMODE1);
-          aic_outb(p, CLRSCSIINT, CLRINT);
-          p->flags &= ~AHC_HANDLING_REQINITS;
-          unpause_sequencer(p, TRUE);
-        }
-        break;
-      }
-    default:
-      {
-        panic("aic7xxx: Unknown REQINIT message type.\n");
-        break;
-      }
-  } /* End of switch(p->msg_type) */
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_handle_scsiint
- *
- * Description:
- *   Interrupt handler for SCSI interrupts (SCSIINT).
- *-F*************************************************************************/
-static void
-aic7xxx_handle_scsiint(struct aic7xxx_host *p, unsigned char intstat)
-{
-  unsigned char scb_index;
-  unsigned char status;
-  struct aic7xxx_scb *scb;
-
-  scb_index = aic_inb(p, SCB_TAG);
-  status = aic_inb(p, SSTAT1);
-
-  if (scb_index < p->scb_data->numscbs)
-  {
-    scb = p->scb_data->scb_array[scb_index];
-    if ((scb->flags & SCB_ACTIVE) == 0)
-    {
-      scb = NULL;
-    }
-  }
-  else
-  {
-    scb = NULL;
-  }
-
-
-  if ((status & SCSIRSTI) != 0)
-  {
-    int channel;
-
-    if ( (p->chip & AHC_CHIPID_MASK) == AHC_AIC7770 )
-      channel = (aic_inb(p, SBLKCTL) & SELBUSB) >> 3;
-    else
-      channel = 0;
-
-    if (aic7xxx_verbose & VERBOSE_RESET)
-      printk(WARN_LEAD "Someone else reset the channel!!\n",
-           p->host_no, channel, -1, -1);
-    if (aic7xxx_panic_on_abort)
-      aic7xxx_panic_abort(p, NULL);
-    /*
-     * Go through and abort all commands for the channel, but do not
-     * reset the channel again.
-     */
-    aic7xxx_reset_channel(p, channel, /* Initiate Reset */ FALSE);
-    aic7xxx_run_done_queue(p, TRUE);
-    scb = NULL;
-  }
-  else if ( ((status & BUSFREE) != 0) && ((status & SELTO) == 0) )
-  {
-    /*
-     * First look at what phase we were last in.  If it's message-out,
-     * chances are pretty good that the bus free was in response to
-     * one of our abort requests.
-     */
-    unsigned char lastphase = aic_inb(p, LASTPHASE);
-    unsigned char saved_tcl = aic_inb(p, SAVED_TCL);
-    unsigned char target = (saved_tcl >> 4) & 0x0F;
-    int channel;
-    int printerror = TRUE;
-
-    if ( (p->chip & AHC_CHIPID_MASK) == AHC_AIC7770 )
-      channel = (aic_inb(p, SBLKCTL) & SELBUSB) >> 3;
-    else
-      channel = 0;
-
-    aic_outb(p, aic_inb(p, SCSISEQ) & (ENSELI|ENRSELI|ENAUTOATNP),
-             SCSISEQ);
-    if (lastphase == P_MESGOUT)
-    {
-      unsigned char message;
-
-      message = aic_inb(p, SINDEX);
-
-      if ((message == MSG_ABORT) || (message == MSG_ABORT_TAG))
-      {
-        if (aic7xxx_verbose & VERBOSE_ABORT_PROCESS)
-          printk(INFO_LEAD "SCB %d abort delivered.\n", p->host_no,
-            CTL_OF_SCB(scb), scb->hscb->tag);
-        aic7xxx_reset_device(p, target, channel, ALL_LUNS,
-                (message == MSG_ABORT) ? SCB_LIST_NULL : scb->hscb->tag );
-        aic7xxx_run_done_queue(p, TRUE);
-        scb = NULL;
-        printerror = 0;
-      }
-      else if (message == MSG_BUS_DEV_RESET)
-      {
-        aic7xxx_handle_device_reset(p, target, channel);
-        scb = NULL;
-        printerror = 0;
-      }
-    }
-    if ( (scb != NULL) &&
-         (scb->cmd == p->dev_dtr_cmnd[TARGET_INDEX(scb->cmd)]) )
-    {
-      /*
-       * This might be a SCSI-3 device that is dropping the bus due to
-       * errors and signalling that we should reduce the transfer speed.
-       * All we have to do is complete this command (since it's a negotiation
-       * command already) and the checksum routine should flag an error and
-       * reduce the speed setting and renegotiate.  We call the reset routing
-       * just to clean out the hardware from this scb.
-       */
-      printerror = 0;
-      aic7xxx_reset_device(p, target, channel, ALL_LUNS, scb->hscb->tag);
-      aic7xxx_run_done_queue(p, TRUE);
-      scb = NULL;
-    }
-    if (printerror != 0)
-    {
-      if (scb != NULL)
-      {
-        unsigned char tag;
-
-        if ((scb->hscb->control & TAG_ENB) != 0)
-        {
-          tag = scb->hscb->tag;
-        }
-        else
-        {
-          tag = SCB_LIST_NULL;
-        }
-        aic7xxx_reset_device(p, target, channel, ALL_LUNS, tag);
-        aic7xxx_run_done_queue(p, TRUE);
-      }
-      else
-      {
-        aic7xxx_reset_device(p, target, channel, ALL_LUNS, SCB_LIST_NULL);
-        aic7xxx_run_done_queue(p, TRUE);
-      }
-      printk(INFO_LEAD "Unexpected busfree, LASTPHASE = 0x%x, "
-             "SEQADDR = 0x%x\n", p->host_no, channel, target, -1, lastphase,
-             (aic_inb(p, SEQADDR1) << 8) | aic_inb(p, SEQADDR0));
-      scb = NULL;
-    }
-    aic_outb(p, MSG_NOOP, MSG_OUT);
-    aic_outb(p, aic_inb(p, SIMODE1) & ~(ENBUSFREE|ENREQINIT),
-      SIMODE1);
-    p->flags &= ~AHC_HANDLING_REQINITS;
-    aic_outb(p, CLRBUSFREE, CLRSINT1);
-    aic_outb(p, CLRSCSIINT, CLRINT);
-    restart_sequencer(p);
-    unpause_sequencer(p, TRUE);
-  }
-  else if ((status & SELTO) != 0)
-  {
-    unsigned char scbptr;
-    unsigned char nextscb;
-    Scsi_Cmnd *cmd;
-
-    scbptr = aic_inb(p, WAITING_SCBH);
-    if (scbptr > p->scb_data->maxhscbs)
-    {
-      /*
-       * I'm still trying to track down exactly how this happens, but until
-       * I find it, this code will make sure we aren't passing bogus values
-       * into the SCBPTR register, even if that register will just wrap
-       * things around, we still don't like having out of range variables.
-       *
-       * NOTE: Don't check the aic7xxx_verbose variable, I want this message
-       * to always be displayed.
-       */
-      printk(INFO_LEAD "Invalid WAITING_SCBH value %d, improvising.\n",
-             p->host_no, -1, -1, -1, scbptr);
-      if (p->scb_data->maxhscbs > 4)
-        scbptr &= (p->scb_data->maxhscbs - 1);
-      else
-        scbptr &= 0x03;
-    }
-    aic_outb(p, scbptr, SCBPTR);
-    scb_index = aic_inb(p, SCB_TAG);
-
-    scb = NULL;
-    if (scb_index < p->scb_data->numscbs)
-    {
-      scb = p->scb_data->scb_array[scb_index];
-      if ((scb->flags & SCB_ACTIVE) == 0)
-      {
-        scb = NULL;
-      }
-    }
-    if (scb == NULL)
-    {
-      printk(WARN_LEAD "Referenced SCB %d not valid during SELTO.\n",
-             p->host_no, -1, -1, -1, scb_index);
-      printk(KERN_WARNING "        SCSISEQ = 0x%x SEQADDR = 0x%x SSTAT0 = 0x%x "
-             "SSTAT1 = 0x%x\n", aic_inb(p, SCSISEQ),
-             aic_inb(p, SEQADDR0) | (aic_inb(p, SEQADDR1) << 8),
-             aic_inb(p, SSTAT0), aic_inb(p, SSTAT1));
-      if (aic7xxx_panic_on_abort)
-        aic7xxx_panic_abort(p, NULL);
-    }
-    else
-    {
-      cmd = scb->cmd;
-      cmd->result = (DID_TIME_OUT << 16);
-
-      /*
-       * Clear out this hardware SCB
-       */
-      aic_outb(p, 0, SCB_CONTROL);
-
-      /*
-       * Clear out a few values in the card that are in an undetermined
-       * state.
-       */
-      aic_outb(p, MSG_NOOP, MSG_OUT);
-
-      /*
-       * Shift the waiting for selection queue forward
-       */
-      nextscb = aic_inb(p, SCB_NEXT);
-      aic_outb(p, nextscb, WAITING_SCBH);
-
-      /*
-       * Put this SCB back on the free list.
-       */
-      aic7xxx_add_curscb_to_free_list(p);
-#ifdef AIC7XXX_VERBOSE_DEBUGGING
-      if (aic7xxx_verbose > 0xffff)
-        printk(INFO_LEAD "Selection Timeout.\n", p->host_no, CTL_OF_SCB(scb));
-#endif
-      if (scb->flags & SCB_QUEUED_ABORT)
-      {
-        /*
-         * We know that this particular SCB had to be the queued abort since
-         * the disconnected SCB would have gotten a reconnect instead.
-         * What we need to do then is to let the command timeout again so
-         * we get a reset since this abort just failed.
-         */
-        cmd->result = 0;
-        scb = NULL;
-      }
-      else if (scb->cmd == p->dev_dtr_cmnd[TARGET_INDEX(scb->cmd)])
-      {
-        /*
-         * Turn off the needsdtr, needwdtr, and needppr bits since this device
-         * doesn't seem to exist.
-         */
-        p->needppr &= ~(0x01 << TARGET_INDEX(scb->cmd));
-        p->needppr_copy &= ~(0x01 << TARGET_INDEX(scb->cmd));
-        p->needsdtr &= ~(0x01 << TARGET_INDEX(scb->cmd));
-        p->needsdtr_copy &= ~(0x01 << TARGET_INDEX(scb->cmd));
-        p->needwdtr &= ~(0x01 << TARGET_INDEX(scb->cmd));
-        p->needwdtr_copy &= ~(0x01 << TARGET_INDEX(scb->cmd));
-      }
-    }
-    /*
-     * Keep the sequencer from trying to restart any selections
-     */
-    aic_outb(p, aic_inb(p, SCSISEQ) & ~ENSELO, SCSISEQ);
-    /*
-     * Make sure the data bits on the bus are released
-     * Don't do this on 7770 chipsets, it makes them give us
-     * a BRKADDRINT and kills the card.
-     */
-    if( (p->chip & ~AHC_CHIPID_MASK) == AHC_PCI )
-      aic_outb(p, 0, SCSIBUSL);
-
-    /*
-     * Delay for the selection timeout delay period then stop the selection
-     */
-    udelay(301);
-    aic_outb(p, CLRSELINGO, CLRSINT0);
-    /*
-     * Clear out all the interrupt status bits
-     */
-    aic_outb(p, aic_inb(p, SIMODE1) & ~(ENREQINIT|ENBUSFREE), SIMODE1);
-    p->flags &= ~AHC_HANDLING_REQINITS;
-    aic_outb(p, CLRSELTIMEO | CLRBUSFREE, CLRSINT1);
-    aic_outb(p, CLRSCSIINT, CLRINT);
-    /*
-     * Restarting the sequencer will stop the selection and make sure devices
-     * are allowed to reselect in.
-     */
-    restart_sequencer(p);
-    unpause_sequencer(p, TRUE);
-  }
-  else if (scb == NULL)
-  {
-    printk(WARN_LEAD "aic7xxx_isr - referenced scb not valid "
-           "during scsiint 0x%x scb(%d)\n"
-           "      SIMODE0 0x%x, SIMODE1 0x%x, SSTAT0 0x%x, SEQADDR 0x%x\n",
-           p->host_no, -1, -1, -1, status, scb_index, aic_inb(p, SIMODE0),
-           aic_inb(p, SIMODE1), aic_inb(p, SSTAT0),
-           (aic_inb(p, SEQADDR1) << 8) | aic_inb(p, SEQADDR0));
-    /*
-     * Turn off the interrupt and set status to zero, so that it
-     * falls through the rest of the SCSIINT code.
-     */
-    aic_outb(p, status, CLRSINT1);
-    aic_outb(p, CLRSCSIINT, CLRINT);
-    unpause_sequencer(p, /* unpause always */ TRUE);
-    scb = NULL;
-  }
-  else if (status & SCSIPERR)
-  {
-    /*
-     * Determine the bus phase and queue an appropriate message.
-     */
-    char  *phase;
-    Scsi_Cmnd *cmd;
-    unsigned char mesg_out = MSG_NOOP;
-    unsigned char lastphase = aic_inb(p, LASTPHASE);
-    unsigned char sstat2 = aic_inb(p, SSTAT2);
-    unsigned char tindex = TARGET_INDEX(scb->cmd);
-
-    cmd = scb->cmd;
-    switch (lastphase)
-    {
-      case P_DATAOUT:
-        phase = "Data-Out";
-        break;
-      case P_DATAIN:
-        phase = "Data-In";
-        mesg_out = MSG_INITIATOR_DET_ERR;
-        break;
-      case P_COMMAND:
-        phase = "Command";
-        break;
-      case P_MESGOUT:
-        phase = "Message-Out";
-        break;
-      case P_STATUS:
-        phase = "Status";
-        mesg_out = MSG_INITIATOR_DET_ERR;
-        break;
-      case P_MESGIN:
-        phase = "Message-In";
-        mesg_out = MSG_PARITY_ERROR;
-        break;
-      default:
-        phase = "unknown";
-        break;
-    }
-
-    /*
-     * A parity error has occurred during a data
-     * transfer phase. Flag it and continue.
-     */
-    if( (p->features & AHC_ULTRA3) && 
-        (aic_inb(p, SCSIRATE) & AHC_SYNCRATE_CRC) &&
-        (lastphase == P_DATAIN) )
-    {
-      printk(WARN_LEAD "CRC error during %s phase.\n",
-             p->host_no, CTL_OF_SCB(scb), phase);
-      if(sstat2 & CRCVALERR)
-      {
-        printk(WARN_LEAD "  CRC error in intermediate CRC packet.\n",
-               p->host_no, CTL_OF_SCB(scb));
-      }
-      if(sstat2 & CRCENDERR)
-      {
-        printk(WARN_LEAD "  CRC error in ending CRC packet.\n",
-               p->host_no, CTL_OF_SCB(scb));
-      }
-      if(sstat2 & CRCREQERR)
-      {
-        printk(WARN_LEAD "  Target incorrectly requested a CRC packet.\n",
-               p->host_no, CTL_OF_SCB(scb));
-      }
-      if(sstat2 & DUAL_EDGE_ERROR)
-      {
-        printk(WARN_LEAD "  Dual Edge transmission error.\n",
-               p->host_no, CTL_OF_SCB(scb));
-      }
-    }
-    else if( (lastphase == P_MESGOUT) &&
-             (cmd == p->dev_dtr_cmnd[tindex]) &&
-             (scb->flags & SCB_MSGOUT_PPR) )
-    {
-      /*
-       * As per the draft specs, any device capable of supporting any of
-       * the option values other than 0 are not allowed to reject the
-       * PPR message.  Instead, they must negotiate out what they do
-       * support instead of rejecting our offering or else they cause
-       * a parity error during msg_out phase to signal that they don't
-       * like our settings.
-       */
-      p->needppr &= ~(1 << tindex);
-      p->needppr_copy &= ~(1 << tindex);
-      aic7xxx_set_width(p, scb->cmd->target, scb->cmd->channel, scb->cmd->lun,
-                        MSG_EXT_WDTR_BUS_8_BIT,
-                        (AHC_TRANS_ACTIVE|AHC_TRANS_CUR|AHC_TRANS_QUITE));
-      aic7xxx_set_syncrate(p, NULL, scb->cmd->target, scb->cmd->channel, 0, 0,
-                           0, AHC_TRANS_ACTIVE|AHC_TRANS_CUR|AHC_TRANS_QUITE);
-      p->transinfo[tindex].goal_options = 0;
-      p->dtr_pending &= ~(1 << tindex);
-      scb->flags &= ~SCB_MSGOUT_BITS;
-      if(aic7xxx_verbose & VERBOSE_NEGOTIATION2)
-      {
-        printk(INFO_LEAD "parity error during PPR message, reverting "
-               "to WDTR/SDTR\n", p->host_no, CTL_OF_SCB(scb));
-      }
-      if ( p->transinfo[tindex].goal_width )
-      {
-        p->needwdtr |= (1 << tindex);
-        p->needwdtr_copy |= (1 << tindex);
-      }
-      if ( p->transinfo[tindex].goal_offset )
-      {
-        if( p->transinfo[tindex].goal_period <= 9 )
-        {
-          p->transinfo[tindex].goal_period = 10;
-        }
-        p->needsdtr |= (1 << tindex);
-        p->needsdtr_copy |= (1 << tindex);
-      }
-      scb = NULL;
-    }
-    else if(p->dev_flags[tindex] & DEVICE_PARITY_ERROR)
-    {
-      struct aic7xxx_syncrate *syncrate;
-      unsigned int period = p->transinfo[tindex].cur_period;
-      unsigned char options = p->transinfo[tindex].cur_options;
-      /*
-       * oops, we had a failure, lower the transfer rate and try again.  It's
-       * worth noting here that it might be wise to also check for typical
-       * wide setting on narrow cable type problems and try disabling wide
-       * instead of slowing down if those exist.  That's hard to do with simple
-       * checksums though.
-       */
-      printk(WARN_LEAD "Parity error during %s phase.\n",
-             p->host_no, CTL_OF_SCB(scb), phase);
-      if((syncrate = aic7xxx_find_syncrate(p, &period, 0, &options)) != NULL)
-      {
-        syncrate++;
-        if( (syncrate->rate[0] != NULL) &&
-            (!(p->features & AHC_ULTRA2) || (syncrate->sxfr_ultra2 == 0)) )
-        {
-          p->transinfo[tindex].goal_period = syncrate->period;
-          if( p->transinfo[tindex].goal_period > 9 )
-          {
-            p->transinfo[tindex].goal_options = 0;
-            p->needppr &= ~(1<<tindex);
-            p->needsdtr |= (1<<tindex);
-            p->needppr_copy &= ~(1<<tindex);
-            p->needsdtr_copy |= (1<<tindex);
-            if (p->transinfo[tindex].goal_width)
-            {
-              p->needwdtr |= (1<<tindex);
-              p->needwdtr_copy |= (1<<tindex);
-            }
-          }
-        }
-        else if (p->transinfo[tindex].goal_width)
-        {
-          p->transinfo[tindex].goal_width = 0;
-          p->needwdtr &= ~(1<<tindex);
-          p->needwdtr_copy &= ~(1<<tindex);
-          p->transinfo[tindex].goal_offset =
-            p->transinfo[tindex].user_offset;
-          p->transinfo[tindex].goal_period =
-            p->transinfo[tindex].user_period;
-          p->transinfo[tindex].goal_options =
-            p->transinfo[tindex].user_options;
-          if( p->transinfo[tindex].goal_period <= 9 )
-          {
-            p->needppr |= (1<<tindex);
-            p->needsdtr &= ~(1<<tindex);
-            p->needppr_copy |= (1<<tindex);
-            p->needsdtr_copy &= ~(1<<tindex);
-          }
-          else
-          {
-            p->needppr &= ~(1<<tindex);
-            p->needsdtr |= (1<<tindex);
-            p->needppr_copy &= ~(1<<tindex);
-            p->needsdtr_copy |= (1<<tindex);
-          }
-        }
-        else
-        {
-          p->transinfo[tindex].goal_offset = 0;
-          p->transinfo[tindex].goal_period = 255;
-          p->transinfo[tindex].goal_options = 0;
-          p->transinfo[tindex].goal_width = 0;
-          p->needppr &= ~(1<<tindex);
-          p->needsdtr &= ~(1<<tindex);
-          p->needwdtr &= ~(1<<tindex);
-          p->needppr_copy &= ~(1<<tindex);
-          p->needsdtr_copy &= ~(1<<tindex);
-          p->needwdtr_copy &= ~(1<<tindex);
-        }
-      }
-      p->dev_flags[tindex] &= ~DEVICE_PARITY_ERROR;
-    }
-    else
-    {
-      p->dev_flags[tindex] |= DEVICE_PARITY_ERROR;
-    }
-
-    /*
-     * We've set the hardware to assert ATN if we get a parity
-     * error on "in" phases, so all we need to do is stuff the
-     * message buffer with the appropriate message.  "In" phases
-     * have set mesg_out to something other than MSG_NOP.
-     */
-    if (mesg_out != MSG_NOOP)
-    {
-      aic_outb(p, mesg_out, MSG_OUT);
-      aic_outb(p, aic_inb(p, SCSISIGI) | ATNO, SCSISIGO);
-      scb = NULL;
-    }
-    aic_outb(p, CLRSCSIPERR, CLRSINT1);
-    aic_outb(p, CLRSCSIINT, CLRINT);
-    unpause_sequencer(p, /* unpause_always */ TRUE);
-  }
-  else if ( (status & REQINIT) &&
-            (p->flags & AHC_HANDLING_REQINITS) )
-  {
-#ifdef AIC7XXX_VERBOSE_DEBUGGING
-    if (aic7xxx_verbose > 0xffff)
-      printk(INFO_LEAD "Handling REQINIT, SSTAT1=0x%x.\n", p->host_no,
-             CTL_OF_SCB(scb), aic_inb(p, SSTAT1));
-#endif
-    aic7xxx_handle_reqinit(p, scb);
-    return;
-  }
-  else
-  {
-    /*
-     * We don't know what's going on. Turn off the
-     * interrupt source and try to continue.
-     */
-    if (aic7xxx_verbose & VERBOSE_SCSIINT)
-      printk(INFO_LEAD "Unknown SCSIINT status, SSTAT1(0x%x).\n",
-        p->host_no, -1, -1, -1, status);
-    aic_outb(p, status, CLRSINT1);
-    aic_outb(p, CLRSCSIINT, CLRINT);
-    unpause_sequencer(p, /* unpause always */ TRUE);
-    scb = NULL;
-  }
-  if (scb != NULL)
-  {
-    aic7xxx_done(p, scb);
-  }
-}
-
-#ifdef AIC7XXX_VERBOSE_DEBUGGING
-static void
-aic7xxx_check_scbs(struct aic7xxx_host *p, char *buffer)
-{
-  unsigned char saved_scbptr, free_scbh, dis_scbh, wait_scbh, temp;
-  int i, bogus, lost;
-  static unsigned char scb_status[AIC7XXX_MAXSCB];
-
-#define SCB_NO_LIST 0
-#define SCB_FREE_LIST 1
-#define SCB_WAITING_LIST 2
-#define SCB_DISCONNECTED_LIST 4
-#define SCB_CURRENTLY_ACTIVE 8
-
-  /*
-   * Note, these checks will fail on a regular basis once the machine moves
-   * beyond the bus scan phase.  The problem is race conditions concerning
-   * the scbs and where they are linked in.  When you have 30 or so commands
-   * outstanding on the bus, and run this twice with every interrupt, the
-   * chances get pretty good that you'll catch the sequencer with an SCB
-   * only partially linked in.  Therefore, once we pass the scan phase
-   * of the bus, we really should disable this function.
-   */
-  bogus = FALSE;
-  memset(&scb_status[0], 0, sizeof(scb_status));
-  pause_sequencer(p);
-  saved_scbptr = aic_inb(p, SCBPTR);
-  if (saved_scbptr >= p->scb_data->maxhscbs)
-  {
-    printk("Bogus SCBPTR %d\n", saved_scbptr);
-    bogus = TRUE;
-  }
-  scb_status[saved_scbptr] = SCB_CURRENTLY_ACTIVE;
-  free_scbh = aic_inb(p, FREE_SCBH);
-  if ( (free_scbh != SCB_LIST_NULL) &&
-       (free_scbh >= p->scb_data->maxhscbs) )
-  {
-    printk("Bogus FREE_SCBH %d\n", free_scbh);
-    bogus = TRUE;
-  }
-  else
-  {
-    temp = free_scbh;
-    while( (temp != SCB_LIST_NULL) && (temp < p->scb_data->maxhscbs) )
-    {
-      if(scb_status[temp] & 0x07)
-      {
-        printk("HSCB %d on multiple lists, status 0x%02x", temp,
-               scb_status[temp] | SCB_FREE_LIST);
-        bogus = TRUE;
-      }
-      scb_status[temp] |= SCB_FREE_LIST;
-      aic_outb(p, temp, SCBPTR);
-      temp = aic_inb(p, SCB_NEXT);
-    }
-  }
-
-  dis_scbh = aic_inb(p, DISCONNECTED_SCBH);
-  if ( (dis_scbh != SCB_LIST_NULL) &&
-       (dis_scbh >= p->scb_data->maxhscbs) )
-  {
-    printk("Bogus DISCONNECTED_SCBH %d\n", dis_scbh);
-    bogus = TRUE;
-  }
-  else
-  {
-    temp = dis_scbh;
-    while( (temp != SCB_LIST_NULL) && (temp < p->scb_data->maxhscbs) )
-    {
-      if(scb_status[temp] & 0x07)
-      {
-        printk("HSCB %d on multiple lists, status 0x%02x", temp,
-               scb_status[temp] | SCB_DISCONNECTED_LIST);
-        bogus = TRUE;
-      }
-      scb_status[temp] |= SCB_DISCONNECTED_LIST;
-      aic_outb(p, temp, SCBPTR);
-      temp = aic_inb(p, SCB_NEXT);
-    }
-  }
-  
-  wait_scbh = aic_inb(p, WAITING_SCBH);
-  if ( (wait_scbh != SCB_LIST_NULL) &&
-       (wait_scbh >= p->scb_data->maxhscbs) )
-  {
-    printk("Bogus WAITING_SCBH %d\n", wait_scbh);
-    bogus = TRUE;
-  }
-  else
-  {
-    temp = wait_scbh;
-    while( (temp != SCB_LIST_NULL) && (temp < p->scb_data->maxhscbs) )
-    {
-      if(scb_status[temp] & 0x07)
-      {
-        printk("HSCB %d on multiple lists, status 0x%02x", temp,
-               scb_status[temp] | SCB_WAITING_LIST);
-        bogus = TRUE;
-      }
-      scb_status[temp] |= SCB_WAITING_LIST;
-      aic_outb(p, temp, SCBPTR);
-      temp = aic_inb(p, SCB_NEXT);
-    }
-  }
-
-  lost=0;
-  for(i=0; i < p->scb_data->maxhscbs; i++)
-  {
-    aic_outb(p, i, SCBPTR);
-    temp = aic_inb(p, SCB_NEXT);
-    if ( ((temp != SCB_LIST_NULL) &&
-          (temp >= p->scb_data->maxhscbs)) )
-    {
-      printk("HSCB %d bad, SCB_NEXT invalid(%d).\n", i, temp);
-      bogus = TRUE;
-    }
-    if ( temp == i )
-    {
-      printk("HSCB %d bad, SCB_NEXT points to self.\n", i);
-      bogus = TRUE;
-    }
-    if (scb_status[i] == 0)
-      lost++;
-    if (lost > 1)
-    {
-      printk("Too many lost scbs.\n");
-      bogus=TRUE;
-    }
-  }
-  aic_outb(p, saved_scbptr, SCBPTR);
-  unpause_sequencer(p, FALSE);
-  if (bogus)
-  {
-    printk("Bogus parameters found in card SCB array structures.\n");
-    printk("%s\n", buffer);
-    aic7xxx_panic_abort(p, NULL);
-  }
-  return;
-}
-#endif
-
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_handle_command_completion_intr
- *
- * Description:
- *   SCSI command completion interrupt handler.
- *-F*************************************************************************/
-static void
-aic7xxx_handle_command_completion_intr(struct aic7xxx_host *p)
-{
-  struct aic7xxx_scb *scb = NULL;
-  Scsi_Cmnd *cmd;
-  unsigned char scb_index, tindex;
-
-#ifdef AIC7XXX_VERBOSE_DEBUGGING
-  if( (p->isr_count < 16) && (aic7xxx_verbose > 0xffff) )
-    printk(INFO_LEAD "Command Complete Int.\n", p->host_no, -1, -1, -1);
-#endif
-    
-  /*
-   * Read the INTSTAT location after clearing the CMDINT bit.  This forces
-   * any posted PCI writes to flush to memory.  Gerard Roudier suggested
-   * this fix to the possible race of clearing the CMDINT bit but not
-   * having all command bytes flushed onto the qoutfifo.
-   */
-  aic_outb(p, CLRCMDINT, CLRINT);
-  aic_inb(p, INTSTAT);
-  /*
-   * The sequencer will continue running when it
-   * issues this interrupt. There may be >1 commands
-   * finished, so loop until we've processed them all.
-   */
-
-  while (p->qoutfifo[p->qoutfifonext] != SCB_LIST_NULL)
-  {
-    scb_index = p->qoutfifo[p->qoutfifonext];
-    p->qoutfifo[p->qoutfifonext++] = SCB_LIST_NULL;
-    if ( scb_index >= p->scb_data->numscbs )
-    {
-      printk(WARN_LEAD "CMDCMPLT with invalid SCB index %d\n", p->host_no,
-        -1, -1, -1, scb_index);
-      continue;
-    }
-    scb = p->scb_data->scb_array[scb_index];
-    if (!(scb->flags & SCB_ACTIVE) || (scb->cmd == NULL))
-    {
-      printk(WARN_LEAD "CMDCMPLT without command for SCB %d, SCB flags "
-        "0x%x, cmd 0x%lx\n", p->host_no, -1, -1, -1, scb_index, scb->flags,
-        (unsigned long) scb->cmd);
-      continue;
-    }
-    tindex = TARGET_INDEX(scb->cmd);
-    if (scb->flags & SCB_QUEUED_ABORT)
-    {
-      pause_sequencer(p);
-      if ( ((aic_inb(p, LASTPHASE) & PHASE_MASK) != P_BUSFREE) &&
-           (aic_inb(p, SCB_TAG) == scb->hscb->tag) )
-      {
-        unpause_sequencer(p, FALSE);
-        continue;
-      }
-      aic7xxx_reset_device(p, scb->cmd->target, scb->cmd->channel,
-        scb->cmd->lun, scb->hscb->tag);
-      scb->flags &= ~(SCB_QUEUED_FOR_DONE | SCB_RESET | SCB_ABORT |
-        SCB_QUEUED_ABORT);
-      unpause_sequencer(p, FALSE);
-    }
-    else if (scb->flags & SCB_ABORT)
-    {
-      /*
-       * We started to abort this, but it completed on us, let it
-       * through as successful
-       */
-      scb->flags &= ~(SCB_ABORT|SCB_RESET);
-    }
-    else if (scb->flags & SCB_SENSE)
-    {
-      char *buffer = &scb->cmd->sense_buffer[0];
-      if (scb->cmd == p->dev_dtr_cmnd[tindex])
-      {
-        struct aic7xxx_scb *old_scb;
-        /*
-         * We have valid sense data, send it back immediately.
-         */
-        old_scb = p->scb_data->scb_array[scb->cmd->next->tag];
-        *old_scb->cmd->sense_buffer = *scb->cmd->sense_buffer;
-        old_scb->hscb->target_status = scb->hscb->target_status;
-        old_scb->cmd->result = scb->hscb->target_status;
-        old_scb->cmd->result |= (DID_ERROR << 16);
-        aic7xxx_status(old_scb->cmd) = scb->hscb->target_status;
-        scbq_remove(&p->waiting_scbs, old_scb);
-        scbq_remove(&p->delayed_scbs[tindex], old_scb);
-        scb->cmd->next = NULL;
-	aic7xxx_done(p, scb);
-        aic7xxx_done(p, old_scb);
-	continue;
-      } 
-      else if (buffer[12] == 0x47 || buffer[12] == 0x54)
-      {
-        /*
-         * SCSI errors, run domain validation and re-run negotiation
-         */
-        p->needdv |= (1<<tindex);
-        /*
-         * Signal that we need to re-negotiate things, this also gets us our
-         * INQUIRY command to re-checksum off of.
-         */
-        p->needppr |= (p->needppr_copy & (1<<tindex));
-        p->needsdtr |= (p->needsdtr_copy & (1<<tindex));
-        p->needwdtr |= (p->needwdtr_copy & (1<<tindex));
-      }
-    }
-    switch (status_byte(scb->hscb->target_status))
-    {
-      case QUEUE_FULL:
-      case BUSY:
-        scb->hscb->target_status = 0;
-        scb->cmd->result = 0;
-        aic7xxx_error(scb->cmd) = DID_OK;
-        break;
-      default:
-        cmd = scb->cmd;
-        if (scb->hscb->residual_SG_segment_count != 0)
-        {
-          aic7xxx_calculate_residual(p, scb);
-        }
-        cmd->result |= (aic7xxx_error(cmd) << 16);
-        aic7xxx_done(p, scb);
-        break;
-    }      
-  }
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_isr
- *
- * Description:
- *   SCSI controller interrupt handler.
- *-F*************************************************************************/
-static void
-aic7xxx_isr(int irq, void *dev_id, struct pt_regs *regs)
-{
-  struct aic7xxx_host *p;
-  unsigned char intstat;
-
-  p = (struct aic7xxx_host *)dev_id;
-
-  /*
-   * Just a few sanity checks.  Make sure that we have an int pending.
-   * Also, if PCI, then we are going to check for a PCI bus error status
-   * should we get too many spurious interrupts.
-   */
-  if (!((intstat = aic_inb(p, INTSTAT)) & INT_PEND))
-  {
-#ifdef CONFIG_PCI
-    if ( (p->chip & AHC_PCI) && (p->spurious_int > 500) &&
-        !(p->flags & AHC_HANDLING_REQINITS) )
-    {
-      if ( aic_inb(p, ERROR) & PCIERRSTAT )
-      {
-        aic7xxx_pci_intr(p);
-      }
-      p->spurious_int = 0;
-    }
-    else if ( !(p->flags & AHC_HANDLING_REQINITS) )
-    {
-      p->spurious_int++;
-    }
-#endif
-    return;
-  }
-
-  p->spurious_int = 0;
-
-  /*
-   * Keep track of interrupts for /proc/scsi
-   */
-  p->isr_count++;
-
-#ifdef AIC7XXX_VERBOSE_DEBUGGING
-  if ( (p->isr_count < 16) && (aic7xxx_verbose > 0xffff) &&
-       (aic7xxx_panic_on_abort) && (p->flags & AHC_PAGESCBS) )
-    aic7xxx_check_scbs(p, "Bogus settings at start of interrupt.");
-#endif
-
-  /*
-   * Handle all the interrupt sources - especially for SCSI
-   * interrupts, we won't get a second chance at them.
-   */
-  if (intstat & CMDCMPLT)
-  {
-    aic7xxx_handle_command_completion_intr(p);
-  }
-
-  if (intstat & BRKADRINT)
-  {
-    int i;
-    unsigned char errno = aic_inb(p, ERROR);
-
-    printk(KERN_ERR "(scsi%d) BRKADRINT error(0x%x):\n", p->host_no, errno);
-    for (i = 0; i < NUMBER(hard_error); i++)
-    {
-      if (errno & hard_error[i].errno)
-      {
-        printk(KERN_ERR "  %s\n", hard_error[i].errmesg);
-      }
-    }
-    printk(KERN_ERR "(scsi%d)   SEQADDR=0x%x\n", p->host_no,
-      (((aic_inb(p, SEQADDR1) << 8) & 0x100) | aic_inb(p, SEQADDR0)));
-    if (aic7xxx_panic_on_abort)
-      aic7xxx_panic_abort(p, NULL);
-#ifdef CONFIG_PCI
-    if (errno & PCIERRSTAT)
-      aic7xxx_pci_intr(p);
-#endif
-    if (errno & (SQPARERR | ILLOPCODE | ILLSADDR))
-    {
-      sti();
-      panic("aic7xxx: unrecoverable BRKADRINT.\n");
-    }
-    if (errno & ILLHADDR)
-    {
-      printk(KERN_ERR "(scsi%d) BUG! Driver accessed chip without first "
-             "pausing controller!\n", p->host_no);
-    }
-#ifdef AIC7XXX_VERBOSE_DEBUGGING
-    if (errno & DPARERR)
-    {
-      if (aic_inb(p, DMAPARAMS) & DIRECTION)
-        printk("(scsi%d) while DMAing SCB from host to card.\n", p->host_no);
-      else
-        printk("(scsi%d) while DMAing SCB from card to host.\n", p->host_no);
-    }
-#endif
-    aic_outb(p, CLRPARERR | CLRBRKADRINT, CLRINT);
-    unpause_sequencer(p, FALSE);
-  }
-
-  if (intstat & SEQINT)
-  {
-    /*
-     * Read the CCSCBCTL register to work around a bug in the Ultra2 cards
-     */
-    if(p->features & AHC_ULTRA2)
-    {
-      aic_inb(p, CCSCBCTL);
-    }
-    aic7xxx_handle_seqint(p, intstat);
-  }
-
-  if (intstat & SCSIINT)
-  {
-    aic7xxx_handle_scsiint(p, intstat);
-  }
-
-#ifdef AIC7XXX_VERBOSE_DEBUGGING
-  if ( (p->isr_count < 16) && (aic7xxx_verbose > 0xffff) &&
-       (aic7xxx_panic_on_abort) && (p->flags & AHC_PAGESCBS) )
-    aic7xxx_check_scbs(p, "Bogus settings at end of interrupt.");
-#endif
-
-}
-
-/*+F*************************************************************************
- * Function:
- *   do_aic7xxx_isr
- *
- * Description:
- *   This is a gross hack to solve a problem in linux kernels 2.1.85 and
- *   above.  Please, children, do not try this at home, and if you ever see
- *   anything like it, please inform the Gross Hack Police immediately
- *-F*************************************************************************/
-static void
-do_aic7xxx_isr(int irq, void *dev_id, struct pt_regs *regs)
-{
-  unsigned long cpu_flags;
-  struct aic7xxx_host *p;
-  
-  p = (struct aic7xxx_host *)dev_id;
-  if(!p)
-    return;
-#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,1,95)
-  if(test_and_set_bit(AHC_IN_ISR_BIT, &p->flags))
-  {
-    return;
-  }
-  spin_lock_irqsave(&io_request_lock, cpu_flags);
-  do
-  {
-    aic7xxx_isr(irq, dev_id, regs);
-  } while ( (aic_inb(p, INTSTAT) & INT_PEND) );
-  aic7xxx_done_cmds_complete(p);
-  aic7xxx_run_waiting_queues(p);
-  clear_bit(AHC_IN_ISR_BIT, &p->flags);
-  spin_unlock_irqrestore(&io_request_lock, cpu_flags);
-#else
-  if(set_bit(AHC_IN_ISR_BIT, (int *)&p->flags))
-  {
-    return;
-  }
-  DRIVER_LOCK
-  do
-  {
-    aic7xxx_isr(irq, dev_id, regs);
-  } while ( (aic_inb(p, INTSTAT) & INT_PEND) );
-  DRIVER_UNLOCK
-  aic7xxx_done_cmds_complete(p);
-  aic7xxx_run_waiting_queues(p);
-  clear_bit(AHC_IN_ISR_BIT, (int *)&p->flags);
-#endif
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_device_queue_depth
- *
- * Description:
- *   Determines the queue depth for a given device.  There are two ways
- *   a queue depth can be obtained for a tagged queueing device.  One
- *   way is the default queue depth which is determined by whether
- *   AIC7XXX_CMDS_PER_DEVICE is defined.  If it is defined, then it is used
- *   as the default queue depth.  Otherwise, we use either 4 or 8 as the
- *   default queue depth (dependent on the number of hardware SCBs).
- *   The other way we determine queue depth is through the use of the
- *   aic7xxx_tag_info array which is enabled by defining
- *   AIC7XXX_TAGGED_QUEUEING_BY_DEVICE.  This array can be initialized
- *   with queue depths for individual devices.  It also allows tagged
- *   queueing to be [en|dis]abled for a specific adapter.
- *-F*************************************************************************/
-static int
-aic7xxx_device_queue_depth(struct aic7xxx_host *p, Scsi_Device *device)
-{
-  int default_depth = 3;
-  unsigned char tindex;
-  unsigned short target_mask;
-
-  tindex = device->id | (device->channel << 3);
-  target_mask = (1 << tindex);
-
-  if (p->dev_max_queue_depth[tindex] > 1)
-  {
-    /*
-     * We've already scanned this device, leave it alone
-     */
-    return(p->dev_max_queue_depth[tindex]);
-  }
-
-  device->queue_depth = default_depth;
-  p->dev_temp_queue_depth[tindex] = 1;
-  p->dev_max_queue_depth[tindex] = 1;
-  p->tagenable &= ~target_mask;
-
-  if (device->tagged_supported)
-  {
-    int tag_enabled = TRUE;
-
-    default_depth = AIC7XXX_CMDS_PER_DEVICE;
- 
-    if (!(p->discenable & target_mask))
-    {
-      if (aic7xxx_verbose & VERBOSE_NEGOTIATION2)
-        printk(INFO_LEAD "Disconnection disabled, unable to "
-             "enable tagged queueing.\n",
-             p->host_no, device->channel, device->id, device->lun);
-    }
-    else
-    {
-      if (p->instance >= NUMBER(aic7xxx_tag_info))
-      {
-        static int print_warning = TRUE;
-        if(print_warning)
-        {
-          printk(KERN_INFO "aic7xxx: WARNING, insufficient tag_info instances for"
-                           " installed controllers.\n");
-          printk(KERN_INFO "aic7xxx: Please update the aic7xxx_tag_info array in"
-                           " the aic7xxx.c source file.\n");
-          print_warning = FALSE;
-        }
-        device->queue_depth = default_depth;
-      }
-      else
-      {
-
-        if (aic7xxx_tag_info[p->instance].tag_commands[tindex] == 255)
-        {
-          tag_enabled = FALSE;
-          device->queue_depth = 3;  /* Tagged queueing is disabled. */
-        }
-        else if (aic7xxx_tag_info[p->instance].tag_commands[tindex] == 0)
-        {
-          device->queue_depth = default_depth;
-        }
-        else
-        {
-          device->queue_depth =
-            aic7xxx_tag_info[p->instance].tag_commands[tindex];
-        }
-      }
-      if ((device->tagged_queue == 0) && tag_enabled)
-      {
-        if (aic7xxx_verbose & VERBOSE_NEGOTIATION2)
-        {
-              printk(INFO_LEAD "Enabled tagged queuing, queue depth %d.\n",
-                p->host_no, device->channel, device->id,
-                device->lun, device->queue_depth);
-        }
-        p->dev_max_queue_depth[tindex] = device->queue_depth;
-        p->dev_temp_queue_depth[tindex] = device->queue_depth;
-        p->tagenable |= target_mask;
-        p->orderedtag |= target_mask;
-        device->tagged_queue = 1;
-        device->current_tag = SCB_LIST_NULL;
-      }
-    }
-  }
-  return(p->dev_max_queue_depth[tindex]);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_select_queue_depth
- *
- * Description:
- *   Sets the queue depth for each SCSI device hanging off the input
- *   host adapter.  We use a queue depth of 2 for devices that do not
- *   support tagged queueing.  If AIC7XXX_CMDS_PER_LUN is defined, we
- *   use that for tagged queueing devices; otherwise we use our own
- *   algorithm for determining the queue depth based on the maximum
- *   SCBs for the controller.
- *-F*************************************************************************/
-static void
-aic7xxx_select_queue_depth(struct Scsi_Host *host,
-    Scsi_Device *scsi_devs)
-{
-  Scsi_Device *device;
-  struct aic7xxx_host *p = (struct aic7xxx_host *) host->hostdata;
-  int scbnum;
-
-  scbnum = 0;
-  for (device = scsi_devs; device != NULL; device = device->next)
-  {
-    if (device->host == host)
-    {
-      scbnum += aic7xxx_device_queue_depth(p, device);
-    }
-  }
-  while (scbnum > p->scb_data->numscbs)
-  {
-    /*
-     * Pre-allocate the needed SCBs to get around the possibility of having
-     * to allocate some when memory is more or less exhausted and we need
-     * the SCB in order to perform a swap operation (possible deadlock)
-     */
-    if ( aic7xxx_allocate_scb(p) == 0 )
-      return;
-  }
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_probe
- *
- * Description:
- *   Probing for EISA boards: it looks like the first two bytes
- *   are a manufacturer code - three characters, five bits each:
- *
- *               BYTE 0   BYTE 1   BYTE 2   BYTE 3
- *              ?1111122 22233333 PPPPPPPP RRRRRRRR
- *
- *   The characters are baselined off ASCII '@', so add that value
- *   to each to get the real ASCII code for it. The next two bytes
- *   appear to be a product and revision number, probably vendor-
- *   specific. This is what is being searched for at each port,
- *   and what should probably correspond to the ID= field in the
- *   ECU's .cfg file for the card - if your card is not detected,
- *   make sure your signature is listed in the array.
- *
- *   The fourth byte's lowest bit seems to be an enabled/disabled
- *   flag (rest of the bits are reserved?).
- *
- * NOTE:  This function is only needed on Intel and Alpha platforms,
- *   the other platforms we support don't have EISA/VLB busses.  So,
- *   we #ifdef this entire function to avoid compiler warnings about
- *   an unused function.
- *-F*************************************************************************/
-#if defined(__i386__) || defined(__alpha__)
-static int
-aic7xxx_probe(int slot, int base, ahc_flag_type *flags)
-{
-  int i;
-  unsigned char buf[4];
-
-  static struct {
-    int n;
-    unsigned char signature[sizeof(buf)];
-    ahc_chip type;
-    int bios_disabled;
-  } AIC7xxx[] = {
-    { 4, { 0x04, 0x90, 0x77, 0x70 },
-      AHC_AIC7770|AHC_EISA, FALSE },  /* mb 7770  */
-    { 4, { 0x04, 0x90, 0x77, 0x71 },
-      AHC_AIC7770|AHC_EISA, FALSE }, /* host adapter 274x */
-    { 4, { 0x04, 0x90, 0x77, 0x56 },
-      AHC_AIC7770|AHC_VL, FALSE }, /* 284x BIOS enabled */
-    { 4, { 0x04, 0x90, 0x77, 0x57 },
-      AHC_AIC7770|AHC_VL, TRUE }   /* 284x BIOS disabled */
-  };
-
-  /*
-   * The VL-bus cards need to be primed by
-   * writing before a signature check.
-   */
-  for (i = 0; i < sizeof(buf); i++)
-  {
-    outb(0x80 + i, base);
-    buf[i] = inb(base + i);
-  }
-
-  for (i = 0; i < NUMBER(AIC7xxx); i++)
-  {
-    /*
-     * Signature match on enabled card?
-     */
-    if (!memcmp(buf, AIC7xxx[i].signature, AIC7xxx[i].n))
-    {
-      if (inb(base + 4) & 1)
-      {
-        if (AIC7xxx[i].bios_disabled)
-        {
-          *flags |= AHC_USEDEFAULTS;
-        }
-        else
-        {
-          *flags |= AHC_BIOS_ENABLED;
-        }
-        return (i);
-      }
-
-      printk("aic7xxx: <Adaptec 7770 SCSI Host Adapter> "
-             "disabled at slot %d, ignored.\n", slot);
-    }
-  }
-
-  return (-1);
-}
-#endif /* (__i386__) || (__alpha__) */
-
-
-/*+F*************************************************************************
- * Function:
- *   read_2840_seeprom
- *
- * Description:
- *   Reads the 2840 serial EEPROM and returns 1 if successful and 0 if
- *   not successful.
- *
- *   See read_seeprom (for the 2940) for the instruction set of the 93C46
- *   chip.
- *
- *   The 2840 interface to the 93C46 serial EEPROM is through the
- *   STATUS_2840 and SEECTL_2840 registers.  The CS_2840, CK_2840, and
- *   DO_2840 bits of the SEECTL_2840 register are connected to the chip
- *   select, clock, and data out lines respectively of the serial EEPROM.
- *   The DI_2840 bit of the STATUS_2840 is connected to the data in line
- *   of the serial EEPROM.  The EEPROM_TF bit of STATUS_2840 register is
- *   useful in that it gives us an 800 nsec timer.  After a read from the
- *   SEECTL_2840 register the timing flag is cleared and goes high 800 nsec
- *   later.
- *-F*************************************************************************/
-static int
-read_284x_seeprom(struct aic7xxx_host *p, struct seeprom_config *sc)
-{
-  int i = 0, k = 0;
-  unsigned char temp;
-  unsigned short checksum = 0;
-  unsigned short *seeprom = (unsigned short *) sc;
-  struct seeprom_cmd {
-    unsigned char len;
-    unsigned char bits[3];
-  };
-  struct seeprom_cmd seeprom_read = {3, {1, 1, 0}};
-
-#define CLOCK_PULSE(p) \
-  while ((aic_inb(p, STATUS_2840) & EEPROM_TF) == 0)        \
-  {                                                \
-    ;  /* Do nothing */                                \
-  }                                                \
-  (void) aic_inb(p, SEECTL_2840);
-
-  /*
-   * Read the first 32 registers of the seeprom.  For the 2840,
-   * the 93C46 SEEPROM is a 1024-bit device with 64 16-bit registers
-   * but only the first 32 are used by Adaptec BIOS.  The loop
-   * will range from 0 to 31.
-   */
-  for (k = 0; k < (sizeof(*sc) / 2); k++)
-  {
-    /*
-     * Send chip select for one clock cycle.
-     */
-    aic_outb(p, CK_2840 | CS_2840, SEECTL_2840);
-    CLOCK_PULSE(p);
-
-    /*
-     * Now we're ready to send the read command followed by the
-     * address of the 16-bit register we want to read.
-     */
-    for (i = 0; i < seeprom_read.len; i++)
-    {
-      temp = CS_2840 | seeprom_read.bits[i];
-      aic_outb(p, temp, SEECTL_2840);
-      CLOCK_PULSE(p);
-      temp = temp ^ CK_2840;
-      aic_outb(p, temp, SEECTL_2840);
-      CLOCK_PULSE(p);
-    }
-    /*
-     * Send the 6 bit address (MSB first, LSB last).
-     */
-    for (i = 5; i >= 0; i--)
-    {
-      temp = k;
-      temp = (temp >> i) & 1;  /* Mask out all but lower bit. */
-      temp = CS_2840 | temp;
-      aic_outb(p, temp, SEECTL_2840);
-      CLOCK_PULSE(p);
-      temp = temp ^ CK_2840;
-      aic_outb(p, temp, SEECTL_2840);
-      CLOCK_PULSE(p);
-    }
-
-    /*
-     * Now read the 16 bit register.  An initial 0 precedes the
-     * register contents which begins with bit 15 (MSB) and ends
-     * with bit 0 (LSB).  The initial 0 will be shifted off the
-     * top of our word as we let the loop run from 0 to 16.
-     */
-    for (i = 0; i <= 16; i++)
-    {
-      temp = CS_2840;
-      aic_outb(p, temp, SEECTL_2840);
-      CLOCK_PULSE(p);
-      temp = temp ^ CK_2840;
-      seeprom[k] = (seeprom[k] << 1) | (aic_inb(p, STATUS_2840) & DI_2840);
-      aic_outb(p, temp, SEECTL_2840);
-      CLOCK_PULSE(p);
-    }
-    /*
-     * The serial EEPROM has a checksum in the last word.  Keep a
-     * running checksum for all words read except for the last
-     * word.  We'll verify the checksum after all words have been
-     * read.
-     */
-    if (k < (sizeof(*sc) / 2) - 1)
-    {
-      checksum = checksum + seeprom[k];
-    }
-
-    /*
-     * Reset the chip select for the next command cycle.
-     */
-    aic_outb(p, 0, SEECTL_2840);
-    CLOCK_PULSE(p);
-    aic_outb(p, CK_2840, SEECTL_2840);
-    CLOCK_PULSE(p);
-    aic_outb(p, 0, SEECTL_2840);
-    CLOCK_PULSE(p);
-  }
-
-#if 0
-  printk("Computed checksum 0x%x, checksum read 0x%x\n", checksum, sc->checksum);
-  printk("Serial EEPROM:");
-  for (k = 0; k < (sizeof(*sc) / 2); k++)
-  {
-    if (((k % 8) == 0) && (k != 0))
-    {
-      printk("\n              ");
-    }
-    printk(" 0x%x", seeprom[k]);
-  }
-  printk("\n");
-#endif
-
-  if (checksum != sc->checksum)
-  {
-    printk("aic7xxx: SEEPROM checksum error, ignoring SEEPROM settings.\n");
-    return (0);
-  }
-
-  return (1);
-#undef CLOCK_PULSE
-}
-
-#define CLOCK_PULSE(p)                                               \
-  do {                                                               \
-    int limit = 0;                                                   \
-    do {                                                             \
-      mb();                                                          \
-      pause_sequencer(p);  /* This is just to generate some PCI */   \
-                           /* traffic so the PCI read is flushed */  \
-                           /* it shouldn't be needed, but some */    \
-                           /* chipsets do indeed appear to need */   \
-                           /* something to force PCI reads to get */ \
-                           /* flushed */                             \
-      udelay(1);           /* Do nothing */                          \
-    } while (((aic_inb(p, SEECTL) & SEERDY) == 0) && (++limit < 1000)); \
-  } while(0)
-
-/*+F*************************************************************************
- * Function:
- *   acquire_seeprom
- *
- * Description:
- *   Acquires access to the memory port on PCI controllers.
- *-F*************************************************************************/
-static int
-acquire_seeprom(struct aic7xxx_host *p)
-{
-
-  /*
-   * Request access of the memory port.  When access is
-   * granted, SEERDY will go high.  We use a 1 second
-   * timeout which should be near 1 second more than
-   * is needed.  Reason: after the 7870 chip reset, there
-   * should be no contention.
-   */
-  aic_outb(p, SEEMS, SEECTL);
-  CLOCK_PULSE(p);
-  if ((aic_inb(p, SEECTL) & SEERDY) == 0)
-  {
-    aic_outb(p, 0, SEECTL);
-    return (0);
-  }
-  return (1);
-}
-
-/*+F*************************************************************************
- * Function:
- *   release_seeprom
- *
- * Description:
- *   Releases access to the memory port on PCI controllers.
- *-F*************************************************************************/
-static void
-release_seeprom(struct aic7xxx_host *p)
-{
-  /*
-   * Make sure the SEEPROM is ready before we release it.
-   */
-  CLOCK_PULSE(p);
-  aic_outb(p, 0, SEECTL);
-}
-
-/*+F*************************************************************************
- * Function:
- *   read_seeprom
- *
- * Description:
- *   Reads the serial EEPROM and returns 1 if successful and 0 if
- *   not successful.
- *
- *   The instruction set of the 93C46/56/66 chips is as follows:
- *
- *               Start  OP
- *     Function   Bit  Code  Address    Data     Description
- *     -------------------------------------------------------------------
- *     READ        1    10   A5 - A0             Reads data stored in memory,
- *                                               starting at specified address
- *     EWEN        1    00   11XXXX              Write enable must precede
- *                                               all programming modes
- *     ERASE       1    11   A5 - A0             Erase register A5A4A3A2A1A0
- *     WRITE       1    01   A5 - A0   D15 - D0  Writes register
- *     ERAL        1    00   10XXXX              Erase all registers
- *     WRAL        1    00   01XXXX    D15 - D0  Writes to all registers
- *     EWDS        1    00   00XXXX              Disables all programming
- *                                               instructions
- *     *Note: A value of X for address is a don't care condition.
- *     *Note: The 93C56 and 93C66 have 8 address bits.
- * 
- *
- *   The 93C46 has a four wire interface: clock, chip select, data in, and
- *   data out.  In order to perform one of the above functions, you need
- *   to enable the chip select for a clock period (typically a minimum of
- *   1 usec, with the clock high and low a minimum of 750 and 250 nsec
- *   respectively.  While the chip select remains high, you can clock in
- *   the instructions (above) starting with the start bit, followed by the
- *   OP code, Address, and Data (if needed).  For the READ instruction, the
- *   requested 16-bit register contents is read from the data out line but
- *   is preceded by an initial zero (leading 0, followed by 16-bits, MSB
- *   first).  The clock cycling from low to high initiates the next data
- *   bit to be sent from the chip.
- *
- *   The 78xx interface to the 93C46 serial EEPROM is through the SEECTL
- *   register.  After successful arbitration for the memory port, the
- *   SEECS bit of the SEECTL register is connected to the chip select.
- *   The SEECK, SEEDO, and SEEDI are connected to the clock, data out,
- *   and data in lines respectively.  The SEERDY bit of SEECTL is useful
- *   in that it gives us an 800 nsec timer.  After a write to the SEECTL
- *   register, the SEERDY goes high 800 nsec later.  The one exception
- *   to this is when we first request access to the memory port.  The
- *   SEERDY goes high to signify that access has been granted and, for
- *   this case, has no implied timing.
- *-F*************************************************************************/
-static int
-read_seeprom(struct aic7xxx_host *p, int offset, 
-    unsigned short *scarray, unsigned int len, seeprom_chip_type chip)
-{
-  int i = 0, k;
-  unsigned char temp;
-  unsigned short checksum = 0;
-  struct seeprom_cmd {
-    unsigned char len;
-    unsigned char bits[3];
-  };
-  struct seeprom_cmd seeprom_read = {3, {1, 1, 0}};
-
-  /*
-   * Request access of the memory port.
-   */
-  if (acquire_seeprom(p) == 0)
-  {
-    return (0);
-  }
-
-  /*
-   * Read 'len' registers of the seeprom.  For the 7870, the 93C46
-   * SEEPROM is a 1024-bit device with 64 16-bit registers but only
-   * the first 32 are used by Adaptec BIOS.  Some adapters use the
-   * 93C56 SEEPROM which is a 2048-bit device.  The loop will range
-   * from 0 to 'len' - 1.
-   */
-  for (k = 0; k < len; k++)
-  {
-    /*
-     * Send chip select for one clock cycle.
-     */
-    aic_outb(p, SEEMS | SEECK | SEECS, SEECTL);
-    CLOCK_PULSE(p);
-
-    /*
-     * Now we're ready to send the read command followed by the
-     * address of the 16-bit register we want to read.
-     */
-    for (i = 0; i < seeprom_read.len; i++)
-    {
-      temp = SEEMS | SEECS | (seeprom_read.bits[i] << 1);
-      aic_outb(p, temp, SEECTL);
-      CLOCK_PULSE(p);
-      temp = temp ^ SEECK;
-      aic_outb(p, temp, SEECTL);
-      CLOCK_PULSE(p);
-    }
-    /*
-     * Send the 6 or 8 bit address (MSB first, LSB last).
-     */
-    for (i = ((int) chip - 1); i >= 0; i--)
-    {
-      temp = k + offset;
-      temp = (temp >> i) & 1;  /* Mask out all but lower bit. */
-      temp = SEEMS | SEECS | (temp << 1);
-      aic_outb(p, temp, SEECTL);
-      CLOCK_PULSE(p);
-      temp = temp ^ SEECK;
-      aic_outb(p, temp, SEECTL);
-      CLOCK_PULSE(p);
-    }
-
-    /*
-     * Now read the 16 bit register.  An initial 0 precedes the
-     * register contents which begins with bit 15 (MSB) and ends
-     * with bit 0 (LSB).  The initial 0 will be shifted off the
-     * top of our word as we let the loop run from 0 to 16.
-     */
-    for (i = 0; i <= 16; i++)
-    {
-      temp = SEEMS | SEECS;
-      aic_outb(p, temp, SEECTL);
-      CLOCK_PULSE(p);
-      temp = temp ^ SEECK;
-      scarray[k] = (scarray[k] << 1) | (aic_inb(p, SEECTL) & SEEDI);
-      aic_outb(p, temp, SEECTL);
-      CLOCK_PULSE(p);
-    }
-
-    /*
-     * The serial EEPROM should have a checksum in the last word.
-     * Keep a running checksum for all words read except for the
-     * last word.  We'll verify the checksum after all words have
-     * been read.
-     */
-    if (k < (len - 1))
-    {
-      checksum = checksum + scarray[k];
-    }
-
-    /*
-     * Reset the chip select for the next command cycle.
-     */
-    aic_outb(p, SEEMS, SEECTL);
-    CLOCK_PULSE(p);
-    aic_outb(p, SEEMS | SEECK, SEECTL);
-    CLOCK_PULSE(p);
-    aic_outb(p, SEEMS, SEECTL);
-    CLOCK_PULSE(p);
-  }
-
-  /*
-   * Release access to the memory port and the serial EEPROM.
-   */
-  release_seeprom(p);
-
-#if 0
-  printk("Computed checksum 0x%x, checksum read 0x%x\n",
-         checksum, scarray[len - 1]);
-  printk("Serial EEPROM:");
-  for (k = 0; k < len; k++)
-  {
-    if (((k % 8) == 0) && (k != 0))
-    {
-      printk("\n              ");
-    }
-    printk(" 0x%x", scarray[k]);
-  }
-  printk("\n");
-#endif
-  if ( (checksum != scarray[len - 1]) || (checksum == 0) )
-  {
-    return (0);
-  }
-
-  return (1);
-}
-
-/*+F*************************************************************************
- * Function:
- *   read_brdctl
- *
- * Description:
- *   Reads the BRDCTL register.
- *-F*************************************************************************/
-static unsigned char
-read_brdctl(struct aic7xxx_host *p)
-{
-  unsigned char brdctl, value;
-
-  /*
-   * Make sure the SEEPROM is ready before we access it
-   */
-  CLOCK_PULSE(p);
-  if (p->features & AHC_ULTRA2)
-  {
-    brdctl = BRDRW_ULTRA2;
-    aic_outb(p, brdctl, BRDCTL);
-    CLOCK_PULSE(p);
-    value = aic_inb(p, BRDCTL);
-    CLOCK_PULSE(p);
-    return(value);
-  }
-  brdctl = BRDRW;
-  if ( !((p->chip & AHC_CHIPID_MASK) == AHC_AIC7895) ||
-        (p->flags & AHC_CHNLB) )
-  {
-    brdctl |= BRDCS;
-  }
-  aic_outb(p, brdctl, BRDCTL);
-  CLOCK_PULSE(p);
-  value = aic_inb(p, BRDCTL);
-  CLOCK_PULSE(p);
-  aic_outb(p, 0, BRDCTL);
-  CLOCK_PULSE(p);
-  return (value);
-}
-
-/*+F*************************************************************************
- * Function:
- *   write_brdctl
- *
- * Description:
- *   Writes a value to the BRDCTL register.
- *-F*************************************************************************/
-static void
-write_brdctl(struct aic7xxx_host *p, unsigned char value)
-{
-  unsigned char brdctl;
-
-  /*
-   * Make sure the SEEPROM is ready before we access it
-   */
-  CLOCK_PULSE(p);
-  if (p->features & AHC_ULTRA2)
-  {
-    brdctl = value;
-    aic_outb(p, brdctl, BRDCTL);
-    CLOCK_PULSE(p);
-    brdctl |= BRDSTB_ULTRA2;
-    aic_outb(p, brdctl, BRDCTL);
-    CLOCK_PULSE(p);
-    brdctl &= ~BRDSTB_ULTRA2;
-    aic_outb(p, brdctl, BRDCTL);
-    CLOCK_PULSE(p);
-    read_brdctl(p);
-    CLOCK_PULSE(p);
-  }
-  else
-  {
-    brdctl = BRDSTB;
-    if ( !((p->chip & AHC_CHIPID_MASK) == AHC_AIC7895) ||
-          (p->flags & AHC_CHNLB) )
-    {
-      brdctl |= BRDCS;
-    }
-    brdctl = BRDSTB | BRDCS;
-    aic_outb(p, brdctl, BRDCTL);
-    CLOCK_PULSE(p);
-    brdctl |= value;
-    aic_outb(p, brdctl, BRDCTL);
-    CLOCK_PULSE(p);
-    brdctl &= ~BRDSTB;
-    aic_outb(p, brdctl, BRDCTL);
-    CLOCK_PULSE(p);
-    brdctl &= ~BRDCS;
-    aic_outb(p, brdctl, BRDCTL);
-    CLOCK_PULSE(p);
-  }
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic785x_cable_detect
- *
- * Description:
- *   Detect the cables that are present on aic785x class controller chips
- *-F*************************************************************************/
-static void
-aic785x_cable_detect(struct aic7xxx_host *p, int *int_50,
-    int *ext_present, int *eeprom)
-{
-  unsigned char brdctl;
-
-  aic_outb(p, BRDRW | BRDCS, BRDCTL);
-  CLOCK_PULSE(p);
-  aic_outb(p, 0, BRDCTL);
-  CLOCK_PULSE(p);
-  brdctl = aic_inb(p, BRDCTL);
-  CLOCK_PULSE(p);
-  *int_50 = !(brdctl & BRDDAT5);
-  *ext_present = !(brdctl & BRDDAT6);
-  *eeprom = (aic_inb(p, SPIOCAP) & EEPROM);
-}
-
-#undef CLOCK_PULSE
-
-/*+F*************************************************************************
- * Function:
- *   aic2940_uwpro_cable_detect
- *
- * Description:
- *   Detect the cables that are present on the 2940-UWPro cards
- *
- * NOTE: This functions assumes the SEEPROM will have already been aquired
- *       prior to invocation of this function.
- *-F*************************************************************************/
-static void
-aic2940_uwpro_wide_cable_detect(struct aic7xxx_host *p, int *int_68,
-    int *ext_68, int *eeprom)
-{
-  unsigned char brdctl;
-
-  /*
-   * First read the status of our cables.  Set the rom bank to
-   * 0 since the bank setting serves as a multiplexor for the
-   * cable detection logic.  BRDDAT5 controls the bank switch.
-   */
-  write_brdctl(p, 0);
-
-  /*
-   * Now we read the state of the internal 68 connector.  BRDDAT6
-   * is don't care, BRDDAT7 is internal 68.  The cable is
-   * present if the bit is 0
-   */
-  brdctl = read_brdctl(p);
-  *int_68 = !(brdctl & BRDDAT7);
-
-  /*
-   * Set the bank bit in brdctl and then read the external cable state
-   * and the EEPROM status
-   */
-  write_brdctl(p, BRDDAT5);
-  brdctl = read_brdctl(p);
-
-  *ext_68 = !(brdctl & BRDDAT6);
-  *eeprom = !(brdctl & BRDDAT7);
-
-  /*
-   * We're done, the calling function will release the SEEPROM for us
-   */
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic787x_cable_detect
- *
- * Description:
- *   Detect the cables that are present on aic787x class controller chips
- *
- * NOTE: This functions assumes the SEEPROM will have already been aquired
- *       prior to invocation of this function.
- *-F*************************************************************************/
-static void
-aic787x_cable_detect(struct aic7xxx_host *p, int *int_50, int *int_68,
-    int *ext_present, int *eeprom)
-{
-  unsigned char brdctl;
-
-  /*
-   * First read the status of our cables.  Set the rom bank to
-   * 0 since the bank setting serves as a multiplexor for the
-   * cable detection logic.  BRDDAT5 controls the bank switch.
-   */
-  write_brdctl(p, 0);
-
-  /*
-   * Now we read the state of the two internal connectors.  BRDDAT6
-   * is internal 50, BRDDAT7 is internal 68.  For each, the cable is
-   * present if the bit is 0
-   */
-  brdctl = read_brdctl(p);
-  *int_50 = !(brdctl & BRDDAT6);
-  *int_68 = !(brdctl & BRDDAT7);
-
-  /*
-   * Set the bank bit in brdctl and then read the external cable state
-   * and the EEPROM status
-   */
-  write_brdctl(p, BRDDAT5);
-  brdctl = read_brdctl(p);
-
-  *ext_present = !(brdctl & BRDDAT6);
-  *eeprom = !(brdctl & BRDDAT7);
-
-  /*
-   * We're done, the calling function will release the SEEPROM for us
-   */
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic787x_ultra2_term_detect
- *
- * Description:
- *   Detect the termination settings present on ultra2 class controllers
- *
- * NOTE: This functions assumes the SEEPROM will have already been aquired
- *       prior to invocation of this function.
- *-F*************************************************************************/
-static void
-aic7xxx_ultra2_term_detect(struct aic7xxx_host *p, int *enableSE_low,
-                           int *enableSE_high, int *enableLVD_low,
-                           int *enableLVD_high, int *eprom_present)
-{
-  unsigned char brdctl;
-
-  brdctl = read_brdctl(p);
-
-  *eprom_present  = (brdctl & BRDDAT7);
-  *enableSE_high  = (brdctl & BRDDAT6);
-  *enableSE_low   = (brdctl & BRDDAT5);
-  *enableLVD_high = (brdctl & BRDDAT4);
-  *enableLVD_low  = (brdctl & BRDDAT3);
-}
-
-/*+F*************************************************************************
- * Function:
- *   configure_termination
- *
- * Description:
- *   Configures the termination settings on PCI adapters that have
- *   SEEPROMs available.
- *-F*************************************************************************/
-static void
-configure_termination(struct aic7xxx_host *p)
-{
-  int internal50_present = 0;
-  int internal68_present = 0;
-  int external_present = 0;
-  int eprom_present = 0;
-  int enableSE_low = 0;
-  int enableSE_high = 0;
-  int enableLVD_low = 0;
-  int enableLVD_high = 0;
-  unsigned char brddat = 0;
-  unsigned char max_target = 0;
-  unsigned char sxfrctl1 = aic_inb(p, SXFRCTL1);
-
-  if (acquire_seeprom(p))
-  {
-    if (p->features & (AHC_WIDE|AHC_TWIN))
-      max_target = 16;
-    else
-      max_target = 8;
-    aic_outb(p, SEEMS | SEECS, SEECTL);
-    sxfrctl1 &= ~STPWEN;
-    /*
-     * The termination/cable detection logic is split into three distinct
-     * groups.  Ultra2 and later controllers, 2940UW-Pro controllers, and
-     * older 7850, 7860, 7870, 7880, and 7895 controllers.  Each has its
-     * own unique way of detecting their cables and writing the results
-     * back to the card.
-     */
-    if (p->features & AHC_ULTRA2)
-    {
-      /*
-       * As long as user hasn't overridden term settings, always check the
-       * cable detection logic
-       */
-      if (aic7xxx_override_term == -1)
-      {
-        aic7xxx_ultra2_term_detect(p, &enableSE_low, &enableSE_high,
-                                   &enableLVD_low, &enableLVD_high,
-                                   &eprom_present);
-      }
-      
-      /*
-       * If the user is overriding settings, then they have been preserved
-       * to here as fake adapter_control entries.  Parse them and allow
-       * them to override the detected settings (if we even did detection).
-       */
-      if (!(p->adapter_control & CFSEAUTOTERM))
-      {
-        enableSE_low = (p->adapter_control & CFSTERM);
-        enableSE_high = (p->adapter_control & CFWSTERM);
-      }
-      if (!(p->adapter_control & CFAUTOTERM))
-      {
-        enableLVD_low = enableLVD_high = (p->adapter_control & CFLVDSTERM);
-      }
-
-      /*
-       * Now take those settings that we have and translate them into the
-       * values that must be written into the registers.
-       *
-       * Flash Enable = BRDDAT7
-       * Secondary High Term Enable = BRDDAT6
-       * Secondary Low Term Enable = BRDDAT5
-       * LVD/Primary High Term Enable = BRDDAT4
-       * LVD/Primary Low Term Enable = STPWEN bit in SXFRCTL1
-       */
-      if (enableLVD_low != 0)
-      {
-        sxfrctl1 |= STPWEN;
-        p->flags |= AHC_TERM_ENB_LVD;
-        if (aic7xxx_verbose & VERBOSE_PROBE2)
-          printk(KERN_INFO "(scsi%d) LVD/Primary Low byte termination "
-                 "Enabled\n", p->host_no);
-      }
-          
-      if (enableLVD_high != 0)
-      {
-        brddat |= BRDDAT4;
-        if (aic7xxx_verbose & VERBOSE_PROBE2)
-          printk(KERN_INFO "(scsi%d) LVD/Primary High byte termination "
-                 "Enabled\n", p->host_no);
-      }
-
-      if (enableSE_low != 0)
-      {
-        brddat |= BRDDAT5;
-        if (aic7xxx_verbose & VERBOSE_PROBE2)
-          printk(KERN_INFO "(scsi%d) Secondary Low byte termination "
-                 "Enabled\n", p->host_no);
-      }
-
-      if (enableSE_high != 0)
-      {
-        brddat |= BRDDAT6;
-        if (aic7xxx_verbose & VERBOSE_PROBE2)
-          printk(KERN_INFO "(scsi%d) Secondary High byte termination "
-                 "Enabled\n", p->host_no);
-      }
-    }
-    else if (p->features & AHC_NEW_AUTOTERM)
-    {
-      /*
-       * The 50 pin connector termination is controlled by STPWEN in the
-       * SXFRCTL1 register.  Since the Adaptec docs typically say the
-       * controller is not allowed to be in the middle of a cable and
-       * this is the only connection on that stub of the bus, there is
-       * no need to even check for narrow termination, it's simply
-       * always on.
-       */
-      sxfrctl1 |= STPWEN;
-      if (aic7xxx_verbose & VERBOSE_PROBE2)
-        printk(KERN_INFO "(scsi%d) Narrow channel termination Enabled\n",
-               p->host_no);
-
-      if (p->adapter_control & CFAUTOTERM)
-      {
-        aic2940_uwpro_wide_cable_detect(p, &internal68_present,
-                                        &external_present,
-                                        &eprom_present);
-        printk(KERN_INFO "(scsi%d) Cables present (Int-50 %s, Int-68 %s, "
-               "Ext-68 %s)\n", p->host_no,
-               "Don't Care",
-               internal68_present ? "YES" : "NO",
-               external_present ? "YES" : "NO");
-        if (aic7xxx_verbose & VERBOSE_PROBE2)
-          printk(KERN_INFO "(scsi%d) EEPROM %s present.\n", p->host_no,
-               eprom_present ? "is" : "is not");
-        if (internal68_present && external_present)
-        {
-          brddat = 0;
-          p->flags &= ~AHC_TERM_ENB_SE_HIGH;
-          if (aic7xxx_verbose & VERBOSE_PROBE2)
-            printk(KERN_INFO "(scsi%d) Wide channel termination Disabled\n",
-                   p->host_no);
-        }
-        else
-        {
-          brddat = BRDDAT6;
-          p->flags |= AHC_TERM_ENB_SE_HIGH;
-          if (aic7xxx_verbose & VERBOSE_PROBE2)
-            printk(KERN_INFO "(scsi%d) Wide channel termination Enabled\n",
-                   p->host_no);
-        }
-      }
-      else
-      {
-        /*
-         * The termination of the Wide channel is done more like normal
-         * though, and the setting of this termination is done by writing
-         * either a 0 or 1 to BRDDAT6 of the BRDDAT register
-         */
-        if (p->adapter_control & CFWSTERM)
-        {
-          brddat = BRDDAT6;
-          p->flags |= AHC_TERM_ENB_SE_HIGH;
-          if (aic7xxx_verbose & VERBOSE_PROBE2)
-            printk(KERN_INFO "(scsi%d) Wide channel termination Enabled\n",
-                   p->host_no);
-        }
-        else
-        {
-          brddat = 0;
-        }
-      }
-    }
-    else
-    {
-      if (p->adapter_control & CFAUTOTERM)
-      {
-        if (p->flags & AHC_MOTHERBOARD)
-        {
-          printk(KERN_INFO "(scsi%d) Warning - detected auto-termination\n",
-                 p->host_no);
-          printk(KERN_INFO "(scsi%d) Please verify driver detected settings "
-            "are correct.\n", p->host_no);
-          printk(KERN_INFO "(scsi%d) If not, then please properly set the "
-            "device termination\n", p->host_no);
-          printk(KERN_INFO "(scsi%d) in the Adaptec SCSI BIOS by hitting "
-            "CTRL-A when prompted\n", p->host_no);
-          printk(KERN_INFO "(scsi%d) during machine bootup.\n", p->host_no);
-        }
-        /* Configure auto termination. */
-
-        if ( (p->chip & AHC_CHIPID_MASK) >= AHC_AIC7870 )
-        {
-          aic787x_cable_detect(p, &internal50_present, &internal68_present,
-            &external_present, &eprom_present);
-        }
-        else
-        {
-          aic785x_cable_detect(p, &internal50_present, &external_present,
-            &eprom_present);
-        }
-
-        if (max_target <= 8)
-          internal68_present = 0;
-
-        if (max_target > 8)
-        {
-          printk(KERN_INFO "(scsi%d) Cables present (Int-50 %s, Int-68 %s, "
-                 "Ext-68 %s)\n", p->host_no,
-                 internal50_present ? "YES" : "NO",
-                 internal68_present ? "YES" : "NO",
-                 external_present ? "YES" : "NO");
-        }
-        else
-        {
-          printk(KERN_INFO "(scsi%d) Cables present (Int-50 %s, Ext-50 %s)\n",
-                 p->host_no,
-                 internal50_present ? "YES" : "NO",
-                 external_present ? "YES" : "NO");
-        }
-        if (aic7xxx_verbose & VERBOSE_PROBE2)
-          printk(KERN_INFO "(scsi%d) EEPROM %s present.\n", p->host_no,
-               eprom_present ? "is" : "is not");
-
-        /*
-         * Now set the termination based on what we found.  BRDDAT6
-         * controls wide termination enable.
-         * Flash Enable = BRDDAT7
-         * SE High Term Enable = BRDDAT6
-         */
-        if (internal50_present && internal68_present && external_present)
-        {
-          printk(KERN_INFO "(scsi%d) Illegal cable configuration!!  Only two\n",
-                 p->host_no);
-          printk(KERN_INFO "(scsi%d) connectors on the SCSI controller may be "
-                 "in use at a time!\n", p->host_no);
-          /*
-           * Force termination (low and high byte) on.  This is safer than
-           * leaving it completely off, especially since this message comes
-           * most often from motherboard controllers that don't even have 3
-           * connectors, but instead are failing the cable detection.
-           */
-          internal50_present = external_present = 0;
-          enableSE_high = enableSE_low = 1;
-        }
-
-        if ((max_target > 8) &&
-            ((external_present == 0) || (internal68_present == 0)) )
-        {
-          brddat |= BRDDAT6;
-          p->flags |= AHC_TERM_ENB_SE_HIGH;
-          if (aic7xxx_verbose & VERBOSE_PROBE2)
-            printk(KERN_INFO "(scsi%d) SE High byte termination Enabled\n",
-                   p->host_no);
-        }
-
-        if ( ((internal50_present ? 1 : 0) +
-              (internal68_present ? 1 : 0) +
-              (external_present   ? 1 : 0)) <= 1 )
-        {
-          sxfrctl1 |= STPWEN;
-          p->flags |= AHC_TERM_ENB_SE_LOW;
-          if (aic7xxx_verbose & VERBOSE_PROBE2)
-            printk(KERN_INFO "(scsi%d) SE Low byte termination Enabled\n",
-                   p->host_no);
-        }
-      }
-      else /* p->adapter_control & CFAUTOTERM */
-      {
-        if (p->adapter_control & CFSTERM)
-        {
-          sxfrctl1 |= STPWEN;
-          if (aic7xxx_verbose & VERBOSE_PROBE2)
-            printk(KERN_INFO "(scsi%d) SE Low byte termination Enabled\n",
-                   p->host_no);
-        }
-
-        if (p->adapter_control & CFWSTERM)
-        {
-          brddat |= BRDDAT6;
-          if (aic7xxx_verbose & VERBOSE_PROBE2)
-            printk(KERN_INFO "(scsi%d) SE High byte termination Enabled\n",
-                   p->host_no);
-        }
-      }
-    }
-
-    aic_outb(p, sxfrctl1, SXFRCTL1);
-    write_brdctl(p, brddat);
-    release_seeprom(p);
-  }
-}
-
-/*+F*************************************************************************
- * Function:
- *   detect_maxscb
- *
- * Description:
- *   Detects the maximum number of SCBs for the controller and returns
- *   the count and a mask in p (p->maxscbs, p->qcntmask).
- *-F*************************************************************************/
-static void
-detect_maxscb(struct aic7xxx_host *p)
-{
-  int i;
-
-  /*
-   * It's possible that we've already done this for multichannel
-   * adapters.
-   */
-  if (p->scb_data->maxhscbs == 0)
-  {
-    /*
-     * We haven't initialized the SCB settings yet.  Walk the SCBs to
-     * determince how many there are.
-     */
-    aic_outb(p, 0, FREE_SCBH);
-
-    for (i = 0; i < AIC7XXX_MAXSCB; i++)
-    {
-      aic_outb(p, i, SCBPTR);
-      aic_outb(p, i, SCB_CONTROL);
-      if (aic_inb(p, SCB_CONTROL) != i)
-        break;
-      aic_outb(p, 0, SCBPTR);
-      if (aic_inb(p, SCB_CONTROL) != 0)
-        break;
-
-      aic_outb(p, i, SCBPTR);
-      aic_outb(p, 0, SCB_CONTROL);   /* Clear the control byte. */
-      aic_outb(p, i + 1, SCB_NEXT);  /* Set the next pointer. */
-      aic_outb(p, SCB_LIST_NULL, SCB_TAG);  /* Make the tag invalid. */
-      aic_outb(p, SCB_LIST_NULL, SCB_BUSYTARGETS);  /* no busy untagged */
-      aic_outb(p, SCB_LIST_NULL, SCB_BUSYTARGETS+1);/* targets active yet */
-      aic_outb(p, SCB_LIST_NULL, SCB_BUSYTARGETS+2);
-      aic_outb(p, SCB_LIST_NULL, SCB_BUSYTARGETS+3);
-    }
-
-    /* Make sure the last SCB terminates the free list. */
-    aic_outb(p, i - 1, SCBPTR);
-    aic_outb(p, SCB_LIST_NULL, SCB_NEXT);
-
-    /* Ensure we clear the first (0) SCBs control byte. */
-    aic_outb(p, 0, SCBPTR);
-    aic_outb(p, 0, SCB_CONTROL);
-
-    p->scb_data->maxhscbs = i;
-    /*
-     * Use direct indexing instead for speed
-     */
-    if ( i == AIC7XXX_MAXSCB )
-      p->flags &= ~AHC_PAGESCBS;
-  }
-
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_register
- *
- * Description:
- *   Register a Adaptec aic7xxx chip SCSI controller with the kernel.
- *-F*************************************************************************/
-static int
-aic7xxx_register(Scsi_Host_Template *template, struct aic7xxx_host *p,
-  int reset_delay)
-{
-  int i, result;
-  int max_targets;
-  int found = 1;
-  unsigned char term, scsi_conf;
-  struct Scsi_Host *host;
-
-  host = p->host;
-
-  p->scb_data->maxscbs = AIC7XXX_MAXSCB;
-  host->can_queue = AIC7XXX_MAXSCB;
-  host->cmd_per_lun = 3;
-  host->sg_tablesize = AIC7XXX_MAX_SG;
-  host->select_queue_depths = aic7xxx_select_queue_depth;
-  host->this_id = p->scsi_id;
-  host->io_port = p->base;
-  host->n_io_port = 0xFF;
-  host->base = (unsigned char *) p->mbase;
-  host->irq = p->irq;
-  if (p->features & AHC_WIDE)
-  {
-    host->max_id = 16;
-  }
-  if (p->features & AHC_TWIN)
-  {
-    host->max_channel = 1;
-  }
-
-  p->host = host;
-  p->host_no = host->host_no;
-  host->unique_id = p->instance;
-  p->isr_count = 0;
-  p->next = NULL;
-  p->completeq.head = NULL;
-  p->completeq.tail = NULL;
-  scbq_init(&p->scb_data->free_scbs);
-  scbq_init(&p->waiting_scbs);
-  init_timer(&p->dev_timer);
-  p->dev_timer.data = (unsigned long)p;
-  p->dev_timer.function = (void *)aic7xxx_timer;
-  p->dev_timer_active = 0;
-
-  for (i = 0; i < NUMBER(p->untagged_scbs); i++)
-  {
-    p->untagged_scbs[i] = SCB_LIST_NULL;
-    p->qinfifo[i] = SCB_LIST_NULL;
-    p->qoutfifo[i] = SCB_LIST_NULL;
-  }
-  /*
-   * We currently have no commands of any type
-   */
-  p->qinfifonext = 0;
-  p->qoutfifonext = 0;
-
-  for (i = 0; i < MAX_TARGETS; i++)
-  {
-    p->dev_commands_sent[i] = 0;
-    p->dev_flags[i] = 0;
-    p->dev_active_cmds[i] = 0;
-    p->dev_last_queue_full[i] = 0;
-    p->dev_last_queue_full_count[i] = 0;
-    p->dev_max_queue_depth[i] = 1;
-    p->dev_temp_queue_depth[i] = 1;
-    p->dev_expires[i] = 0;
-    scbq_init(&p->delayed_scbs[i]);
-  }
-
-  printk(KERN_INFO "(scsi%d) <%s> found at ", p->host_no,
-    board_names[p->board_name_index]);
-  switch(p->chip)
-  {
-    case (AHC_AIC7770|AHC_EISA):
-      printk("EISA slot %d\n", p->pci_device_fn);
-      break;
-    case (AHC_AIC7770|AHC_VL):
-      printk("VLB slot %d\n", p->pci_device_fn);
-      break;
-    default:
-      printk("PCI %d/%d/%d\n", p->pci_bus, PCI_SLOT(p->pci_device_fn),
-        PCI_FUNC(p->pci_device_fn));
-      break;
-  }
-  if (p->features & AHC_TWIN)
-  {
-    printk(KERN_INFO "(scsi%d) Twin Channel, A SCSI ID %d, B SCSI ID %d, ",
-           p->host_no, p->scsi_id, p->scsi_id_b);
-  }
-  else
-  {
-    char *channel;
-
-    channel = "";
-
-    if ((p->flags & AHC_MULTI_CHANNEL) != 0)
-    {
-      channel = " A";
-
-      if ( (p->flags & (AHC_CHNLB|AHC_CHNLC)) != 0 )
-      {
-        channel = (p->flags & AHC_CHNLB) ? " B" : " C";
-      }
-    }
-    if (p->features & AHC_WIDE)
-    {
-      printk(KERN_INFO "(scsi%d) Wide ", p->host_no);
-    }
-    else
-    {
-      printk(KERN_INFO "(scsi%d) Narrow ", p->host_no);
-    }
-    printk("Channel%s, SCSI ID=%d, ", channel, p->scsi_id);
-  }
-  aic_outb(p, 0, SEQ_FLAGS);
-
-  detect_maxscb(p);
-
-  printk("%d/%d SCBs\n", p->scb_data->maxhscbs, p->scb_data->maxscbs);
-  if (aic7xxx_verbose & VERBOSE_PROBE2)
-  {
-    printk(KERN_INFO "(scsi%d) BIOS %sabled, IO Port 0x%lx, IRQ %d\n",
-      p->host_no, (p->flags & AHC_BIOS_ENABLED) ? "en" : "dis",
-      p->base, p->irq);
-    printk(KERN_INFO "(scsi%d) IO Memory at 0x%lx, MMAP Memory at 0x%lx\n",
-      p->host_no, p->mbase, (unsigned long)p->maddr);
-  }
-
-#ifdef CONFIG_PCI
-  /*
-   * Now that we know our instance number, we can set the flags we need to
-   * force termination if need be.
-   */
-  if (aic7xxx_stpwlev != -1)
-  {
-    /*
-     * This option only applies to PCI controllers.
-     */
-    if ( (p->chip & ~AHC_CHIPID_MASK) == AHC_PCI)
-    {
-      unsigned char devconfig;
-
-#if LINUX_KERNEL_VERSION > KERNEL_VERSION(2,1,92)
-      pci_read_config_byte(p->pdev, DEVCONFIG, &devconfig);
-#else
-      pcibios_read_config_byte(p->pci_bus, p->pci_device_fn,
-                               DEVCONFIG, &devconfig);
-#endif
-      if ( (aic7xxx_stpwlev >> p->instance) & 0x01 )
-      {
-        devconfig |= STPWLEVEL;
-        if (aic7xxx_verbose & VERBOSE_PROBE2)
-          printk("(scsi%d) Force setting STPWLEVEL bit\n", p->host_no);
-      }
-      else
-      {
-        devconfig &= ~STPWLEVEL;
-        if (aic7xxx_verbose & VERBOSE_PROBE2)
-          printk("(scsi%d) Force clearing STPWLEVEL bit\n", p->host_no);
-      }
-#if LINUX_KERNEL_VERSION > KERNEL_VERSION(2,1,92)
-      pci_write_config_byte(p->pdev, DEVCONFIG, devconfig);
-#else
-      pcibios_write_config_byte(p->pci_bus, p->pci_device_fn,
-                                DEVCONFIG, devconfig);
-#endif
-    }
-  }
-#endif
-
-  /*
-   * That took care of devconfig and stpwlev, now for the actual termination
-   * settings.
-   */
-  if (aic7xxx_override_term != -1)
-  {
-    /*
-     * Again, this only applies to PCI controllers.  We don't have problems
-     * with the termination on 274x controllers to the best of my knowledge.
-     */
-    if ( (p->chip & ~AHC_CHIPID_MASK) == AHC_PCI)
-    {
-      unsigned char term_override;
-
-      term_override = ( (aic7xxx_override_term >> (p->instance * 4)) & 0x0f);
-      p->adapter_control &= 
-        ~(CFSTERM|CFWSTERM|CFLVDSTERM|CFAUTOTERM|CFSEAUTOTERM);
-      if ( (p->features & AHC_ULTRA2) && (term_override & 0x0c) )
-      {
-        p->adapter_control |= CFLVDSTERM;
-      }
-      if (term_override & 0x02)
-      {
-        p->adapter_control |= CFWSTERM;
-      }
-      if (term_override & 0x01)
-      {
-        p->adapter_control |= CFSTERM;
-      }
-    }
-  }
-
-  if ( (p->flags & AHC_SEEPROM_FOUND) || (aic7xxx_override_term != -1) )
-  {
-    if (p->features & AHC_SPIOCAP)
-    {
-      if ( aic_inb(p, SPIOCAP) & SSPIOCPS )
-      /*
-       * Update the settings in sxfrctl1 to match the termination
-       * settings.
-       */
-        configure_termination(p);
-    }
-    else if ((p->chip & AHC_CHIPID_MASK) >= AHC_AIC7870)
-    {
-      configure_termination(p);
-    }
-  }
-
-  /*
-   * Set the SCSI Id, SXFRCTL0, SXFRCTL1, and SIMODE1, for both channels
-   */
-  if (p->features & AHC_TWIN)
-  {
-    /* Select channel B */
-    aic_outb(p, aic_inb(p, SBLKCTL) | SELBUSB, SBLKCTL);
-
-    if ((p->flags & AHC_SEEPROM_FOUND) || (aic7xxx_override_term != -1))
-      term = (aic_inb(p, SXFRCTL1) & STPWEN);
-    else
-      term = ((p->flags & AHC_TERM_ENB_B) ? STPWEN : 0);
-
-    aic_outb(p, p->scsi_id_b, SCSIID);
-    scsi_conf = aic_inb(p, SCSICONF + 1);
-    aic_outb(p, DFON | SPIOEN, SXFRCTL0);
-    aic_outb(p, (scsi_conf & ENSPCHK) | aic7xxx_seltime | term | 
-         ENSTIMER | ACTNEGEN, SXFRCTL1);
-    aic_outb(p, 0, SIMODE0);
-    aic_outb(p, ENSELTIMO | ENSCSIRST | ENSCSIPERR, SIMODE1);
-    aic_outb(p, 0, SCSIRATE);
-
-    /* Select channel A */
-    aic_outb(p, aic_inb(p, SBLKCTL) & ~SELBUSB, SBLKCTL);
-  }
-
-  if (p->features & AHC_ULTRA2)
-  {
-    aic_outb(p, p->scsi_id, SCSIID_ULTRA2);
-  }
-  else
-  {
-    aic_outb(p, p->scsi_id, SCSIID);
-  }
-  if ((p->flags & AHC_SEEPROM_FOUND) || (aic7xxx_override_term != -1))
-    term = (aic_inb(p, SXFRCTL1) & STPWEN);
-  else
-    term = ((p->flags & (AHC_TERM_ENB_A|AHC_TERM_ENB_LVD)) ? STPWEN : 0);
-  scsi_conf = aic_inb(p, SCSICONF);
-  aic_outb(p, DFON | SPIOEN, SXFRCTL0);
-  aic_outb(p, (scsi_conf & ENSPCHK) | aic7xxx_seltime | term | 
-       ENSTIMER | ACTNEGEN, SXFRCTL1);
-  aic_outb(p, 0, SIMODE0);
-  /*
-   * If we are a cardbus adapter then don't enable SCSI reset detection.
-   * We shouldn't likely be sharing SCSI busses with someone else, and
-   * if we don't have a cable currently plugged into the controller then
-   * we won't have a power source for the SCSI termination, which means
-   * we'll see infinite incoming bus resets.
-   */
-  if(p->flags & AHC_NO_STPWEN)
-    aic_outb(p, ENSELTIMO | ENSCSIPERR, SIMODE1);
-  else
-    aic_outb(p, ENSELTIMO | ENSCSIRST | ENSCSIPERR, SIMODE1);
-  aic_outb(p, 0, SCSIRATE);
-  if ( p->features & AHC_ULTRA2)
-    aic_outb(p, 0, SCSIOFFSET);
-
-  /*
-   * Look at the information that board initialization or the board
-   * BIOS has left us. In the lower four bits of each target's
-   * scratch space any value other than 0 indicates that we should
-   * initiate synchronous transfers. If it's zero, the user or the
-   * BIOS has decided to disable synchronous negotiation to that
-   * target so we don't activate the needsdtr flag.
-   */
-  if ((p->features & (AHC_TWIN|AHC_WIDE)) == 0)
-  {
-    max_targets = 8;
-  }
-  else
-  {
-    max_targets = 16;
-  }
-
-  if (!(aic7xxx_no_reset))
-  {
-    /*
-     * If we reset the bus, then clear the transfer settings, else leave
-     * them be
-     */
-    for (i = 0; i < max_targets; i++)
-    {
-      aic_outb(p, 0, TARG_SCSIRATE + i);
-      if (p->features & AHC_ULTRA2)
-      {
-        aic_outb(p, 0, TARG_OFFSET + i);
-      }
-      p->transinfo[i].cur_offset = 0;
-      p->transinfo[i].cur_period = 0;
-      p->transinfo[i].cur_width = MSG_EXT_WDTR_BUS_8_BIT;
-    }
-
-    /*
-     * If we reset the bus, then clear the transfer settings, else leave
-     * them be.
-     */
-    aic_outb(p, 0, ULTRA_ENB);
-    aic_outb(p, 0, ULTRA_ENB + 1);
-    p->ultraenb = 0;
-  }
-
-  /*
-   * Allocate enough hardware scbs to handle the maximum number of
-   * concurrent transactions we can have.  We have to make sure that
-   * the allocated memory is contiguous memory.  The Linux kmalloc
-   * routine should only allocate contiguous memory, but note that
-   * this could be a problem if kmalloc() is changed.
-   */
-  {
-    size_t array_size;
-    unsigned int hscb_physaddr;
-    unsigned long temp;
-
-    array_size = p->scb_data->maxscbs * sizeof(struct aic7xxx_hwscb);
-    if (p->scb_data->hscbs == NULL)
-    {
-      /*
-       * A little padding so we can align thing the way we want
-       */
-      p->scb_data->hscbs = kmalloc(array_size + 0x1f, GFP_ATOMIC);
-    }
-    if (p->scb_data->hscbs == NULL)
-    {
-      printk("(scsi%d) Unable to allocate hardware SCB array; "
-             "failing detection.\n", p->host_no);
-      aic_outb(p, 0, SIMODE1);
-      p->irq = 0;
-      return(0);
-    }
-    /*
-     * Save the actual kmalloc buffer pointer off, then align our
-     * buffer to a 32 byte boundary
-     */
-    p->scb_data->hscb_kmalloc_ptr = p->scb_data->hscbs;
-    temp = (unsigned long)p->scb_data->hscbs;
-    temp += 0x1f;
-    temp &= ~0x1f;
-    p->scb_data->hscbs = (struct aic7xxx_hwscb *)temp;
-    /* At least the control byte of each SCB needs to be 0. */
-    memset(p->scb_data->hscbs, 0, array_size);
-
-    /* Tell the sequencer where it can find the hardware SCB array. */
-    hscb_physaddr = VIRT_TO_BUS(p->scb_data->hscbs);
-    aic_outb(p, hscb_physaddr & 0xFF, HSCB_ADDR);
-    aic_outb(p, (hscb_physaddr >> 8) & 0xFF, HSCB_ADDR + 1);
-    aic_outb(p, (hscb_physaddr >> 16) & 0xFF, HSCB_ADDR + 2);
-    aic_outb(p, (hscb_physaddr >> 24) & 0xFF, HSCB_ADDR + 3);
-
-    /* Set up the fifo areas at the same time */
-    hscb_physaddr = VIRT_TO_BUS(&p->untagged_scbs[0]);
-    aic_outb(p, hscb_physaddr & 0xFF, SCBID_ADDR);
-    aic_outb(p, (hscb_physaddr >> 8) & 0xFF, SCBID_ADDR + 1);
-    aic_outb(p, (hscb_physaddr >> 16) & 0xFF, SCBID_ADDR + 2);
-    aic_outb(p, (hscb_physaddr >> 24) & 0xFF, SCBID_ADDR + 3);
-  }
-
-  /* The Q-FIFOs we just set up are all empty */
-  aic_outb(p, 0, QINPOS);
-  aic_outb(p, 0, KERNEL_QINPOS);
-  aic_outb(p, 0, QOUTPOS);
-
-  if(p->features & AHC_QUEUE_REGS)
-  {
-    aic_outb(p, SCB_QSIZE_256, QOFF_CTLSTA);
-    aic_outb(p, 0, SDSCB_QOFF);
-    aic_outb(p, 0, SNSCB_QOFF);
-    aic_outb(p, 0, HNSCB_QOFF);
-  }
-
-  /*
-   * We don't have any waiting selections or disconnected SCBs.
-   */
-  aic_outb(p, SCB_LIST_NULL, WAITING_SCBH);
-  aic_outb(p, SCB_LIST_NULL, DISCONNECTED_SCBH);
-
-  /*
-   * Message out buffer starts empty
-   */
-  aic_outb(p, MSG_NOOP, MSG_OUT);
-  aic_outb(p, MSG_NOOP, LAST_MSG);
-
-  /*
-   * Set all the other asundry items that haven't been set yet.
-   * This includes just dumping init values to a lot of registers simply
-   * to make sure they've been touched and are ready for use parity wise
-   * speaking.
-   */
-  aic_outb(p, 0, TMODE_CMDADDR);
-  aic_outb(p, 0, TMODE_CMDADDR + 1);
-  aic_outb(p, 0, TMODE_CMDADDR + 2);
-  aic_outb(p, 0, TMODE_CMDADDR + 3);
-  aic_outb(p, 0, TMODE_CMDADDR_NEXT);
-
-  /*
-   * Link us into the list of valid hosts
-   */
-  p->next = first_aic7xxx;
-  first_aic7xxx = p;
-
-  /*
-   * Allocate the first set of scbs for this controller.  This is to stream-
-   * line code elsewhere in the driver.  If we have to check for the existence
-   * of scbs in certain code sections, it slows things down.  However, as
-   * soon as we register the IRQ for this card, we could get an interrupt that
-   * includes possibly the SCSI_RSTI interrupt.  If we catch that interrupt
-   * then we are likely to segfault if we don't have at least one chunk of
-   * SCBs allocated or add checks all through the reset code to make sure
-   * that the SCBs have been allocated which is an invalid running condition
-   * and therefore I think it's preferable to simply pre-allocate the first
-   * chunk of SCBs.
-   */
-  aic7xxx_allocate_scb(p);
-
-  /*
-   * Load the sequencer program, then re-enable the board -
-   * resetting the AIC-7770 disables it, leaving the lights
-   * on with nobody home.
-   */
-  aic7xxx_loadseq(p);
-
-  /*
-   * Make sure the AUTOFLUSHDIS bit is *not* set in the SBLKCTL register
-   */
-  aic_outb(p, aic_inb(p, SBLKCTL) & ~AUTOFLUSHDIS, SBLKCTL);
-
-  if ( (p->chip & AHC_CHIPID_MASK) == AHC_AIC7770 )
-  {
-    aic_outb(p, ENABLE, BCTL);  /* Enable the boards BUS drivers. */
-  }
-
-  if ( !(aic7xxx_no_reset) )
-  {
-    if (p->features & AHC_TWIN)
-    {
-      if (aic7xxx_verbose & VERBOSE_PROBE2)
-        printk(KERN_INFO "(scsi%d) Resetting channel B\n", p->host_no);
-      aic_outb(p, aic_inb(p, SBLKCTL) | SELBUSB, SBLKCTL);
-      aic7xxx_reset_current_bus(p);
-      aic_outb(p, aic_inb(p, SBLKCTL) & ~SELBUSB, SBLKCTL);
-    }
-    /* Reset SCSI bus A. */
-    if (aic7xxx_verbose & VERBOSE_PROBE2)
-    {  /* In case we are a 3940, 3985, or 7895, print the right channel */
-      char *channel = "";
-      if (p->flags & AHC_MULTI_CHANNEL)
-      {
-        channel = " A";
-        if (p->flags & (AHC_CHNLB|AHC_CHNLC))
-          channel = (p->flags & AHC_CHNLB) ? " B" : " C";
-      }
-      printk(KERN_INFO "(scsi%d) Resetting channel%s\n", p->host_no, channel);
-    }
-    
-    aic7xxx_reset_current_bus(p);
-
-    /*
-     * Delay for the reset delay by setting the timer, this will delay
-     * future commands sent to any devices.
-     */
-    p->flags |= AHC_RESET_DELAY;
-    for(i=0; i<MAX_TARGETS; i++)
-    {
-      p->dev_expires[i] = jiffies + (4 * HZ);
-      p->dev_timer_active |= (0x01 << i);
-    }
-    p->dev_timer.expires = p->dev_expires[p->scsi_id];
-    add_timer(&p->dev_timer);
-    p->dev_timer_active |= (0x01 << MAX_TARGETS);
-  }
-  else
-  {
-    if (!reset_delay)
-    {
-      printk(KERN_INFO "(scsi%d) Not resetting SCSI bus.  Note: Don't use "
-             "the no_reset\n", p->host_no);
-      printk(KERN_INFO "(scsi%d) option unless you have a verifiable need "
-             "for it.\n", p->host_no);
-    }
-  }
-  
-  /*
-   * Register IRQ with the kernel.  Only allow sharing IRQs with
-   * PCI devices.
-   */
-  if (!(p->chip & AHC_PCI))
-  {
-    result = (request_irq(p->irq, do_aic7xxx_isr, 0, "aic7xxx", p));
-  }
-  else
-  {
-    result = (request_irq(p->irq, do_aic7xxx_isr, SA_SHIRQ,
-              "aic7xxx", p));
-    if (result < 0)
-    {
-      result = (request_irq(p->irq, do_aic7xxx_isr, SA_INTERRUPT | SA_SHIRQ,
-              "aic7xxx", p));
-    }
-  }
-  if (result < 0)
-  {
-    printk(KERN_WARNING "(scsi%d) Couldn't register IRQ %d, ignoring "
-           "controller.\n", p->host_no, p->irq);
-    aic_outb(p, 0, SIMODE1);
-    p->irq = 0;
-    return (0);
-  }
-
-  if(aic_inb(p, INTSTAT) & INT_PEND)
-    printk(INFO_LEAD "spurious interrupt during configuration, cleared.\n",
-      p->host_no, -1, -1 , -1);
-  aic7xxx_clear_intstat(p);
-
-  unpause_sequencer(p, /* unpause_always */ TRUE);
-
-  return (found);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_chip_reset
- *
- * Description:
- *   Perform a chip reset on the aic7xxx SCSI controller.  The controller
- *   is paused upon return.
- *-F*************************************************************************/
-int
-aic7xxx_chip_reset(struct aic7xxx_host *p)
-{
-  unsigned char sblkctl;
-  int wait;
-
-  /*
-   * For some 274x boards, we must clear the CHIPRST bit and pause
-   * the sequencer. For some reason, this makes the driver work.
-   */
-  aic_outb(p, PAUSE | CHIPRST, HCNTRL);
-
-  /*
-   * In the future, we may call this function as a last resort for
-   * error handling.  Let's be nice and not do any unecessary delays.
-   */
-  wait = 1000;  /* 1 msec (1000 * 1 msec) */
-  while (--wait && !(aic_inb(p, HCNTRL) & CHIPRSTACK))
-  {
-    udelay(1);  /* 1 usec */
-  }
-
-  pause_sequencer(p);
-
-  sblkctl = aic_inb(p, SBLKCTL) & (SELBUSB|SELWIDE);
-  if (p->chip & AHC_PCI)
-    sblkctl &= ~SELBUSB;
-  switch( sblkctl )
-  {
-    case 0:  /* normal narrow card */
-      break;
-    case 2:  /* Wide card */
-      p->features |= AHC_WIDE;
-      break;
-    case 8:  /* Twin card */
-      p->features |= AHC_TWIN;
-      p->flags |= AHC_MULTI_CHANNEL;
-      break;
-    default: /* hmmm...we don't know what this is */
-      printk(KERN_WARNING "aic7xxx: Unsupported adapter type %d, ignoring.\n",
-        aic_inb(p, SBLKCTL) & 0x0a);
-      return(-1);
-  }
-  return(0);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_alloc
- *
- * Description:
- *   Allocate and initialize a host structure.  Returns NULL upon error
- *   and a pointer to a aic7xxx_host struct upon success.
- *-F*************************************************************************/
-static struct aic7xxx_host *
-aic7xxx_alloc(Scsi_Host_Template *sht, struct aic7xxx_host *temp)
-{
-  struct aic7xxx_host *p = NULL;
-  struct Scsi_Host *host;
-  int i;
-
-  /*
-   * Allocate a storage area by registering us with the mid-level
-   * SCSI layer.
-   */
-  host = scsi_register(sht, sizeof(struct aic7xxx_host));
-
-  if (host != NULL)
-  {
-    p = (struct aic7xxx_host *) host->hostdata;
-    memset(p, 0, sizeof(struct aic7xxx_host));
-    *p = *temp;
-    p->host = host;
-
-    p->scb_data = kmalloc(sizeof(scb_data_type), GFP_ATOMIC);
-    if (p->scb_data != NULL)
-    {
-      memset(p->scb_data, 0, sizeof(scb_data_type));
-      scbq_init (&p->scb_data->free_scbs);
-    }
-    else
-    {
-      /*
-       * For some reason we don't have enough memory.  Free the
-       * allocated memory for the aic7xxx_host struct, and return NULL.
-       */
-      release_region(p->base, MAXREG - MINREG);
-      scsi_unregister(host);
-      return(NULL);
-    }
-    p->host_no = host->host_no;
-    p->tagenable = 0;
-    p->orderedtag = 0;
-    for (i=0; i<MAX_TARGETS; i++)
-    {
-      p->transinfo[i].goal_period = 255;
-      p->transinfo[i].goal_offset = 0;
-      p->transinfo[i].goal_options = 0;
-      p->transinfo[i].goal_width = MSG_EXT_WDTR_BUS_8_BIT;
-    }
-    DRIVER_LOCK_INIT
-  }
-  return (p);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_free
- *
- * Description:
- *   Frees and releases all resources associated with an instance of
- *   the driver (struct aic7xxx_host *).
- *-F*************************************************************************/
-static void
-aic7xxx_free(struct aic7xxx_host *p)
-{
-  int i;
-
-  /*
-   * Free the allocated hardware SCB space.
-   */
-  if (p->scb_data != NULL)
-  {
-    if (p->scb_data->hscbs != NULL)
-    {
-      kfree(p->scb_data->hscb_kmalloc_ptr);
-      p->scb_data->hscbs = p->scb_data->hscb_kmalloc_ptr = NULL;
-    }
-    /*
-     * Free the driver SCBs.  These were allocated on an as-need
-     * basis.  We allocated these in groups depending on how many
-     * we could fit into a given amount of RAM.  The tail SCB for
-     * these allocations has a pointer to the alloced area.
-     */
-    for (i = 0; i < p->scb_data->numscbs; i++)
-    {
-      if (p->scb_data->scb_array[i]->kmalloc_ptr != NULL)
-        kfree(p->scb_data->scb_array[i]->kmalloc_ptr);
-      p->scb_data->scb_array[i] = NULL;
-    }
-  
-    /*
-     * Free the SCB data area.
-     */
-    kfree(p->scb_data);
-  }
-
-  /*
-   * Free any alloced Scsi_Cmnd structures that might be around for
-   * negotiation purposes....
-   */
-  for (i = 0; i < MAX_TARGETS; i++)
-  {
-    if(p->dev_dtr_cmnd[i])
-    {
-      if(p->dev_dtr_cmnd[i]->request_buffer)
-      {
-        kfree(p->dev_dtr_cmnd[i]->request_buffer);
-      }
-      kfree(p->dev_dtr_cmnd[i]);
-    }
-  }
-
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_load_seeprom
- *
- * Description:
- *   Load the seeprom and configure adapter and target settings.
- *   Returns 1 if the load was successful and 0 otherwise.
- *-F*************************************************************************/
-static void
-aic7xxx_load_seeprom(struct aic7xxx_host *p, unsigned char *sxfrctl1)
-{
-  int have_seeprom = 0;
-  int i, max_targets, mask;
-  unsigned char scsirate, scsi_conf;
-  unsigned short scarray[128];
-  struct seeprom_config *sc = (struct seeprom_config *) scarray;
-
-  if (aic7xxx_verbose & VERBOSE_PROBE2)
-  {
-    printk(KERN_INFO "aic7xxx: Loading serial EEPROM...");
-  }
-  switch (p->chip)
-  {
-    case (AHC_AIC7770|AHC_EISA):  /* None of these adapters have seeproms. */
-      if (aic_inb(p, SCSICONF) & TERM_ENB)
-        p->flags |= AHC_TERM_ENB_A;
-      if ( (p->features & AHC_TWIN) && (aic_inb(p, SCSICONF + 1) & TERM_ENB) )
-        p->flags |= AHC_TERM_ENB_B;
-      break;
-
-    case (AHC_AIC7770|AHC_VL):
-      have_seeprom = read_284x_seeprom(p, (struct seeprom_config *) scarray);
-      break;
-
-    default:
-      have_seeprom = read_seeprom(p, (p->flags & (AHC_CHNLB|AHC_CHNLC)),
-                                  scarray, p->sc_size, p->sc_type);
-      if (!have_seeprom)
-      {
-        if(p->sc_type == C46)
-          have_seeprom = read_seeprom(p, (p->flags & (AHC_CHNLB|AHC_CHNLC)),
-                                      scarray, p->sc_size, C56_66);
-        else
-          have_seeprom = read_seeprom(p, (p->flags & (AHC_CHNLB|AHC_CHNLC)),
-                                      scarray, p->sc_size, C46);
-      }
-      if (!have_seeprom)
-      {
-        p->sc_size = 128;
-        have_seeprom = read_seeprom(p, 4*(p->flags & (AHC_CHNLB|AHC_CHNLC)),
-                                    scarray, p->sc_size, p->sc_type);
-        if (!have_seeprom)
-        {
-          if(p->sc_type == C46)
-            have_seeprom = read_seeprom(p, 4*(p->flags & (AHC_CHNLB|AHC_CHNLC)),
-                                        scarray, p->sc_size, C56_66);
-          else
-            have_seeprom = read_seeprom(p, 4*(p->flags & (AHC_CHNLB|AHC_CHNLC)),
-                                        scarray, p->sc_size, C46);
-        }
-      }
-      break;
-  }
-
-  if (!have_seeprom)
-  {
-    if (aic7xxx_verbose & VERBOSE_PROBE2)
-    {
-      printk("\naic7xxx: No SEEPROM available.\n");
-    }
-    p->flags |= AHC_NEWEEPROM_FMT;
-    if (aic_inb(p, SCSISEQ) == 0)
-    {
-      p->flags |= AHC_USEDEFAULTS;
-      p->flags &= ~AHC_BIOS_ENABLED;
-      p->scsi_id = p->scsi_id_b = 7;
-      *sxfrctl1 |= STPWEN;
-      if (aic7xxx_verbose & VERBOSE_PROBE2)
-      {
-        printk("aic7xxx: Using default values.\n");
-      }
-    }
-    else if (aic7xxx_verbose & VERBOSE_PROBE2)
-    {
-      printk("aic7xxx: Using leftover BIOS values.\n");
-    }
-    if ( ((p->chip & ~AHC_CHIPID_MASK) == AHC_PCI) && (*sxfrctl1 & STPWEN) )
-    {
-      p->flags |= AHC_TERM_ENB_SE_LOW | AHC_TERM_ENB_SE_HIGH;
-      sc->adapter_control &= ~CFAUTOTERM;
-      sc->adapter_control |= CFSTERM | CFWSTERM | CFLVDSTERM;
-    }
-    if (aic7xxx_extended)
-      p->flags |= (AHC_EXTEND_TRANS_A | AHC_EXTEND_TRANS_B);
-    else
-      p->flags &= ~(AHC_EXTEND_TRANS_A | AHC_EXTEND_TRANS_B);
-  }
-  else
-  {
-    if (aic7xxx_verbose & VERBOSE_PROBE2)
-    {
-      printk("done\n");
-    }
-
-    /*
-     * Note things in our flags
-     */
-    p->flags |= AHC_SEEPROM_FOUND;
-
-    /*
-     * Update the settings in sxfrctl1 to match the termination settings.
-     */
-    *sxfrctl1 = 0;
-
-    /*
-     * Get our SCSI ID from the SEEPROM setting...
-     */
-    p->scsi_id = (sc->brtime_id & CFSCSIID);
-
-    /*
-     * First process the settings that are different between the VLB
-     * and PCI adapter seeproms.
-     */
-    if ((p->chip & AHC_CHIPID_MASK) == AHC_AIC7770)
-    {
-      /* VLB adapter seeproms */
-      if (sc->bios_control & CF284XEXTEND)
-        p->flags |= AHC_EXTEND_TRANS_A;
-
-      if (sc->adapter_control & CF284XSTERM)
-      {
-        *sxfrctl1 |= STPWEN;
-        p->flags |= AHC_TERM_ENB_SE_LOW | AHC_TERM_ENB_SE_HIGH;
-      }
-    }
-    else
-    {
-      /* PCI adapter seeproms */
-      if (sc->bios_control & CFEXTEND)
-        p->flags |= AHC_EXTEND_TRANS_A;
-      if (sc->bios_control & CFBIOSEN)
-        p->flags |= AHC_BIOS_ENABLED;
-      else
-        p->flags &= ~AHC_BIOS_ENABLED;
-
-      if (sc->adapter_control & CFSTERM)
-      {
-        *sxfrctl1 |= STPWEN;
-        p->flags |= AHC_TERM_ENB_SE_LOW | AHC_TERM_ENB_SE_HIGH;
-      }
-    }
-    p->sc = *sc;
-  }
-
-  p->discenable = 0;
-    
-  /*
-   * Limit to 16 targets just in case.  The 2842 for one is known to
-   * blow the max_targets setting, future cards might also.
-   */
-  max_targets = ((p->features & (AHC_TWIN | AHC_WIDE)) ? 16 : 8);
-
-  if (have_seeprom)
-  {
-    for (i = 0; i < max_targets; i++)
-    {
-      if( ((p->features & AHC_ULTRA) &&
-          !(sc->adapter_control & CFULTRAEN) &&
-           (sc->device_flags[i] & CFSYNCHISULTRA)) ||
-          (sc->device_flags[i] & CFNEWULTRAFORMAT) )
-      {
-        p->flags |= AHC_NEWEEPROM_FMT;
-        break;
-      }
-    }
-  }
-
-  for (i = 0; i < max_targets; i++)
-  {
-    mask = (0x01 << i);
-    if (!have_seeprom)
-    {
-      if (aic_inb(p, SCSISEQ) != 0)
-      {
-        /*
-         * OK...the BIOS set things up and left behind the settings we need.
-         * Just make our sc->device_flags[i] entry match what the card has
-         * set for this device.
-         */
-        p->discenable = 
-          ~(aic_inb(p, DISC_DSB) | (aic_inb(p, DISC_DSB + 1) << 8) );
-        p->ultraenb =
-          (aic_inb(p, ULTRA_ENB) | (aic_inb(p, ULTRA_ENB + 1) << 8) );
-        sc->device_flags[i] = (p->discenable & mask) ? CFDISC : 0;
-        if (aic_inb(p, TARG_SCSIRATE + i) & WIDEXFER)
-          sc->device_flags[i] |= CFWIDEB;
-        if (p->features & AHC_ULTRA2)
-        {
-          if (aic_inb(p, TARG_OFFSET + i))
-          {
-            sc->device_flags[i] |= CFSYNCH;
-            sc->device_flags[i] |= (aic_inb(p, TARG_SCSIRATE + i) & 0x07);
-            if ( (aic_inb(p, TARG_SCSIRATE + i) & 0x18) == 0x18 )
-              sc->device_flags[i] |= CFSYNCHISULTRA;
-          }
-        }
-        else
-        {
-          if (aic_inb(p, TARG_SCSIRATE + i) & ~WIDEXFER)
-          {
-            sc->device_flags[i] |= CFSYNCH;
-            if (p->features & AHC_ULTRA)
-              sc->device_flags[i] |= ((p->ultraenb & mask) ?
-                                      CFSYNCHISULTRA : 0);
-          }
-        }
-      }
-      else
-      {
-        /*
-         * Assume the BIOS has NOT been run on this card and nothing between
-         * the card and the devices is configured yet.
-         */
-        sc->device_flags[i] = CFDISC;
-        if (p->features & AHC_WIDE)
-          sc->device_flags[i] |= CFWIDEB;
-        if (p->features & AHC_ULTRA3)
-          sc->device_flags[i] |= 2;
-        else if (p->features & AHC_ULTRA2)
-          sc->device_flags[i] |= 3;
-        else if (p->features & AHC_ULTRA)
-          sc->device_flags[i] |= CFSYNCHISULTRA;
-        sc->device_flags[i] |= CFSYNCH;
-        aic_outb(p, 0, TARG_SCSIRATE + i);
-        if (p->features & AHC_ULTRA2)
-          aic_outb(p, 0, TARG_OFFSET + i);
-      }
-    }
-    if (sc->device_flags[i] & CFDISC)
-    {
-      p->discenable |= mask;
-    }
-    if (p->flags & AHC_NEWEEPROM_FMT)
-    {
-      if ( !(p->features & AHC_ULTRA2) )
-      {
-        /*
-         * I know of two different Ultra BIOSes that do this differently.
-         * One on the Gigabyte 6BXU mb that wants flags[i] & CFXFER to
-         * be == to 0x03 and SYNCHISULTRA to be true to mean 40MByte/s
-         * while on the IBM Netfinity 5000 they want the same thing
-         * to be something else, while flags[i] & CFXFER == 0x03 and
-         * SYNCHISULTRA false should be 40MByte/s.  So, we set both to
-         * 40MByte/s and the lower speeds be damned.  People will have
-         * to select around the conversely mapped lower speeds in order
-         * to select lower speeds on these boards.
-         */
-        if ( (sc->device_flags[i] & CFNEWULTRAFORMAT) &&
-            ((sc->device_flags[i] & CFXFER) == 0x03) )
-        {
-          sc->device_flags[i] &= ~CFXFER;
-          sc->device_flags[i] |= CFSYNCHISULTRA;
-        }
-        if (sc->device_flags[i] & CFSYNCHISULTRA)
-        {
-          p->ultraenb |= mask;
-        }
-      }
-      else if ( !(sc->device_flags[i] & CFNEWULTRAFORMAT) &&
-                 (p->features & AHC_ULTRA2) &&
-                 (sc->device_flags[i] & CFSYNCHISULTRA) )
-      {
-        p->ultraenb |= mask;
-      }
-    }
-    else if (sc->adapter_control & CFULTRAEN)
-    {
-      p->ultraenb |= mask;
-    }
-    if ( (sc->device_flags[i] & CFSYNCH) == 0)
-    {
-      sc->device_flags[i] &= ~CFXFER;
-      p->ultraenb &= ~mask;
-      p->transinfo[i].user_offset = 0;
-      p->transinfo[i].user_period = 0;
-      p->transinfo[i].user_options = 0;
-      p->transinfo[i].cur_offset = 0;
-      p->transinfo[i].cur_period = 0;
-      p->transinfo[i].cur_options = 0;
-      p->needsdtr_copy &= ~mask;
-    }
-    else
-    {
-      if (p->features & AHC_ULTRA3)
-      {
-        p->transinfo[i].user_offset = MAX_OFFSET_ULTRA2;
-        p->transinfo[i].cur_offset = aic_inb(p, TARG_OFFSET + i);
-        if( (sc->device_flags[i] & CFXFER) < 0x03 )
-        {
-          scsirate = (sc->device_flags[i] & CFXFER);
-          p->transinfo[i].user_options = MSG_EXT_PPR_OPTION_DT_CRC;
-          if( (aic_inb(p, TARG_SCSIRATE + i) & CFXFER) < 0x03 )
-          {
-            p->transinfo[i].cur_options = 
-              ((aic_inb(p, TARG_SCSIRATE + i) & 0x40) ?
-                 MSG_EXT_PPR_OPTION_DT_CRC : MSG_EXT_PPR_OPTION_DT_UNITS);
-          }
-          else
-          {
-            p->transinfo[i].cur_options = 0;
-          }
-        }
-        else
-        {
-          scsirate = (sc->device_flags[i] & CFXFER) |
-                     ((p->ultraenb & mask) ? 0x18 : 0x10);
-          p->transinfo[i].user_options = 0;
-          p->transinfo[i].cur_options = 0;
-        }
-        p->transinfo[i].user_period = aic7xxx_find_period(p, scsirate,
-                                       AHC_SYNCRATE_ULTRA3);
-        p->transinfo[i].cur_period = aic7xxx_find_period(p,
-                                       aic_inb(p, TARG_SCSIRATE + i),
-                                       AHC_SYNCRATE_ULTRA3);
-      }
-      else if (p->features & AHC_ULTRA2)
-      {
-        p->transinfo[i].user_offset = MAX_OFFSET_ULTRA2;
-        p->transinfo[i].cur_offset = aic_inb(p, TARG_OFFSET + i);
-        scsirate = (sc->device_flags[i] & CFXFER) |
-                   ((p->ultraenb & mask) ? 0x18 : 0x10);
-        p->transinfo[i].user_options = 0;
-        p->transinfo[i].cur_options = 0;
-        p->transinfo[i].user_period = aic7xxx_find_period(p, scsirate,
-                                       AHC_SYNCRATE_ULTRA2);
-        p->transinfo[i].cur_period = aic7xxx_find_period(p,
-                                       aic_inb(p, TARG_SCSIRATE + i),
-                                       AHC_SYNCRATE_ULTRA2);
-      }
-      else
-      {
-        scsirate = (sc->device_flags[i] & CFXFER) << 4;
-        p->transinfo[i].user_options = 0;
-        p->transinfo[i].cur_options = 0;
-        p->transinfo[i].user_offset = MAX_OFFSET_8BIT;
-        if (p->features & AHC_ULTRA)
-        {
-          short ultraenb;
-          ultraenb = aic_inb(p, ULTRA_ENB) |
-            (aic_inb(p, ULTRA_ENB + 1) << 8);
-          p->transinfo[i].user_period = aic7xxx_find_period(p,
-                                          scsirate,
-                                          (p->ultraenb & mask) ?
-                                          AHC_SYNCRATE_ULTRA :
-                                          AHC_SYNCRATE_FAST);
-          p->transinfo[i].cur_period = aic7xxx_find_period(p,
-                                         aic_inb(p, TARG_SCSIRATE + i),
-                                         (ultraenb & mask) ? 
-                                         AHC_SYNCRATE_ULTRA :
-                                         AHC_SYNCRATE_FAST);
-        }
-        else
-          p->transinfo[i].user_period = aic7xxx_find_period(p,
-                                          scsirate, AHC_SYNCRATE_FAST);
-      }
-      p->needsdtr_copy |= mask;
-    }
-    if ( (sc->device_flags[i] & CFWIDEB) && (p->features & AHC_WIDE) )
-    {
-      p->transinfo[i].user_width = MSG_EXT_WDTR_BUS_16_BIT;
-      p->needwdtr_copy |= mask;
-    }
-    else
-    {
-      p->transinfo[i].user_width = MSG_EXT_WDTR_BUS_8_BIT;
-      p->needwdtr_copy &= ~mask;
-    }
-    p->transinfo[i].cur_width =
-      (aic_inb(p, TARG_SCSIRATE + i) & WIDEXFER) ?
-      MSG_EXT_WDTR_BUS_16_BIT : MSG_EXT_WDTR_BUS_8_BIT;
-  }
-  aic_outb(p, ~(p->discenable & 0xFF), DISC_DSB);
-  aic_outb(p, ~((p->discenable >> 8) & 0xFF), DISC_DSB + 1);
-  p->needppr = p->needppr_copy = p->needdv = 0;
-  p->needwdtr = p->needwdtr_copy;
-  p->needsdtr = p->needsdtr_copy;
-  p->dtr_pending = 0;
-
-  /*
-   * We set the p->ultraenb from the SEEPROM to begin with, but now we make
-   * it match what is already down in the card.  If we are doing a reset
-   * on the card then this will get put back to a default state anyway.
-   * This allows us to not have to pre-emptively negotiate when using the
-   * no_reset option.
-   */
-  if (p->features & AHC_ULTRA)
-    p->ultraenb = aic_inb(p, ULTRA_ENB) | (aic_inb(p, ULTRA_ENB + 1) << 8);
-
-  
-  scsi_conf = (p->scsi_id & HSCSIID);
-
-  if(have_seeprom)
-  {
-    p->adapter_control = sc->adapter_control;
-    p->bios_control = sc->bios_control;
-
-    switch (p->chip & AHC_CHIPID_MASK)
-    {
-      case AHC_AIC7895:
-      case AHC_AIC7896:
-      case AHC_AIC7899:
-        if (p->adapter_control & CFBPRIMARY)
-          p->flags |= AHC_CHANNEL_B_PRIMARY;
-      default:
-        break;
-    }
-
-    if (sc->adapter_control & CFSPARITY)
-      scsi_conf |= ENSPCHK;
-  }
-  else
-  {
-    scsi_conf |= ENSPCHK | RESET_SCSI;
-  }
-
-  /*
-   * Only set the SCSICONF and SCSICONF + 1 registers if we are a PCI card.
-   * The 2842 and 2742 cards already have these registers set and we don't
-   * want to muck with them since we don't set all the bits they do.
-   */
-  if ( (p->chip & ~AHC_CHIPID_MASK) == AHC_PCI )
-  {
-    /* Set the host ID */
-    aic_outb(p, scsi_conf, SCSICONF);
-    /* In case we are a wide card */
-    aic_outb(p, p->scsi_id, SCSICONF + 1);
-  }
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_detect
- *
- * Description:
- *   Try to detect and register an Adaptec 7770 or 7870 SCSI controller.
- *
- * XXX - This should really be called aic7xxx_probe().  A sequence of
- *       probe(), attach()/detach(), and init() makes more sense than
- *       one do-it-all function.  This may be useful when (and if) the
- *       mid-level SCSI code is overhauled.
- *-F*************************************************************************/
-int
-aic7xxx_detect(Scsi_Host_Template *template)
-{
-  struct aic7xxx_host *temp_p = NULL;
-  struct aic7xxx_host *current_p = NULL;
-  struct aic7xxx_host *list_p = NULL;
-  int found = 0;
-#if defined(__i386__) || defined(__alpha__)
-  ahc_flag_type flags = 0;
-  int type;
-#endif
-  unsigned char sxfrctl1;
-#if defined(__i386__) || defined(__alpha__)
-  unsigned char hcntrl, hostconf;
-  unsigned int slot, base;
-#endif
-
-#ifdef MODULE
-  /*
-   * If we are called as a module, the aic7xxx pointer may not be null
-   * and it would point to our bootup string, just like on the lilo
-   * command line.  IF not NULL, then process this config string with
-   * aic7xxx_setup
-   */
-  if(aic7xxx)
-    aic7xxx_setup(aic7xxx, NULL);
-  if(dummy_buffer[0] != 'P')
-    printk(KERN_WARNING "aic7xxx: Please read the file /usr/src/linux/drivers"
-      "/scsi/README.aic7xxx\n"
-      "aic7xxx: to see the proper way to specify options to the aic7xxx "
-      "module\n"
-      "aic7xxx: Specifically, don't use any commas when passing arguments to\n"
-      "aic7xxx: insmod or else it might trash certain memory areas.\n");
-#endif
-
-  template->proc_dir = &proc_scsi_aic7xxx;
-  template->sg_tablesize = AIC7XXX_MAX_SG;
-
-
-#ifdef CONFIG_PCI
-  /*
-   * PCI-bus probe.
-   */
-#if LINUX_VERSION_CODE > KERNEL_VERSION(2,1,92)
-  if (pci_present())
-#else
-  if (pcibios_present())
-#endif
-  {
-    struct
-    {
-      unsigned short      vendor_id;
-      unsigned short      device_id;
-      ahc_chip            chip;
-      ahc_flag_type       flags;
-      ahc_feature         features;
-      int                 board_name_index;
-      unsigned short      seeprom_size;
-      unsigned short      seeprom_type;
-    } const aic_pdevs[] = {
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_7810, AHC_NONE,
-       AHC_FNONE, AHC_FENONE,                                1,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_7850, AHC_AIC7850,
-       AHC_PAGESCBS, AHC_AIC7850_FE,                         5,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_7855, AHC_AIC7850,
-       AHC_PAGESCBS, AHC_AIC7850_FE,                         6,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_7821, AHC_AIC7860,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED,
-       AHC_AIC7860_FE,                                       7,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_3860, AHC_AIC7860,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED,
-       AHC_AIC7860_FE,                                       7,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_38602, AHC_AIC7860,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED,
-       AHC_AIC7860_FE,                                       7,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_7860, AHC_AIC7860,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED | AHC_MOTHERBOARD,
-       AHC_AIC7860_FE,                                       7,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_7861, AHC_AIC7860,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED,
-       AHC_AIC7860_FE,                                       8,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_7870, AHC_AIC7870,
-       AHC_PAGESCBS | AHC_BIOS_ENABLED | AHC_MOTHERBOARD,
-       AHC_AIC7870_FE,                                       9,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_7871, AHC_AIC7870,
-       AHC_PAGESCBS | AHC_BIOS_ENABLED, AHC_AIC7870_FE,     10,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_7872, AHC_AIC7870,
-       AHC_PAGESCBS | AHC_BIOS_ENABLED | AHC_MULTI_CHANNEL,
-       AHC_AIC7870_FE,                                      11,
-       32, C56_66 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_7873, AHC_AIC7870,
-       AHC_PAGESCBS | AHC_BIOS_ENABLED | AHC_MULTI_CHANNEL,
-       AHC_AIC7870_FE,                                      12,
-       32, C56_66 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_7874, AHC_AIC7870,
-       AHC_PAGESCBS | AHC_BIOS_ENABLED, AHC_AIC7870_FE,     13,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_7880, AHC_AIC7880,
-       AHC_PAGESCBS | AHC_BIOS_ENABLED | AHC_MOTHERBOARD,
-       AHC_AIC7880_FE,                                      14,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_7881, AHC_AIC7880,
-       AHC_PAGESCBS | AHC_BIOS_ENABLED, AHC_AIC7880_FE,     15,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_7882, AHC_AIC7880,
-       AHC_PAGESCBS | AHC_BIOS_ENABLED | AHC_MULTI_CHANNEL,
-       AHC_AIC7880_FE,                                      16,
-       32, C56_66 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_7883, AHC_AIC7880,
-       AHC_PAGESCBS | AHC_BIOS_ENABLED | AHC_MULTI_CHANNEL,
-       AHC_AIC7880_FE,                                      17,
-       32, C56_66 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_7884, AHC_AIC7880,
-       AHC_PAGESCBS | AHC_BIOS_ENABLED, AHC_AIC7880_FE,     18,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_7885, AHC_AIC7880,
-       AHC_PAGESCBS | AHC_BIOS_ENABLED, AHC_AIC7880_FE,     18,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_7886, AHC_AIC7880,
-       AHC_PAGESCBS | AHC_BIOS_ENABLED, AHC_AIC7880_FE,     18,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_7887, AHC_AIC7880,
-       AHC_PAGESCBS | AHC_BIOS_ENABLED, AHC_AIC7880_FE | AHC_NEW_AUTOTERM, 19,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_7888, AHC_AIC7880,
-       AHC_PAGESCBS | AHC_BIOS_ENABLED, AHC_AIC7880_FE,     18,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_7895, AHC_AIC7895,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED | AHC_MULTI_CHANNEL,
-       AHC_AIC7895_FE,                                      20,
-       32, C56_66 },
-      {PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_7890, AHC_AIC7890,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED,
-       AHC_AIC7890_FE,                                      21,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_7890B, AHC_AIC7890,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED,
-       AHC_AIC7890_FE,                                      21,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_2930U2, AHC_AIC7890,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED,
-       AHC_AIC7890_FE,                                      22,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_2940U2, AHC_AIC7890,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED,
-       AHC_AIC7890_FE,                                      23,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_7896, AHC_AIC7896,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED | AHC_MULTI_CHANNEL,
-       AHC_AIC7896_FE,                                      24,
-       32, C56_66 },
-      {PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_3940U2, AHC_AIC7896,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED | AHC_MULTI_CHANNEL,
-       AHC_AIC7896_FE,                                      25,
-       32, C56_66 },
-      {PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_3950U2D, AHC_AIC7896,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED | AHC_MULTI_CHANNEL,
-       AHC_AIC7896_FE,                                      26,
-       32, C56_66 },
-      {PCI_VENDOR_ID_ADAPTEC, PCI_DEVICE_ID_ADAPTEC_1480A, AHC_AIC7860,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED | AHC_NO_STPWEN,
-       AHC_AIC7860_FE,                                      27,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_7892A, AHC_AIC7892,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED,
-       AHC_AIC7892_FE,                                      28,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_7892B, AHC_AIC7892,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED,
-       AHC_AIC7892_FE,                                      28,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_7892D, AHC_AIC7892,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED,
-       AHC_AIC7892_FE,                                      28,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_7892P, AHC_AIC7892,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED,
-       AHC_AIC7892_FE,                                      28,
-       32, C46 },
-      {PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_7899A, AHC_AIC7899,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED | AHC_MULTI_CHANNEL,
-       AHC_AIC7899_FE,                                      29,
-       32, C56_66 },
-      {PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_7899B, AHC_AIC7899,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED | AHC_MULTI_CHANNEL,
-       AHC_AIC7899_FE,                                      29,
-       32, C56_66 },
-      {PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_7899D, AHC_AIC7899,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED | AHC_MULTI_CHANNEL,
-       AHC_AIC7899_FE,                                      29,
-       32, C56_66 },
-      {PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_7899P, AHC_AIC7899,
-       AHC_PAGESCBS | AHC_NEWEEPROM_FMT | AHC_BIOS_ENABLED | AHC_MULTI_CHANNEL,
-       AHC_AIC7899_FE,                                      29,
-       32, C56_66 },
-    };
-
-    unsigned short command;
-    unsigned int  devconfig, i, oldverbose;
-#if LINUX_VERSION_CODE > KERNEL_VERSION(2,1,92)
-    struct pci_dev *pdev = NULL;
-#else
-    int index;
-    unsigned int piobase, mmapbase;
-    unsigned char pci_bus, pci_devfn, pci_irq;
-#endif
-
-    for (i = 0; i < NUMBER(aic_pdevs); i++)
-    {
-#if LINUX_VERSION_CODE > KERNEL_VERSION(2,1,92)
-      pdev = NULL;
-      while ((pdev = pci_find_device(aic_pdevs[i].vendor_id,
-                                     aic_pdevs[i].device_id,
-                                     pdev)))
-#else
-      index = 0;
-      while (!(pcibios_find_device(aic_pdevs[i].vendor_id,
-                                   aic_pdevs[i].device_id,
-                                   index++, &pci_bus, &pci_devfn)) )
-#endif
-      {
-        if ( i == 0 ) /* We found one, but it's the 7810 RAID cont. */
-        {
-          if (aic7xxx_verbose & (VERBOSE_PROBE|VERBOSE_PROBE2))
-          {
-            printk(KERN_INFO "aic7xxx: The 7810 RAID controller is not "
-              "supported by\n");
-            printk(KERN_INFO "         this driver, we are ignoring it.\n");
-          }
-        }
-        else if ( (temp_p = kmalloc(sizeof(struct aic7xxx_host),
-                                    GFP_ATOMIC)) != NULL )
-        {
-          memset(temp_p, 0, sizeof(struct aic7xxx_host));
-          temp_p->chip = aic_pdevs[i].chip | AHC_PCI;
-          temp_p->flags = aic_pdevs[i].flags;
-          temp_p->features = aic_pdevs[i].features;
-          temp_p->board_name_index = aic_pdevs[i].board_name_index;
-          temp_p->sc_size = aic_pdevs[i].seeprom_size;
-          temp_p->sc_type = aic_pdevs[i].seeprom_type;
-
-          /*
-           * Read sundry information from PCI BIOS.
-           */
-#if LINUX_VERSION_CODE > KERNEL_VERSION(2,1,92)
-          temp_p->irq = pdev->irq;
-          temp_p->pdev = pdev;
-          temp_p->pci_bus = pdev->bus->number;
-          temp_p->pci_device_fn = pdev->devfn;
-          temp_p->base = pdev->base_address[0];
-          temp_p->mbase = pdev->base_address[1];
-          temp_p->base &= PCI_BASE_ADDRESS_IO_MASK;
-          temp_p->mbase &= PCI_BASE_ADDRESS_MEM_MASK;
-          current_p = list_p;
-          while(current_p && temp_p)
-          {
-            if ( ((current_p->pci_bus == temp_p->pci_bus) &&
-                  (current_p->pci_device_fn == temp_p->pci_device_fn)) ||
-                 (temp_p->base && (current_p->base == temp_p->base)) ||
-                 (temp_p->mbase && (current_p->mbase == temp_p->mbase)) )
-            {
-              /* duplicate PCI entry, skip it */
-              kfree(temp_p);
-              temp_p = NULL;
-            }
-            current_p = current_p->next;
-          }
-          if ( temp_p == NULL )
-            continue;
-          if (aic7xxx_verbose & VERBOSE_PROBE2)
-            printk("aic7xxx: <%s> at PCI %d/%d/%d\n", 
-              board_names[aic_pdevs[i].board_name_index],
-              temp_p->pci_bus,
-              PCI_SLOT(temp_p->pci_device_fn),
-              PCI_FUNC(temp_p->pci_device_fn));
-          pci_read_config_word(pdev, PCI_COMMAND, &command);
-          if (aic7xxx_verbose & VERBOSE_PROBE2)
-          {
-            printk("aic7xxx: Initial PCI_COMMAND value was 0x%x\n",
-              (int)command);
-          }
-#ifdef AIC7XXX_STRICT_PCI_SETUP
-          command |= PCI_COMMAND_SERR | PCI_COMMAND_PARITY |
-            PCI_COMMAND_MASTER | PCI_COMMAND_MEMORY | PCI_COMMAND_IO;
-#else
-          command |= PCI_COMMAND_MASTER | PCI_COMMAND_MEMORY | PCI_COMMAND_IO;
-#endif
-          command &= ~PCI_COMMAND_INVALIDATE;
-          if (aic7xxx_pci_parity == 0)
-            command &= ~(PCI_COMMAND_SERR | PCI_COMMAND_PARITY);
-          pci_write_config_word(pdev, PCI_COMMAND, command);
-#ifdef AIC7XXX_STRICT_PCI_SETUP
-          pci_read_config_dword(pdev, DEVCONFIG, &devconfig);
-          if (aic7xxx_verbose & VERBOSE_PROBE2)
-          {
-            printk("aic7xxx: Initial DEVCONFIG value was 0x%x\n", devconfig);
-          }
-          devconfig |= 0x80000040;
-          pci_write_config_dword(pdev, DEVCONFIG, devconfig);
-#endif /* AIC7XXX_STRICT_PCI_SETUP */
-#else  /* LINUX_VERSION_CODE > KERNEL_VERSION(2,1,92) */
-          temp_p->pci_bus = pci_bus;
-          temp_p->pci_device_fn = pci_devfn;
-          pcibios_read_config_byte(pci_bus, pci_devfn, PCI_INTERRUPT_LINE,
-            &pci_irq);
-          temp_p->irq = pci_irq;
-          pcibios_read_config_dword(pci_bus, pci_devfn, PCI_BASE_ADDRESS_0,
-            &piobase);
-          temp_p->base = piobase;
-          pcibios_read_config_dword(pci_bus, pci_devfn, PCI_BASE_ADDRESS_1,
-            &mmapbase);
-          temp_p->mbase = mmapbase;
-          temp_p->base &= PCI_BASE_ADDRESS_IO_MASK;
-          temp_p->mbase &= PCI_BASE_ADDRESS_MEM_MASK;
-          current_p = list_p;
-          while(current_p)
-          {
-            if ( ((current_p->pci_bus == temp_p->pci_bus) &&
-                  (current_p->pci_device_fn == temp_p->pci_device_fn)) ||
-                 (temp_p->base && (current_p->base == temp_p->base)) ||
-                 (temp_p->mbase && (current_p->mbase == temp_p->mbase)) )
-            {
-              /* duplicate PCI entry, skip it */
-              kfree(temp_p);
-              temp_p = NULL;
-            }
-            current_p = current_p->next;
-          }
-          if ( temp_p == NULL )
-            continue;
-          if (aic7xxx_verbose & VERBOSE_PROBE2)
-            printk("aic7xxx: <%s> at PCI %d/%d/%d\n", 
-              board_names[aic_pdevs[i].board_name_index],
-              temp_p->pci_bus,
-              PCI_SLOT(temp_p->pci_device_fn),
-              PCI_FUNC(temp_p->pci_device_fn));
-          pcibios_read_config_word(pci_bus, pci_devfn, PCI_COMMAND, &command);
-          if (aic7xxx_verbose & VERBOSE_PROBE2)
-          {
-            printk("aic7xxx: Initial PCI_COMMAND value was 0x%x\n",
-              (int)command);
-          }
-#ifdef AIC7XXX_STRICT_PCI_SETUP
-          command |= PCI_COMMAND_SERR | PCI_COMMAND_PARITY |
-            PCI_COMMAND_MASTER | PCI_COMMAND_MEMORY | PCI_COMMAND_IO;
-#else
-          command |= PCI_COMMAND_MASTER | PCI_COMMAND_MEMORY | PCI_COMMAND_IO;
-#endif
-          command &= ~PCI_COMMAND_INVALIDATE;
-          if (aic7xxx_pci_parity == 0)
-            command &= ~(PCI_COMMAND_SERR | PCI_COMMAND_PARITY);
-          pcibios_write_config_word(pci_bus, pci_devfn, PCI_COMMAND, command);
-#ifdef AIC7XXX_STRICT_PCI_SETUP
-          pcibios_read_config_dword(pci_bus, pci_devfn, DEVCONFIG, &devconfig);
-          if (aic7xxx_verbose & VERBOSE_PROBE2)
-          {
-            printk("aic7xxx: Initial DEVCONFIG value was 0x%x\n", devconfig);
-          }
-          devconfig |= 0x80000040;
-          pcibios_write_config_dword(pci_bus, pci_devfn, DEVCONFIG, devconfig);
-#endif /* AIC7XXX_STRICT_PCI_SETUP */
-#endif /* LINUIX_VERSION_CODE > KERNEL_VERSION(2,1,92) */
-
-          if(temp_p->base && check_region(temp_p->base, MAXREG - MINREG))
-          {
-            printk("aic7xxx: <%s> at PCI %d/%d/%d\n", 
-              board_names[aic_pdevs[i].board_name_index],
-              temp_p->pci_bus,
-              PCI_SLOT(temp_p->pci_device_fn),
-              PCI_FUNC(temp_p->pci_device_fn));
-            printk("aic7xxx: I/O ports already in use, ignoring.\n");
-            kfree(temp_p);
-            temp_p = NULL;
-            continue;
-          }
-
-          temp_p->unpause = INTEN;
-          temp_p->pause = temp_p->unpause | PAUSE;
-          if ( ((temp_p->base == 0) &&
-                (temp_p->mbase == 0)) ||
-               (temp_p->irq == 0) )
-          {
-            printk("aic7xxx: <%s> at PCI %d/%d/%d\n", 
-              board_names[aic_pdevs[i].board_name_index],
-              temp_p->pci_bus,
-              PCI_SLOT(temp_p->pci_device_fn),
-              PCI_FUNC(temp_p->pci_device_fn));
-            printk("aic7xxx: Controller disabled by BIOS, ignoring.\n");
-            kfree(temp_p);
-            temp_p = NULL;
-            continue;
-          }
-
-#ifdef MMAPIO
-          if ( !(temp_p->base) || !(temp_p->flags & AHC_MULTI_CHANNEL) ||
-               ((temp_p->chip != (AHC_AIC7870 | AHC_PCI)) &&
-                (temp_p->chip != (AHC_AIC7880 | AHC_PCI))) )
-          {
-            unsigned long page_offset, base;
-
-            base = temp_p->mbase & PAGE_MASK;
-            page_offset = temp_p->mbase - base;
-#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,1,0)
-            temp_p->maddr = ioremap_nocache(base, page_offset + 256);
-#else
-            temp_p->maddr = vremap(base, page_offset + 256);
-#endif
-            if(temp_p->maddr)
-            {
-              temp_p->maddr += page_offset;
-              /*
-               * We need to check the I/O with the MMAPed address.  Some machines
-               * simply fail to work with MMAPed I/O and certain controllers.
-               */
-              if(aic_inb(temp_p, HCNTRL) == 0xff)
-              {
-                /*
-                 * OK.....we failed our test....go back to programmed I/O
-                 */
-                printk(KERN_INFO "aic7xxx: <%s> at PCI %d/%d/%d\n", 
-                  board_names[aic_pdevs[i].board_name_index],
-                  temp_p->pci_bus,
-                  PCI_SLOT(temp_p->pci_device_fn),
-                  PCI_FUNC(temp_p->pci_device_fn));
-                printk(KERN_INFO "aic7xxx: MMAPed I/O failed, reverting to "
-                                 "Programmed I/O.\n");
-#if LINUX_VERSION_CODE > KERNEL_VERSION(2,1,0)
-                iounmap((void *) (((unsigned long) temp_p->maddr) & PAGE_MASK));
-#else
-                vfree((void *) (((unsigned long) temp_p->maddr) & PAGE_MASK));
-#endif
-                temp_p->maddr = 0;
-                if(temp_p->base == 0)
-                {
-                  printk("aic7xxx: <%s> at PCI %d/%d/%d\n", 
-                    board_names[aic_pdevs[i].board_name_index],
-                    temp_p->pci_bus,
-                    PCI_SLOT(temp_p->pci_device_fn),
-                    PCI_FUNC(temp_p->pci_device_fn));
-                  printk("aic7xxx: Controller disabled by BIOS, ignoring.\n");
-                  kfree(temp_p);
-                  temp_p = NULL;
-                  continue;
-                }
-              }
-            }
-          }
-#endif
-
-          /*
-           * Lock out other contenders for our i/o space.
-           */
-          if(temp_p->base)
-            request_region(temp_p->base, MAXREG - MINREG, "aic7xxx");
-
-          /*
-           * We HAVE to make sure the first pause_sequencer() and all other
-           * subsequent I/O that isn't PCI config space I/O takes place
-           * after the MMAPed I/O region is configured and tested.  The
-           * problem is the PowerPC architecture that doesn't support
-           * programmed I/O at all, so we have to have the MMAP I/O set up
-           * for this pause to even work on those machines.
-           */
-          pause_sequencer(temp_p);
-
-          /*
-           * Clear out any pending PCI error status messages.  Also set
-           * verbose to 0 so that we don't emit strange PCI error messages
-           * while cleaning out the current status bits.
-           */
-          oldverbose = aic7xxx_verbose;
-          aic7xxx_verbose = 0;
-          aic7xxx_pci_intr(temp_p);
-          aic7xxx_verbose = oldverbose;
-
-          temp_p->bios_address = 0;
-
-          /*
-           * Remember how the card was setup in case there is no seeprom.
-           */
-          if (temp_p->features & AHC_ULTRA2)
-            temp_p->scsi_id = aic_inb(temp_p, SCSIID_ULTRA2) & OID;
-          else
-            temp_p->scsi_id = aic_inb(temp_p, SCSIID) & OID;
-          /*
-           * Get current termination setting
-           */
-          sxfrctl1 = aic_inb(temp_p, SXFRCTL1);
-
-          if (aic7xxx_chip_reset(temp_p) == -1)
-          {
-            release_region(temp_p->base, MAXREG - MINREG);
-            kfree(temp_p);
-            temp_p = NULL;
-            continue;
-          }
-          /*
-           * Very quickly put the term setting back into the register since
-           * the chip reset may cause odd things to happen.  This is to keep
-           * LVD busses with lots of drives from draining the power out of
-           * the diffsense line before we get around to running the
-           * configure_termination() function.  Also restore the STPWLEVEL
-           * bit of DEVCONFIG
-           */
-          aic_outb(temp_p, sxfrctl1, SXFRCTL1);
-#if LINUX_VERSION_CODE > KERNEL_VERSION(2,1,92)
-          pcibios_write_config_dword(temp_p->pci_bus, temp_p->pci_device_fn,
-            DEVCONFIG, devconfig);
-#else
-          pci_write_config_dword(temp_p->pdev, DEVCONFIG, devconfig);
-#endif
-          sxfrctl1 &= STPWEN;
-
-          /*
-           * We need to set the CHNL? assignments before loading the SEEPROM
-           * The 3940 and 3985 cards (original stuff, not any of the later
-           * stuff) are 7870 and 7880 class chips.  The Ultra2 stuff falls
-           * under 7896 and 7897.  The 7895 is in a class by itself :)
-           */
-          switch (temp_p->chip & AHC_CHIPID_MASK)
-          {
-            case AHC_AIC7870: /* 3840 / 3985 */
-            case AHC_AIC7880: /* 3840 UW / 3985 UW */
-              if(temp_p->flags & AHC_MULTI_CHANNEL)
-              {
-                switch(PCI_SLOT(temp_p->pci_device_fn))
-                {
-                  case 5:
-                    temp_p->flags |= AHC_CHNLB;
-                    break;
-                  case 8:
-                    temp_p->flags |= AHC_CHNLB;
-                    break;
-                  case 12:
-                    temp_p->flags |= AHC_CHNLC;
-                    break;
-                  default:
-                    break;
-                }
-              }
-              break;
-
-            case AHC_AIC7895: /* 7895 */
-            case AHC_AIC7896: /* 7896/7 */
-            case AHC_AIC7899: /* 7899 */
-#if LINUX_VERSION_CODE > KERNEL_VERSION(2,1,92)
-              if (PCI_FUNC(temp_p->pdev->devfn) != 0)
-              {
-                temp_p->flags |= AHC_CHNLB;
-              }
-              /*
-               * The 7895 is the only chipset that sets the SCBSIZE32 param
-               * in the DEVCONFIG register.  The Ultra2 chipsets use
-               * the DSCOMMAND0 register instead.
-               */
-              if ((temp_p->chip & AHC_CHIPID_MASK) == AHC_AIC7895)
-              {
-                pci_read_config_dword(pdev, DEVCONFIG, &devconfig);
-                devconfig |= SCBSIZE32;
-                pci_write_config_dword(pdev, DEVCONFIG, devconfig);
-              }
-#else
-              if (PCI_FUNC(temp_p->pci_device_fn) != 0)
-              {
-                temp_p->flags |= AHC_CHNLB;
-              }
-              /*
-               * The 7895 is the only chipset that sets the SCBSIZE32 param
-               * in the DEVCONFIG register.  The Ultra2 chipsets use
-               * the DSCOMMAND0 register instead.
-               */
-              if ((temp_p->chip & AHC_CHIPID_MASK) == AHC_AIC7895)
-              {
-                pcibios_read_config_dword(pci_bus, pci_devfn, DEVCONFIG,
-                  &devconfig);
-                devconfig |= SCBSIZE32;
-                pcibios_write_config_dword(pci_bus, pci_devfn, DEVCONFIG,
-                  devconfig);
-              }
-#endif
-              break;
-            default:
-              break;
-          }
-
-          /*
-           * Loading of the SEEPROM needs to come after we've set the flags
-           * to indicate possible CHNLB and CHNLC assigments.  Otherwise,
-           * on 394x and 398x cards we'll end up reading the wrong settings
-           * for channels B and C
-           */
-          switch (temp_p->chip & AHC_CHIPID_MASK)
-          {
-            case AHC_AIC7892:
-            case AHC_AIC7899:
-              aic_outb(temp_p, 0, SCAMCTL);
-              /*
-               * Switch to the alt mode of the chip...
-               */
-              aic_outb(temp_p, aic_inb(temp_p, SFUNCT) | ALT_MODE, SFUNCT);
-              /*
-               * Set our options...the last two items set our CRC after x byte
-               * count in target mode...
-               */
-              aic_outb(temp_p, AUTO_MSGOUT_DE | DIS_MSGIN_DUALEDGE, OPTIONMODE);
-              aic_outb(temp_p, 0x00, 0x0b);
-              aic_outb(temp_p, 0x10, 0x0a);
-              /*
-               * switch back to normal mode...
-               */
-              aic_outb(temp_p, aic_inb(temp_p, SFUNCT) & ~ALT_MODE, SFUNCT);
-              aic_outb(temp_p, CRCVALCHKEN | CRCENDCHKEN | CRCREQCHKEN |
-                               TARGCRCENDEN | TARGCRCCNTEN,
-                       CRCCONTROL1);
-              aic_outb(temp_p, ((aic_inb(temp_p, DSCOMMAND0) | USCBSIZE32 |
-                                 MPARCKEN | CIOPARCKEN | CACHETHEN) & 
-                               ~DPARCKEN), DSCOMMAND0);
-              aic7xxx_load_seeprom(temp_p, &sxfrctl1);
-              break;
-            case AHC_AIC7890:
-            case AHC_AIC7896:
-              aic_outb(temp_p, 0, SCAMCTL);
-              aic_outb(temp_p, (aic_inb(temp_p, DSCOMMAND0) |
-                                CACHETHEN | MPARCKEN | USCBSIZE32 |
-                                CIOPARCKEN) & ~DPARCKEN, DSCOMMAND0);
-              aic7xxx_load_seeprom(temp_p, &sxfrctl1);
-              break;
-            case AHC_AIC7850:
-            case AHC_AIC7860:
-              /*
-               * Set the DSCOMMAND0 register on these cards different from
-               * on the 789x cards.  Also, read the SEEPROM as well.
-               */
-              aic_outb(temp_p, (aic_inb(temp_p, DSCOMMAND0) |
-                                CACHETHEN | MPARCKEN) & ~DPARCKEN,
-                       DSCOMMAND0);
-              /* FALLTHROUGH */
-            default:
-              aic7xxx_load_seeprom(temp_p, &sxfrctl1);
-              break;
-            case AHC_AIC7880:
-              /*
-               * Check the rev of the chipset before we change DSCOMMAND0
-               */
-#if LINUX_VERSION_CODE > KERNEL_VERSION(2,1,92)
-              pci_read_config_dword(pdev, DEVCONFIG, &devconfig);
-#else
-              pcibios_read_config_dword(pci_bus, pci_devfn, DEVCONFIG,
-                                        &devconfig);
-#endif
-              if ((devconfig & 0xff) >= 1)
-              {
-                aic_outb(temp_p, (aic_inb(temp_p, DSCOMMAND0) |
-                                  CACHETHEN | MPARCKEN) & ~DPARCKEN,
-                         DSCOMMAND0);
-              }
-              aic7xxx_load_seeprom(temp_p, &sxfrctl1);
-              break;
-          }
-          
-
-          /*
-           * and then we need another switch based on the type in order to
-           * make sure the channel B primary flag is set properly on 7895
-           * controllers....Arrrgggghhh!!!  We also have to catch the fact
-           * that when you disable the BIOS on the 7895 on the Intel DK440LX
-           * motherboard, and possibly others, it only sets the BIOS disabled
-           * bit on the A channel...I think I'm starting to lean towards
-           * going postal....
-           */
-          switch(temp_p->chip & AHC_CHIPID_MASK)
-          {
-            case AHC_AIC7895:
-            case AHC_AIC7896:
-            case AHC_AIC7899:
-              current_p = list_p;
-              while(current_p != NULL)
-              {
-                if ( (current_p->pci_bus == temp_p->pci_bus) &&
-                     (PCI_SLOT(current_p->pci_device_fn) ==
-                      PCI_SLOT(temp_p->pci_device_fn)) )
-                {
-                  if ( PCI_FUNC(current_p->pci_device_fn) == 0 )
-                  {
-                    temp_p->flags |= 
-                      (current_p->flags & AHC_CHANNEL_B_PRIMARY);
-                    temp_p->flags &= ~(AHC_BIOS_ENABLED|AHC_USEDEFAULTS);
-                    temp_p->flags |=
-                      (current_p->flags & (AHC_BIOS_ENABLED|AHC_USEDEFAULTS));
-                  }
-                  else
-                  {
-                    current_p->flags |=
-                      (temp_p->flags & AHC_CHANNEL_B_PRIMARY);
-                    current_p->flags &= ~(AHC_BIOS_ENABLED|AHC_USEDEFAULTS);
-                    current_p->flags |=
-                      (temp_p->flags & (AHC_BIOS_ENABLED|AHC_USEDEFAULTS));
-                  }
-                }
-                current_p = current_p->next;
-              }
-              break;
-            default:
-              break;
-          }
-
-          /*
-           * We only support external SCB RAM on the 7895/6/7 chipsets.
-           * We could support it on the 7890/1 easy enough, but I don't
-           * know of any 7890/1 based cards that have it.  I do know
-           * of 7895/6/7 cards that have it and they work properly.
-           */
-          switch(temp_p->chip & AHC_CHIPID_MASK)
-          {
-            default:
-              break;
-            case AHC_AIC7895:
-            case AHC_AIC7896:
-            case AHC_AIC7899:
-#if LINUX_VERSION_CODE > KERNEL_VERSION(2,1,92)
-              pci_read_config_dword(pdev, DEVCONFIG, &devconfig);
-#else
-              pcibios_read_config_dword(pci_bus, pci_devfn, DEVCONFIG,
-                                        &devconfig);
-#endif
-              if (temp_p->features & AHC_ULTRA2)
-              {
-                if ( (aic_inb(temp_p, DSCOMMAND0) & RAMPSM_ULTRA2) &&
-                     (aic7xxx_scbram) )
-                {
-                  aic_outb(temp_p,
-                           aic_inb(temp_p, DSCOMMAND0) & ~SCBRAMSEL_ULTRA2,
-                           DSCOMMAND0);
-                  temp_p->flags |= AHC_EXTERNAL_SRAM;
-                  devconfig |= EXTSCBPEN;
-                }
-                else if (aic_inb(temp_p, DSCOMMAND0) & RAMPSM_ULTRA2)
-                {
-                  printk(KERN_INFO "aic7xxx: <%s> at PCI %d/%d/%d\n", 
-                    board_names[aic_pdevs[i].board_name_index],
-                    temp_p->pci_bus,
-                    PCI_SLOT(temp_p->pci_device_fn),
-                    PCI_FUNC(temp_p->pci_device_fn));
-                  printk("aic7xxx: external SCB RAM detected, "
-                         "but not enabled\n");
-                }
-              }
-              else
-              {
-                if ((devconfig & RAMPSM) && (aic7xxx_scbram))
-                {
-                  devconfig &= ~SCBRAMSEL;
-                  devconfig |= EXTSCBPEN;
-                  temp_p->flags |= AHC_EXTERNAL_SRAM;
-                }
-                else if (devconfig & RAMPSM)
-                {
-                  printk(KERN_INFO "aic7xxx: <%s> at PCI %d/%d/%d\n", 
-                    board_names[aic_pdevs[i].board_name_index],
-                    temp_p->pci_bus,
-                    PCI_SLOT(temp_p->pci_device_fn),
-                    PCI_FUNC(temp_p->pci_device_fn));
-                  printk("aic7xxx: external SCB RAM detected, "
-                         "but not enabled\n");
-                }
-              }
-#if LINUX_VERSION_CODE > KERNEL_VERSION(2,1,92)
-              pci_write_config_dword(pdev, DEVCONFIG, devconfig);
-#else
-              pcibios_write_config_dword(pci_bus, pci_devfn, DEVCONFIG,
-                                         devconfig);
-#endif
-              if ( (temp_p->flags & AHC_EXTERNAL_SRAM) &&
-                   (temp_p->flags & AHC_CHNLB) )
-                aic_outb(temp_p, 1, CCSCBBADDR);
-              break;
-          }
-
-          /*
-           * Take the LED out of diagnostic mode
-           */
-          aic_outb(temp_p, 
-            (aic_inb(temp_p, SBLKCTL) & ~(DIAGLEDEN | DIAGLEDON)),
-            SBLKCTL);
-
-          /*
-           * We don't know where this is set in the SEEPROM or by the
-           * BIOS, so we default to 100%.  On Ultra2 controllers, use 75%
-           * instead.
-           */
-          if (temp_p->features & AHC_ULTRA2)
-          {
-            aic_outb(temp_p, RD_DFTHRSH_MAX | WR_DFTHRSH_MAX, DFF_THRSH);
-          }
-          else
-          {
-            aic_outb(temp_p, DFTHRSH_100, DSPCISTATUS);
-          }
-
-          if ( list_p == NULL )
-          {
-            list_p = current_p = temp_p;
-          }
-          else
-          {
-            current_p = list_p;
-            while(current_p->next != NULL)
-              current_p = current_p->next;
-            current_p->next = temp_p;
-          }
-          temp_p->next = NULL;
-          found++;
-        }  /* Found an Adaptec PCI device. */
-        else /* Well, we found one, but we couldn't get any memory */
-        {
-          printk("aic7xxx: Found <%s>\n", 
-            board_names[aic_pdevs[i].board_name_index]);
-          printk(KERN_INFO "aic7xxx: Unable to allocate device memory, "
-            "skipping.\n");
-        }
-      } /* while(pdev=....) */
-    } /* for PCI_DEVICES */
-  } /* PCI BIOS present */
-#endif CONFIG_PCI
-
-#if defined(__i386__) || defined(__alpha__)
-  /*
-   * EISA/VL-bus card signature probe.
-   */
-  slot = MINSLOT;
-  while ( (slot <= MAXSLOT) && 
-         !(aic7xxx_no_probe) )
-  {
-    base = SLOTBASE(slot) + MINREG;
-
-    if (check_region(base, MAXREG - MINREG))
-    {
-      /*
-       * Some other driver has staked a
-       * claim to this i/o region already.
-       */
-      slot++;
-      continue; /* back to the beginning of the for loop */
-    }
-    flags = 0;
-    type = aic7xxx_probe(slot, base + AHC_HID0, &flags);
-    if (type == -1)
-    {
-      slot++;
-      continue;
-    }
-    temp_p = kmalloc(sizeof(struct aic7xxx_host), GFP_ATOMIC);
-    if (temp_p == NULL)
-    {
-      printk(KERN_WARNING "aic7xxx: Unable to allocate device space.\n");
-      slot++;
-      continue; /* back to the beginning of the while loop */
-    }
-    /*
-     * Lock out other contenders for our i/o space.
-     */
-    request_region(base, MAXREG - MINREG, "aic7xxx");
-
-    /*
-     * Pause the card preserving the IRQ type.  Allow the operator
-     * to override the IRQ trigger.
-     */
-    if (aic7xxx_irq_trigger == 1)
-      hcntrl = IRQMS;  /* Level */
-    else if (aic7xxx_irq_trigger == 0)
-      hcntrl = 0;  /* Edge */
-    else
-      hcntrl = inb(base + HCNTRL) & IRQMS;  /* Default */
-    memset(temp_p, 0, sizeof(struct aic7xxx_host));
-    temp_p->unpause = hcntrl | INTEN;
-    temp_p->pause = hcntrl | PAUSE | INTEN;
-    temp_p->base = base;
-    temp_p->mbase = 0;
-    temp_p->maddr = 0;
-    temp_p->pci_bus = 0;
-    temp_p->pci_device_fn = slot;
-    aic_outb(temp_p, hcntrl | PAUSE, HCNTRL);
-    while( (aic_inb(temp_p, HCNTRL) & PAUSE) == 0 ) ;
-    if (aic7xxx_chip_reset(temp_p) == -1)
-      temp_p->irq = 0;
-    else
-      temp_p->irq = aic_inb(temp_p, INTDEF) & 0x0F;
-    temp_p->flags |= AHC_PAGESCBS;
-
-    switch (temp_p->irq)
-    {
-      case 9:
-      case 10:
-      case 11:
-      case 12:
-      case 14:
-      case 15:
-        break;
-
-      default:
-        printk(KERN_WARNING "aic7xxx: Host adapter uses unsupported IRQ "
-          "level %d, ignoring.\n", temp_p->irq);
-        kfree(temp_p);
-        release_region(base, MAXREG - MINREG);
-        slot++;
-        continue; /* back to the beginning of the while loop */
-    }
-
-    /*
-     * We are commited now, everything has been checked and this card
-     * has been found, now we just set it up
-     */
-
-    /*
-     * Insert our new struct into the list at the end
-     */
-    if (list_p == NULL)
-    {
-      list_p = current_p = temp_p;
-    }
-    else
-    {
-      current_p = list_p;
-      while (current_p->next != NULL)
-        current_p = current_p->next;
-      current_p->next = temp_p;
-    }
-
-    switch (type)
-    {
-      case 0:
-        temp_p->board_name_index = 2;
-        if (aic7xxx_verbose & VERBOSE_PROBE2)
-          printk("aic7xxx: <%s> at EISA %d\n",
-               board_names[2], slot);
-        /* FALLTHROUGH */
-      case 1:
-      {
-        temp_p->chip = AHC_AIC7770 | AHC_EISA;
-        temp_p->features |= AHC_AIC7770_FE;
-        temp_p->bios_control = aic_inb(temp_p, HA_274_BIOSCTRL);
-
-        /*
-         * Get the primary channel information.  Right now we don't
-         * do anything with this, but someday we will be able to inform
-         * the mid-level SCSI code which channel is primary.
-         */
-        if (temp_p->board_name_index == 0)
-        {
-          temp_p->board_name_index = 3;
-          if (aic7xxx_verbose & VERBOSE_PROBE2)
-            printk("aic7xxx: <%s> at EISA %d\n",
-                 board_names[3], slot);
-        }
-        if (temp_p->bios_control & CHANNEL_B_PRIMARY)
-        {
-          temp_p->flags |= AHC_CHANNEL_B_PRIMARY;
-        }
-
-        if ((temp_p->bios_control & BIOSMODE) == BIOSDISABLED)
-        {
-          temp_p->flags &= ~AHC_BIOS_ENABLED;
-        }
-        else
-        {
-          temp_p->flags &= ~AHC_USEDEFAULTS;
-          temp_p->flags |= AHC_BIOS_ENABLED;
-          if ( (temp_p->bios_control & 0x20) == 0 )
-          {
-            temp_p->bios_address = 0xcc000;
-            temp_p->bios_address += (0x4000 * (temp_p->bios_control & 0x07));
-          }
-          else
-          {
-            temp_p->bios_address = 0xd0000;
-            temp_p->bios_address += (0x8000 * (temp_p->bios_control & 0x06));
-          }
-        }
-        temp_p->adapter_control = aic_inb(temp_p, SCSICONF) << 8;
-        temp_p->adapter_control |= aic_inb(temp_p, SCSICONF + 1);
-        if (temp_p->features & AHC_WIDE)
-        {
-          temp_p->scsi_id = temp_p->adapter_control & HWSCSIID;
-          temp_p->scsi_id_b = temp_p->scsi_id;
-        }
-        else
-        {
-          temp_p->scsi_id = (temp_p->adapter_control >> 8) & HSCSIID;
-          temp_p->scsi_id_b = temp_p->adapter_control & HSCSIID;
-        }
-        aic7xxx_load_seeprom(temp_p, &sxfrctl1);
-        break;
-      }
-
-      case 2:
-      case 3:
-        temp_p->chip = AHC_AIC7770 | AHC_VL;
-        temp_p->features |= AHC_AIC7770_FE;
-        if (type == 2)
-          temp_p->flags |= AHC_BIOS_ENABLED;
-        else
-          temp_p->flags &= ~AHC_BIOS_ENABLED;
-        if (aic_inb(temp_p, SCSICONF) & TERM_ENB)
-          sxfrctl1 = STPWEN;
-        aic7xxx_load_seeprom(temp_p, &sxfrctl1);
-        temp_p->board_name_index = 4;
-        if (aic7xxx_verbose & VERBOSE_PROBE2)
-          printk("aic7xxx: <%s> at VLB %d\n",
-               board_names[2], slot);
-        switch( aic_inb(temp_p, STATUS_2840) & BIOS_SEL )
-        {
-          case 0x00:
-            temp_p->bios_address = 0xe0000;
-            break;
-          case 0x20:
-            temp_p->bios_address = 0xc8000;
-            break;
-          case 0x40:
-            temp_p->bios_address = 0xd0000;
-            break;
-          case 0x60:
-            temp_p->bios_address = 0xd8000;
-            break;
-          default:
-            break; /* can't get here */
-        }
-        break;
-
-      default:  /* Won't get here. */
-        break;
-    }
-    if (aic7xxx_verbose & VERBOSE_PROBE2)
-    {
-      printk(KERN_INFO "aic7xxx: BIOS %sabled, IO Port 0x%lx, IRQ %d (%s)\n",
-        (temp_p->flags & AHC_USEDEFAULTS) ? "dis" : "en", temp_p->base,
-        temp_p->irq,
-        (temp_p->pause & IRQMS) ? "level sensitive" : "edge triggered");
-      printk(KERN_INFO "aic7xxx: Extended translation %sabled.\n",
-             (temp_p->flags & AHC_EXTEND_TRANS_A) ? "en" : "dis");
-    }
-
-    /*
-     * Set the FIFO threshold and the bus off time.
-     */
-    hostconf = aic_inb(temp_p, HOSTCONF);
-    aic_outb(temp_p, hostconf & DFTHRSH, BUSSPD);
-    aic_outb(temp_p, (hostconf << 2) & BOFF, BUSTIME);
-    slot++;
-    found++;
-  }
-
-#endif /* defined(__i386__) || defined(__alpha__) */
-
-  /*
-   * Now, we re-order the probed devices by BIOS address and BUS class.
-   * In general, we follow this algorithm to make the adapters show up
-   * in the same order under linux that the computer finds them.
-   *  1: All VLB/EISA cards with BIOS_ENABLED first, according to BIOS
-   *     address, going from lowest to highest.
-   *  2: All PCI controllers with BIOS_ENABLED next, according to BIOS
-   *     address, going from lowest to highest.
-   *  3: Remaining VLB/EISA controllers going in slot order.
-   *  4: Remaining PCI controllers, going in PCI device order (reversable)
-   */
-
-  {
-    struct aic7xxx_host *sort_list[4] = { NULL, NULL, NULL, NULL };
-    struct aic7xxx_host *vlb, *pci;
-    struct aic7xxx_host *prev_p;
-    struct aic7xxx_host *p;
-    unsigned char left;
-
-    prev_p = vlb = pci = NULL;
-
-    temp_p = list_p;
-    while (temp_p != NULL)
-    {
-      switch(temp_p->chip & ~AHC_CHIPID_MASK)
-      {
-        case AHC_EISA:
-        case AHC_VL:
-        {
-          p = temp_p;
-          if (p->flags & AHC_BIOS_ENABLED)
-            vlb = sort_list[0];
-          else
-            vlb = sort_list[2];
-
-          if (vlb == NULL)
-          {
-            vlb = temp_p;
-            temp_p = temp_p->next;
-            vlb->next = NULL;
-          }
-          else
-          {
-            current_p = vlb;
-            prev_p = NULL;
-            while ( (current_p != NULL) &&
-                    (current_p->bios_address < temp_p->bios_address))
-            {
-              prev_p = current_p;
-              current_p = current_p->next;
-            }
-            if (prev_p != NULL)
-            {
-              prev_p->next = temp_p;
-              temp_p = temp_p->next;
-              prev_p->next->next = current_p;
-            }
-            else
-            {
-              vlb = temp_p;
-              temp_p = temp_p->next;
-              vlb->next = current_p;
-            }
-          }
-          
-          if (p->flags & AHC_BIOS_ENABLED)
-            sort_list[0] = vlb;
-          else
-            sort_list[2] = vlb;
-          
-          break;
-        }
-        default:  /* All PCI controllers fall through to default */
-        {
-
-          p = temp_p;
-          if (p->flags & AHC_BIOS_ENABLED) 
-            pci = sort_list[1];
-          else
-            pci = sort_list[3];
-
-          if (pci == NULL)
-          {
-            pci = temp_p;
-            temp_p = temp_p->next;
-            pci->next = NULL;
-          }
-          else
-          {
-            current_p = pci;
-            prev_p = NULL;
-            if (!aic7xxx_reverse_scan)
-            {
-              while ( (current_p != NULL) &&
-                      ( (PCI_SLOT(current_p->pci_device_fn) |
-                        (current_p->pci_bus << 8)) < 
-                        (PCI_SLOT(temp_p->pci_device_fn) |
-                        (temp_p->pci_bus << 8)) ) )
-              {
-                prev_p = current_p;
-                current_p = current_p->next;
-              }
-            }
-            else
-            {
-              while ( (current_p != NULL) &&
-                      ( (PCI_SLOT(current_p->pci_device_fn) |
-                        (current_p->pci_bus << 8)) > 
-                        (PCI_SLOT(temp_p->pci_device_fn) |
-                        (temp_p->pci_bus << 8)) ) )
-              {
-                prev_p = current_p;
-                current_p = current_p->next;
-              }
-            }
-            /*
-             * Are we dealing with a 7895/6/7/9 where we need to sort the
-             * channels as well, if so, the bios_address values should
-             * be the same
-             */
-            if ( (current_p) && (temp_p->flags & AHC_MULTI_CHANNEL) &&
-                 (temp_p->pci_bus == current_p->pci_bus) &&
-                 (PCI_SLOT(temp_p->pci_device_fn) ==
-                  PCI_SLOT(current_p->pci_device_fn)) )
-            {
-              if (temp_p->flags & AHC_CHNLB)
-              {
-                if ( !(temp_p->flags & AHC_CHANNEL_B_PRIMARY) )
-                {
-                  prev_p = current_p;
-                  current_p = current_p->next;
-                }
-              }
-              else
-              {
-                if (temp_p->flags & AHC_CHANNEL_B_PRIMARY)
-                {
-                  prev_p = current_p;
-                  current_p = current_p->next;
-                }
-              }
-            }
-            if (prev_p != NULL)
-            {
-              prev_p->next = temp_p;
-              temp_p = temp_p->next;
-              prev_p->next->next = current_p;
-            }
-            else
-            {
-              pci = temp_p;
-              temp_p = temp_p->next;
-              pci->next = current_p;
-            }
-          }
-
-          if (p->flags & AHC_BIOS_ENABLED)
-            sort_list[1] = pci;
-          else
-            sort_list[3] = pci;
-
-          break;
-        }
-      }  /* End of switch(temp_p->type) */
-    } /* End of while (temp_p != NULL) */
-    /*
-     * At this point, the cards have been broken into 4 sorted lists, now
-     * we run through the lists in order and register each controller
-     */
-    {
-      int i;
-      
-      left = found;
-      for (i=0; i<NUMBER(sort_list); i++)
-      {
-        temp_p = sort_list[i];
-        while(temp_p != NULL)
-        {
-          template->name = board_names[temp_p->board_name_index];
-          p = aic7xxx_alloc(template, temp_p);
-          if (p != NULL)
-          {
-            p->instance = found - left;
-            if (aic7xxx_register(template, p, (--left)) == 0)
-            {
-              found--;
-              aic7xxx_release(p->host);
-              scsi_unregister(p->host);
-            }
-            else if (aic7xxx_dump_card)
-            {
-              pause_sequencer(p);
-              aic7xxx_print_card(p);
-              aic7xxx_print_scratch_ram(p);
-              unpause_sequencer(p, TRUE);
-            }
-          }
-          current_p = temp_p;
-          temp_p = (struct aic7xxx_host *)temp_p->next;
-          kfree(current_p);
-        }
-      }
-    }
-  }
-  return (found);
-}
-
-static void aic7xxx_build_negotiation_cmnd(struct aic7xxx_host *p,
-                                           Scsi_Cmnd *old_cmd, int tindex);
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_allocate_negotiation_command
- *
- * Description:
- *   allocate the actual command struct and fill in the gaps...
- *-F*************************************************************************/
-static Scsi_Cmnd *
-aic7xxx_allocate_negotiation_command(struct aic7xxx_host *p,
-                                     Scsi_Cmnd *old_cmd, int tindex)
-{
-  Scsi_Cmnd *cmd;
-  char *buffer;
-
-  if (!(p->dev_dtr_cmnd[tindex] = kmalloc(sizeof(Scsi_Cmnd), GFP_ATOMIC)) )
-  {
-    return(NULL);
-  }
-  if (!(buffer = kmalloc(256, GFP_ATOMIC)))
-  {
-    kfree(p->dev_dtr_cmnd[tindex]);
-    p->dev_dtr_cmnd[tindex] = NULL;
-    return(NULL);
-  }
-  cmd = p->dev_dtr_cmnd[tindex];
-  memset(cmd, 0, sizeof(Scsi_Cmnd));
-  memcpy(cmd, old_cmd, sizeof(Scsi_Cmnd));
-  memset(&cmd->cmnd[0], 0, sizeof(cmd->cmnd));
-  memset(&cmd->data_cmnd[0], 0, sizeof(cmd->data_cmnd));
-  cmd->lun = 0;
-  cmd->request_bufflen = 255;
-  cmd->request_buffer = buffer;
-  cmd->use_sg = cmd->old_use_sg = cmd->sglist_len = 0;
-  cmd->bufflen = 0;
-  cmd->buffer = NULL;
-  cmd->underflow = 0;
-  cmd->cmd_len = 6;
-  cmd->cmnd[0] = cmd->data_cmnd[0] = INQUIRY;
-  cmd->cmnd[1] = cmd->data_cmnd[1] = 0;
-  cmd->cmnd[2] = cmd->data_cmnd[2] = 0;
-  cmd->cmnd[3] = cmd->data_cmnd[3] = 0;
-  cmd->cmnd[4] = cmd->data_cmnd[4] = 255; /* match what scsi.c does here */
-  cmd->cmnd[5] = cmd->data_cmnd[5] = 0;
-  return(cmd);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_negotiation_complete
- *
- * Description:
- *   Handle completion events for our Negotiation commands.  Clear out the
- *   struct and get it ready for its next use.
- *-F*************************************************************************/
-static void
-aic7xxx_negotiation_complete(Scsi_Cmnd *cmd)
-{
-  unsigned int checksum;
-  int i;
-  int *ibuffer;
-  struct aic7xxx_host *p = (struct aic7xxx_host *)cmd->host->hostdata;
-  int tindex = TARGET_INDEX(cmd);
-  struct aic7xxx_syncrate *syncrate;
-
-  /*
-   * perform our minimalistic domain validation
-   */
-  if(p->dev_flags[tindex] & DEVICE_SCANNED)
-  {
-    ibuffer = (int *)cmd->request_buffer;
-    checksum = 0;
-    for(i = 0; i < (cmd->request_bufflen >> 2); i++)
-    {
-      checksum += ibuffer[i];
-    }
-    if( (checksum != p->dev_checksum[tindex]) &&
-        (p->transinfo[tindex].cur_offset != 0) )
-    {
-      unsigned int period = p->transinfo[tindex].cur_period;
-      unsigned char options = p->transinfo[tindex].cur_options;
-
-      if (p->needdv & (1<<tindex))
-      {
-        /*
-         * oops, we had a failure, lower the transfer rate and try again.  It's
-         * worth noting here that it might be wise to also check for typical
-         * wide setting on narrow cable type problems and try disabling wide
-         * instead of slowing down if those exist.  That's hard to do with simple
-         * checksums though.
-         */
-        if(aic7xxx_verbose & VERBOSE_NEGOTIATION2) 
-        {
-          printk(INFO_LEAD "reducing SCSI transfer speed due to Domain "
-                 "validation failure.\n", p->host_no, CTL_OF_CMD(cmd));
-        }
-        if((syncrate = aic7xxx_find_syncrate(p, &period, 0, &options)) != NULL)
-        {
-          syncrate++;
-          if( (syncrate->rate[0] != NULL) &&
-              (!(p->features & AHC_ULTRA2) || (syncrate->sxfr_ultra2 == 0)) )
-          {
-            p->transinfo[tindex].goal_period = syncrate->period;
-            if( p->transinfo[tindex].goal_period > 9 )
-            {
-              p->transinfo[tindex].goal_options = 0;
-              p->needppr &= ~(1<<tindex);
-              p->needsdtr |= (1<<tindex);
-              p->needppr_copy &= ~(1<<tindex);
-              p->needsdtr_copy |= (1<<tindex);
-              if (p->transinfo[tindex].goal_width)
-              {
-                p->needwdtr |= (1<<tindex);
-                p->needwdtr_copy |= (1<<tindex);
-              }
-            }
-          }
-          else if (p->transinfo[tindex].goal_width)
-          {
-            p->transinfo[tindex].goal_width = 0;
-            p->needwdtr &= ~(1<<tindex);
-            p->needwdtr_copy &= ~(1<<tindex);
-            p->transinfo[tindex].goal_offset =
-              p->transinfo[tindex].user_offset;
-            p->transinfo[tindex].goal_period =
-              p->transinfo[tindex].user_period;
-            p->transinfo[tindex].goal_options =
-              p->transinfo[tindex].user_options;
-            if( p->transinfo[tindex].goal_period <= 9 )
-            {
-              p->needppr |= (1<<tindex);
-              p->needsdtr &= ~(1<<tindex);
-              p->needppr_copy |= (1<<tindex);
-              p->needsdtr_copy &= ~(1<<tindex);
-            }
-            else
-            {
-              p->needppr &= ~(1<<tindex);
-              p->needsdtr |= (1<<tindex);
-              p->needppr_copy &= ~(1<<tindex);
-              p->needsdtr_copy |= (1<<tindex);
-            }
-          }
-          else
-          {
-            p->transinfo[tindex].goal_offset = 0;
-            p->transinfo[tindex].goal_period = 255;
-            p->transinfo[tindex].goal_options = 0;
-            p->transinfo[tindex].goal_width = 0;
-            p->needppr &= ~(1<<tindex);
-            p->needsdtr &= ~(1<<tindex);
-            p->needwdtr &= ~(1<<tindex);
-            p->needppr_copy &= ~(1<<tindex);
-            p->needsdtr_copy &= ~(1<<tindex);
-            p->needwdtr_copy &= ~(1<<tindex);
-          }
-        }
-        p->needdv &= ~(1<<tindex);
-      }
-      else
-      {
-        if(aic7xxx_verbose & VERBOSE_NEGOTIATION2) 
-        {
-          printk(INFO_LEAD "Performing Domain validation.\n",
-                 p->host_no, CTL_OF_CMD(cmd));
-        }
-        /*
-         * Update the checksum in case the INQUIRY data has changed, maybe
-         * in relation to a change in the mode pages, or whatever.
-         */
-        p->dev_checksum[tindex] = checksum;
-        /*
-         * Signal that we are trying out the domain validation
-         */
-        p->needdv |= (1<<tindex);
-        /*
-         * Signal that we need to re-negotiate things, this also gets us our
-         * INQUIRY command to re-checksum off of.
-         */
-        p->needppr |= (p->needppr_copy & (1<<tindex));
-        p->needsdtr |= (p->needsdtr_copy & (1<<tindex));
-        p->needwdtr |= (p->needwdtr_copy & (1<<tindex));
-      }
-    } 
-    else
-    {
-      if( (aic7xxx_verbose & VERBOSE_NEGOTIATION2) &&
-          (p->needdv & (1<<tindex)) )
-      {
-        printk(INFO_LEAD "Successfully completed Domain validation.\n",
-               p->host_no, CTL_OF_CMD(cmd));
-      }
-      /*
-       * We successfully did our checksum, so don't leave the needdv flag set
-       * in case we might have set it last time through.
-       */
-      p->needdv &= ~(1<<tindex);
-    }
-  }
-
-  p->dtr_pending &= ~(0x01 << tindex);
-  /*
-   * This looks recursive in the extreme, but if this was a WDTR negotiation
-   * and we didn't follow up with SDTR yet, then this will get it started.
-   * For all other cases, this should work out to be a no-op, unless we are
-   * doing domain validation and happen to need a new negotiation command.
-   *
-   * In case we don't want this to go any further, the cmdcmplt interrupt
-   * handler will NULL out the cmd->next entry so that the real SCSI command
-   * can be sent back to the mid layer code with SENSE data intact.  We'll
-   * finish things up when the cmd gets sent back down to us, so no worries.
-   */
-  if(cmd->next)
-  {
-    aic7xxx_build_negotiation_cmnd(p, cmd->next, tindex);
-  }
-  return;
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_build_negotiation_command
- *
- * Description:
- *   Build a Scsi_Cmnd structure to perform negotiation with or else send
- *   a pre-built command specifically for this purpose.
- *-F*************************************************************************/
-static void
-aic7xxx_build_negotiation_cmnd(struct aic7xxx_host *p, Scsi_Cmnd *old_cmd,
-  int tindex)
-{
-
-  if ( !(p->dtr_pending & (1<<tindex)) &&
-       ( (p->needppr & (1<<tindex)) ||
-         (p->needwdtr & (1<<tindex)) ||
-         (p->needsdtr & (1<<tindex)) ) )
-  {
-    if ( (p->dev_dtr_cmnd[tindex] == NULL) &&
-         (aic7xxx_allocate_negotiation_command(p, old_cmd, tindex) == NULL) )
-    {
-      return;
-    }
-    /*
-     * Before sending this thing out, we also make the cmd->next pointer
-     * point to the real command so we can stuff any possible SENSE data
-     * into the real command instead of this fake command.  This has to be
-     * done each time the command is built, not just the first time, hence
-     * it's outside of the above if()...
-     */
-    p->dev_dtr_cmnd[tindex]->next = old_cmd;
-    /*
-     * Clear the buffer so checksums come out right....
-     */
-    memset(p->dev_dtr_cmnd[tindex]->request_buffer, 0,
-           p->dev_dtr_cmnd[tindex]->request_bufflen);
-    /*
-     * Remove any commands for this particular device that might be on the
-     * waiting_scbs queue or qinfifo so that this command goes out first.
-     * This is vital for our implementation of domain validation.
-     */
-    pause_sequencer(p);
-    aic7xxx_search_qinfifo(p, old_cmd->target, old_cmd->channel, ALL_LUNS,
-                SCB_LIST_NULL, 0, TRUE, &p->delayed_scbs[tindex]);
-    unpause_sequencer(p, FALSE);
-    {
-      struct aic7xxx_scb *scb, *next;
-
-      scb = p->waiting_scbs.head;
-      while(scb != NULL)
-      {
-        if( aic7xxx_match_scb(p, scb, old_cmd->target, old_cmd->channel,
-                              ALL_LUNS, SCB_LIST_NULL) )
-        {
-          next = scb->q_next;
-          scbq_remove(&p->waiting_scbs, scb);
-          scbq_insert_tail(&p->delayed_scbs[tindex], scb);
-          scb = next;
-        }
-        else
-        {
-          scb = scb->q_next;
-        }
-      }
-    }
-    aic7xxx_queue(p->dev_dtr_cmnd[tindex], 
-                  aic7xxx_negotiation_complete);
-  }
-}
-
-#ifdef AIC7XXX_VERBOSE_DEBUGGING
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_print_scb
- *
- * Description:
- *   Dump the byte codes for an about to be sent SCB.
- *-F*************************************************************************/
-static void
-aic7xxx_print_scb(struct aic7xxx_host *p, struct aic7xxx_scb *scb)
-{
-  int i;
-  unsigned char *x;  
-
-  x = (unsigned char *)&scb->hscb->control;
-
-  for(i=0; i<32; i++)
-  {
-    printk("%02x ", x[i]);
-  }
-  printk("\n");
-}
-#endif
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_buildscb
- *
- * Description:
- *   Build a SCB.
- *-F*************************************************************************/
-static void
-aic7xxx_buildscb(struct aic7xxx_host *p, Scsi_Cmnd *cmd,
-    struct aic7xxx_scb *scb)
-{
-  unsigned short mask;
-  struct aic7xxx_hwscb *hscb;
-  unsigned char tindex = TARGET_INDEX(cmd);
-
-  mask = (0x01 << tindex);
-  hscb = scb->hscb;
-
-  /*
-   * Setup the control byte if we need negotiation and have not
-   * already requested it.
-   */
-  hscb->control = 0;
-  scb->tag_action = 0;
-  cmd->tag = hscb->tag;
-  if (p->discenable & mask)
-  {
-    hscb->control |= DISCENB;
-    if ( (p->tagenable & mask) &&
-         (cmd->cmnd[0] != TEST_UNIT_READY) )
-    {
-      p->dev_commands_sent[tindex]++;
-      if (p->dev_commands_sent[tindex] < 200)
-      {
-        hscb->control |= MSG_SIMPLE_Q_TAG;
-        scb->tag_action = MSG_SIMPLE_Q_TAG;
-      }
-      else
-      {
-        if (p->orderedtag & mask)
-        {
-          hscb->control |= MSG_ORDERED_Q_TAG;
-          scb->tag_action = MSG_ORDERED_Q_TAG;
-        }
-        else
-        {
-          hscb->control |= MSG_SIMPLE_Q_TAG;
-          scb->tag_action = MSG_SIMPLE_Q_TAG;
-        }
-        p->dev_commands_sent[tindex] = 0;
-      }
-    }
-  }
-  if ( cmd == p->dev_dtr_cmnd[tindex] )
-  {
-    p->dtr_pending |= mask;
-    scb->tag_action = 0;
-    if (p->dev_flags[tindex] & DEVICE_SCANNED)
-    {
-      hscb->control &= DISCENB;
-      hscb->control |= MK_MESSAGE;
-      if(p->needppr & mask)
-      {
-        scb->flags |= SCB_MSGOUT_PPR;
-      }
-      else if(p->needwdtr & mask)
-      {
-        scb->flags |= SCB_MSGOUT_WDTR;
-      }
-      else if(p->needsdtr & mask)
-      {
-        scb->flags |= SCB_MSGOUT_SDTR;
-      }
-    }
-  }
-  if ( !(p->dtr_pending & mask) &&
-        ( (p->needppr & mask) ||
-          (p->needwdtr & mask) ||
-          (p->needsdtr & mask) ) )
-  {
-    aic7xxx_build_negotiation_cmnd(p, cmd, tindex);
-  }
-  hscb->target_channel_lun = ((cmd->target << 4) & 0xF0) |
-        ((cmd->channel & 0x01) << 3) | (cmd->lun & 0x07);
-
-  /*
-   * The interpretation of request_buffer and request_bufflen
-   * changes depending on whether or not use_sg is zero; a
-   * non-zero use_sg indicates the number of elements in the
-   * scatter-gather array.
-   */
-
-  /*
-   * XXX - this relies on the host data being stored in a
-   *       little-endian format.
-   */
-  hscb->SCSI_cmd_length = cmd->cmd_len;
-  hscb->SCSI_cmd_pointer = cpu_to_le32(VIRT_TO_BUS(cmd->cmnd));
-
-  if (cmd->use_sg)
-  {
-    struct scatterlist *sg;  /* Must be mid-level SCSI code scatterlist */
-
-    /*
-     * We must build an SG list in adapter format, as the kernel's SG list
-     * cannot be used directly because of data field size (__alpha__)
-     * differences and the kernel SG list uses virtual addresses where
-     * we need physical addresses.
-     */
-    int i;
-
-    sg = (struct scatterlist *)cmd->request_buffer;
-    scb->sg_length = 0;
-    /*
-     * Copy the segments into the SG array.  NOTE!!! - We used to
-     * have the first entry both in the data_pointer area and the first
-     * SG element.  That has changed somewhat.  We still have the first
-     * entry in both places, but now we download the address of
-     * scb->sg_list[1] instead of 0 to the sg pointer in the hscb.
-     */
-    for (i = 0; i < cmd->use_sg; i++)
-    {
-      scb->sg_list[i].address = cpu_to_le32(VIRT_TO_BUS(sg[i].address));
-      scb->sg_list[i].length = cpu_to_le32(sg[i].length);
-      scb->sg_length += sg[i].length;
-    }
-    /* Copy the first SG into the data pointer area. */
-    hscb->data_pointer = scb->sg_list[0].address;
-    hscb->data_count = scb->sg_list[0].length;
-    scb->sg_count = cmd->use_sg;
-    hscb->SG_segment_count = cmd->use_sg;
-    hscb->SG_list_pointer = cpu_to_le32(VIRT_TO_BUS(&scb->sg_list[1]));
-  }
-  else
-  {
-    if (cmd->request_bufflen)
-    {
-      scb->sg_count = 1;
-      scb->sg_list[0].address = cpu_to_le32(VIRT_TO_BUS(cmd->request_buffer));
-      scb->sg_list[0].length = cpu_to_le32(cmd->request_bufflen);
-      scb->sg_length = cmd->request_bufflen;
-      hscb->SG_segment_count = 1;
-      hscb->SG_list_pointer = cpu_to_le32(VIRT_TO_BUS(&scb->sg_list[0]));
-      hscb->data_count = scb->sg_list[0].length;
-      hscb->data_pointer = scb->sg_list[0].address;
-    }
-    else
-    {
-      scb->sg_count = 0;
-      scb->sg_length = 0;
-      hscb->SG_segment_count = 0;
-      hscb->SG_list_pointer = 0;
-      hscb->data_count = 0;
-      hscb->data_pointer = 0;
-    }
-  }
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_queue
- *
- * Description:
- *   Queue a SCB to the controller.
- *-F*************************************************************************/
-int
-aic7xxx_queue(Scsi_Cmnd *cmd, void (*fn)(Scsi_Cmnd *))
-{
-  struct aic7xxx_host *p;
-  struct aic7xxx_scb *scb;
-#ifdef AIC7XXX_VERBOSE_DEBUGGING
-  int tindex = TARGET_INDEX(cmd);
-#endif
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,1,95)
-  unsigned long cpu_flags = 0;
-#endif
-
-  p = (struct aic7xxx_host *) cmd->host->hostdata;
-  /*
-   * Check to see if channel was scanned.
-   */
-  
-#ifdef AIC7XXX_VERBOSE_DEBUGGING
-  if (!(p->flags & AHC_A_SCANNED) && (cmd->channel == 0))
-  {
-    if (aic7xxx_verbose & VERBOSE_PROBE2)
-      printk(INFO_LEAD "Scanning channel for devices.\n",
-        p->host_no, 0, -1, -1);
-    p->flags |= AHC_A_SCANNED;
-  }
-  else
-  {
-    if (!(p->flags & AHC_B_SCANNED) && (cmd->channel == 1))
-    {
-      if (aic7xxx_verbose & VERBOSE_PROBE2)
-        printk(INFO_LEAD "Scanning channel for devices.\n",
-          p->host_no, 1, -1, -1);
-      p->flags |= AHC_B_SCANNED;
-    }
-  }
-
-  if (p->dev_active_cmds[tindex] > (cmd->device->queue_depth + 1))
-  {
-    printk(WARN_LEAD "Commands queued exceeds queue "
-           "depth, active=%d\n",
-           p->host_no, CTL_OF_CMD(cmd), 
-           p->dev_active_cmds[tindex]);
-    if ( p->dev_active_cmds[tindex] > 220 )
-      p->dev_active_cmds[tindex] = 0;
-  }
-#endif
-
-  scb = scbq_remove_head(&p->scb_data->free_scbs);
-  if (scb == NULL)
-  {
-    DRIVER_LOCK
-    aic7xxx_allocate_scb(p);
-    DRIVER_UNLOCK
-    scb = scbq_remove_head(&p->scb_data->free_scbs);
-  }
-  if (scb == NULL)
-  {
-    printk(WARN_LEAD "Couldn't get a free SCB.\n", p->host_no,
-           CTL_OF_CMD(cmd));
-    cmd->result = (DID_BUS_BUSY << 16);
-    DRIVER_LOCK
-    aic7xxx_queue_cmd_complete(p, cmd);
-    DRIVER_UNLOCK
-    return 0;
-  }
-  else
-  {
-    scb->cmd = cmd;
-    aic7xxx_position(cmd) = scb->hscb->tag;
-
-    /*
-     * Construct the SCB beforehand, so the sequencer is
-     * paused a minimal amount of time.
-     */
-    aic7xxx_buildscb(p, cmd, scb);
-
-    /*
-     * Make sure the Scsi_Cmnd pointer is saved, the struct it points to
-     * is set up properly, and the parity error flag is reset, then send
-     * the SCB to the sequencer and watch the fun begin.
-     */
-    cmd->scsi_done = fn;
-    cmd->result = DID_OK;
-    memset(cmd->sense_buffer, 0, sizeof(cmd->sense_buffer));
-    aic7xxx_error(cmd) = DID_OK;
-    aic7xxx_status(cmd) = 0;
-    cmd->host_scribble = NULL;
-
-    scb->flags |= SCB_ACTIVE | SCB_WAITINGQ;
-
-    DRIVER_LOCK
-    scbq_insert_tail(&p->waiting_scbs, scb);
-    if ( (p->flags & (AHC_IN_ISR | AHC_IN_ABORT | AHC_IN_RESET)) == 0)
-    {
-      aic7xxx_run_waiting_queues(p);
-    }
-    DRIVER_UNLOCK
-  }
-  return (0);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_bus_device_reset
- *
- * Description:
- *   Abort or reset the current SCSI command(s).  If the scb has not
- *   previously been aborted, then we attempt to send a BUS_DEVICE_RESET
- *   message to the target.  If the scb has previously been unsuccessfully
- *   aborted, then we will reset the channel and have all devices renegotiate.
- *   Returns an enumerated type that indicates the status of the operation.
- *-F*************************************************************************/
-static int
-aic7xxx_bus_device_reset(struct aic7xxx_host *p, Scsi_Cmnd *cmd)
-{
-  struct aic7xxx_scb   *scb;
-  struct aic7xxx_hwscb *hscb;
-  int result = -1;
-  int channel;
-  unsigned char saved_scbptr, lastphase;
-  unsigned char hscb_index;
-  int disconnected;
-
-  scb = (p->scb_data->scb_array[aic7xxx_position(cmd)]);
-  hscb = scb->hscb;
-
-  lastphase = aic_inb(p, LASTPHASE);
-  if (aic7xxx_verbose & VERBOSE_RESET_PROCESS)
-  {
-    printk(INFO_LEAD "Bus Device reset, scb flags 0x%x, ",
-         p->host_no, CTL_OF_SCB(scb), scb->flags);
-    switch (lastphase)
-    {
-      case P_DATAOUT:
-        printk("Data-Out phase\n");
-        break;
-      case P_DATAIN:
-        printk("Data-In phase\n");
-        break;
-      case P_COMMAND:
-        printk("Command phase\n");
-        break;
-      case P_MESGOUT:
-        printk("Message-Out phase\n");
-        break;
-      case P_STATUS:
-        printk("Status phase\n");
-        break;
-      case P_MESGIN:
-        printk("Message-In phase\n");
-        break;
-      default:
-      /*
-       * We're not in a valid phase, so assume we're idle.
-       */
-        printk("while idle, LASTPHASE = 0x%x\n", lastphase);
-        break;
-    }
-    printk(INFO_LEAD "SCSISIGI 0x%x, SEQADDR 0x%x, SSTAT0 0x%x, SSTAT1 "
-         "0x%x\n", p->host_no, CTL_OF_SCB(scb),
-         aic_inb(p, SCSISIGI),
-         aic_inb(p, SEQADDR0) | (aic_inb(p, SEQADDR1) << 8),
-         aic_inb(p, SSTAT0), aic_inb(p, SSTAT1));
-  }
-
-  channel = cmd->channel;
-
-    /*
-     * Send a Device Reset Message:
-     * The target that is holding up the bus may not be the same as
-     * the one that triggered this timeout (different commands have
-     * different timeout lengths).  Our strategy here is to queue an
-     * abort message to the timed out target if it is disconnected.
-     * Otherwise, if we have an active target we stuff the message buffer
-     * with an abort message and assert ATN in the hopes that the target
-     * will let go of the bus and go to the mesgout phase.  If this
-     * fails, we'll get another timeout a few seconds later which will
-     * attempt a bus reset.
-     */
-  saved_scbptr = aic_inb(p, SCBPTR);
-  disconnected = FALSE;
-
-  if (lastphase != P_BUSFREE)
-  {
-    if (aic_inb(p, SCB_TAG) >= p->scb_data->numscbs)
-    {
-      printk(WARN_LEAD "Invalid SCB ID %d is active, "
-             "SCB flags = 0x%x.\n", p->host_no,
-            CTL_OF_CMD(cmd), scb->hscb->tag, scb->flags);
-      return(SCSI_RESET_ERROR);
-    }
-    if (scb->hscb->tag == aic_inb(p, SCB_TAG))
-    { 
-      if ( (lastphase != P_MESGOUT) && (lastphase != P_MESGIN) )
-      {
-        if (aic7xxx_verbose & VERBOSE_RESET_PROCESS)
-          printk(INFO_LEAD "Device reset message in "
-                "message buffer\n", p->host_no, CTL_OF_SCB(scb));
-        scb->flags |= SCB_RESET | SCB_DEVICE_RESET;
-        aic7xxx_error(scb->cmd) = DID_RESET;
-        p->dev_flags[TARGET_INDEX(scb->cmd)] |= 
-                BUS_DEVICE_RESET_PENDING;
-        /* Send the abort message to the active SCB. */
-        aic_outb(p, HOST_MSG, MSG_OUT);
-        aic_outb(p, lastphase | ATNO, SCSISIGO);
-        return(SCSI_RESET_PENDING);
-      }
-      else
-      {
-        /* We want to send out the message, but it could screw an already */
-        /* in place and being used message.  Instead, we return an error  */
-        /* to try and start the bus reset phase since this command is     */
-        /* probably hung (aborts failed, and now reset is failing).  We   */
-        /* also make sure to set BUS_DEVICE_RESET_PENDING so we won't try */
-        /* any more on this device, but instead will escalate to a bus or */
-        /* host reset (additionally, we won't try to abort any more).     */
-        printk(WARN_LEAD "Device reset, Message buffer "
-                "in use\n", p->host_no, CTL_OF_SCB(scb));
-        scb->flags |= SCB_RESET | SCB_DEVICE_RESET;
-        aic7xxx_error(scb->cmd) = DID_RESET;
-        p->dev_flags[TARGET_INDEX(scb->cmd)] |= 
-                BUS_DEVICE_RESET_PENDING;
-        return(SCSI_RESET_ERROR);
-      }
-    }
-  } /* if (last_phase != P_BUSFREE).....indicates we are idle and can work */
-  hscb_index = aic7xxx_find_scb(p, scb);
-  if (hscb_index == SCB_LIST_NULL)
-  {
-    disconnected = (aic7xxx_scb_on_qoutfifo(p, scb)) ? FALSE : TRUE;
-  }
-  else
-  {
-    aic_outb(p, hscb_index, SCBPTR);
-    if (aic_inb(p, SCB_CONTROL) & DISCONNECTED)
-    {
-      disconnected = TRUE;
-    }
-  }
-  if (disconnected)
-  {
-        /*
-         * Simply set the MK_MESSAGE flag and the SEQINT handler will do
-         * the rest on a reconnect.
-         */
-    scb->hscb->control |= MK_MESSAGE;
-    scb->flags |= SCB_RESET | SCB_DEVICE_RESET;
-    p->dev_flags[TARGET_INDEX(scb->cmd)] |= 
-        BUS_DEVICE_RESET_PENDING;
-    if (hscb_index != SCB_LIST_NULL)
-    {
-      unsigned char scb_control;
-
-      aic_outb(p, hscb_index, SCBPTR);
-      scb_control = aic_inb(p, SCB_CONTROL);
-      aic_outb(p, scb_control | MK_MESSAGE, SCB_CONTROL);
-    }
-        /*
-         * Actually requeue this SCB in case we can select the
-         * device before it reconnects.  If the transaction we
-         * want to abort is not tagged, then this will be the only
-         * outstanding command and we can simply shove it on the
-         * qoutfifo and be done.  If it is tagged, then it goes right
-         * in with all the others, no problem :)  We need to add it
-         * to the qinfifo and let the sequencer know it is there.
-         * Now, the only problem left to deal with is, *IF* this
-         * command completes, in spite of the MK_MESSAGE bit in the
-         * control byte, then we need to pick that up in the interrupt
-         * routine and clean things up.  This *shouldn't* ever happen.
-         */
-    if (aic7xxx_verbose & VERBOSE_RESET_PROCESS)
-      printk(INFO_LEAD "Queueing device reset "
-           "command.\n", p->host_no, CTL_OF_SCB(scb));
-    p->qinfifo[p->qinfifonext++] = scb->hscb->tag;
-    if (p->features & AHC_QUEUE_REGS)
-      aic_outb(p, p->qinfifonext, HNSCB_QOFF);
-    else
-      aic_outb(p, p->qinfifonext, KERNEL_QINPOS);
-    scb->flags |= SCB_QUEUED_ABORT;
-    result = SCSI_RESET_PENDING;
-  }
-  else if (result == -1)
-  {
-    result = SCSI_RESET_ERROR;
-  }
-  aic_outb(p, saved_scbptr, SCBPTR);
-  return (result);
-}
-
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_panic_abort
- *
- * Description:
- *   Abort the current SCSI command(s).
- *-F*************************************************************************/
-void
-aic7xxx_panic_abort(struct aic7xxx_host *p, Scsi_Cmnd *cmd)
-{
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,1,0)
-  int i, mask, found, need_tag;
-  struct aic7xxx_scb *scb;
-  unsigned char qinpos, hscbp;
-
-  found = FALSE;
-#endif
-
-  printk("aic7xxx driver version %s/%s\n", AIC7XXX_C_VERSION,
-         UTS_RELEASE);
-  printk("Controller type:\n    %s\n", board_names[p->board_name_index]);
-  printk("p->flags=0x%x, p->chip=0x%x, p->features=0x%x, "
-         "sequencer %s paused\n",
-     p->flags, p->chip, p->features,
-    (aic_inb(p, HCNTRL) & PAUSE) ? "is" : "isn't" );
-  pause_sequencer(p);
-  disable_irq(p->irq);
-  aic7xxx_print_card(p);
-  aic7xxx_print_scratch_ram(p);
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,1,0)
-  for(i=0; i<MAX_TARGETS; i++)
-  {
-    if(p->dev_flags[i] & DEVICE_PRESENT)
-    {
-      mask = (0x01 << i);
-      printk(INFO_LEAD "dev_flags=0x%x, Pending:%c, PPR:%c/%c, WDTR:%c/%c, "
-             "SDTR:%c/%c, q_depth=%d:%d\n",
-        p->host_no, 0, i, 0, p->dev_flags[i],
-        (p->dtr_pending & mask) ? 'Y' : 'N',
-        (p->needppr & mask) ? 'Y' : 'N',
-        (p->needppr_copy & mask) ? 'Y' : 'N',
-        (p->needwdtr & mask) ? 'Y' : 'N',
-        (p->needwdtr_copy & mask) ? 'Y' : 'N',
-        (p->needsdtr & mask) ? 'Y' : 'N',
-        (p->needsdtr_copy & mask) ? 'Y' : 'N',
-        p->dev_active_cmds[i],
-        p->dev_max_queue_depth[i] );
-      printk(INFO_LEAD "targ_scsirate=0x%x", p->host_no, 0, i, 0,
-             aic_inb(p, TARG_SCSIRATE + i));
-      if (p->features & AHC_ULTRA2)
-        printk(", targ_offset=%d", aic_inb(p, TARG_OFFSET + i));
-      printk("\n");
-    }
-  }
-  /*
-   * Search for this command and see if we can't track it down, it's the
-   * one causing the timeout.  Print out this command first, then all other
-   * active commands afterwords.
-   */
-  need_tag = -1;
-  if ( cmd )
-  {
-    scb = p->scb_data->scb_array[aic7xxx_position(cmd)];
-    if ( (scb->flags & SCB_ACTIVE) && (scb->cmd == cmd) )
-    {
-      printk("Timed out command is scb #%d:\n", scb->hscb->tag);
-      printk("Tag%d: flags=0x%x, control=0x%x, TCL=0x%x, %s\n", scb->hscb->tag,
-             scb->flags, scb->hscb->control, scb->hscb->target_channel_lun,
-             (scb->flags & SCB_WAITINGQ) ? "WAITINGQ" : "Sent" );
-      need_tag = scb->hscb->tag;
-      if (scb->flags & SCB_WAITINGQ) found=TRUE;
-    }
-  }
-  printk("QINFIFO: (TAG) ");
-  qinpos = aic_inb(p, QINPOS);
-  while ( qinpos != p->qinfifonext )
-  {
-    if (p->qinfifo[qinpos] == need_tag)
-      found=TRUE;
-    printk("%d ", p->qinfifo[qinpos++]);
-  }  
-  printk("\n");
-  printk("Current SCB: (SCBPTR/TAG/CONTROL) %d/%d/0x%x\n", aic_inb(p, SCBPTR),
-         aic_inb(p, SCB_TAG), aic_inb(p, SCB_CONTROL) );
-  if (aic_inb(p, SCB_TAG) == need_tag)  found=TRUE;
-  printk("WAITING_SCBS: (SCBPTR/TAG/CONTROL) %d->",
-         hscbp = aic_inb(p, WAITING_SCBH));
-  while (hscbp != SCB_LIST_NULL)
-  {
-    aic_outb(p, hscbp, SCBPTR);
-    printk("%d/%d/0x%x ", hscbp, aic_inb(p, SCB_TAG), aic_inb(p, SCB_CONTROL));
-    hscbp = aic_inb(p, SCB_NEXT);
-    if (aic_inb(p, SCB_TAG) == need_tag)  found=TRUE;
-  }
-  printk("\n");
-  printk("DISCONNECTED_SCBS: (SCBPTR/TAG/CONTROL) %d->",
-         hscbp = aic_inb(p, DISCONNECTED_SCBH));
-  while (hscbp != SCB_LIST_NULL)
-  {
-    aic_outb(p, hscbp, SCBPTR);
-    printk("%d/%d/0x%x ", hscbp, aic_inb(p, SCB_TAG), aic_inb(p, SCB_CONTROL));
-    hscbp = aic_inb(p, SCB_NEXT);
-    if (aic_inb(p, SCB_TAG) == need_tag)  found=TRUE;
-  }
-  printk("\n");
-  printk("FREE_SCBS: (SCBPTR/TAG/CONTROL) %d->",
-         hscbp = aic_inb(p, FREE_SCBH));
-  while (hscbp != SCB_LIST_NULL)
-  {
-    aic_outb(p, hscbp, SCBPTR);
-    printk("%d/%d/0x%x ", hscbp, aic_inb(p, SCB_TAG), aic_inb(p, SCB_CONTROL));
-    hscbp = aic_inb(p, SCB_NEXT);
-  }
-  printk("\n");
-
-  if (found == FALSE)
-  {
-    /*
-     * We haven't found the offending SCB yet, and it should be around
-     * somewhere, so go look for it in the cards SCBs.
-     */
-    printk("SCBPTR CONTROL TAG NEXT\n");
-    for(i=0; i<p->scb_data->maxhscbs; i++)
-    {
-      aic_outb(p, i, SCBPTR);
-      printk("   %3d      %02x  %02x   %02x\n", i,
-             aic_inb(p, SCB_CONTROL), aic_inb(p, SCB_TAG),
-             aic_inb(p, SCB_NEXT));
-    }
-  }
-  
-
-  for (i=0; i < p->scb_data->numscbs; i++)
-  {
-    scb = p->scb_data->scb_array[i];
-    if ( (scb->flags & SCB_ACTIVE) && (scb->cmd != cmd) )
-    {
-      printk("Tag%d: flags=0x%x, control=0x%x, TCL=0x%x, %s\n", scb->hscb->tag,
-             scb->flags, scb->hscb->control, scb->hscb->target_channel_lun,
-             (scb->flags & SCB_WAITINGQ) ? "WAITINGQ" : "Sent" );
-    }
-  }
-  sti();
-#else
-  spin_unlock_irq(&io_request_lock);
-#endif
-  for(;;) barrier();
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_abort
- *
- * Description:
- *   Abort the current SCSI command(s).
- *-F*************************************************************************/
-int
-aic7xxx_abort(Scsi_Cmnd *cmd)
-{
-  struct aic7xxx_scb  *scb = NULL;
-  struct aic7xxx_host *p;
-  int    result, found=0;
-  unsigned char tmp_char, saved_hscbptr, next_hscbptr, prev_hscbptr;
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,1,95)
-  unsigned long cpu_flags = 0;
-#endif
-  Scsi_Cmnd *cmd_next, *cmd_prev;
-
-  p = (struct aic7xxx_host *) cmd->host->hostdata;
-  scb = (p->scb_data->scb_array[aic7xxx_position(cmd)]);
-
-  /*
-   * I added a new config option to the driver: "panic_on_abort" that will
-   * cause the driver to panic and the machine to stop on the first abort
-   * or reset call into the driver.  At that point, it prints out a lot of
-   * usefull information for me which I can then use to try and debug the
-   * problem.  Simply enable the boot time prompt in order to activate this
-   * code.
-   */
-  if (aic7xxx_panic_on_abort)
-    aic7xxx_panic_abort(p, cmd);
-
-  DRIVER_LOCK
-
-/*
- *  Run the isr to grab any command in the QOUTFIFO and any other misc.
- *  assundry tasks.  This should also set up the bh handler if there is
- *  anything to be done, but it won't run until we are done here since
- *  we are following a straight code path without entering the scheduler
- *  code.
- */
-
-  pause_sequencer(p);
-  while ( (aic_inb(p, INTSTAT) & INT_PEND) && !(p->flags & AHC_IN_ISR))
-  {
-    aic7xxx_isr(p->irq, p, (void *)NULL);
-    pause_sequencer(p);
-    aic7xxx_done_cmds_complete(p);
-  }
-
-  if ((scb == NULL) || (cmd->serial_number != cmd->serial_number_at_timeout))
-                      /*  Totally bogus cmd since it points beyond our  */
-  {                   /*  valid SCB range or doesn't even match it's own*/
-                      /*  timeout serial number.                        */
-    if (aic7xxx_verbose & VERBOSE_ABORT_MID)
-      printk(INFO_LEAD "Abort called with bogus Scsi_Cmnd "
-        "pointer.\n", p->host_no, CTL_OF_CMD(cmd));
-    unpause_sequencer(p, FALSE);
-    DRIVER_UNLOCK
-    return(SCSI_ABORT_NOT_RUNNING);
-  }
-  if (scb->cmd != cmd)  /*  Hmmm...either this SCB is currently free with a */
-  {                     /*  NULL cmd pointer (NULLed out when freed) or it  */
-                        /*  has already been recycled for another command   */
-                        /*  Either way, this SCB has nothing to do with this*/
-                        /*  command and we need to deal with cmd without    */
-                        /*  touching the SCB.                               */
-                        /*  The theory here is to return a value that will  */
-                        /*  make the queued for complete command actually   */
-                        /*  finish successfully, or to indicate that we     */
-                        /*  don't have this cmd any more and the mid level  */
-                        /*  code needs to find it.                          */
-    cmd_next = p->completeq.head;
-    cmd_prev = NULL;
-    while (cmd_next != NULL) 
-    {
-      if (cmd_next == cmd) 
-      {
-        if (aic7xxx_verbose & VERBOSE_ABORT_PROCESS)
-          printk(INFO_LEAD "Abort called for command "
-          "on completeq, completing.\n", p->host_no, CTL_OF_CMD(cmd));
-        if ( cmd_prev == NULL )
-          p->completeq.head = (Scsi_Cmnd *)cmd_next->host_scribble;
-        else
-          cmd_prev->host_scribble = cmd_next->host_scribble;
-        cmd_next->scsi_done(cmd_next);
-        unpause_sequencer(p, FALSE);
-        DRIVER_UNLOCK
-        return(SCSI_ABORT_NOT_RUNNING); /* It's already back as a successful
-                                         * completion */
-      }                                  
-      cmd_prev = cmd_next;
-      cmd_next = (Scsi_Cmnd *)cmd_next->host_scribble;
-    }
-    if (aic7xxx_verbose & VERBOSE_ABORT_MID)
-      printk(INFO_LEAD "Abort called for already completed"
-        " command.\n", p->host_no, CTL_OF_CMD(cmd));
-    unpause_sequencer(p, FALSE);
-    DRIVER_UNLOCK
-    return(SCSI_ABORT_NOT_RUNNING);
-  }
-    
-/*   At this point we know the following:
- *     the SCB pointer is valid
- *     the command pointer passed in to us and the scb->cmd pointer match
- *     this then means that the command we need to abort is the same as the
- *     command held by the scb pointer and is a valid abort request.
- *   Now, we just have to figure out what to do from here.  Current plan is:
- *     if we have already been here on this command, escalate to a reset
- *     if scb is on waiting list or QINFIFO, send it back as aborted, but
- *       we also need to be aware of the possibility that we could be using
- *       a faked negotiation command that is holding this command up,  if
- *       so we need to take care of that command instead, which means we
- *       would then treat this one like it was sitting around disconnected
- *       instead.
- *     if scb is on WAITING_SCB list in sequencer, free scb and send back
- *     if scb is disconnected and not completed, abort with abort message
- *     if scb is currently running, then it may be causing the bus to hang
- *       so we want a return value that indicates a reset would be appropriate
- *       if the command does not finish shortly
- *     if scb is already complete but not on completeq, we're screwed because
- *       this can't happen (except if the command is in the QOUTFIFO, in which
- *       case we would like it to complete successfully instead of having to
- *       to be re-done)
- *   All other scenarios already dealt with by previous code.
- */
-
-  if ( scb->flags & (SCB_ABORT | SCB_RESET | SCB_QUEUED_ABORT) )
-  {
-    if (aic7xxx_verbose & VERBOSE_ABORT_PROCESS)
-      printk(INFO_LEAD "SCB aborted once already, "
-        "escalating.\n", p->host_no, CTL_OF_SCB(scb));
-    unpause_sequencer(p, FALSE);
-    DRIVER_UNLOCK
-    return(SCSI_ABORT_SNOOZE);
-  }
-  if ( (p->flags & (AHC_RESET_PENDING | AHC_ABORT_PENDING)) || 
-          (p->dev_flags[TARGET_INDEX(scb->cmd)] & 
-           BUS_DEVICE_RESET_PENDING) )
-  {
-    if (aic7xxx_verbose & VERBOSE_ABORT_PROCESS)
-      printk(INFO_LEAD "Reset/Abort pending for this "
-        "device, not wasting our time.\n", p->host_no, CTL_OF_SCB(scb));
-    unpause_sequencer(p, FALSE);
-    DRIVER_UNLOCK
-    return(SCSI_ABORT_PENDING);
-  }
-
-  found = 0;
-  p->flags |= AHC_IN_ABORT;
-  if (aic7xxx_verbose & VERBOSE_ABORT)
-    printk(INFO_LEAD "Aborting scb %d, flags 0x%x\n",
-         p->host_no, CTL_OF_SCB(scb), scb->hscb->tag, scb->flags);
-
-/*
- *   First, let's check to see if the currently running command is our target
- *    since if it is, the return is fairly easy and quick since we don't want
- *    to touch the command in case it might complete, but we do want a timeout
- *    in case it's actually hung, so we really do nothing, but tell the mid
- *    level code to reset the timeout.
- */
-
-  if ( scb->hscb->tag == aic_inb(p, SCB_TAG) )
-  {
-   /*
-    *  Check to see if the sequencer is just sitting on this command, or
-    *   if it's actively being run.
-    */
-    result = aic_inb(p, LASTPHASE);
-    switch (result)
-    {
-      case P_DATAOUT:    /*    For any of these cases, we can assume we are */
-      case P_DATAIN:     /*    an active command and act according.  For    */
-      case P_COMMAND:    /*    anything else we are going to fall on through*/
-      case P_STATUS:     /*    The SCSI_ABORT_SNOOZE will give us two abort */
-      case P_MESGOUT:    /*    chances to finish and then escalate to a     */
-      case P_MESGIN:     /*    reset call                                   */
-        if (aic7xxx_verbose & VERBOSE_ABORT_PROCESS)
-          printk(INFO_LEAD "SCB is currently active.  "
-                "Waiting on completion.\n", p->host_no, CTL_OF_SCB(scb));
-        unpause_sequencer(p, FALSE);
-        p->flags &= ~AHC_IN_ABORT;
-        scb->flags |= SCB_RECOVERY_SCB; /*  Note the fact that we've been  */
-        p->flags |= AHC_ABORT_PENDING;  /*  here so we will know not to    */
-        DRIVER_UNLOCK                   /*  muck with other SCBs if this   */
-        return(SCSI_ABORT_PENDING);     /*  one doesn't complete and clear */
-        break;                          /*  out.                           */
-      default:
-        break;
-    }
-  }
-
-  if ((found == 0) && (scb->flags & SCB_WAITINGQ))
-  {
-    int tindex = TARGET_INDEX(cmd);
-    unsigned short mask;
-
-    mask = (1 << tindex);
-
-    if (p->dtr_pending & mask)
-    {
-      if (p->dev_dtr_cmnd[tindex]->next != cmd)
-        found = 1;
-      else
-        found = 0;
-    }
-    else
-    {
-      found = 1;
-    }
-    if (found == 0)
-    {
-      /*
-       * OK..this means the command we are currently getting an abort
-       * for has an outstanding negotiation command in front of it.
-       * We don't really have a way to tie back into the negotiation
-       * commands, so we just send this back as pending, then it
-       * will get reset in 2 seconds.
-       */
-      unpause_sequencer(p, TRUE);
-      scb->flags |= SCB_ABORT;
-      DRIVER_UNLOCK
-      return(SCSI_ABORT_PENDING);
-    }
-    if (aic7xxx_verbose & VERBOSE_ABORT_PROCESS) 
-      printk(INFO_LEAD "SCB found on waiting list and "
-          "aborted.\n", p->host_no, CTL_OF_SCB(scb));
-    scbq_remove(&p->waiting_scbs, scb);
-    scbq_remove(&p->delayed_scbs[tindex], scb);
-    p->dev_active_cmds[tindex]++;
-    p->activescbs++;
-    scb->flags &= ~(SCB_WAITINGQ | SCB_ACTIVE);
-    scb->flags |= SCB_ABORT | SCB_QUEUED_FOR_DONE;
-    found = 1;
-  }
-
-/*
- *  We just checked the waiting_q, now for the QINFIFO
- */
-  if ( found == 0 )
-  {
-    if ( ((found = aic7xxx_search_qinfifo(p, cmd->target, 
-                     cmd->channel,
-                     cmd->lun, scb->hscb->tag, SCB_ABORT | SCB_QUEUED_FOR_DONE,
-                     FALSE, NULL)) != 0) &&
-                    (aic7xxx_verbose & VERBOSE_ABORT_PROCESS))
-      printk(INFO_LEAD "SCB found in QINFIFO and "
-        "aborted.\n", p->host_no, CTL_OF_SCB(scb));
-  }
-
-/*
- *  QINFIFO, waitingq, completeq done.  Next, check WAITING_SCB list in card
- */
-
-  if ( found == 0 )
-  {
-    unsigned char scb_next_ptr;
-    prev_hscbptr = SCB_LIST_NULL;
-    saved_hscbptr = aic_inb(p, SCBPTR);
-    next_hscbptr = aic_inb(p, WAITING_SCBH);
-    while ( next_hscbptr != SCB_LIST_NULL )
-    {
-      aic_outb(p,  next_hscbptr, SCBPTR );
-      if ( scb->hscb->tag == aic_inb(p, SCB_TAG) )
-      {
-        found = 1;
-        if (aic7xxx_verbose & VERBOSE_ABORT_PROCESS)
-          printk(INFO_LEAD "SCB found on hardware waiting"
-            " list and aborted.\n", p->host_no, CTL_OF_SCB(scb));
-        if ( prev_hscbptr == SCB_LIST_NULL )
-        {
-            aic_outb(p, aic_inb(p, SCB_NEXT), WAITING_SCBH);
-            /* stop the selection since we just
-             * grabbed the scb out from under the
-             * card
-             */
-            aic_outb(p, aic_inb(p, SCSISEQ) & ~ENSELO, SCSISEQ);
-            aic_outb(p, CLRSELTIMEO, CLRSINT1);
-        }
-        else
-        {
-            scb_next_ptr = aic_inb(p, SCB_NEXT);
-            aic_outb(p, prev_hscbptr, SCBPTR);
-            aic_outb(p, scb_next_ptr, SCB_NEXT);
-            aic_outb(p, next_hscbptr, SCBPTR);
-        }
-        aic_outb(p, SCB_LIST_NULL, SCB_TAG);
-        aic_outb(p, 0, SCB_CONTROL);
-        aic7xxx_add_curscb_to_free_list(p);
-        scb->flags = SCB_ABORT | SCB_QUEUED_FOR_DONE;
-        break;
-      }
-      prev_hscbptr = next_hscbptr;
-      next_hscbptr = aic_inb(p, SCB_NEXT);
-    }
-    aic_outb(p,  saved_hscbptr, SCBPTR );
-  }
-        
-/*
- *  Hmmm...completeq, QOUTFIFO, QINFIFO, WAITING_SCBH, waitingq all checked.
- *  OK...the sequencer's paused, interrupts are off, and we haven't found the
- *  command anyplace where it could be easily aborted.  Time for the hard
- *  work.  We also know the command is valid.  This essentially means the
- *  command is disconnected, or connected but not into any phases yet, which
- *  we know due to the tests we ran earlier on the current active scb phase.
- *  At this point we can queue the abort tag and go on with life.
- */
-
-  if ( found == 0 )
-  {
-    p->flags |= AHC_ABORT_PENDING;
-    scb->flags |= SCB_QUEUED_ABORT | SCB_ABORT | SCB_RECOVERY_SCB;
-    scb->hscb->control |= MK_MESSAGE;
-    result=aic7xxx_find_scb(p, scb);
-    if ( result != SCB_LIST_NULL ) 
-    {
-      saved_hscbptr = aic_inb(p, SCBPTR);
-      aic_outb(p, result, SCBPTR);
-      tmp_char = aic_inb(p, SCB_CONTROL);
-      aic_outb(p,  tmp_char | MK_MESSAGE, SCB_CONTROL);
-      aic_outb(p, saved_hscbptr, SCBPTR);
-    }
-    if (aic7xxx_verbose & VERBOSE_ABORT_PROCESS)
-      printk(INFO_LEAD "SCB disconnected.  Queueing Abort"
-        " SCB.\n", p->host_no, CTL_OF_SCB(scb));
-    p->qinfifo[p->qinfifonext++] = scb->hscb->tag;
-    if (p->features & AHC_QUEUE_REGS)
-      aic_outb(p, p->qinfifonext, HNSCB_QOFF);
-    else
-      aic_outb(p, p->qinfifonext, KERNEL_QINPOS);
-  }
-  if (found)
-  {
-    aic7xxx_run_done_queue(p, TRUE);
-    aic7xxx_run_waiting_queues(p);
-  }
-  p->flags &= ~AHC_IN_ABORT;
-  unpause_sequencer(p, FALSE);
-  DRIVER_UNLOCK
-
-/*
- *  On the return value.  If we found the command and aborted it, then we know
- *  it's already sent back and there is no reason for a further timeout, so
- *  we use SCSI_ABORT_SUCCESS.  On the queued abort side, we aren't so certain
- *  there hasn't been a bus hang or something that might keep the abort from
- *  from completing.  Therefore, we use SCSI_ABORT_PENDING.  The first time this
- *  is passed back, the timeout on the command gets extended, the second time
- *  we pass this back, the mid level SCSI code calls our reset function, which
- *  would shake loose a hung bus.
- */
-  if ( found != 0 )
-    return(SCSI_ABORT_SUCCESS);
-  else
-    return(SCSI_ABORT_PENDING); 
-}
-
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_reset
- *
- * Description:
- *   Resetting the bus always succeeds - is has to, otherwise the
- *   kernel will panic! Try a surgical technique - sending a BUS
- *   DEVICE RESET message - on the offending target before pulling
- *   the SCSI bus reset line.
- *-F*************************************************************************/
-int
-aic7xxx_reset(Scsi_Cmnd *cmd, unsigned int flags)
-{
-  struct aic7xxx_scb *scb = NULL;
-  struct aic7xxx_host *p;
-  int    tindex;
-  int    result = -1;
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,1,95)
-  unsigned long cpu_flags = 0;
-#endif
-#define DEVICE_RESET 0x01
-#define BUS_RESET    0x02
-#define HOST_RESET   0x04
-#define FAIL         0x08
-#define RESET_DELAY  0x10
-  int        action;
-  Scsi_Cmnd *cmd_prev, *cmd_next;
-
-
-  if ( cmd == NULL )
-  {
-    printk(KERN_WARNING "(scsi?:?:?:?) Reset called with NULL Scsi_Cmnd "
-      "pointer, failing.\n");
-    return(SCSI_RESET_SNOOZE);
-  }
-
-  p = (struct aic7xxx_host *) cmd->host->hostdata;
-  scb = (p->scb_data->scb_array[aic7xxx_position(cmd)]);
-  tindex = TARGET_INDEX(cmd);
-
-  /*
-   * I added a new config option to the driver: "panic_on_abort" that will
-   * cause the driver to panic and the machine to stop on the first abort
-   * or reset call into the driver.  At that point, it prints out a lot of
-   * usefull information for me which I can then use to try and debug the
-   * problem.  Simply enable the boot time prompt in order to activate this
-   * code.
-   */
-  if (aic7xxx_panic_on_abort)
-    aic7xxx_panic_abort(p, cmd);
-
-  DRIVER_LOCK
-
-  pause_sequencer(p);
-  while ( (aic_inb(p, INTSTAT) & INT_PEND) && !(p->flags & AHC_IN_ISR))
-  {
-    aic7xxx_isr(p->irq, p, (void *)NULL );
-    pause_sequencer(p);
-    aic7xxx_done_cmds_complete(p);
-  }
-
-  if (scb == NULL)
-  {
-    if (aic7xxx_verbose & VERBOSE_RESET_MID)
-      printk(INFO_LEAD "Reset called with bogus Scsi_Cmnd"
-           "->SCB mapping, improvising.\n", p->host_no, CTL_OF_CMD(cmd));
-    if ( flags & SCSI_RESET_SUGGEST_HOST_RESET )
-    {
-      action = HOST_RESET;
-    }
-    else
-    {
-      action = BUS_RESET;
-    }
-  }
-  else if (scb->cmd != cmd) 
-  {
-    if (aic7xxx_verbose & VERBOSE_RESET_MID)
-    printk(INFO_LEAD "Reset called with recycled SCB "
-        "for cmd.\n", p->host_no, CTL_OF_CMD(cmd));
-    cmd_prev = NULL;
-    cmd_next = p->completeq.head;
-    while ( cmd_next != NULL )
-    {
-      if (cmd_next == cmd)
-      {
-        if (aic7xxx_verbose & VERBOSE_RESET_RETURN)
-          printk(INFO_LEAD "Reset, found cmd on completeq"
-          ", completing.\n", p->host_no, CTL_OF_CMD(cmd));
-        unpause_sequencer(p, FALSE);
-        DRIVER_UNLOCK
-        return(SCSI_RESET_NOT_RUNNING);
-      }
-      cmd_prev = cmd_next;
-      cmd_next = (Scsi_Cmnd *)cmd_next->host_scribble;
-    }
-    if ( !(flags & SCSI_RESET_SYNCHRONOUS) )
-    {
-      if (aic7xxx_verbose & VERBOSE_RESET_RETURN)
-        printk(INFO_LEAD "Reset, cmd not found,"
-          " failing.\n", p->host_no, CTL_OF_CMD(cmd));
-      unpause_sequencer(p, FALSE);
-      DRIVER_UNLOCK
-      return(SCSI_RESET_NOT_RUNNING);
-    }
-    else
-    {
-      if (aic7xxx_verbose & VERBOSE_RESET_MID)
-        printk(INFO_LEAD "Reset called, no scb, "
-          "flags 0x%x\n", p->host_no, CTL_OF_CMD(cmd), flags);
-      scb = NULL;
-      action = HOST_RESET;
-    }
-  }
-  else
-  {
-    if (aic7xxx_verbose & VERBOSE_RESET_MID)
-      printk(INFO_LEAD "Reset called, scb %d, flags "
-        "0x%x\n", p->host_no, CTL_OF_SCB(scb), scb->hscb->tag, scb->flags);
-    if ( aic7xxx_scb_on_qoutfifo(p, scb) )
-    {
-      if(aic7xxx_verbose & VERBOSE_RESET_RETURN)
-        printk(INFO_LEAD "SCB on qoutfifo, completing.\n", p->host_no,
-          CTL_OF_SCB(scb));
-      if ((aic_inb(p,INTSTAT) & CMDCMPLT) == 0)
-        printk(INFO_LEAD "missed CMDCMPLT interrupt!\n", p->host_no,
-          CTL_OF_SCB(scb));
-      aic7xxx_handle_command_completion_intr(p);
-      aic7xxx_done_cmds_complete(p);
-      aic7xxx_run_waiting_queues(p);
-      unpause_sequencer(p, FALSE);
-      DRIVER_UNLOCK
-      return(SCSI_RESET_SUCCESS);
-    }
-    if ( flags & SCSI_RESET_SUGGEST_HOST_RESET )
-    {
-      action = HOST_RESET;
-    }
-    else if ( flags & SCSI_RESET_SUGGEST_BUS_RESET )
-    {
-      action = BUS_RESET;
-    }
-    else 
-    {
-      action = DEVICE_RESET;
-    }
-  }
-  if ( (action & DEVICE_RESET) && 
-        (p->dev_flags[tindex] & BUS_DEVICE_RESET_PENDING) )
-  {
-    if (aic7xxx_verbose & VERBOSE_RESET_PROCESS)
-      printk(INFO_LEAD "Bus device reset already sent to "
-        "device, escalating.\n", p->host_no, CTL_OF_CMD(cmd));
-    action = BUS_RESET;
-  }
-  if ( (action & DEVICE_RESET) &&
-       (scb->flags & SCB_QUEUED_ABORT) )
-  {
-    if (aic7xxx_verbose & VERBOSE_RESET_PROCESS)
-    {
-      printk(INFO_LEAD "Have already attempted to reach "
-        "device with queued\n", p->host_no, CTL_OF_CMD(cmd));
-      printk(INFO_LEAD "message, will escalate to bus "
-        "reset.\n", p->host_no, CTL_OF_CMD(cmd));
-    }
-    action = BUS_RESET;
-  }
-  if ( (action & DEVICE_RESET) && 
-       (p->flags & (AHC_RESET_PENDING | AHC_ABORT_PENDING)) )
-  {
-    if (aic7xxx_verbose & VERBOSE_RESET_PROCESS)
-     printk(INFO_LEAD "Bus device reset stupid when "
-        "other action has failed.\n", p->host_no, CTL_OF_CMD(cmd));
-    action = BUS_RESET;
-  }
-  if ( (action & BUS_RESET) && !(p->features & AHC_TWIN) )
-  {
-    action = HOST_RESET;
-  }
-  if ( (p->dev_flags[tindex] & DEVICE_RESET_DELAY) &&
-       !(action & (HOST_RESET | BUS_RESET)))
-  {
-    if (aic7xxx_verbose & VERBOSE_RESET_PROCESS)
-    {
-      printk(INFO_LEAD "Reset called too soon after last "
-        "reset without requesting\n", p->host_no, CTL_OF_CMD(cmd));
-      printk(INFO_LEAD "bus or host reset, escalating.\n", p->host_no,
-        CTL_OF_CMD(cmd));
-    }
-    action = BUS_RESET;
-  }
-  if ( (p->flags & AHC_RESET_DELAY) &&
-       (action & (HOST_RESET | BUS_RESET)) )
-  {
-    if (aic7xxx_verbose & VERBOSE_RESET_PROCESS)
-      printk(INFO_LEAD "Reset called too soon after "
-        "last bus reset, delaying.\n", p->host_no, CTL_OF_CMD(cmd));
-    action = RESET_DELAY;
-  }
-/*
- *  By this point, we want to already know what we are going to do and
- *  only have the following code implement our course of action.
- */
-  switch (action)
-  {
-    case RESET_DELAY:
-      unpause_sequencer(p, FALSE);
-      DRIVER_UNLOCK
-      return(SCSI_RESET_PENDING);
-      break;
-    case FAIL:
-      unpause_sequencer(p, FALSE);
-      DRIVER_UNLOCK
-      return(SCSI_RESET_ERROR);
-      break;
-    case DEVICE_RESET:
-      p->flags |= AHC_IN_RESET;
-      result = aic7xxx_bus_device_reset(p, cmd);
-      aic7xxx_run_done_queue(p, TRUE);
-      /*  We can't rely on run_waiting_queues to unpause the sequencer for
-       *  PCI based controllers since we use AAP */
-      aic7xxx_run_waiting_queues(p);
-      unpause_sequencer(p, FALSE);
-      p->flags &= ~AHC_IN_RESET;
-      DRIVER_UNLOCK
-      return(result);
-      break;
-    case BUS_RESET:
-    case HOST_RESET:
-    default:
-      p->flags |= AHC_IN_RESET | AHC_RESET_DELAY;
-      p->dev_expires[p->scsi_id] = jiffies + (3 * HZ);
-      p->dev_timer_active |= (0x01 << p->scsi_id);
-      if ( !(p->dev_timer_active & (0x01 << MAX_TARGETS)) ||
-            time_after_eq(p->dev_timer.expires, p->dev_expires[p->scsi_id]) )
-      {
-        del_timer(&p->dev_timer);
-        p->dev_timer.expires = p->dev_expires[p->scsi_id];
-        add_timer(&p->dev_timer);
-        p->dev_timer_active |= (0x01 << MAX_TARGETS);
-      }
-      aic7xxx_reset_channel(p, cmd->channel, TRUE);
-      if ( (p->features & AHC_TWIN) && (action & HOST_RESET) )
-      {
-        aic7xxx_reset_channel(p, cmd->channel ^ 0x01, TRUE);
-        restart_sequencer(p);
-      }
-      if (action != HOST_RESET)
-        result = SCSI_RESET_SUCCESS | SCSI_RESET_BUS_RESET;
-      else
-      {
-        result = SCSI_RESET_SUCCESS | SCSI_RESET_HOST_RESET;
-        aic_outb(p,  aic_inb(p, SIMODE1) & ~(ENREQINIT|ENBUSFREE),
-          SIMODE1);
-        aic7xxx_clear_intstat(p);
-        p->flags &= ~AHC_HANDLING_REQINITS;
-        p->msg_type = MSG_TYPE_NONE;
-        p->msg_index = 0;
-        p->msg_len = 0;
-      }
-      aic7xxx_run_done_queue(p, TRUE);
-      /*
-       * If this a SCSI_RESET_SYNCHRONOUS then the command we were given is
-       * in need of being re-started, so send it on through to aic7xxx_queue
-       * and let it set until the delay is over.  This keeps it from dying
-       * entirely and avoids getting a bogus dead command back through the
-       * mid-level code due to too many retries.
-       */
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,1,132)
-      if ( flags & SCSI_RESET_SYNCHRONOUS )
-      {
-        cmd->result = DID_BUS_BUSY << 16;
-        cmd->done(cmd);
-      }
-#endif
-      p->flags &= ~AHC_IN_RESET;
-      /*
-       * We can't rely on run_waiting_queues to unpause the sequencer for
-       * PCI based controllers since we use AAP.  NOTE: this also sets
-       * the timer for the one command we might have queued in the case
-       * of a synch reset.
-       */
-      aic7xxx_run_waiting_queues(p);
-      unpause_sequencer(p, FALSE);
-      DRIVER_UNLOCK
-      return(result);
-      break;
-  }
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_biosparam
- *
- * Description:
- *   Return the disk geometry for the given SCSI device.
- *-F*************************************************************************/
-int
-aic7xxx_biosparam(Disk *disk, kdev_t dev, int geom[])
-{
-  int heads, sectors, cylinders, ret;
-  struct aic7xxx_host *p;
-  struct buffer_head *bh;
-
-  p = (struct aic7xxx_host *) disk->device->host->hostdata;
-  bh = bread(MKDEV(MAJOR(dev), MINOR(dev)&~0xf), 0, 1024);
-
-  if ( bh )
-  {
-    ret = scsi_partsize(bh, disk->capacity, &geom[2], &geom[0], &geom[1]);
-    brelse(bh);
-    if ( ret != -1 )
-      return(ret);
-  }
-  
-  heads = 64;
-  sectors = 32;
-  cylinders = disk->capacity / (heads * sectors);
-
-  if ((p->flags & AHC_EXTEND_TRANS_A) && (cylinders > 1024))
-  {
-    heads = 255;
-    sectors = 63;
-    cylinders = disk->capacity / (heads * sectors);
-  }
-
-  geom[0] = heads;
-  geom[1] = sectors;
-  geom[2] = cylinders;
-
-  return (0);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_release
- *
- * Description:
- *   Free the passed in Scsi_Host memory structures prior to unloading the
- *   module.
- *-F*************************************************************************/
-int
-aic7xxx_release(struct Scsi_Host *host)
-{
-  struct aic7xxx_host *p = (struct aic7xxx_host *) host->hostdata;
-  struct aic7xxx_host *next, *prev;
-
-  if(p->irq)
-    free_irq(p->irq, p);
-  if(p->base)
-    release_region(p->base, MAXREG - MINREG);
-#ifdef MMAPIO
-  if(p->maddr)
-  {
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,1,0)
-    vfree((void *) (((unsigned long) p->maddr) & PAGE_MASK));
-#else
-    iounmap((void *) (((unsigned long) p->maddr) & PAGE_MASK));
-#endif
-  }
-#endif /* MMAPIO */
-  prev = NULL;
-  next = first_aic7xxx;
-  while(next != NULL)
-  {
-    if(next == p)
-    {
-      if(prev == NULL)
-        first_aic7xxx = next->next;
-      else
-        prev->next = next->next;
-    }
-    else
-    {
-      prev = next;
-    }
-    next = next->next;
-  }
-  aic7xxx_free(p);
-  return(0);
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_print_card
- *
- * Description:
- *   Print out all of the control registers on the card
- *
- *   NOTE: This function is not yet safe for use on the VLB and EISA
- *   controllers, so it isn't used on those controllers at all.
- *-F*************************************************************************/
-static void
-aic7xxx_print_card(struct aic7xxx_host *p)
-{
-  int i, j, k, chip;
-  static struct register_ranges {
-    int num_ranges;
-    int range_val[32];
-  } cards_ds[] = {
-    { 0, {0,} }, /* none */
-    {10, {0x00, 0x05, 0x08, 0x11, 0x18, 0x19, 0x1f, 0x1f, 0x60, 0x60, /*7771*/
-          0x62, 0x66, 0x80, 0x8e, 0x90, 0x95, 0x97, 0x97, 0x9b, 0x9f} },
-    { 9, {0x00, 0x05, 0x08, 0x11, 0x18, 0x1f, 0x60, 0x60, 0x62, 0x66, /*7850*/
-          0x80, 0x8e, 0x90, 0x95, 0x97, 0x97, 0x9a, 0x9f} },
-    { 9, {0x00, 0x05, 0x08, 0x11, 0x18, 0x1f, 0x60, 0x60, 0x62, 0x66, /*7860*/
-          0x80, 0x8e, 0x90, 0x95, 0x97, 0x97, 0x9a, 0x9f} },
-    {10, {0x00, 0x05, 0x08, 0x11, 0x18, 0x19, 0x1c, 0x1f, 0x60, 0x60, /*7870*/
-          0x62, 0x66, 0x80, 0x8e, 0x90, 0x95, 0x97, 0x97, 0x9a, 0x9f} },
-    {10, {0x00, 0x05, 0x08, 0x11, 0x18, 0x1a, 0x1c, 0x1f, 0x60, 0x60, /*7880*/
-          0x62, 0x66, 0x80, 0x8e, 0x90, 0x95, 0x97, 0x97, 0x9a, 0x9f} },
-    {16, {0x00, 0x05, 0x08, 0x11, 0x18, 0x1f, 0x60, 0x60, 0x62, 0x66, /*7890*/
-          0x84, 0x8e, 0x90, 0x95, 0x97, 0x97, 0x9a, 0x9a, 0x9f, 0x9f,
-          0xe0, 0xf1, 0xf4, 0xf4, 0xf6, 0xf6, 0xf8, 0xf8, 0xfa, 0xfc,
-          0xfe, 0xff} },
-    {12, {0x00, 0x05, 0x08, 0x11, 0x18, 0x19, 0x1b, 0x1f, 0x60, 0x60, /*7895*/
-          0x62, 0x66, 0x80, 0x8e, 0x90, 0x95, 0x97, 0x97, 0x9a, 0x9a,
-          0x9f, 0x9f, 0xe0, 0xf1} },
-    {16, {0x00, 0x05, 0x08, 0x11, 0x18, 0x1f, 0x60, 0x60, 0x62, 0x66, /*7896*/
-          0x84, 0x8e, 0x90, 0x95, 0x97, 0x97, 0x9a, 0x9a, 0x9f, 0x9f,
-          0xe0, 0xf1, 0xf4, 0xf4, 0xf6, 0xf6, 0xf8, 0xf8, 0xfa, 0xfc,
-          0xfe, 0xff} },
-    {12, {0x00, 0x05, 0x08, 0x11, 0x18, 0x1f, 0x60, 0x60, 0x62, 0x66, /*7892*/
-          0x84, 0x8e, 0x90, 0x95, 0x97, 0x97, 0x9a, 0x9a, 0x9c, 0x9f,
-          0xe0, 0xf1, 0xf4, 0xfc} },
-    {12, {0x00, 0x05, 0x08, 0x11, 0x18, 0x1f, 0x60, 0x60, 0x62, 0x66, /*7899*/
-          0x84, 0x8e, 0x90, 0x95, 0x97, 0x97, 0x9a, 0x9a, 0x9c, 0x9f,
-          0xe0, 0xf1, 0xf4, 0xfc} },
-  };
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,1,0)
-#ifdef CONFIG_PCI
-  static struct register_ranges cards_ns[] = {
-    { 0, {0,} }, /* none */
-    { 0, {0,} }, /* 7771 */
-    { 7, {0x04, 0x08, 0x0c, 0x0e, 0x10, 0x17, 0x28, 0x2b, 0x30, 0x33,
-          0x3c, 0x41, 0x43, 0x47} },
-    { 7, {0x04, 0x08, 0x0c, 0x0e, 0x10, 0x17, 0x28, 0x2b, 0x30, 0x33,
-          0x3c, 0x41, 0x43, 0x47} },
-    { 5, {0x04, 0x08, 0x0c, 0x0e, 0x10, 0x17, 0x30, 0x33, 0x3c, 0x41} },
-    { 5, {0x04, 0x08, 0x0c, 0x0e, 0x10, 0x17, 0x30, 0x34, 0x3c, 0x47} },
-    { 5, {0x04, 0x08, 0x0c, 0x1b, 0x30, 0x34, 0x3c, 0x43, 0xdc, 0xe3} },
-    { 6, {0x04, 0x08, 0x0c, 0x0e, 0x10, 0x17, 0x30, 0x34, 0x3c, 0x47,
-          0xdc, 0xe3} },
-    { 6, {0x04, 0x08, 0x0c, 0x1b, 0x30, 0x34, 0x3c, 0x43, 0xdc, 0xe3,
-          0xff, 0xff} },
-    { 6, {0x04, 0x08, 0x0c, 0x1b, 0x30, 0x34, 0x3c, 0x43, 0xdc, 0xe3,
-          0xff, 0xff} },
-    { 6, {0x04, 0x08, 0x0c, 0x1b, 0x30, 0x34, 0x3c, 0x43, 0xdc, 0xe3,
-          0xff, 0xff} }
-  };
-#endif
-#endif
-  chip = p->chip & AHC_CHIPID_MASK;
-  /*
-   * Let's run through the PCI space first....
-   */
-  printk("%s at ",
-         board_names[p->board_name_index]);
-  switch(p->chip & ~AHC_CHIPID_MASK)
-  {
-    case AHC_VL:
-      printk("VLB Slot %d.\n", p->pci_device_fn);
-      break;
-    case AHC_EISA:
-      printk("EISA Slot %d.\n", p->pci_device_fn);
-      break;
-    case AHC_PCI:
-    default:
-      printk("PCI %d/%d/%d.\n", p->pci_bus, PCI_SLOT(p->pci_device_fn),
-             PCI_FUNC(p->pci_device_fn));
-      break;
-  }
-
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,1,0)
-#ifdef CONFIG_PCI
-  {
-    unsigned char temp;
-    
-    printk("PCI Dump:\n");
-    k=0;
-    for(i=0; i<cards_ns[chip].num_ranges; i++)
-    {
-      for(j  = cards_ns[chip].range_val[ i * 2 ];
-          j <= cards_ns[chip].range_val[ i * 2 + 1 ] ;
-          j++)
-      {
-        pcibios_read_config_byte(p->pci_bus, p->pci_device_fn, j, &temp);
-        printk("%02x:%02x ", j, temp);
-        if(++k == 13)
-        {
-          printk("\n");
-          k = 0;
-        }
-      }
-    }
-  }
-  if(k != 0)
-    printk("\n");
-#endif /* CONFIG_PCI */
-#endif
-
-  /*
-   * Now the registers on the card....
-   */
-  printk("Card Dump:\n");
-  k = 0;
-  for(i=0; i<cards_ds[chip].num_ranges; i++)
-  {
-    for(j  = cards_ds[chip].range_val[ i * 2 ];
-        j <= cards_ds[chip].range_val[ i * 2 + 1 ] ;
-        j++)
-    {
-      printk("%02x:%02x ", j, aic_inb(p, j));
-      if(++k == 13)
-      {
-        printk("\n");
-        k=0;
-      }
-    }
-  }
-  if(k != 0)
-    printk("\n");
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,1,0)
-  if (p->flags & AHC_SEEPROM_FOUND)
-  {
-    unsigned short *sc1;
-    sc1 = (unsigned short *)&p->sc;
-    
-    printk("SEEPROM dump.\n");
-    for(i=1; i<=32; i++)
-    {
-      printk("0x%04x", sc1[i-1]);
-      if ( (i % 8) == 0 )
-        printk("\n");
-      else
-        printk("  ");
-    }
-  }
-#endif
-
-  /*
-   * If this was an Ultra2 controller, then we just hosed the card in terms
-   * of the QUEUE REGS.  This function is only called at init time or by
-   * the panic_abort function, so it's safe to assume a generic init time
-   * setting here
-   */
-
-  if(p->features & AHC_QUEUE_REGS)
-  {
-    aic_outb(p, 0, SDSCB_QOFF);
-    aic_outb(p, 0, SNSCB_QOFF);
-    aic_outb(p, 0, HNSCB_QOFF);
-  }
-
-}
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_print_scratch_ram
- *
- * Description:
- *   Print out the scratch RAM values on the card.
- *-F*************************************************************************/
-static void
-aic7xxx_print_scratch_ram(struct aic7xxx_host *p)
-{
-  int i, k;
-
-  k = 0;
-  printk("Scratch RAM:\n");
-  for(i = SRAM_BASE; i < SEQCTL; i++)
-  {
-    printk("%02x:%02x ", i, aic_inb(p, i));
-    if(++k == 13)
-    {
-      printk("\n");
-      k=0;
-    }
-  }
-  if (p->features & AHC_MORE_SRAM)
-  {
-    for(i = TARG_OFFSET; i < 0x80; i++)
-    {
-      printk("%02x:%02x ", i, aic_inb(p, i));
-      if(++k == 13)
-      {
-        printk("\n");
-        k=0;
-      }
-    }
-  }
-  printk("\n");
-}
-
-
-#include "aic7xxx_proc.c"
-
-#ifdef MODULE
-/* Eventually this will go into an include file, but this will be later */
-Scsi_Host_Template driver_template = AIC7XXX;
-
-#include "scsi_module.c"
-#endif
-
-/*
- * Overrides for Emacs so that we almost follow Linus's tabbing style.
- * Emacs will notice this stuff at the end of the file and automatically
- * adjust the settings for this buffer only.  This must remain at the end
- * of the file.
- * ---------------------------------------------------------------------------
- * Local variables:
- * c-indent-level: 2
- * c-brace-imaginary-offset: 0
- * c-brace-offset: -2
- * c-argdecl-indent: 2
- * c-label-offset: -2
- * c-continued-statement-offset: 2
- * c-continued-brace-offset: 0
- * indent-tabs-mode: nil
- * tab-width: 8
- * End:
- */
diff -urN linux/drivers/scsi/aic7xxx.h /tmp/linux/drivers/scsi/aic7xxx.h
--- linux/drivers/scsi/aic7xxx.h	Tue May 11 11:36:28 1999
+++ /tmp/linux/drivers/scsi/aic7xxx.h	Wed Dec 31 17:00:00 1969
@@ -1,114 +0,0 @@
-/*+M*************************************************************************
- * Adaptec AIC7xxx device driver for Linux.
- *
- * Copyright (c) 1994 John Aycock
- *   The University of Calgary Department of Computer Science.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2, or (at your option)
- * any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; see the file COPYING.  If not, write to
- * the Free Software Foundation, 675 Mass Ave, Cambridge, MA 02139, USA.
- * 
- * $Id: aic7xxx.h,v 3.2 1996/07/23 03:37:26 deang Exp $
- *-M*************************************************************************/
-#ifndef _aic7xxx_h
-#define _aic7xxx_h
-
-#define AIC7XXX_H_VERSION  "3.2.4"
-
-#ifndef LINUX_VERSION_CODE
-#include <linux/version.h>
-#endif
-
-#ifndef KERNEL_VERSION
-#define KERNEL_VERSION(x,y,z) (((x)<<16)+((y)<<8)+(z))
-#endif
-
-#if defined(__i386__)
-#  define AIC7XXX_BIOSPARAM aic7xxx_biosparam
-#else
-#  define AIC7XXX_BIOSPARAM NULL
-#endif
-
-/*
- * Scsi_Host_Template (see hosts.h) for AIC-7xxx - some fields
- * to do with card config are filled in after the card is detected.
- */
-#if LINUX_VERSION_CODE > KERNEL_VERSION(2,1,65)
-#define AIC7XXX	{						\
-	next: NULL,						\
-	module: NULL,						\
-	proc_dir: NULL,						\
-	proc_info: aic7xxx_proc_info,				\
-	name: NULL,						\
-	detect: aic7xxx_detect,					\
-	release: aic7xxx_release,				\
-	info: aic7xxx_info,					\
-	command: NULL,						\
-	queuecommand: aic7xxx_queue,				\
-	eh_strategy_handler: NULL,				\
-	eh_abort_handler: NULL,					\
-	eh_device_reset_handler: NULL,				\
-	eh_bus_reset_handler: NULL,				\
-	eh_host_reset_handler: NULL,				\
-	abort: aic7xxx_abort,					\
-	reset: aic7xxx_reset,					\
-	slave_attach: NULL,					\
-	bios_param: AIC7XXX_BIOSPARAM,				\
-	can_queue: 255,		/* max simultaneous cmds      */\
-	this_id: -1,		/* scsi id of host adapter    */\
-	sg_tablesize: 0,	/* max scatter-gather cmds    */\
-	cmd_per_lun: 3,		/* cmds per lun (linked cmds) */\
-	present: 0,		/* number of 7xxx's present   */\
-	unchecked_isa_dma: 0,	/* no memory DMA restrictions */\
-	use_clustering: ENABLE_CLUSTERING,			\
-	use_new_eh_code: 0					\
-}
-#else
-#define AIC7XXX	{						\
-	next: NULL,						\
-	usage_count: NULL,					\
-	proc_dir: NULL, 					\
-	proc_info: aic7xxx_proc_info,				\
-	name: NULL,						\
-	detect: aic7xxx_detect,					\
-	release: aic7xxx_release,				\
-	info: aic7xxx_info,					\
-	command: NULL,						\
-	queuecommand: aic7xxx_queue,				\
-	abort: aic7xxx_abort,					\
-	reset: aic7xxx_reset,					\
-	slave_attach: NULL,					\
-	bios_param: AIC7XXX_BIOSPARAM,				\
-	can_queue: 255,		/* max simultaneous cmds      */\
-	this_id: -1,		/* scsi id of host adapter    */\
-	sg_tablesize: 0,	/* max scatter-gather cmds    */\
-	cmd_per_lun: 3,		/* cmds per lun (linked cmds) */\
-	present: 0,		/* number of 7xxx's present   */\
-	unchecked_isa_dma: 0,	/* no memory DMA restrictions */\
-	use_clustering: ENABLE_CLUSTERING			\
-}
-#endif
-
-extern int aic7xxx_queue(Scsi_Cmnd *, void (*)(Scsi_Cmnd *));
-extern int aic7xxx_biosparam(Disk *, kdev_t, int[]);
-extern int aic7xxx_detect(Scsi_Host_Template *);
-extern int aic7xxx_command(Scsi_Cmnd *);
-extern int aic7xxx_reset(Scsi_Cmnd *, unsigned int);
-extern int aic7xxx_abort(Scsi_Cmnd *);
-extern int aic7xxx_release(struct Scsi_Host *);
-
-extern const char *aic7xxx_info(struct Scsi_Host *);
-
-extern int aic7xxx_proc_info(char *, char **, off_t, int, int, int);
-
-#endif /* _aic7xxx_h */
diff -urN linux/drivers/scsi/aic7xxx_proc.c /tmp/linux/drivers/scsi/aic7xxx_proc.c
--- linux/drivers/scsi/aic7xxx_proc.c	Wed May  3 18:16:44 2000
+++ /tmp/linux/drivers/scsi/aic7xxx_proc.c	Wed Dec 31 17:00:00 1969
@@ -1,414 +0,0 @@
-/*+M*************************************************************************
- * Adaptec AIC7xxx device driver proc support for Linux.
- *
- * Copyright (c) 1995, 1996 Dean W. Gehnert
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2, or (at your option)
- * any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; see the file COPYING.  If not, write to
- * the Free Software Foundation, 675 Mass Ave, Cambridge, MA 02139, USA.
- *
- * ----------------------------------------------------------------
- *  o Modified from the EATA-DMA /proc support.
- *  o Additional support for device block statistics provided by
- *    Matthew Jacob.
- *  o Correction of overflow by Heinz Mauelshagen
- *  o Adittional corrections by Doug Ledford
- *
- *  Dean W. Gehnert, deang@teleport.com, 05/01/96
- *
- *  $Id: aic7xxx_proc.c,v 4.1 1997/06/97 08:23:42 deang Exp $
- *-M*************************************************************************/
-
-#include <linux/config.h>
-
-#define	BLS	(&aic7xxx_buffer[size])
-#define HDRB \
-"             < 2K      2K+     4K+     8K+    16K+    32K+    64K+   128K+"
-
-#ifdef PROC_DEBUG
-extern int vsprintf(char *, const char *, va_list);
-
-static void
-proc_debug(const char *fmt, ...)
-{
-  va_list ap;
-  char buf[256];
-
-  va_start(ap, fmt);
-  vsprintf(buf, fmt, ap);
-  printk(buf);
-  va_end(ap);
-}
-#else /* PROC_DEBUG */
-#  define proc_debug(fmt, args...)
-#endif /* PROC_DEBUG */
-
-static int aic7xxx_buffer_size = 0;
-static char *aic7xxx_buffer = NULL;
-
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_set_info
- *
- * Description:
- *   Set parameters for the driver from the /proc filesystem.
- *-F*************************************************************************/
-int
-aic7xxx_set_info(char *buffer, int length, struct Scsi_Host *HBAptr)
-{
-  proc_debug("aic7xxx_set_info(): %s\n", buffer);
-  return (-ENOSYS);  /* Currently this is a no-op */
-}
-
-
-/*+F*************************************************************************
- * Function:
- *   aic7xxx_proc_info
- *
- * Description:
- *   Return information to handle /proc support for the driver.
- *-F*************************************************************************/
-int
-aic7xxx_proc_info ( char *buffer, char **start, off_t offset, int length, 
-                    int hostno, int inout)
-{
-  struct Scsi_Host *HBAptr;
-  struct aic7xxx_host *p;
-  int    size = 0;
-  unsigned char i;
-  struct aic7xxx_xferstats *sp;
-  unsigned char target;
-
-  HBAptr = NULL;
-
-  for(p=first_aic7xxx; p->host->host_no != hostno; p=p->next)
-    ;
-
-  if (!p)
-  {
-    size += sprintf(buffer, "Can't find adapter for host number %d\n", hostno);
-    if (size > length)
-    {
-      return (size);
-    }
-    else
-    {
-      return (length);
-    }
-  }
-
-  HBAptr = p->host;
-
-  if (inout == TRUE) /* Has data been written to the file? */ 
-  {
-    return (aic7xxx_set_info(buffer, length, HBAptr));
-  }
-
-  p = (struct aic7xxx_host *) HBAptr->hostdata;
-
-  /*
-   * It takes roughly 1K of space to hold all relevant card info, not
-   * counting any proc stats, so we start out with a 1.5k buffer size and
-   * if proc_stats is defined, then we sweep the stats structure to see
-   * how many drives we will be printing out for and add 384 bytes per
-   * device with active stats.
-   *
-   * Hmmmm...that 1.5k seems to keep growing as items get added so they
-   * can be easily viewed for debugging purposes.  So, we bumped that
-   * 1.5k to 4k so we can quit having to bump it all the time.
-   */
-
-  size = 4096;
-  for (target = 0; target < MAX_TARGETS; target++)
-  {
-    if (p->dev_flags[target] & DEVICE_PRESENT)
-#ifdef AIC7XXX_PROC_STATS
-      size += 512;
-#else
-      size += 256;
-#endif
-  }
-  if (aic7xxx_buffer_size != size)
-  {
-    if (aic7xxx_buffer != NULL) 
-    {
-      kfree(aic7xxx_buffer);
-      aic7xxx_buffer_size = 0;
-    }
-    aic7xxx_buffer = kmalloc(size, GFP_KERNEL);
-  }
-  if (aic7xxx_buffer == NULL)
-  {
-    size = sprintf(buffer, "AIC7xxx - kmalloc error at line %d\n",
-        __LINE__);
-    return size;
-  }
-  aic7xxx_buffer_size = size;
-
-  size = 0;
-  size += sprintf(BLS, "Adaptec AIC7xxx driver version: ");
-  size += sprintf(BLS, "%s/", AIC7XXX_C_VERSION);
-  size += sprintf(BLS, "%s", AIC7XXX_H_VERSION);
-  size += sprintf(BLS, "\n");
-  size += sprintf(BLS, "Compile Options:\n");
-#ifdef CONFIG_AIC7XXX_TCQ_ON_BY_DEFAULT
-  size += sprintf(BLS, "  TCQ Enabled By Default : Enabled\n");
-#else
-  size += sprintf(BLS, "  TCQ Enabled By Default : Disabled\n");
-#endif
-#ifdef AIC7XXX_PROC_STATS
-  size += sprintf(BLS, "  AIC7XXX_PROC_STATS     : Enabled\n");
-#else
-  size += sprintf(BLS, "  AIC7XXX_PROC_STATS     : Disabled\n");
-#endif
-  size += sprintf(BLS, "  AIC7XXX_RESET_DELAY    : %d\n", AIC7XXX_RESET_DELAY);
-  size += sprintf(BLS, "\n");
-  size += sprintf(BLS, "Adapter Configuration:\n");
-  size += sprintf(BLS, "           SCSI Adapter: %s\n",
-      board_names[p->board_name_index]);
-  if (p->flags & AHC_TWIN)
-    size += sprintf(BLS, "                         Twin Channel Controller ");
-  else
-  {
-    char *channel = "";
-    char *ultra = "";
-    char *wide = "Narrow ";
-    if (p->flags & AHC_MULTI_CHANNEL)
-    {
-      channel = " Channel A";
-      if (p->flags & (AHC_CHNLB|AHC_CHNLC))
-        channel = (p->flags & AHC_CHNLB) ? " Channel B" : " Channel C";
-    }
-    if (p->features & AHC_WIDE)
-      wide = "Wide ";
-    if (p->features & AHC_ULTRA3)
-    {
-      switch(p->chip & AHC_CHIPID_MASK)
-      {
-        case AHC_AIC7892:
-        case AHC_AIC7899:
-          ultra = "Ultra-160/m LVD/SE ";
-          break;
-        default:
-          ultra = "Ultra-3 LVD/SE ";
-          break;
-      }
-    }
-    else if (p->features & AHC_ULTRA2)
-      ultra = "Ultra-2 LVD/SE ";
-    else if (p->features & AHC_ULTRA)
-      ultra = "Ultra ";
-    size += sprintf(BLS, "                           %s%sController%s ",
-      ultra, wide, channel);
-  }
-  switch(p->chip & ~AHC_CHIPID_MASK)
-  {
-    case AHC_VL:
-      size += sprintf(BLS, "at VLB slot %d\n", p->pci_device_fn);
-      break;
-    case AHC_EISA:
-      size += sprintf(BLS, "at EISA slot %d\n", p->pci_device_fn);
-      break;
-    default:
-      size += sprintf(BLS, "at PCI %d/%d/%d\n", p->pci_bus,
-        PCI_SLOT(p->pci_device_fn), PCI_FUNC(p->pci_device_fn));
-      break;
-  }
-  if( !(p->maddr) )
-  {
-    size += sprintf(BLS, "    Programmed I/O Base: %lx\n", p->base);
-  }
-  else
-  {
-    size += sprintf(BLS, "    PCI MMAPed I/O Base: 0x%lx\n", p->mbase);
-  }
-  if( (p->chip & (AHC_VL | AHC_EISA)) )
-  {
-    size += sprintf(BLS, "    BIOS Memory Address: 0x%08x\n", p->bios_address);
-  }
-  size += sprintf(BLS, " Adapter SEEPROM Config: %s\n",
-          (p->flags & AHC_SEEPROM_FOUND) ? "SEEPROM found and used." :
-         ((p->flags & AHC_USEDEFAULTS) ? "SEEPROM not found, using defaults." :
-           "SEEPROM not found, using leftover BIOS values.") );
-  size += sprintf(BLS, "      Adaptec SCSI BIOS: %s\n",
-          (p->flags & AHC_BIOS_ENABLED) ? "Enabled" : "Disabled");
-  size += sprintf(BLS, "                    IRQ: %d\n", HBAptr->irq);
-  size += sprintf(BLS, "                   SCBs: Active %d, Max Active %d,\n",
-            p->activescbs, p->max_activescbs);
-  size += sprintf(BLS, "                         Allocated %d, HW %d, "
-            "Page %d\n", p->scb_data->numscbs, p->scb_data->maxhscbs,
-            p->scb_data->maxscbs);
-  if (p->flags & AHC_EXTERNAL_SRAM)
-    size += sprintf(BLS, "                         Using External SCB SRAM\n");
-  size += sprintf(BLS, "             Interrupts: %ld", p->isr_count);
-  if (p->chip & AHC_EISA)
-  {
-    size += sprintf(BLS, " %s\n",
-        (p->pause & IRQMS) ? "(Level Sensitive)" : "(Edge Triggered)");
-  }
-  else
-  {
-    size += sprintf(BLS, "\n");
-  }
-  size += sprintf(BLS, "      BIOS Control Word: 0x%04x\n",
-            p->bios_control);
-  size += sprintf(BLS, "   Adapter Control Word: 0x%04x\n",
-            p->adapter_control);
-  size += sprintf(BLS, "   Extended Translation: %sabled\n",
-      (p->flags & AHC_EXTEND_TRANS_A) ? "En" : "Dis");
-  size += sprintf(BLS, "Disconnect Enable Flags: 0x%04x\n", p->discenable);
-  if (p->features & (AHC_ULTRA | AHC_ULTRA2))
-  {
-    size += sprintf(BLS, "     Ultra Enable Flags: 0x%04x\n", p->ultraenb);
-  }
-  size += sprintf(BLS, " Tag Queue Enable Flags: 0x%04x\n", p->tagenable);
-  size += sprintf(BLS, "Ordered Queue Tag Flags: 0x%04x\n", p->orderedtag);
-  size += sprintf(BLS, "Default Tag Queue Depth: %d\n", AIC7XXX_CMDS_PER_DEVICE);
-  size += sprintf(BLS, "    Tagged Queue By Device array for aic7xxx host "
-                       "instance %d:\n", p->instance);
-  size += sprintf(BLS, "      {");
-  for(i=0; i < (MAX_TARGETS - 1); i++)
-    size += sprintf(BLS, "%d,",aic7xxx_tag_info[p->instance].tag_commands[i]);
-  size += sprintf(BLS, "%d}\n",aic7xxx_tag_info[p->instance].tag_commands[i]);
-  size += sprintf(BLS, "    Actual queue depth per device for aic7xxx host "
-                       "instance %d:\n", p->instance);
-  size += sprintf(BLS, "      {");
-  for(i=0; i < (MAX_TARGETS - 1); i++)
-    size += sprintf(BLS, "%d,", p->dev_max_queue_depth[i]);
-  size += sprintf(BLS, "%d}\n", p->dev_max_queue_depth[i]);
-
-  size += sprintf(BLS, "\n");
-  size += sprintf(BLS, "Statistics:\n\n");
-  for (target = 0; target < MAX_TARGETS; target++)
-  {
-    sp = &p->stats[target];
-    if ((p->dev_flags[target] & DEVICE_PRESENT) == 0)
-    {
-      continue;
-    }
-    if (p->features & AHC_TWIN)
-    {
-      size += sprintf(BLS, "(scsi%d:%d:%d:%d)\n",
-          p->host_no, (target >> 3), (target & 0x7), 0);
-    }
-    else
-    {
-      size += sprintf(BLS, "(scsi%d:%d:%d:%d)\n",
-          p->host_no, 0, target, 0);
-    }
-    size += sprintf(BLS, "  Device using %s/%s",
-          (p->transinfo[target].cur_width == MSG_EXT_WDTR_BUS_16_BIT) ?
-          "Wide" : "Narrow",
-          (p->transinfo[target].cur_offset != 0) ?
-          "Sync transfers at " : "Async transfers.\n" );
-    if (p->transinfo[target].cur_offset != 0)
-    {
-      struct aic7xxx_syncrate *sync_rate;
-      unsigned char options = p->transinfo[target].cur_options;
-      int period = p->transinfo[target].cur_period;
-      int rate = (p->transinfo[target].cur_width ==
-                  MSG_EXT_WDTR_BUS_16_BIT) ? 1 : 0;
-
-      sync_rate = aic7xxx_find_syncrate(p, &period, 0, &options);
-      if (sync_rate != NULL)
-      {
-        size += sprintf(BLS, "%s MByte/sec, offset %d\n",
-                        sync_rate->rate[rate],
-                        p->transinfo[target].cur_offset );
-      }
-      else
-      {
-        size += sprintf(BLS, "3.3 MByte/sec, offset %d\n",
-                        p->transinfo[target].cur_offset );
-      }
-    }
-    size += sprintf(BLS, "  Transinfo settings: ");
-    size += sprintf(BLS, "current(%d/%d/%d/%d), ",
-                    p->transinfo[target].cur_period,
-                    p->transinfo[target].cur_offset,
-                    p->transinfo[target].cur_width,
-                    p->transinfo[target].cur_options);
-    size += sprintf(BLS, "goal(%d/%d/%d/%d), ",
-                    p->transinfo[target].goal_period,
-                    p->transinfo[target].goal_offset,
-                    p->transinfo[target].goal_width,
-                    p->transinfo[target].goal_options);
-    size += sprintf(BLS, "user(%d/%d/%d/%d)\n",
-                    p->transinfo[target].user_period,
-                    p->transinfo[target].user_offset,
-                    p->transinfo[target].user_width,
-                    p->transinfo[target].user_options);
-#ifdef AIC7XXX_PROC_STATS
-    size += sprintf(BLS, "  Total transfers %ld (%ld reads and %ld writes)\n",
-        sp->r_total + sp->w_total, sp->r_total, sp->w_total);
-    size += sprintf(BLS, "%s\n", HDRB);
-    size += sprintf(BLS, "   Reads:");
-    for (i = 0; i < NUMBER(sp->r_bins); i++)
-    {
-      size += sprintf(BLS, " %7ld", sp->r_bins[i]);
-    }
-    size += sprintf(BLS, "\n");
-    size += sprintf(BLS, "  Writes:");
-    for (i = 0; i < NUMBER(sp->w_bins); i++)
-    {
-      size += sprintf(BLS, " %7ld", sp->w_bins[i]);
-    }
-    size += sprintf(BLS, "\n");
-#else
-    size += sprintf(BLS, "  Total transfers %ld (%ld reads and %ld writes)\n",
-        sp->r_total + sp->w_total, sp->r_total, sp->w_total);
-#endif /* AIC7XXX_PROC_STATS */
-    size += sprintf(BLS, "\n\n");
-  }
-
-  if (size >= aic7xxx_buffer_size)
-  {
-    printk(KERN_WARNING "aic7xxx: Overflow in aic7xxx_proc.c\n");
-  }
-
-  if (offset > size - 1)
-  {
-    kfree(aic7xxx_buffer);
-    aic7xxx_buffer = NULL;
-    aic7xxx_buffer_size = length = 0;
-    *start = NULL;
-  }
-  else
-  {
-    *start = buffer;
-    length = MIN(length, size - offset);
-    memcpy(buffer, &aic7xxx_buffer[offset], length);
-  }
-
-  return (length);
-}
-
-/*
- * Overrides for Emacs so that we follow Linus's tabbing style.
- * Emacs will notice this stuff at the end of the file and automatically
- * adjust the settings for this buffer only.  This must remain at the end
- * of the file.
- * ---------------------------------------------------------------------------
- * Local variables:
- * c-indent-level: 2
- * c-brace-imaginary-offset: 0
- * c-brace-offset: -2
- * c-argdecl-indent: 2
- * c-label-offset: -2
- * c-continued-statement-offset: 2
- * c-continued-brace-offset: 0
- * indent-tabs-mode: nil
- * tab-width: 8
- * End:
- */
diff -urN linux/drivers/scsi/aic7xxx_reg.h /tmp/linux/drivers/scsi/aic7xxx_reg.h
--- linux/drivers/scsi/aic7xxx_reg.h	Wed May  3 18:16:44 2000
+++ /tmp/linux/drivers/scsi/aic7xxx_reg.h	Wed Dec 31 17:00:00 1969
@@ -1,629 +0,0 @@
-/*
-  * DO NOT EDIT - This file is automatically generated.
-  */
-
-#define	SCSISEQ         		0x00
-#define		TEMODE          	0x80
-#define		ENSELO          	0x40
-#define		ENSELI          	0x20
-#define		ENRSELI         	0x10
-#define		ENAUTOATNO      	0x08
-#define		ENAUTOATNI      	0x04
-#define		ENAUTOATNP      	0x02
-#define		SCSIRSTO        	0x01
-
-#define	SXFRCTL0        		0x01
-#define		DFON            	0x80
-#define		DFPEXP          	0x40
-#define		FAST20          	0x20
-#define		CLRSTCNT        	0x10
-#define		SPIOEN          	0x08
-#define		SCAMEN          	0x04
-#define		CLRCHN          	0x02
-
-#define	SXFRCTL1        		0x02
-#define		BITBUCKET       	0x80
-#define		SWRAPEN         	0x40
-#define		ENSPCHK         	0x20
-#define		STIMESEL        	0x18
-#define		ENSTIMER        	0x04
-#define		ACTNEGEN        	0x02
-#define		STPWEN          	0x01
-
-#define	SCSISIGO        		0x03
-#define		CDO             	0x80
-#define		IOO             	0x40
-#define		MSGO            	0x20
-#define		ATNO            	0x10
-#define		SELO            	0x08
-#define		BSYO            	0x04
-#define		REQO            	0x02
-#define		ACKO            	0x01
-
-#define	SCSISIGI        		0x03
-#define		ATNI            	0x10
-#define		SELI            	0x08
-#define		BSYI            	0x04
-#define		REQI            	0x02
-#define		ACKI            	0x01
-
-#define	SCSIRATE        		0x04
-#define		WIDEXFER        	0x80
-#define		SXFR_ULTRA2     	0x7f
-#define		SXFR            	0x70
-#define		SOFS            	0x0f
-
-#define	SCSIID          		0x05
-#define	SCSIOFFSET      		0x05
-#define		SOFS_ULTRA2     	0x7f
-
-#define	SCSIDATL        		0x06
-
-#define	SCSIDATH        		0x07
-
-#define	STCNT           		0x08
-
-#define	OPTIONMODE      		0x08
-#define		AUTORATEEN      	0x80
-#define		AUTOACKEN       	0x40
-#define		ATNMGMNTEN      	0x20
-#define		BUSFREEREV      	0x10
-#define		EXPPHASEDIS     	0x08
-#define		SCSIDATL_IMGEN  	0x04
-#define		AUTO_MSGOUT_DE  	0x02
-#define		DIS_MSGIN_DUALEDGE	0x01
-
-#define	CLRSINT0        		0x0b
-#define		CLRSELDO        	0x40
-#define		CLRSELDI        	0x20
-#define		CLRSELINGO      	0x10
-#define		CLRSWRAP        	0x08
-#define		CLRSPIORDY      	0x02
-
-#define	SSTAT0          		0x0b
-#define		TARGET          	0x80
-#define		SELDO           	0x40
-#define		SELDI           	0x20
-#define		SELINGO         	0x10
-#define		IOERR           	0x08
-#define		SWRAP           	0x08
-#define		SDONE           	0x04
-#define		SPIORDY         	0x02
-#define		DMADONE         	0x01
-
-#define	CLRSINT1        		0x0c
-#define		CLRSELTIMEO     	0x80
-#define		CLRATNO         	0x40
-#define		CLRSCSIRSTI     	0x20
-#define		CLRBUSFREE      	0x08
-#define		CLRSCSIPERR     	0x04
-#define		CLRPHASECHG     	0x02
-#define		CLRREQINIT      	0x01
-
-#define	SSTAT1          		0x0c
-#define		SELTO           	0x80
-#define		ATNTARG         	0x40
-#define		SCSIRSTI        	0x20
-#define		PHASEMIS        	0x10
-#define		BUSFREE         	0x08
-#define		SCSIPERR        	0x04
-#define		PHASECHG        	0x02
-#define		REQINIT         	0x01
-
-#define	SSTAT2          		0x0d
-#define		OVERRUN         	0x80
-#define		SHVALID         	0x40
-#define		WIDE_RES        	0x20
-#define		SFCNT           	0x1f
-#define		EXP_ACTIVE      	0x10
-#define		CRCVALERR       	0x08
-#define		CRCENDERR       	0x04
-#define		CRCREQERR       	0x02
-#define		DUAL_EDGE_ERROR 	0x01
-
-#define	SSTAT3          		0x0e
-#define		SCSICNT         	0xf0
-#define		OFFCNT          	0x0f
-
-#define	SCSIID_ULTRA2   		0x0f
-#define		OID             	0x0f
-
-#define	SIMODE0         		0x10
-#define		ENSELDO         	0x40
-#define		ENSELDI         	0x20
-#define		ENSELINGO       	0x10
-#define		ENIOERR         	0x08
-#define		ENSWRAP         	0x08
-#define		ENSDONE         	0x04
-#define		ENSPIORDY       	0x02
-#define		ENDMADONE       	0x01
-
-#define	SIMODE1         		0x11
-#define		ENSELTIMO       	0x80
-#define		ENATNTARG       	0x40
-#define		ENSCSIRST       	0x20
-#define		ENPHASEMIS      	0x10
-#define		ENBUSFREE       	0x08
-#define		ENSCSIPERR      	0x04
-#define		ENPHASECHG      	0x02
-#define		ENREQINIT       	0x01
-
-#define	SCSIBUSL        		0x12
-
-#define	SCSIBUSH        		0x13
-
-#define	SHADDR          		0x14
-
-#define	SELTIMER        		0x18
-#define		STAGE6          	0x20
-#define		STAGE5          	0x10
-#define		STAGE4          	0x08
-#define		STAGE3          	0x04
-#define		STAGE2          	0x02
-#define		STAGE1          	0x01
-
-#define	SELID           		0x19
-#define		SELID_MASK      	0xf0
-#define		ONEBIT          	0x08
-
-#define	SPIOCAP         		0x1b
-#define		SOFT1           	0x80
-#define		SOFT0           	0x40
-#define		SOFTCMDEN       	0x20
-#define		HAS_BRDCTL      	0x10
-#define		SEEPROM         	0x08
-#define		EEPROM          	0x04
-#define		ROM             	0x02
-#define		SSPIOCPS        	0x01
-
-#define	BRDCTL          		0x1d
-#define		BRDDAT7         	0x80
-#define		BRDDAT6         	0x40
-#define		BRDDAT5         	0x20
-#define		BRDDAT4         	0x10
-#define		BRDSTB          	0x10
-#define		BRDCS           	0x08
-#define		BRDDAT3         	0x08
-#define		BRDDAT2         	0x04
-#define		BRDRW           	0x04
-#define		BRDRW_ULTRA2    	0x02
-#define		BRDCTL1         	0x02
-#define		BRDCTL0         	0x01
-#define		BRDSTB_ULTRA2   	0x01
-
-#define	SEECTL          		0x1e
-#define		EXTARBACK       	0x80
-#define		EXTARBREQ       	0x40
-#define		SEEMS           	0x20
-#define		SEERDY          	0x10
-#define		SEECS           	0x08
-#define		SEECK           	0x04
-#define		SEEDO           	0x02
-#define		SEEDI           	0x01
-
-#define	SBLKCTL         		0x1f
-#define		DIAGLEDEN       	0x80
-#define		DIAGLEDON       	0x40
-#define		AUTOFLUSHDIS    	0x20
-#define		ENAB40          	0x08
-#define		ENAB20          	0x04
-#define		SELWIDE         	0x02
-#define		XCVR            	0x01
-
-#define	SRAM_BASE       		0x20
-
-#define	TARG_SCSIRATE   		0x20
-
-#define	ULTRA_ENB       		0x30
-
-#define	DISC_DSB        		0x32
-
-#define	MSG_OUT         		0x34
-
-#define	DMAPARAMS       		0x35
-#define		PRELOADEN       	0x80
-#define		WIDEODD         	0x40
-#define		SCSIEN          	0x20
-#define		SDMAENACK       	0x10
-#define		SDMAEN          	0x10
-#define		HDMAEN          	0x08
-#define		HDMAENACK       	0x08
-#define		DIRECTION       	0x04
-#define		FIFOFLUSH       	0x02
-#define		FIFORESET       	0x01
-
-#define	SEQ_FLAGS       		0x36
-#define		IDENTIFY_SEEN   	0x80
-#define		SCBPTR_VALID    	0x20
-#define		DPHASE          	0x10
-#define		AMTARGET        	0x08
-#define		WIDE_BUS        	0x02
-#define		TWIN_BUS        	0x01
-
-#define	SAVED_TCL       		0x37
-
-#define	SG_COUNT        		0x38
-
-#define	SG_NEXT         		0x39
-
-#define	LASTPHASE       		0x3d
-#define		P_MESGIN        	0xe0
-#define		PHASE_MASK      	0xe0
-#define		P_STATUS        	0xc0
-#define		P_MESGOUT       	0xa0
-#define		P_COMMAND       	0x80
-#define		CDI             	0x80
-#define		IOI             	0x40
-#define		P_DATAIN        	0x40
-#define		MSGI            	0x20
-#define		P_BUSFREE       	0x01
-#define		P_DATAOUT       	0x00
-
-#define	WAITING_SCBH    		0x3e
-
-#define	DISCONNECTED_SCBH		0x3f
-
-#define	FREE_SCBH       		0x40
-
-#define	HSCB_ADDR       		0x41
-
-#define	SCBID_ADDR      		0x45
-
-#define	TMODE_CMDADDR   		0x49
-
-#define	KERNEL_QINPOS   		0x4d
-
-#define	QINPOS          		0x4e
-
-#define	QOUTPOS         		0x4f
-
-#define	TMODE_CMDADDR_NEXT		0x50
-
-#define	ARG_1           		0x51
-#define	RETURN_1        		0x51
-#define		SEND_MSG        	0x80
-#define		SEND_SENSE      	0x40
-#define		SEND_REJ        	0x20
-#define		MSGOUT_PHASEMIS 	0x10
-
-#define	ARG_2           		0x52
-#define	RETURN_2        		0x52
-
-#define	LAST_MSG        		0x53
-
-#define	PREFETCH_CNT    		0x54
-
-#define	SCSICONF        		0x5a
-#define		TERM_ENB        	0x80
-#define		RESET_SCSI      	0x40
-#define		HWSCSIID        	0x0f
-#define		HSCSIID         	0x07
-
-#define	HOSTCONF        		0x5d
-
-#define	HA_274_BIOSCTRL 		0x5f
-#define		BIOSMODE        	0x30
-#define		BIOSDISABLED    	0x30
-#define		CHANNEL_B_PRIMARY	0x08
-
-#define	SEQCTL          		0x60
-#define		PERRORDIS       	0x80
-#define		PAUSEDIS        	0x40
-#define		FAILDIS         	0x20
-#define		FASTMODE        	0x10
-#define		BRKADRINTEN     	0x08
-#define		STEP            	0x04
-#define		SEQRESET        	0x02
-#define		LOADRAM         	0x01
-
-#define	SEQRAM          		0x61
-
-#define	SEQADDR0        		0x62
-
-#define	SEQADDR1        		0x63
-#define		SEQADDR1_MASK   	0x01
-
-#define	ACCUM           		0x64
-
-#define	SINDEX          		0x65
-
-#define	DINDEX          		0x66
-
-#define	ALLONES         		0x69
-
-#define	ALLZEROS        		0x6a
-
-#define	NONE            		0x6a
-
-#define	FLAGS           		0x6b
-#define		ZERO            	0x02
-#define		CARRY           	0x01
-
-#define	SINDIR          		0x6c
-
-#define	DINDIR          		0x6d
-
-#define	FUNCTION1       		0x6e
-
-#define	STACK           		0x6f
-
-#define	TARG_OFFSET     		0x70
-
-#define	BCTL            		0x84
-#define		ACE             	0x08
-#define		ENABLE          	0x01
-
-#define	DSCOMMAND0      		0x84
-#define		INTSCBRAMSEL    	0x08
-#define		RAMPS           	0x04
-#define		USCBSIZE32      	0x02
-#define		CIOPARCKEN      	0x01
-
-#define	DSCOMMAND       		0x84
-#define		CACHETHEN       	0x80
-#define		DPARCKEN        	0x40
-#define		MPARCKEN        	0x20
-#define		EXTREQLCK       	0x10
-
-#define	BUSTIME         		0x85
-#define		BOFF            	0xf0
-#define		BON             	0x0f
-
-#define	BUSSPD          		0x86
-#define		DFTHRSH         	0xc0
-#define		STBOFF          	0x38
-#define		STBON           	0x07
-
-#define	DSPCISTATUS     		0x86
-#define		DFTHRSH_100     	0xc0
-
-#define	HCNTRL          		0x87
-#define		POWRDN          	0x40
-#define		SWINT           	0x10
-#define		IRQMS           	0x08
-#define		PAUSE           	0x04
-#define		INTEN           	0x02
-#define		CHIPRST         	0x01
-#define		CHIPRSTACK      	0x01
-
-#define	HADDR           		0x88
-
-#define	HCNT            		0x8c
-
-#define	SCBPTR          		0x90
-
-#define	INTSTAT         		0x91
-#define		SEQINT_MASK     	0xf1
-#define		DATA_OVERRUN    	0xe1
-#define		MSGIN_PHASEMIS  	0xd1
-#define		TRACEPOINT2     	0xc1
-#define		TRACEPOINT      	0xb1
-#define		AWAITING_MSG    	0xa1
-#define		RESIDUAL        	0x81
-#define		BAD_STATUS      	0x71
-#define		REJECT_MSG      	0x61
-#define		WIDE_RESIDUE    	0x51
-#define		EXTENDED_MSG    	0x41
-#define		NO_MATCH        	0x31
-#define		NO_IDENT        	0x21
-#define		SEND_REJECT     	0x11
-#define		INT_PEND        	0x0f
-#define		BRKADRINT       	0x08
-#define		SCSIINT         	0x04
-#define		CMDCMPLT        	0x02
-#define		BAD_PHASE       	0x01
-#define		SEQINT          	0x01
-
-#define	CLRINT          		0x92
-#define		CLRPARERR       	0x10
-#define		CLRBRKADRINT    	0x08
-#define		CLRSCSIINT      	0x04
-#define		CLRCMDINT       	0x02
-#define		CLRSEQINT       	0x01
-
-#define	ERROR           		0x92
-#define		CIOPARERR       	0x80
-#define		PCIERRSTAT      	0x40
-#define		MPARERR         	0x20
-#define		DPARERR         	0x10
-#define		SQPARERR        	0x08
-#define		ILLOPCODE       	0x04
-#define		DSCTMOUT        	0x02
-#define		ILLSADDR        	0x02
-#define		ILLHADDR        	0x01
-
-#define	DFCNTRL         		0x93
-
-#define	DFSTATUS        		0x94
-#define		PRELOAD_AVAIL   	0x80
-#define		DWORDEMP        	0x20
-#define		MREQPEND        	0x10
-#define		HDONE           	0x08
-#define		DFTHRESH        	0x04
-#define		FIFOFULL        	0x02
-#define		FIFOEMP         	0x01
-
-#define	DFDAT           		0x99
-
-#define	SCBCNT          		0x9a
-#define		SCBAUTO         	0x80
-#define		SCBCNT_MASK     	0x1f
-
-#define	QINFIFO         		0x9b
-
-#define	QINCNT          		0x9c
-
-#define	SCSIDATL_IMG    		0x9c
-
-#define	QOUTFIFO        		0x9d
-
-#define	CRCCONTROL1     		0x9d
-#define		CRCONSEEN       	0x80
-#define		CRCVALCHKEN     	0x40
-#define		CRCENDCHKEN     	0x20
-#define		CRCREQCHKEN     	0x10
-#define		TARGCRCENDEN    	0x08
-#define		TARGCRCCNTEN    	0x04
-
-#define	QOUTCNT         		0x9e
-
-#define	SCSIPHASE       		0x9e
-#define		SP_STATUS       	0x20
-#define		SP_COMMAND      	0x10
-#define		SP_MSG_IN       	0x08
-#define		SP_MSG_OUT      	0x04
-#define		SP_DATA_IN      	0x02
-#define		SP_DATA_OUT     	0x01
-
-#define	SFUNCT          		0x9f
-#define		ALT_MODE        	0x80
-
-#define	SCB_CONTROL     		0xa0
-#define		MK_MESSAGE      	0x80
-#define		DISCENB         	0x40
-#define		TAG_ENB         	0x20
-#define		DISCONNECTED    	0x04
-#define		SCB_TAG_TYPE    	0x03
-
-#define	SCB_BASE        		0xa0
-
-#define	SCB_TCL         		0xa1
-#define		TID             	0xf0
-#define		SELBUSB         	0x08
-#define		LID             	0x07
-
-#define	SCB_TARGET_STATUS		0xa2
-
-#define	SCB_SGCOUNT     		0xa3
-
-#define	SCB_SGPTR       		0xa4
-
-#define	SCB_RESID_SGCNT 		0xa8
-
-#define	SCB_RESID_DCNT  		0xa9
-
-#define	SCB_DATAPTR     		0xac
-
-#define	SCB_DATACNT     		0xb0
-
-#define	SCB_CMDPTR      		0xb4
-
-#define	SCB_CMDLEN      		0xb8
-
-#define	SCB_TAG         		0xb9
-
-#define	SCB_NEXT        		0xba
-
-#define	SCB_PREV        		0xbb
-
-#define	SCB_BUSYTARGETS 		0xbc
-
-#define	SEECTL_2840     		0xc0
-#define		CS_2840         	0x04
-#define		CK_2840         	0x02
-#define		DO_2840         	0x01
-
-#define	STATUS_2840     		0xc1
-#define		EEPROM_TF       	0x80
-#define		BIOS_SEL        	0x60
-#define		ADSEL           	0x1e
-#define		DI_2840         	0x01
-
-#define	CCHADDR         		0xe0
-
-#define	CCHCNT          		0xe8
-
-#define	CCSGRAM         		0xe9
-
-#define	CCSGADDR        		0xea
-
-#define	CCSGCTL         		0xeb
-#define		CCSGDONE        	0x80
-#define		CCSGEN          	0x08
-#define		FLAG            	0x02
-#define		CCSGRESET       	0x01
-
-#define	CCSCBRAM        		0xec
-
-#define	CCSCBADDR       		0xed
-
-#define	CCSCBCTL        		0xee
-#define		CCSCBDONE       	0x80
-#define		ARRDONE         	0x40
-#define		CCARREN         	0x10
-#define		CCSCBEN         	0x08
-#define		CCSCBDIR        	0x04
-#define		CCSCBRESET      	0x01
-
-#define	CCSCBCNT        		0xef
-
-#define	CCSCBPTR        		0xf1
-
-#define	HNSCB_QOFF      		0xf4
-
-#define	HESCB_QOFF      		0xf5
-
-#define	SNSCB_QOFF      		0xf6
-
-#define	SESCB_QOFF      		0xf7
-
-#define	SDSCB_QOFF      		0xf8
-
-#define	QOFF_CTLSTA     		0xfa
-#define		ESTABLISH_SCB_AVAIL	0x80
-#define		SCB_AVAIL       	0x40
-#define		SNSCB_ROLLOVER  	0x20
-#define		SDSCB_ROLLOVER  	0x10
-#define		SESCB_ROLLOVER  	0x08
-#define		SCB_QSIZE       	0x07
-#define		SCB_QSIZE_256   	0x06
-
-#define	DFF_THRSH       		0xfb
-#define		WR_DFTHRSH      	0x70
-#define		WR_DFTHRSH_MAX  	0x70
-#define		WR_DFTHRSH_90   	0x60
-#define		WR_DFTHRSH_85   	0x50
-#define		WR_DFTHRSH_75   	0x40
-#define		WR_DFTHRSH_63   	0x30
-#define		WR_DFTHRSH_50   	0x20
-#define		WR_DFTHRSH_25   	0x10
-#define		RD_DFTHRSH_MAX  	0x07
-#define		RD_DFTHRSH      	0x07
-#define		RD_DFTHRSH_90   	0x06
-#define		RD_DFTHRSH_85   	0x05
-#define		RD_DFTHRSH_75   	0x04
-#define		RD_DFTHRSH_63   	0x03
-#define		RD_DFTHRSH_50   	0x02
-#define		RD_DFTHRSH_25   	0x01
-#define		RD_DFTHRSH_MIN  	0x00
-#define		WR_DFTHRSH_MIN  	0x00
-
-#define	SG_CACHEPTR     		0xfc
-#define		SG_USER_DATA    	0xfc
-#define		LAST_SEG        	0x02
-#define		LAST_SEG_DONE   	0x01
-
-
-#define	CMD_GROUP_CODE_SHIFT	0x05
-#define	BUS_8_BIT	0x00
-#define	QOUTFIFO_OFFSET	0x01
-#define	CCSGRAM_MAXSEGS	0x10
-#define	CMD_GROUP2_BYTE_DELTA	0xfa
-#define	MAX_OFFSET_8BIT	0x0f
-#define	BUS_16_BIT	0x01
-#define	QINFIFO_OFFSET	0x02
-#define	CMD_GROUP5_BYTE_DELTA	0x0b
-#define	MAX_OFFSET_ULTRA2	0x7f
-#define	MAX_OFFSET_16BIT	0x08
-#define	UNTAGGEDSCB_OFFSET	0x00
-#define	SCB_LIST_NULL	0xff
-#define	SG_SIZEOF	0x08
-#define	CMD_GROUP4_BYTE_DELTA	0x04
-#define	CMD_GROUP0_BYTE_DELTA	0xfc
-#define	HOST_MSG	0xff
-#define	BUS_32_BIT	0x02
-#define	CCSGADDR_MAX	0x80
-
-
-/* Downloaded Constant Definitions */
-#define	TMODE_NUMCMDS	0x00
diff -urN linux/drivers/scsi/aic7xxx_seq.c /tmp/linux/drivers/scsi/aic7xxx_seq.c
--- linux/drivers/scsi/aic7xxx_seq.c	Mon Sep  4 11:39:21 2000
+++ /tmp/linux/drivers/scsi/aic7xxx_seq.c	Wed Dec 31 17:00:00 1969
@@ -1,745 +0,0 @@
-/*
-  * DO NOT EDIT - This file is automatically generated.
-  */
-static unsigned char seqprog[] = {
-	0xff, 0x6a, 0x06, 0x08,
-	0x7f, 0x02, 0x04, 0x08,
-	0x12, 0x6a, 0x00, 0x00,
-	0xff, 0x6a, 0xd6, 0x09,
-	0xff, 0x6a, 0xdc, 0x09,
-	0x00, 0x65, 0xca, 0x58,
-	0xf7, 0x01, 0x02, 0x08,
-	0xff, 0x4e, 0xc8, 0x08,
-	0xbf, 0x60, 0xc0, 0x08,
-	0x60, 0x0b, 0x86, 0x68,
-	0x40, 0x00, 0x0c, 0x68,
-	0x08, 0x1f, 0x3e, 0x10,
-	0x60, 0x0b, 0x86, 0x68,
-	0x40, 0x00, 0x0c, 0x68,
-	0x08, 0x1f, 0x3e, 0x10,
-	0xff, 0x3e, 0x48, 0x60,
-	0x40, 0xfa, 0x10, 0x78,
-	0xff, 0xf6, 0xd4, 0x08,
-	0x01, 0x4e, 0x9c, 0x18,
-	0x40, 0x60, 0xc0, 0x00,
-	0x00, 0x4d, 0x10, 0x70,
-	0x01, 0x4e, 0x9c, 0x18,
-	0xbf, 0x60, 0xc0, 0x08,
-	0x00, 0x6a, 0x3e, 0x5c,
-	0xff, 0x4e, 0xc8, 0x18,
-	0x02, 0x6a, 0x54, 0x5b,
-	0xff, 0x52, 0x20, 0x09,
-	0x0d, 0x6a, 0x6a, 0x00,
-	0x00, 0x52, 0xca, 0x5b,
-	0x03, 0xb0, 0x52, 0x31,
-	0xff, 0xb0, 0x52, 0x09,
-	0xff, 0xb1, 0x54, 0x09,
-	0xff, 0xb2, 0x56, 0x09,
-	0xff, 0xa3, 0x50, 0x09,
-	0xff, 0x3e, 0x74, 0x09,
-	0xff, 0x90, 0x7c, 0x08,
-	0xff, 0x3e, 0x20, 0x09,
-	0x00, 0x65, 0x4e, 0x58,
-	0x00, 0x65, 0x0c, 0x40,
-	0xf7, 0x1f, 0xca, 0x08,
-	0x08, 0xa1, 0xc8, 0x08,
-	0x00, 0x65, 0xca, 0x00,
-	0xff, 0x65, 0x3e, 0x08,
-	0xf0, 0xa1, 0xc8, 0x08,
-	0x0f, 0x0f, 0x1e, 0x08,
-	0x00, 0x0f, 0x1e, 0x00,
-	0xf0, 0xa1, 0xc8, 0x08,
-	0x0f, 0x05, 0x0a, 0x08,
-	0x00, 0x05, 0x0a, 0x00,
-	0xff, 0x6a, 0x0c, 0x08,
-	0x5a, 0x6a, 0x00, 0x04,
-	0x12, 0x65, 0x02, 0x00,
-	0x31, 0x6a, 0xca, 0x00,
-	0x80, 0x37, 0x6e, 0x68,
-	0xff, 0x65, 0xca, 0x18,
-	0xff, 0x37, 0xdc, 0x08,
-	0xff, 0x6e, 0xc8, 0x08,
-	0x00, 0x6c, 0x76, 0x78,
-	0x20, 0x01, 0x02, 0x00,
-	0x4c, 0x37, 0xc8, 0x28,
-	0x08, 0x1f, 0x7e, 0x78,
-	0x08, 0x37, 0x6e, 0x00,
-	0x08, 0x64, 0xc8, 0x00,
-	0x70, 0x64, 0xca, 0x18,
-	0xff, 0x6c, 0x0a, 0x08,
-	0x20, 0x64, 0xca, 0x18,
-	0xff, 0x6c, 0x08, 0x0c,
-	0x40, 0x0b, 0x96, 0x68,
-	0x20, 0x6a, 0x16, 0x00,
-	0xf0, 0x19, 0x6e, 0x08,
-	0x08, 0x6a, 0x18, 0x00,
-	0x08, 0x11, 0x22, 0x00,
-	0x08, 0x6a, 0x66, 0x58,
-	0x08, 0x6a, 0x68, 0x00,
-	0x00, 0x65, 0xaa, 0x40,
-	0x12, 0x6a, 0x00, 0x00,
-	0x40, 0x6a, 0x16, 0x00,
-	0xff, 0x3e, 0x20, 0x09,
-	0xff, 0xba, 0x7c, 0x08,
-	0xff, 0xa1, 0x6e, 0x08,
-	0x08, 0x6a, 0x18, 0x00,
-	0x08, 0x11, 0x22, 0x00,
-	0x08, 0x6a, 0x66, 0x58,
-	0x80, 0x6a, 0x68, 0x00,
-	0x80, 0x36, 0x6c, 0x00,
-	0x00, 0x65, 0x9e, 0x5b,
-	0xff, 0x3d, 0xc8, 0x08,
-	0xbf, 0x64, 0xe2, 0x78,
-	0x80, 0x64, 0xac, 0x71,
-	0xa0, 0x64, 0xdc, 0x71,
-	0xc0, 0x64, 0xd4, 0x71,
-	0xe0, 0x64, 0x1c, 0x72,
-	0x01, 0x6a, 0x22, 0x01,
-	0x00, 0x65, 0xaa, 0x40,
-	0xf7, 0x11, 0x22, 0x08,
-	0x00, 0x65, 0xca, 0x58,
-	0xff, 0x06, 0xd4, 0x08,
-	0xf7, 0x01, 0x02, 0x08,
-	0x09, 0x0c, 0xc4, 0x78,
-	0x08, 0x0c, 0x0c, 0x68,
-	0x01, 0x6a, 0x22, 0x01,
-	0xff, 0x6a, 0x26, 0x09,
-	0x02, 0x6a, 0x08, 0x30,
-	0xff, 0x6a, 0x08, 0x08,
-	0xdf, 0x01, 0x02, 0x08,
-	0x01, 0x6a, 0x7a, 0x00,
-	0xff, 0x6a, 0x6c, 0x0c,
-	0x04, 0x14, 0x10, 0x31,
-	0x03, 0xa9, 0x18, 0x31,
-	0x03, 0xa9, 0x10, 0x30,
-	0x08, 0x6a, 0xcc, 0x00,
-	0xa9, 0x6a, 0xb4, 0x5b,
-	0x00, 0x65, 0x02, 0x41,
-	0xa8, 0x6a, 0x6a, 0x00,
-	0x79, 0x6a, 0x6a, 0x00,
-	0x40, 0x3d, 0xea, 0x68,
-	0x04, 0x35, 0x6a, 0x00,
-	0x00, 0x65, 0x0e, 0x5b,
-	0x80, 0x6a, 0xd4, 0x01,
-	0x10, 0x36, 0xd6, 0x68,
-	0x10, 0x36, 0x6c, 0x00,
-	0x07, 0xac, 0x10, 0x31,
-	0x03, 0x8c, 0x10, 0x30,
-	0x05, 0xa3, 0x70, 0x30,
-	0x88, 0x6a, 0xcc, 0x00,
-	0xac, 0x6a, 0xac, 0x5b,
-	0x00, 0x65, 0xa6, 0x5b,
-	0x38, 0x6a, 0xcc, 0x00,
-	0xa3, 0x6a, 0xb0, 0x5b,
-	0xff, 0x38, 0x12, 0x69,
-	0x80, 0x02, 0x04, 0x00,
-	0xe7, 0x35, 0x6a, 0x08,
-	0x03, 0x69, 0x18, 0x31,
-	0x03, 0x69, 0x10, 0x30,
-	0xff, 0x6a, 0x10, 0x00,
-	0xff, 0x6a, 0x12, 0x00,
-	0xff, 0x6a, 0x14, 0x00,
-	0x01, 0x38, 0x18, 0x61,
-	0xbf, 0x35, 0x6a, 0x08,
-	0x02, 0x6a, 0xf8, 0x01,
-	0xff, 0x69, 0xca, 0x08,
-	0xff, 0x35, 0x26, 0x09,
-	0x04, 0x0b, 0x1c, 0x69,
-	0x04, 0x0b, 0x28, 0x69,
-	0x10, 0x0c, 0x1e, 0x79,
-	0x04, 0x0b, 0x28, 0x69,
-	0xff, 0x6a, 0xca, 0x08,
-	0x00, 0x35, 0xee, 0x5a,
-	0x80, 0x02, 0x7c, 0x69,
-	0xff, 0x65, 0x6c, 0x79,
-	0xff, 0x38, 0x70, 0x18,
-	0xff, 0x38, 0x6c, 0x79,
-	0x80, 0xea, 0x48, 0x61,
-	0xef, 0x38, 0xc8, 0x18,
-	0x80, 0x6a, 0xc8, 0x00,
-	0x00, 0x65, 0x3a, 0x49,
-	0x33, 0x38, 0xc8, 0x28,
-	0xff, 0x64, 0xd0, 0x09,
-	0x04, 0x39, 0xc0, 0x31,
-	0x09, 0x6a, 0xd6, 0x01,
-	0x80, 0xeb, 0x40, 0x79,
-	0xf7, 0xeb, 0xd6, 0x09,
-	0x08, 0xeb, 0x44, 0x69,
-	0x01, 0x6a, 0xd6, 0x01,
-	0x08, 0xe9, 0x10, 0x31,
-	0x03, 0x8c, 0x10, 0x30,
-	0x88, 0x6a, 0xcc, 0x00,
-	0x39, 0x6a, 0xb2, 0x5b,
-	0x08, 0x6a, 0x18, 0x01,
-	0xff, 0x6a, 0x1a, 0x09,
-	0xff, 0x6a, 0x1c, 0x09,
-	0x0d, 0x93, 0x26, 0x01,
-	0x00, 0x65, 0x30, 0x5c,
-	0x88, 0x6a, 0x20, 0x5c,
-	0x00, 0x65, 0xa6, 0x5b,
-	0xff, 0x6a, 0xc8, 0x08,
-	0x08, 0x39, 0x72, 0x18,
-	0x00, 0x3a, 0x74, 0x20,
-	0x01, 0x0c, 0x64, 0x79,
-	0x10, 0x0c, 0x02, 0x79,
-	0xff, 0x35, 0x26, 0x09,
-	0x04, 0x0b, 0x6a, 0x69,
-	0x00, 0x65, 0x84, 0x59,
-	0x03, 0x08, 0x52, 0x31,
-	0xff, 0x38, 0x50, 0x09,
-	0xff, 0x08, 0x52, 0x09,
-	0xff, 0x09, 0x54, 0x09,
-	0xff, 0x0a, 0x56, 0x09,
-	0xff, 0x38, 0x50, 0x09,
-	0x00, 0x65, 0xaa, 0x40,
-	0x00, 0x65, 0x84, 0x59,
-	0x7f, 0x02, 0x04, 0x08,
-	0xe1, 0x6a, 0x22, 0x01,
-	0x00, 0x65, 0xaa, 0x40,
-	0x04, 0x93, 0x9a, 0x69,
-	0xdf, 0x93, 0x26, 0x09,
-	0x20, 0x93, 0x88, 0x69,
-	0x02, 0x93, 0x26, 0x01,
-	0x01, 0x94, 0x8a, 0x79,
-	0x01, 0x94, 0x8a, 0x79,
-	0x01, 0x94, 0x8a, 0x79,
-	0x01, 0x94, 0x8a, 0x79,
-	0x01, 0x94, 0x8a, 0x79,
-	0x01, 0x94, 0x8a, 0x79,
-	0x10, 0x94, 0x98, 0x69,
-	0x7f, 0x05, 0xa0, 0x69,
-	0x02, 0x03, 0xa0, 0x79,
-	0x11, 0x0c, 0x9c, 0x79,
-	0xd7, 0x93, 0x26, 0x09,
-	0x28, 0x93, 0xa2, 0x69,
-	0x03, 0x08, 0x52, 0x31,
-	0xff, 0x38, 0x50, 0x09,
-	0x12, 0x01, 0x02, 0x00,
-	0xff, 0x6a, 0xd4, 0x0c,
-	0x00, 0x65, 0x0e, 0x5b,
-	0x05, 0xb4, 0x10, 0x31,
-	0x02, 0x6a, 0x1a, 0x31,
-	0x03, 0x8c, 0x10, 0x30,
-	0x88, 0x6a, 0xcc, 0x00,
-	0xb4, 0x6a, 0xb0, 0x5b,
-	0xff, 0x6a, 0x1a, 0x09,
-	0xff, 0x6a, 0x1c, 0x09,
-	0x00, 0x65, 0xa6, 0x5b,
-	0x3d, 0x6a, 0xee, 0x5a,
-	0xac, 0x6a, 0x26, 0x01,
-	0x04, 0x0b, 0xc2, 0x69,
-	0x04, 0x0b, 0xc8, 0x69,
-	0x10, 0x0c, 0xc4, 0x79,
-	0x02, 0x03, 0xcc, 0x79,
-	0x11, 0x0c, 0xc8, 0x79,
-	0xd7, 0x93, 0x26, 0x09,
-	0x28, 0x93, 0xce, 0x69,
-	0x12, 0x01, 0x02, 0x00,
-	0x00, 0x65, 0xaa, 0x40,
-	0x00, 0x65, 0x0e, 0x5b,
-	0xff, 0x06, 0x44, 0x09,
-	0x00, 0x65, 0xaa, 0x40,
-	0x10, 0x3d, 0x06, 0x00,
-	0xff, 0x34, 0xca, 0x08,
-	0x80, 0x65, 0x00, 0x62,
-	0x0f, 0xa1, 0xca, 0x08,
-	0x07, 0xa1, 0xca, 0x08,
-	0x40, 0xa0, 0xc8, 0x08,
-	0x00, 0x65, 0xca, 0x00,
-	0x80, 0x65, 0xca, 0x00,
-	0x80, 0xa0, 0xf0, 0x79,
-	0xff, 0x65, 0x0c, 0x08,
-	0x00, 0x65, 0x02, 0x42,
-	0x20, 0xa0, 0x08, 0x7a,
-	0xff, 0x65, 0x0c, 0x08,
-	0x00, 0x65, 0x9e, 0x5b,
-	0xa0, 0x3d, 0x10, 0x62,
-	0x23, 0xa0, 0x0c, 0x08,
-	0x00, 0x65, 0x9e, 0x5b,
-	0xa0, 0x3d, 0x10, 0x62,
-	0x00, 0xb9, 0x08, 0x42,
-	0xff, 0x65, 0x08, 0x62,
-	0xa1, 0x6a, 0x22, 0x01,
-	0xff, 0x6a, 0xd4, 0x08,
-	0x10, 0x51, 0x10, 0x72,
-	0x40, 0x6a, 0x18, 0x00,
-	0xff, 0x65, 0x0c, 0x08,
-	0x00, 0x65, 0x9e, 0x5b,
-	0xa0, 0x3d, 0xda, 0x71,
-	0x40, 0x6a, 0x18, 0x00,
-	0xff, 0x34, 0xa6, 0x08,
-	0x80, 0x34, 0x18, 0x62,
-	0x7f, 0xa0, 0x40, 0x09,
-	0x08, 0x6a, 0x68, 0x00,
-	0x00, 0x65, 0xaa, 0x40,
-	0x64, 0x6a, 0xe4, 0x5a,
-	0x80, 0x64, 0x8e, 0x6a,
-	0x04, 0x64, 0x70, 0x72,
-	0x02, 0x64, 0x76, 0x72,
-	0x00, 0x6a, 0x38, 0x72,
-	0x03, 0x64, 0x8a, 0x72,
-	0x01, 0x64, 0x6c, 0x72,
-	0x07, 0x64, 0xcc, 0x72,
-	0x08, 0x64, 0x34, 0x72,
-	0x23, 0x64, 0xd0, 0x72,
-	0x11, 0x6a, 0x22, 0x01,
-	0x07, 0x6a, 0xd6, 0x5a,
-	0xff, 0x06, 0xd4, 0x08,
-	0x00, 0x65, 0xaa, 0x40,
-	0xff, 0xa8, 0x3c, 0x6a,
-	0xff, 0xa2, 0x54, 0x7a,
-	0x01, 0x6a, 0x6a, 0x00,
-	0x00, 0xb9, 0xca, 0x5b,
-	0xff, 0xa2, 0x54, 0x7a,
-	0x71, 0x6a, 0x22, 0x01,
-	0xff, 0x6a, 0xd4, 0x08,
-	0x40, 0x51, 0x54, 0x62,
-	0x0d, 0x6a, 0x6a, 0x00,
-	0x00, 0xb9, 0xca, 0x5b,
-	0xff, 0x3e, 0x74, 0x09,
-	0xff, 0x90, 0x7c, 0x08,
-	0x00, 0x65, 0x4e, 0x58,
-	0x00, 0x65, 0xbc, 0x40,
-	0x20, 0xa0, 0x5c, 0x6a,
-	0xff, 0x37, 0xc8, 0x08,
-	0x00, 0x6a, 0x74, 0x5b,
-	0xff, 0x6a, 0x8a, 0x5b,
-	0xff, 0xf8, 0xc8, 0x08,
-	0xff, 0x4f, 0xc8, 0x08,
-	0x01, 0x6a, 0x74, 0x5b,
-	0x00, 0xb9, 0x8a, 0x5b,
-	0x01, 0x4f, 0x9e, 0x18,
-	0x02, 0x6a, 0x22, 0x01,
-	0x00, 0x65, 0x38, 0x5c,
-	0x00, 0x65, 0xbc, 0x40,
-	0x41, 0x6a, 0x22, 0x01,
-	0x00, 0x65, 0xaa, 0x40,
-	0x04, 0xa0, 0x40, 0x01,
-	0x00, 0x65, 0x50, 0x5c,
-	0x00, 0x65, 0xbc, 0x40,
-	0x10, 0x36, 0x34, 0x7a,
-	0x05, 0x38, 0x46, 0x31,
-	0x04, 0x14, 0x58, 0x31,
-	0x03, 0xa9, 0x60, 0x31,
-	0xa3, 0x6a, 0xcc, 0x00,
-	0x38, 0x6a, 0xb0, 0x5b,
-	0xac, 0x6a, 0xcc, 0x00,
-	0x14, 0x6a, 0xb2, 0x5b,
-	0xa9, 0x6a, 0xb4, 0x5b,
-	0x00, 0x65, 0x34, 0x42,
-	0xef, 0x36, 0x6c, 0x08,
-	0x00, 0x65, 0x34, 0x42,
-	0x0f, 0x64, 0xc8, 0x08,
-	0x07, 0x64, 0xc8, 0x08,
-	0x00, 0x37, 0x6e, 0x00,
-	0xff, 0x6a, 0xa4, 0x00,
-	0x00, 0x65, 0x44, 0x5b,
-	0xff, 0x51, 0xa0, 0x72,
-	0x20, 0x36, 0xaa, 0x7a,
-	0x00, 0x90, 0x32, 0x5b,
-	0x00, 0x65, 0xac, 0x42,
-	0xff, 0x06, 0xd4, 0x08,
-	0x00, 0x65, 0x9e, 0x5b,
-	0xe0, 0x3d, 0xc6, 0x62,
-	0x20, 0x12, 0xc6, 0x62,
-	0x51, 0x6a, 0xda, 0x5a,
-	0x00, 0x65, 0x2c, 0x5b,
-	0xff, 0x37, 0xc8, 0x08,
-	0x00, 0xa1, 0xbe, 0x62,
-	0x04, 0xa0, 0xbe, 0x7a,
-	0xfb, 0xa0, 0x40, 0x09,
-	0x80, 0x36, 0x6c, 0x00,
-	0x80, 0xa0, 0x34, 0x7a,
-	0x7f, 0xa0, 0x40, 0x09,
-	0xff, 0x6a, 0xd6, 0x5a,
-	0x00, 0x65, 0x34, 0x42,
-	0x04, 0xa0, 0xc4, 0x7a,
-	0x00, 0x65, 0x50, 0x5c,
-	0x00, 0x65, 0xc6, 0x42,
-	0x00, 0x65, 0x38, 0x5c,
-	0x31, 0x6a, 0x22, 0x01,
-	0x0c, 0x6a, 0xd6, 0x5a,
-	0x00, 0x65, 0x34, 0x42,
-	0x61, 0x6a, 0x22, 0x01,
-	0x00, 0x65, 0x34, 0x42,
-	0x51, 0x6a, 0xda, 0x5a,
-	0x51, 0x6a, 0x22, 0x01,
-	0x00, 0x65, 0x34, 0x42,
-	0x10, 0x3d, 0x06, 0x00,
-	0xff, 0x65, 0x68, 0x0c,
-	0xff, 0x06, 0xd4, 0x08,
-	0x01, 0x0c, 0xdc, 0x7a,
-	0x04, 0x0c, 0xde, 0x6a,
-	0xe0, 0x03, 0x7a, 0x08,
-	0xe0, 0x3d, 0xea, 0x62,
-	0xff, 0x65, 0xcc, 0x08,
-	0xff, 0x12, 0xda, 0x0c,
-	0xff, 0x06, 0xd4, 0x0c,
-	0xd1, 0x6a, 0x22, 0x01,
-	0x00, 0x65, 0xaa, 0x40,
-	0xff, 0x65, 0x26, 0x09,
-	0x01, 0x0b, 0xfe, 0x6a,
-	0x10, 0x0c, 0xf0, 0x7a,
-	0x04, 0x0b, 0xf8, 0x6a,
-	0xff, 0x6a, 0xca, 0x08,
-	0x04, 0x93, 0xfc, 0x6a,
-	0x01, 0x94, 0xfa, 0x7a,
-	0x10, 0x94, 0xfc, 0x6a,
-	0x80, 0x3d, 0x02, 0x73,
-	0x0f, 0x04, 0x06, 0x6b,
-	0x02, 0x03, 0x06, 0x7b,
-	0x11, 0x0c, 0x02, 0x7b,
-	0xc7, 0x93, 0x26, 0x09,
-	0xff, 0x99, 0xd4, 0x08,
-	0x38, 0x93, 0x08, 0x6b,
-	0xff, 0x6a, 0xd4, 0x0c,
-	0x80, 0x36, 0x0c, 0x6b,
-	0x21, 0x6a, 0x22, 0x05,
-	0xff, 0x65, 0x20, 0x09,
-	0xff, 0x51, 0x1a, 0x63,
-	0xff, 0x37, 0xc8, 0x08,
-	0xa1, 0x6a, 0x26, 0x43,
-	0xff, 0x51, 0xc8, 0x08,
-	0xb9, 0x6a, 0x26, 0x43,
-	0xff, 0x90, 0xa4, 0x08,
-	0xff, 0xba, 0x2a, 0x73,
-	0xff, 0xba, 0x20, 0x09,
-	0xff, 0x65, 0xca, 0x18,
-	0x00, 0x6c, 0x1e, 0x63,
-	0xff, 0x90, 0xca, 0x0c,
-	0xff, 0x6a, 0xca, 0x04,
-	0x20, 0x36, 0x3e, 0x7b,
-	0x00, 0x90, 0x12, 0x5b,
-	0xff, 0x65, 0x3e, 0x73,
-	0xff, 0x52, 0x3c, 0x73,
-	0xff, 0xba, 0xcc, 0x08,
-	0xff, 0x52, 0x20, 0x09,
-	0xff, 0x66, 0x74, 0x09,
-	0xff, 0x65, 0x20, 0x0d,
-	0xff, 0xba, 0x7e, 0x0c,
-	0x00, 0x6a, 0x3e, 0x5c,
-	0x0d, 0x6a, 0x6a, 0x00,
-	0x00, 0x51, 0xca, 0x43,
-	0xff, 0x3f, 0x98, 0x73,
-	0xff, 0x6a, 0xa2, 0x00,
-	0x00, 0x3f, 0x12, 0x5b,
-	0xff, 0x65, 0x98, 0x73,
-	0x20, 0x36, 0x6c, 0x00,
-	0x20, 0xa0, 0x52, 0x6b,
-	0xff, 0xb9, 0xa2, 0x0c,
-	0xff, 0x6a, 0xa2, 0x04,
-	0xff, 0x65, 0xa4, 0x08,
-	0xe0, 0x6a, 0xcc, 0x00,
-	0x45, 0x6a, 0xbe, 0x5b,
-	0x01, 0x6a, 0xd0, 0x01,
-	0x09, 0x6a, 0xd6, 0x01,
-	0x80, 0xeb, 0x5e, 0x7b,
-	0x01, 0x6a, 0xd6, 0x01,
-	0x01, 0xe9, 0xa4, 0x34,
-	0x88, 0x6a, 0xcc, 0x00,
-	0x45, 0x6a, 0xbe, 0x5b,
-	0x01, 0x6a, 0x18, 0x01,
-	0xff, 0x6a, 0x1a, 0x09,
-	0xff, 0x6a, 0x1c, 0x09,
-	0x0d, 0x6a, 0x26, 0x01,
-	0x00, 0x65, 0x30, 0x5c,
-	0xff, 0x99, 0xa4, 0x0c,
-	0xff, 0x65, 0xa4, 0x08,
-	0xe0, 0x6a, 0xcc, 0x00,
-	0x45, 0x6a, 0xbe, 0x5b,
-	0x01, 0x6a, 0xd0, 0x01,
-	0x01, 0x6a, 0xdc, 0x05,
-	0x88, 0x6a, 0xcc, 0x00,
-	0x45, 0x6a, 0xbe, 0x5b,
-	0x01, 0x6a, 0x18, 0x01,
-	0xff, 0x6a, 0x1a, 0x09,
-	0xff, 0x6a, 0x1c, 0x09,
-	0x01, 0x6a, 0x26, 0x05,
-	0x01, 0x65, 0xd8, 0x31,
-	0x09, 0xee, 0xdc, 0x01,
-	0x80, 0xee, 0x8e, 0x7b,
-	0xff, 0x6a, 0xdc, 0x0d,
-	0xff, 0x65, 0x32, 0x09,
-	0x0a, 0x93, 0x26, 0x01,
-	0x00, 0x65, 0x30, 0x44,
-	0xff, 0x37, 0xc8, 0x08,
-	0x00, 0x6a, 0x54, 0x5b,
-	0xff, 0x52, 0xa2, 0x0c,
-	0x01, 0x0c, 0x9e, 0x7b,
-	0x04, 0x0c, 0x9e, 0x6b,
-	0xe0, 0x03, 0x06, 0x08,
-	0xe0, 0x03, 0x7a, 0x0c,
-	0xff, 0x8c, 0x10, 0x08,
-	0xff, 0x8d, 0x12, 0x08,
-	0xff, 0x8e, 0x14, 0x0c,
-	0xff, 0x6c, 0xda, 0x08,
-	0xff, 0x6c, 0xda, 0x08,
-	0xff, 0x6c, 0xda, 0x08,
-	0xff, 0x6c, 0xda, 0x08,
-	0xff, 0x6c, 0xda, 0x08,
-	0xff, 0x6c, 0xda, 0x08,
-	0xff, 0x6c, 0xda, 0x0c,
-	0x3d, 0x64, 0xa4, 0x28,
-	0x55, 0x64, 0xc8, 0x28,
-	0x00, 0x6c, 0xda, 0x18,
-	0xff, 0x52, 0xc8, 0x08,
-	0x00, 0x6c, 0xda, 0x20,
-	0xff, 0x6a, 0xc8, 0x08,
-	0x00, 0x6c, 0xda, 0x20,
-	0x00, 0x6c, 0xda, 0x24,
-	0xff, 0x65, 0xc8, 0x08,
-	0xe0, 0x6a, 0xcc, 0x00,
-	0x41, 0x6a, 0xba, 0x5b,
-	0xff, 0x90, 0xe2, 0x09,
-	0x20, 0x6a, 0xd0, 0x01,
-	0x04, 0x35, 0xdc, 0x7b,
-	0x1d, 0x6a, 0xdc, 0x01,
-	0xdc, 0xee, 0xd8, 0x63,
-	0x00, 0x65, 0xe8, 0x43,
-	0x01, 0x6a, 0xdc, 0x01,
-	0x20, 0xa0, 0xd8, 0x31,
-	0x09, 0xee, 0xdc, 0x01,
-	0x80, 0xee, 0xe2, 0x7b,
-	0x19, 0x6a, 0xdc, 0x01,
-	0xd8, 0xee, 0xe6, 0x63,
-	0xff, 0x6a, 0xdc, 0x09,
-	0x18, 0xee, 0xea, 0x6b,
-	0xff, 0x6a, 0xd4, 0x0c,
-	0x88, 0x6a, 0xcc, 0x00,
-	0x41, 0x6a, 0xba, 0x5b,
-	0x20, 0x6a, 0x18, 0x01,
-	0xff, 0x6a, 0x1a, 0x09,
-	0xff, 0x6a, 0x1c, 0x09,
-	0xff, 0x35, 0x26, 0x09,
-	0x04, 0x35, 0x14, 0x6c,
-	0xa0, 0x6a, 0xca, 0x00,
-	0x20, 0x65, 0xc8, 0x18,
-	0xff, 0x6c, 0x32, 0x09,
-	0xff, 0x6c, 0x32, 0x09,
-	0xff, 0x6c, 0x32, 0x09,
-	0xff, 0x6c, 0x32, 0x09,
-	0xff, 0x6c, 0x32, 0x09,
-	0xff, 0x6c, 0x32, 0x09,
-	0xff, 0x6c, 0x32, 0x09,
-	0xff, 0x6c, 0x32, 0x09,
-	0x00, 0x65, 0x00, 0x64,
-	0x0a, 0x93, 0x26, 0x01,
-	0x00, 0x65, 0x30, 0x5c,
-	0x04, 0x35, 0x0c, 0x7b,
-	0xa0, 0x6a, 0x20, 0x5c,
-	0x00, 0x65, 0x22, 0x5c,
-	0x00, 0x65, 0x22, 0x5c,
-	0x00, 0x65, 0x22, 0x44,
-	0xff, 0x65, 0xcc, 0x08,
-	0xff, 0x99, 0xda, 0x08,
-	0xff, 0x99, 0xda, 0x08,
-	0xff, 0x99, 0xda, 0x08,
-	0xff, 0x99, 0xda, 0x08,
-	0xff, 0x99, 0xda, 0x08,
-	0xff, 0x99, 0xda, 0x08,
-	0xff, 0x99, 0xda, 0x0c,
-	0x08, 0x94, 0x30, 0x7c,
-	0xf7, 0x93, 0x26, 0x09,
-	0x08, 0x93, 0x34, 0x6c,
-	0xff, 0x6a, 0xd4, 0x0c,
-	0xff, 0x40, 0x74, 0x09,
-	0xff, 0x90, 0x80, 0x08,
-	0xff, 0x6a, 0x72, 0x05,
-	0xff, 0x40, 0x4c, 0x64,
-	0xff, 0x3f, 0x44, 0x64,
-	0xff, 0x6a, 0xca, 0x04,
-	0xff, 0x3f, 0x20, 0x09,
-	0x01, 0x6a, 0x6a, 0x00,
-	0x00, 0xb9, 0xca, 0x5b,
-	0xff, 0xba, 0x7e, 0x0c,
-	0xff, 0x40, 0x20, 0x09,
-	0xff, 0xba, 0x80, 0x0c,
-	0xff, 0x3f, 0x74, 0x09,
-	0xff, 0x90, 0x7e, 0x0c,
-};
-
-static int aic7xxx_patch12_func(struct aic7xxx_host *p);
-
-static int
-aic7xxx_patch12_func(struct aic7xxx_host *p)
-{
-	return ((p->features & AHC_WIDE) != 0);
-}
-
-static int aic7xxx_patch11_func(struct aic7xxx_host *p);
-
-static int
-aic7xxx_patch11_func(struct aic7xxx_host *p)
-{
-	return ((p->features & AHC_ULTRA2) == 0);
-}
-
-static int aic7xxx_patch10_func(struct aic7xxx_host *p);
-
-static int
-aic7xxx_patch10_func(struct aic7xxx_host *p)
-{
-	return ((p->features & AHC_CMD_CHAN) == 0);
-}
-
-static int aic7xxx_patch9_func(struct aic7xxx_host *p);
-
-static int
-aic7xxx_patch9_func(struct aic7xxx_host *p)
-{
-	return ((p->chip & AHC_CHIPID_MASK) == AHC_AIC7895);
-}
-
-static int aic7xxx_patch8_func(struct aic7xxx_host *p);
-
-static int
-aic7xxx_patch8_func(struct aic7xxx_host *p)
-{
-	return ((p->features & AHC_ULTRA) != 0);
-}
-
-static int aic7xxx_patch7_func(struct aic7xxx_host *p);
-
-static int
-aic7xxx_patch7_func(struct aic7xxx_host *p)
-{
-	return ((p->features & AHC_ULTRA2) != 0);
-}
-
-static int aic7xxx_patch6_func(struct aic7xxx_host *p);
-
-static int
-aic7xxx_patch6_func(struct aic7xxx_host *p)
-{
-	return ((p->flags & AHC_PAGESCBS) == 0);
-}
-
-static int aic7xxx_patch5_func(struct aic7xxx_host *p);
-
-static int
-aic7xxx_patch5_func(struct aic7xxx_host *p)
-{
-	return ((p->flags & AHC_PAGESCBS) != 0);
-}
-
-static int aic7xxx_patch4_func(struct aic7xxx_host *p);
-
-static int
-aic7xxx_patch4_func(struct aic7xxx_host *p)
-{
-	return ((p->features & AHC_QUEUE_REGS) != 0);
-}
-
-static int aic7xxx_patch3_func(struct aic7xxx_host *p);
-
-static int
-aic7xxx_patch3_func(struct aic7xxx_host *p)
-{
-	return ((p->features & AHC_TWIN) != 0);
-}
-
-static int aic7xxx_patch2_func(struct aic7xxx_host *p);
-
-static int
-aic7xxx_patch2_func(struct aic7xxx_host *p)
-{
-	return ((p->features & AHC_QUEUE_REGS) == 0);
-}
-
-static int aic7xxx_patch1_func(struct aic7xxx_host *p);
-
-static int
-aic7xxx_patch1_func(struct aic7xxx_host *p)
-{
-	return ((p->features & AHC_CMD_CHAN) != 0);
-}
-
-static int aic7xxx_patch0_func(struct aic7xxx_host *p);
-
-static int
-aic7xxx_patch0_func(struct aic7xxx_host *p)
-{
-	return (0);
-}
-
-struct sequencer_patch {
-	int		(*patch_func)(struct aic7xxx_host *);
-	unsigned int	begin	   :10,
-			skip_instr :10,
-			skip_patch :12;
-} sequencer_patches[] = {
-	{ aic7xxx_patch1_func, 3, 2, 1 },
-	{ aic7xxx_patch2_func, 7, 1, 1 },
-	{ aic7xxx_patch2_func, 8, 1, 1 },
-	{ aic7xxx_patch3_func, 11, 4, 1 },
-	{ aic7xxx_patch4_func, 16, 3, 2 },
-	{ aic7xxx_patch0_func, 19, 4, 1 },
-	{ aic7xxx_patch5_func, 23, 1, 1 },
-	{ aic7xxx_patch6_func, 26, 1, 1 },
-	{ aic7xxx_patch1_func, 29, 1, 2 },
-	{ aic7xxx_patch0_func, 30, 3, 1 },
-	{ aic7xxx_patch3_func, 39, 4, 1 },
-	{ aic7xxx_patch7_func, 43, 3, 2 },
-	{ aic7xxx_patch0_func, 46, 3, 1 },
-	{ aic7xxx_patch8_func, 52, 7, 1 },
-	{ aic7xxx_patch3_func, 60, 3, 1 },
-	{ aic7xxx_patch7_func, 63, 2, 1 },
-	{ aic7xxx_patch7_func, 102, 1, 2 },
-	{ aic7xxx_patch0_func, 103, 2, 1 },
-	{ aic7xxx_patch7_func, 107, 2, 1 },
-	{ aic7xxx_patch9_func, 109, 1, 1 },
-	{ aic7xxx_patch10_func, 110, 2, 1 },
-	{ aic7xxx_patch7_func, 113, 1, 2 },
-	{ aic7xxx_patch0_func, 114, 1, 1 },
-	{ aic7xxx_patch1_func, 118, 1, 1 },
-	{ aic7xxx_patch1_func, 121, 3, 2 },
-	{ aic7xxx_patch0_func, 124, 5, 1 },
-	{ aic7xxx_patch1_func, 132, 2, 3 },
-	{ aic7xxx_patch7_func, 132, 1, 1 },
-	{ aic7xxx_patch0_func, 134, 3, 1 },
-	{ aic7xxx_patch11_func, 138, 1, 2 },
-	{ aic7xxx_patch0_func, 139, 1, 1 },
-	{ aic7xxx_patch7_func, 140, 7, 2 },
-	{ aic7xxx_patch0_func, 147, 1, 1 },
-	{ aic7xxx_patch1_func, 152, 14, 3 },
-	{ aic7xxx_patch11_func, 165, 1, 1 },
-	{ aic7xxx_patch0_func, 166, 9, 1 },
-	{ aic7xxx_patch7_func, 180, 2, 1 },
-	{ aic7xxx_patch7_func, 182, 1, 1 },
-	{ aic7xxx_patch11_func, 183, 6, 3 },
-	{ aic7xxx_patch1_func, 183, 2, 2 },
-	{ aic7xxx_patch0_func, 185, 4, 1 },
-	{ aic7xxx_patch7_func, 190, 1, 1 },
-	{ aic7xxx_patch7_func, 194, 20, 1 },
-	{ aic7xxx_patch1_func, 215, 3, 3 },
-	{ aic7xxx_patch11_func, 217, 1, 1 },
-	{ aic7xxx_patch0_func, 218, 5, 1 },
-	{ aic7xxx_patch11_func, 223, 1, 2 },
-	{ aic7xxx_patch0_func, 224, 9, 1 },
-	{ aic7xxx_patch12_func, 240, 1, 2 },
-	{ aic7xxx_patch0_func, 241, 1, 1 },
-	{ aic7xxx_patch4_func, 302, 1, 2 },
-	{ aic7xxx_patch0_func, 303, 1, 1 },
-	{ aic7xxx_patch2_func, 306, 1, 1 },
-	{ aic7xxx_patch1_func, 316, 3, 2 },
-	{ aic7xxx_patch0_func, 319, 5, 1 },
-	{ aic7xxx_patch12_func, 327, 1, 2 },
-	{ aic7xxx_patch0_func, 328, 1, 1 },
-	{ aic7xxx_patch5_func, 333, 1, 1 },
-	{ aic7xxx_patch11_func, 375, 15, 1 },
-	{ aic7xxx_patch1_func, 427, 7, 2 },
-	{ aic7xxx_patch0_func, 434, 8, 1 },
-	{ aic7xxx_patch1_func, 443, 4, 2 },
-	{ aic7xxx_patch0_func, 447, 6, 1 },
-	{ aic7xxx_patch1_func, 453, 4, 2 },
-	{ aic7xxx_patch0_func, 457, 3, 1 },
-	{ aic7xxx_patch10_func, 467, 10, 1 },
-	{ aic7xxx_patch1_func, 486, 17, 4 },
-	{ aic7xxx_patch9_func, 494, 4, 2 },
-	{ aic7xxx_patch0_func, 498, 2, 1 },
-	{ aic7xxx_patch0_func, 503, 33, 1 },
-	{ aic7xxx_patch10_func, 536, 4, 1 },
-	{ aic7xxx_patch5_func, 540, 2, 1 },
-	{ aic7xxx_patch5_func, 543, 9, 1 },
-
-};
diff -urN linux/drivers/scsi/hosts.c /tmp/linux/drivers/scsi/hosts.c
--- linux/drivers/scsi/hosts.c	Sun Dec 10 17:49:43 2000
+++ /tmp/linux/drivers/scsi/hosts.c	Fri Feb  2 19:06:15 2001
@@ -136,7 +136,7 @@
 #endif
 
 #ifdef CONFIG_SCSI_AIC7XXX
-#include "aic7xxx.h"
+#include "aic7xxx/aic7xxx_linux_host.h"
 #endif
 
 #ifdef CONFIG_SCSI_IPS
diff -urN linux/fs/Config.in /tmp/linux/fs/Config.in
--- linux/fs/Config.in	Sun Dec 10 17:49:44 2000
+++ /tmp/linux/fs/Config.in	Fri Feb  2 19:05:42 2001
@@ -54,6 +54,13 @@
 if [ "$CONFIG_UFS_FS" != "n" ]; then
   bool '   UFS filesystem write support (experimental)' CONFIG_UFS_FS_WRITE
 fi
+
+#reiserfs support
+tristate 'Reiserfs support' CONFIG_REISERFS_FS 
+if [ "$CONFIG_REISERFS_FS" != "n" ]; then
+  bool '   Enable ReiserFS internal checks' CONFIG_REISERFS_CHECK
+fi
+
 if [ "$CONFIG_EXPERIMENTAL" = "y" ]; then
   tristate 'SGI EFS filesystem support (read only) (experimental)' CONFIG_EFS_FS
   if [ "$CONFIG_EFS_FS" != "n" ]; then
diff -urN linux/fs/Makefile /tmp/linux/fs/Makefile
--- linux/fs/Makefile	Wed Aug 25 18:29:49 1999
+++ /tmp/linux/fs/Makefile	Fri Feb  2 19:05:42 2001
@@ -17,6 +17,7 @@
 
 MOD_LIST_NAME := FS_MODULES
 ALL_SUB_DIRS = coda minix ext2 fat msdos vfat proc isofs nfs umsdos ntfs \
+		reiserfs \
 		hpfs sysv smbfs ncpfs ufs affs romfs autofs hfs lockd \
 		nfsd nls devpts adfs qnx4 efs
 
@@ -244,6 +245,14 @@
 else
   ifeq ($(CONFIG_DEVPTS_FS),m)
   MOD_SUB_DIRS += devpts
+  endif
+endif
+
+ifeq ($(CONFIG_REISERFS_FS),y)
+SUB_DIRS += reiserfs
+else
+  ifeq ($(CONFIG_REISERFS_FS),m)
+  MOD_SUB_DIRS += reiserfs
   endif
 endif
 
diff -urN linux/fs/binfmt_aout.c /tmp/linux/fs/binfmt_aout.c
--- linux/fs/binfmt_aout.c	Tue Jan  4 11:12:22 2000
+++ /tmp/linux/fs/binfmt_aout.c	Fri Feb  2 19:06:42 2001
@@ -62,9 +62,9 @@
 static int dump_write(struct file *file, const void *addr, int nr)
 {
 	int r;
-	down(&file->f_dentry->d_inode->i_sem);
+	fs_down(&file->f_dentry->d_inode->i_sem);
 	r = file->f_op->write(file, addr, nr, &file->f_pos) == nr;
-	up(&file->f_dentry->d_inode->i_sem);
+	fs_up(&file->f_dentry->d_inode->i_sem);
 	return r;
 }
 
diff -urN linux/fs/binfmt_elf.c /tmp/linux/fs/binfmt_elf.c
--- linux/fs/binfmt_elf.c	Sun Dec 10 17:49:44 2000
+++ /tmp/linux/fs/binfmt_elf.c	Fri Feb  2 19:06:42 2001
@@ -948,9 +948,9 @@
 static int dump_write(struct file *file, const void *addr, int nr)
 {
 	int r;
-	down(&file->f_dentry->d_inode->i_sem);
+	fs_down(&file->f_dentry->d_inode->i_sem);
 	r = file->f_op->write(file, addr, nr, &file->f_pos) == nr;
-	up(&file->f_dentry->d_inode->i_sem);
+	fs_up(&file->f_dentry->d_inode->i_sem);
 	return r;
 }
 
diff -urN linux/fs/buffer.c /tmp/linux/fs/buffer.c
--- linux/fs/buffer.c	Sun Dec 10 17:49:44 2000
+++ /tmp/linux/fs/buffer.c	Fri Feb  2 19:06:42 2001
@@ -27,10 +27,6 @@
 /* invalidate_buffers/set_blocksize/sync_dev race conditions and
    fs corruption fixes, 1999, Andrea Arcangeli <andrea@suse.de> */
 
-/* Wait for dirty buffers to sync in sync_page_buffers.
- * 2000, Marcelo Tosatti <marcelo@conectiva.com.br>
- */
-
 #include <linux/malloc.h>
 #include <linux/locks.h>
 #include <linux/errno.h>
@@ -83,6 +79,7 @@
 
 static int nr_buffers = 0;
 static int nr_buffers_type[NR_LIST] = {0,};
+static unsigned long size_buffers_type[NR_LIST];
 static int nr_buffer_heads = 0;
 static int nr_unused_buffer_heads = 0;
 static int nr_hashed_buffers = 0;
@@ -359,9 +356,9 @@
 		goto out_putf;
 
 	/* We need to protect against concurrent writers.. */
-	down(&inode->i_sem);
+	fs_down(&inode->i_sem);
 	err = file->f_op->fsync(file, dentry);
-	up(&inode->i_sem);
+	fs_up(&inode->i_sem);
 
 out_putf:
 	fput(file);
@@ -396,9 +393,9 @@
 		goto out_putf;
 
 	/* this needs further work, at the moment it is identical to fsync() */
-	down(&inode->i_sem);
+	fs_down(&inode->i_sem);
 	err = file->f_op->fsync(file, dentry);
-	up(&inode->i_sem);
+	fs_up(&inode->i_sem);
 
 out_putf:
 	fput(file);
@@ -474,6 +471,7 @@
 		return;
 	}
 	nr_buffers_type[bh->b_list]--;
+	size_buffers_type[bh->b_list] -= bh->b_size;
 	remove_from_hash_queue(bh);
 	remove_from_lru_list(bh);
 }
@@ -523,6 +521,7 @@
 		(*bhp)->b_prev_free = bh;
 
 		nr_buffers_type[bh->b_list]++;
+		size_buffers_type[bh->b_list] += bh->b_size;
 
 		/* Put the buffer in new hash-queue if it has a device. */
 		bh->b_next = NULL;
@@ -571,8 +570,10 @@
 {
 	struct buffer_head * bh;
 	bh = find_buffer(dev,block,size);
-	if (bh)
+	if (bh) {
 		bh->b_count++;
+		touch_buffer(bh);
+	}
 	return bh;
 }
 
@@ -816,6 +817,46 @@
 	insert_into_queues(bh);
 }
 
+/* -1 -> no need to flush
+    0 -> async flush
+    1 -> sync flush (wait for I/O completation) */
+static int balance_dirty_state(kdev_t dev)
+{
+	unsigned long dirty, tot, hard_dirty_limit, soft_dirty_limit;
+
+	dirty = size_buffers_type[BUF_DIRTY] >> PAGE_SHIFT;
+	tot = (buffermem >> PAGE_SHIFT) + nr_free_pages;
+	tot -= size_buffers_type[BUF_PROTECTED] >> PAGE_SHIFT;
+
+	dirty *= 200;
+	soft_dirty_limit = tot * bdf_prm.b_un.nfract;
+	hard_dirty_limit = soft_dirty_limit * 2;
+
+	if (dirty > soft_dirty_limit)
+	{
+		if (dirty > hard_dirty_limit)
+			return 1;
+		return 0;
+	}
+	return -1;
+}
+
+/*
+ * if a new dirty buffer is created we need to balance bdflush.
+ *
+ * in the future we might want to make bdflush aware of different
+ * pressures on different devices - thus the (currently unused)
+ * 'dev' parameter.
+ */
+void balance_dirty(kdev_t dev)
+{
+	int state = balance_dirty_state(dev);
+
+	if (state < 0)
+		return;
+	wakeup_bdflush(state);
+}
+
 /*
  * A buffer may need to be moved from one buffer list to another
  * (e.g. in case it is not shared any more). Handle this.
@@ -828,7 +869,9 @@
 		printk("Attempt to refile free buffer\n");
 		return;
 	}
-	if (buffer_dirty(buf))
+	if (buffer_protected(buf))
+		dispose = BUF_PROTECTED;
+	else if (buffer_dirty(buf))
 		dispose = BUF_DIRTY;
 	else if (buffer_locked(buf))
 		dispose = BUF_LOCKED;
@@ -837,13 +880,7 @@
 	if(dispose != buf->b_list) {
 		file_buffer(buf, dispose);
 		if(dispose == BUF_DIRTY) {
-			int too_many = (nr_buffers * bdf_prm.b_un.nfract/100);
-
-			/* This buffer is dirty, maybe we need to start flushing.
-			 * If too high a percentage of the buffers are dirty...
-			 */
-			if (nr_buffers_type[BUF_DIRTY] > too_many)
-				wakeup_bdflush(1);
+			balance_dirty(buf->b_dev);
 
 			/* If this is a loop device, and
 			 * more than half of the buffers are dirty...
@@ -864,7 +901,6 @@
 	/* If dirty, mark the time this buffer should be written back. */
 	set_writetime(buf, 0);
 	refile_buffer(buf);
-	touch_buffer(buf);
 
 	if (buf->b_count) {
 		buf->b_count--;
@@ -1457,6 +1493,7 @@
 	}
 	tmp->b_this_page = bh;
 	free_list[isize] = bh;
+	mem_map[MAP_NR(page)].flags = 0;
 	mem_map[MAP_NR(page)].buffers = bh;
 	buffermem += PAGE_SIZE;
 	return 1;
@@ -1468,33 +1505,34 @@
 #define BUFFER_BUSY_BITS	((1<<BH_Dirty) | (1<<BH_Lock) | (1<<BH_Protected))
 #define buffer_busy(bh)		((bh)->b_count || ((bh)->b_state & BUFFER_BUSY_BITS))
 
-static int sync_page_buffers(struct page * page, int wait)
+static void sync_page_buffers(struct page * page)
 {
-	struct buffer_head * bh = page->buffers;
-	struct buffer_head * tmp = bh;
+	struct buffer_head * tmp, * bh = page->buffers;
 
+	/*
+	 * Here we'll probably sleep and so we must make sure that
+	 * the page doesn't go away from under us. We also prefer any
+	 * concurrent try_to_free_buffers() not to work in any way on
+	 * our current page from under us since we're just working on it.
+	 * As always in 2.2.x we're serialized by the big kernel lock
+	 * during those hacky page-visibility manipulations.
+	 *
+	 * SUBTLE NOTE: for things like LVM snapshotting WRITEA will block too!
+	 */
 	page->buffers = NULL;
 
+	tmp = bh;
 	do {
 		struct buffer_head *p = tmp;
 		tmp = tmp->b_this_page;
-		if (buffer_locked(p)) {
-			if (wait)
-				__wait_on_buffer(p);
-		} else if (buffer_dirty(p))
-			ll_rw_block(WRITE, 1, &p);
-	} while (tmp != bh);
-
-	page->buffers = bh;
 
-	do {
-		struct buffer_head *p = tmp;
-		tmp = tmp->b_this_page;
-		if (buffer_busy(p))
-			return 1;
+		if (buffer_dirty(p))
+			if (test_and_set_bit(BH_Wait_IO, &p->b_state))
+				ll_rw_block(WRITE, 1, &p);
 	} while (tmp != bh);
 
-	return 0;
+	/* Restore the visibility of the page before returning. */
+	page->buffers = bh;
 }
 
 /*
@@ -1504,10 +1542,9 @@
  * Wake up bdflush() if this fails - if we're running low on memory due
  * to dirty buffers, we need to flush them out as quickly as possible.
  */
-int try_to_free_buffers(struct page * page_map, int wait)
+int try_to_free_buffers(struct page * page_map, int gfp_mask)
 {
 	struct buffer_head * tmp, * bh = page_map->buffers;
-	int too_many;
 
 	tmp = bh;
 	do {
@@ -1516,8 +1553,6 @@
 		tmp = tmp->b_this_page;
 	} while (tmp != bh);
 
- succeed:
-	tmp = bh;
 	do {
 		struct buffer_head * p = tmp;
 		tmp = tmp->b_this_page;
@@ -1536,25 +1571,12 @@
 	return 1;
 
  busy:
-	too_many = (nr_buffers * bdf_prm.b_un.nfract/100);
-
-	if (!sync_page_buffers(page_map, wait)) {
+	if (gfp_mask & __GFP_IO)
+		sync_page_buffers(page_map);
 
-		/* If a high percentage of the buffers are dirty, 
-		 * wake kflushd 
-		 */
-		if (nr_buffers_type[BUF_DIRTY] > too_many)
-			wakeup_bdflush(0);
-			
-		/*
-		 * We can jump after the busy check because
-		 * we rely on the kernel lock.
-		 */
-		goto succeed;
-	}
-
-	if(nr_buffers_type[BUF_DIRTY] > too_many)
+	if (balance_dirty_state(NODEV) >= 0)
 		wakeup_bdflush(0);
+
 	return 0;
 }
 
@@ -1566,7 +1588,7 @@
 	int found = 0, locked = 0, dirty = 0, used = 0, lastused = 0;
 	int protected = 0;
 	int nlist;
-	static char *buf_types[NR_LIST] = {"CLEAN","LOCKED","DIRTY"};
+	static char *buf_types[NR_LIST] = {"CLEAN","LOCKED","DIRTY","PROTECTED",};
 
 	printk("Buffer memory:   %8ldkB\n",buffermem>>10);
 	printk("Buffer heads:    %6d\n",nr_buffer_heads);
@@ -1590,7 +1612,7 @@
 			used++, lastused = found;
 		bh = bh->b_next_free;
 	  } while (bh != lru_list[nlist]);
-	  printk("%8s: %d buffers, %d used (last=%d), "
+	  printk("%9s: %d buffers, %d used (last=%d), "
 		 "%d locked, %d protected, %d dirty\n",
 		 buf_types[nlist], found, used, lastused,
 		 locked, protected, dirty);
@@ -1935,7 +1957,8 @@
 		
 		/* If there are still a lot of dirty buffers around, skip the sleep
 		   and flush some more */
-		if(ndirty == 0 || nr_buffers_type[BUF_DIRTY] <= nr_buffers * bdf_prm.b_un.nfract/100) {
+		if (!ndirty || balance_dirty_state(NODEV) < 0)
+		{
 			spin_lock_irq(&current->sigmask_lock);
 			flush_signals(current);
 			spin_unlock_irq(&current->sigmask_lock);
@@ -1983,3 +2006,5 @@
 		sync_old_buffers();
 	}
 }
+
+#include "reiserfs/buffer.c"
diff -urN linux/fs/coda/file.c /tmp/linux/fs/coda/file.c
--- linux/fs/coda/file.c	Mon Sep  4 11:39:22 2000
+++ /tmp/linux/fs/coda/file.c	Fri Feb  2 19:06:42 2001
@@ -190,10 +190,10 @@
                 return -1;
         }
 
-	down(&cont_inode->i_sem);
+	fs_down(&cont_inode->i_sem);
         result = cont_file.f_op->write(&cont_file , buff, count, 
 				       &(cont_file.f_pos));
-	up(&cont_inode->i_sem);
+	fs_up(&cont_inode->i_sem);
         coda_restore_codafile(coda_inode, coda_file, cont_inode, &cont_file);
 	
 	if (result)
@@ -228,14 +228,14 @@
         coda_prepare_openfile(coda_inode, coda_file, cont_inode, 
 			      &cont_file, &cont_dentry);
 
-	down(&cont_inode->i_sem);
+	fs_down(&cont_inode->i_sem);
 
         result = file_fsync(&cont_file ,&cont_dentry);
 	if ( result == 0 ) {
 		result = venus_fsync(coda_inode->i_sb, &(cnp->c_fid));
 	}
 
-	up(&cont_inode->i_sem);
+	fs_up(&cont_inode->i_sem);
 
         coda_restore_codafile(coda_inode, coda_file, cont_inode, &cont_file);
         return result;
diff -urN linux/fs/dcache.c /tmp/linux/fs/dcache.c
--- linux/fs/dcache.c	Wed Jun  7 15:26:43 2000
+++ /tmp/linux/fs/dcache.c	Fri Feb  2 19:06:42 2001
@@ -475,9 +475,9 @@
  */
 void shrink_dcache_memory(int priority, unsigned int gfp_mask)
 {
-	if (gfp_mask & __GFP_IO) {
+	if (gfp_mask & __GFP_IO && !current->fs_locks) {
 		int count = 0;
-		if (priority)
+		if (priority > 1)
 			count = dentry_stat.nr_unused / priority;
 		prune_dcache(count, -1);
 	}
diff -urN linux/fs/filesystems.c /tmp/linux/fs/filesystems.c
--- linux/fs/filesystems.c	Sun Dec 10 17:49:44 2000
+++ /tmp/linux/fs/filesystems.c	Fri Feb  2 19:05:42 2001
@@ -29,6 +29,7 @@
 #include <linux/ntfs_fs.h>
 #include <linux/hfs_fs.h>
 #include <linux/devpts_fs.h>
+#include <linux/reiserfs_fs.h>
 #include <linux/major.h>
 #include <linux/smp.h>
 #include <linux/smp_lock.h>
@@ -60,6 +61,10 @@
 
 #ifdef CONFIG_MINIX_FS
 	init_minix_fs();
+#endif
+
+#ifdef CONFIG_REISERFS_FS
+	init_reiserfs_fs();
 #endif
 
 #ifdef CONFIG_ROMFS_FS
diff -urN linux/fs/inode.c /tmp/linux/fs/inode.c
--- linux/fs/inode.c	Sun Dec 10 17:49:44 2000
+++ /tmp/linux/fs/inode.c	Fri Feb  2 19:05:42 2001
@@ -100,7 +100,7 @@
 	}
 }
 
-static void __wait_on_inode(struct inode * inode)
+/*static*/ void __wait_on_inode(struct inode * inode)
 {
 	struct wait_queue wait = { current, NULL };
 
diff -urN linux/fs/open.c /tmp/linux/fs/open.c
--- linux/fs/open.c	Sun Dec 10 17:49:44 2000
+++ /tmp/linux/fs/open.c	Fri Feb  2 19:06:42 2001
@@ -73,7 +73,7 @@
 	if ((off_t) length < 0)
 		return -EINVAL;
 
-	down(&inode->i_sem);
+	fs_down(&inode->i_sem);
 	newattrs.ia_size = length;
 	newattrs.ia_valid = ATTR_SIZE | ATTR_CTIME;
 	error = notify_change(dentry, &newattrs);
@@ -83,7 +83,7 @@
 		if (inode->i_op && inode->i_op->truncate)
 			inode->i_op->truncate(inode);
 	}
-	up(&inode->i_sem);
+	fs_up(&inode->i_sem);
 	return error;
 }
 
diff -urN linux/fs/read_write.c /tmp/linux/fs/read_write.c
--- linux/fs/read_write.c	Mon Sep  4 11:39:27 2000
+++ /tmp/linux/fs/read_write.c	Fri Feb  2 19:06:42 2001
@@ -166,9 +166,9 @@
 	if (!file->f_op || !(write = file->f_op->write))
 		goto out;
 
-	down(&inode->i_sem);
+	fs_down(&inode->i_sem);
 	ret = write(file, buf, count, &file->f_pos);
-	up(&inode->i_sem);
+	fs_up(&inode->i_sem);
 out:
 	fput(file);
 bad_file:
@@ -314,9 +314,9 @@
 	if (!file)
 		goto bad_file;
 	if (file->f_op && file->f_op->write && (file->f_mode & FMODE_WRITE)) {
-		down(&file->f_dentry->d_inode->i_sem);
+		fs_down(&file->f_dentry->d_inode->i_sem);
 		ret = do_readv_writev(VERIFY_READ, file, vector, count);
-		up(&file->f_dentry->d_inode->i_sem);
+		fs_up(&file->f_dentry->d_inode->i_sem);
 	}
 	fput(file);
 
@@ -386,9 +386,9 @@
 	if (pos < 0)
 		goto out;
 
-	down(&file->f_dentry->d_inode->i_sem);
+	fs_down(&file->f_dentry->d_inode->i_sem);
 	ret = write(file, buf, count, &pos);
-	up(&file->f_dentry->d_inode->i_sem);
+	fs_up(&file->f_dentry->d_inode->i_sem);
 
 out:
 	fput(file);
diff -urN linux/fs/reiserfs/Makefile /tmp/linux/fs/reiserfs/Makefile
--- linux/fs/reiserfs/Makefile	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/Makefile	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,19 @@
+#
+# Makefile for the linux reiser-filesystem routines.
+#
+# Note! Dependencies are done automagically by 'make dep', which also
+# removes any old dependencies. DON'T put your own dependencies here
+# unless it's something special (ie not a .c file).
+#
+# Note 2! The CFLAGS definitions are now in the main makefile...
+
+O_TARGET := reiserfs.o
+O_OBJS   := bitmap.o do_balan.o namei.o inode.o file.o dir.o symlink.o fix_node.o super.o prints.o objectid.o version.o \
+lbalance.o ibalance.o stree.o hashes.o buffer2.o journal.o resize.o ioctl.o
+M_OBJS   := $(O_TARGET)
+
+include $(TOPDIR)/Rules.make
+
+TAGS:
+	etags *.c
+
diff -urN linux/fs/reiserfs/README /tmp/linux/fs/reiserfs/README
--- linux/fs/reiserfs/README	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/README	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,157 @@
+[LICENSING] 
+
+ReiserFS is hereby licensed under the GNU General
+Public License version 2.
+
+Source code files that contain the phrase "licensing governed by
+reiserfs/README" are "governed files" throughout this file.  Governed
+files are licensed under the GPL.  The portions of them owned by Hans
+Reiser, or authorized to be licensed by him, have been in the past,
+and likely will be in the future, licensed to other parties under
+other licenses.  If you add your code to governed files, and don't
+want it to be owned by Hans Reiser, put your copyright label on that
+code so the poor blight and his customers can keep things straight.
+All portions of governed files not labeled otherwise are owned by Hans
+Reiser, and by adding your code to it, widely distributing it to
+others or sending us a patch, and leaving the sentence in stating that
+licensing is governed by the statement in this file, you accept this.
+It will be a kindness if you identify whether Hans Reiser is allowed
+to license code labeled as owned by you on your behalf other than
+under the GPL, because he wants to know if it is okay to do so and put
+a check in the mail to you (for non-trivial improvements) when he
+makes his next sale.  He makes no guarantees as to the amount if any,
+though he feels motivated to motivate contributors, and you can surely
+discuss this with him before or after contributing.  You have the
+right to decline to allow him to license your code contribution other
+than under the GPL.
+
+Further licensing options are available for commercial and/or other
+interests directly from Hans Reiser: hans@reiser.to.  If you interpret
+the GPL as not allowing those additional licensing options, you read
+it wrongly, and Richard Stallman agrees with me, when carefully read
+you can see that those restrictions on additional terms do not apply
+to the owner of the copyright, and my interpretation of this shall
+govern for this license.  
+
+Finally, nothing in this license shall be interpreted to allow you to
+fail to fairly credit me, or to remove my credits, without my
+permission, unless you are an end user not redistributing to others.
+If you have doubts about how to properly do that, or about what is
+fair, ask.  (Last I spoke with him Richard was contemplating how best
+to address the fair crediting issue in the next GPL version.)
+
+[END LICENSING]
+
+Reiserfs is a file system based on balanced tree algorithms, which is
+described at http://devlinux.com/namesys.
+
+Stop reading here.  Go there, then return.
+
+Send bug reports to yura@namesys.botik.ru.
+
+mkreiserfs and other utilities are in reiserfs/utils, or wherever your
+Linux provider put them.  There is some disagreement about how useful
+it is for users to get their fsck and mkreiserfs out of sync with the
+version of reiserfs that is in their kernel, with many important
+distributors wanting them out of sync.:-) Please try to remember to
+recompile and reinstall fsck and mkreiserfs with every update of
+reiserfs, this is a common source of confusion.  Note that some of the
+utilities cannot be compiled without accessing the balancing code
+which is in the kernel code, and relocating the utilities may require
+you to specify where that code can be found.
+
+Yes, if you update your reiserfs kernel module you do have to
+recompile your kernel, most of the time.  The errors you get will be
+quite cryptic if your forget to do so.
+
+Real users, as opposed to folks who want to hack and then understand
+what went wrong, will want REISERFS_CHECK off.
+
+Hideous Commercial Pitch: Spread your development costs across other OS
+vendors.  Select from the best in the world, not the best in your
+building, by buying from third party OS component suppliers.  Leverage
+the software component development power of the internet.  Be the most
+aggressive in taking advantage of the commercial possibilities of
+decentralized internet development, and add value through your branded
+integration that you sell as an operating system.  Let your competitors
+be the ones to compete against the entire internet by themselves.  Be
+hip, get with the new economic trend, before your competitors do.  Send
+email to hans@reiser.to.
+
+To understand the code, after reading the website, start reading the
+code by reading reiserfs_fs.h first.
+
+Hans Reiser was the project initiator, primary architect, source of all
+funding for the first 5.5 years, and one of the programmers.  He owns
+the copyright.
+
+Vladimir Saveljev was one of the programmers, and he worked long hours
+writing the cleanest code.  He always made the effort to be the best he
+could be, and to make his code the best that it could be.  What resulted
+was quite remarkable. I don't think that money can ever motivate someone
+to work the way he did, he is one of the most selfless men I know.
+
+Yura helps with benchmarking, coding hashes, and block pre-allocation
+code.
+
+Anatoly Pinchuk is a former member of our team who worked closely with
+Vladimir throughout the project's development.  He wrote a quite
+substantial portion of the total code.  He realized that there was a
+space problem with packing tails of files for files larger than a node
+that start on a node aligned boundary (there are reasons to want to node
+align files), and he invented and implemented indirect items and
+unformatted nodes as the solution.
+
+Konstantin Shvachko, with the help of the Russian version of a VC,
+tried to put me in a position where I was forced into giving control
+of the project to him.  (Fortunately, as the person paying the money
+for all salaries from my dayjob I owned all copyrights, and you can't
+really force takeovers of sole proprietorships.)  This was something
+curious, because he never really understood the value of our project,
+why we should do what we do, or why innovation was possible in
+general, but he was sure that he ought to be controlling it.  Every
+innovation had to be forced past him while he was with us.  He added
+two years to the time required to complete reiserfs, and was a net
+loss for me.  Mikhail Gilula was a brilliant innovator who also left
+in a destructive way that erased the value of his contributions, and
+that he was shown much generosity just makes it more painful.
+
+Grigory Zaigralin was an extremely effective system administrator for
+our group.
+
+Igor Krasheninnikov was wonderful at hardware procurement, repair, and
+network installation.
+
+Jeremy Fitzhardinge wrote the teahash.c code, and he gives credit to a
+textbook he got the algorithm from in the code.  Note that his analysis
+of how we could use the hashing code in making 32 bit NFS cookies work
+was probably more important than the actual algorithm.  Colin Plumb also
+contributed to it.
+
+Chris Mason dived right into our code, and in just a few months produced
+the journaling code that dramatically increased the value of ReiserFS.
+He is just an amazing programmer.
+
+Igor Zagorovsky is writing much of the new item handler and extent code
+for our next major release.
+
+Alexander Zarochentcev (sometimes known as zam, or sasha), wrote the
+resizer, and is hard at work on implementing allocate on flush.  SGI
+implemented allocate on flush before us for XFS, and generously took
+the time to convince me we should do it also.  They are great people,
+and a great company.
+
+Yuri Shevchuk and Nikita Danilov are doing squid cache optimization.
+
+Vitaly Fertman is doing fsck.
+
+SuSE, IntegratedLinux.com, Ecila, MP3.com, bigstorage.com, and the
+Alpha PC Company made it possible for me to not have a day job
+anymore, and to dramatically increase our staffing.  Ecila funded
+hypertext feature development, MP3.com funded journaling, SuSE funded
+core development, IntegratedLinux.com funded squid web cache
+appliances, bigstorage.com funded HSM, and the alpha PC company funded
+the alpha port.  Many of these tasks were helped by sponsors other
+than the ones just named.  SuSE has helped in much more than just
+funding....
+
diff -urN linux/fs/reiserfs/bitmap.c /tmp/linux/fs/reiserfs/bitmap.c
--- linux/fs/reiserfs/bitmap.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/bitmap.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,413 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+#ifdef __KERNEL__
+
+#include <linux/sched.h>
+#include <linux/reiserfs_fs.h>
+#include <linux/locks.h>
+#include <asm/bitops.h>
+
+#else
+
+#include "nokernel.h"
+
+#endif
+
+
+#ifdef CONFIG_REISERFS_CHECK
+#if 0
+static void check_bitmap (struct super_block * s)
+{
+  int i = 0;
+  int free = 0;
+  char * buf;
+
+  while (i < SB_BLOCK_COUNT (s)) {
+    buf = SB_AP_BITMAP (s)[i / (s->s_blocksize * 8)]->b_data;
+    if (!test_bit (i % (s->s_blocksize * 8), buf))
+      free ++;
+    i ++;
+  }
+
+  if (free != SB_FREE_BLOCKS (s))
+    reiserfs_warning ("vs-4000: check_bitmap: %d free blocks, must be %d\n",
+		      free, SB_FREE_BLOCKS (s));
+}
+#endif
+
+
+/* this checks, that block can be reused, and it has correct state
+   (free or busy) */
+int is_reusable (struct super_block * s, unsigned long block, int bit_value)
+{
+  int i, j;
+  
+  if (block == 0 || block >= SB_BLOCK_COUNT (s)) {
+    printk ("REISERFS: block number is out of range %lu (%u)\n",
+	    block, SB_BLOCK_COUNT (s));
+    return 0;
+  }
+
+  /* it can't be one of the bitmap blocks */
+  for (i = 0; i < le16_to_cpu (SB_BMAP_NR (s)); i ++)
+    if (block == SB_AP_BITMAP (s)[i]->b_blocknr) {
+      printk ("REISERFS: bitmap block %lu(%u) can't be freed or reused\n", block, le16_to_cpu (SB_BMAP_NR (s)));
+      return 0;
+    }
+  
+  i = block / (s->s_blocksize << 3);
+  if (i >= le32_to_cpu (SB_BMAP_NR (s))) {
+    printk ("REISERFS: there is no so many bitmap blocks: block=%lu, bitmap_nr=%d\n", block, i);
+    return 0;
+  }
+
+  j = block % (s->s_blocksize << 3);
+  if ((bit_value == 0 && test_bit (j, SB_AP_BITMAP (s)[i]->b_data)) ||
+      (bit_value == 1 && test_bit (j, SB_AP_BITMAP (s)[i]->b_data) == 0)) {
+    printk ("REISERFS: corresponding bit of block %lu does not match required value (i==%d, j==%d) test_bit==%d\n",
+	    block, i, j, test_bit (j, SB_AP_BITMAP (s)[i]->b_data));
+    return 0;
+  }
+
+  if (bit_value == 0 && block == SB_ROOT_BLOCK (s)) {
+    printk ("REISERFS: this is root block (%u), it must be busy", SB_ROOT_BLOCK (s));
+    return 0;
+  }
+
+  return 1;
+}
+
+#endif /* CONFIG_REISERFS_CHECK */
+
+
+/* get address of corresponding bit (bitmap block number and offset in it) */
+static inline void get_bit_address (struct super_block * s, unsigned long block, int * bmap_nr, int * offset)
+{
+                                /* It is in the bitmap block number equal to the block number divided by the number of
+                                   bits in a block. */
+  *bmap_nr = block / (s->s_blocksize << 3);
+                                /* Within that bitmap block it is located at bit offset *offset. */
+  *offset = block % (s->s_blocksize << 3);
+  return;
+}
+
+
+/* There would be a modest performance benefit if we write a version
+   to free a list of blocks at once. -Hans */
+void reiserfs_free_block (struct reiserfs_transaction_handle *th, struct super_block * s, unsigned long block)
+{
+  struct reiserfs_super_block * rs;
+  struct buffer_head * sbh;
+  struct buffer_head ** apbh;
+  int nr, offset;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (!s)
+    reiserfs_panic (s, "vs-4005: reiserfs_free_block: trying to free block on nonexistent device");
+
+  if (is_reusable (s, block, 1) == 0)
+    reiserfs_panic (s, "vs-4010: reiserfs_free_block: can not free such block");
+#endif
+
+  rs = SB_DISK_SUPER_BLOCK (s);
+  sbh = SB_BUFFER_WITH_SB (s);
+  apbh = SB_AP_BITMAP (s);
+
+  get_bit_address (s, block, &nr, &offset);
+
+  /* mark it before we clear it, just in case */
+  journal_mark_freed(th, s, block) ;
+
+  /* clear bit for the given block in bit map */
+  if (!test_and_clear_bit (offset, apbh[nr]->b_data)) {
+    printk ("bitmap-124: reiserfs_free_block: free_block (%04x:%lu)[dev:blocknr]: bit already cleared\n", 
+	    s->s_dev, block);
+  }
+
+  /* clear bit in cautious bitmap */
+  /* clear_bit (offset, SB_AP_CAUTIOUS_BITMAP (s)[nr]->b_data); journal victim */
+
+  /* update super block */
+  rs->s_free_blocks = cpu_to_le32 (le32_to_cpu (rs->s_free_blocks) + 1);
+
+  journal_mark_dirty (th, s, sbh);/* no need to place buffer on preserve list */
+  journal_mark_dirty (th, s, apbh[nr]);/* no need to place buffer on preserve list */
+  s->s_dirt = 1;
+}
+
+
+
+/* beginning from offset-th bit in bmap_nr-th bitmap block,
+   find_forward finds the closest zero bit. It returns 1 and zero
+   bit address (bitmap, offset) if zero bit found or 0 if there is no
+   zero bits in forward direction */
+static int find_forward (struct super_block * s, int * bmap_nr, int * offset, int * repeat, int for_unformatted)
+{
+  int i, j;
+  struct buffer_head * bh;
+  unsigned long block_to_try = 0;
+  unsigned long next_block_to_try = 0 ;
+
+  for (i = *bmap_nr; i < SB_BMAP_NR (s); i ++, *offset = 0) {
+    /* get corresponding bitmap block */
+    bh = SB_AP_BITMAP (s)[i];
+    (*repeat) |= test_and_wait_on_buffer(bh) ;
+retry:
+    j = find_next_zero_bit ((unsigned long *)bh->b_data, s->s_blocksize << 3, *offset);
+
+    /* wow, this really needs to be redone.  We can't allocate a block if
+    ** it is in the journal somehow.  reiserfs_in_journal makes a suggestion
+    ** for a good block if the one you ask for is in the journal.  Note,
+    ** reiserfs_in_journal might reject the block it suggests.  The big
+    ** gain from the suggestion is when a big file has been deleted, and
+    ** many blocks show free in the real bitmap, but are all not free
+    ** in the journal list bitmaps.
+    **
+    ** this whole system sucks.  The bitmaps should reflect exactly what
+    ** can and can't be allocated, and the journal should update them as
+    ** it goes.  TODO.
+    */
+    if (j < (s->s_blocksize << 3)) {
+      block_to_try = (i * (s->s_blocksize << 3)) + j; 
+
+      /* the block is not in the journal, we can proceed */
+      if (!(reiserfs_in_journal(s, s->s_dev, block_to_try, s->s_blocksize, for_unformatted, &next_block_to_try))) {
+	*bmap_nr = i;
+	*offset = j;
+	return 1;
+      } 
+      /* the block is in the journal */
+      else if ((j+1) < (s->s_blocksize << 3)) { /* try again */
+	/* reiserfs_in_journal suggested a new block to try */
+	if (next_block_to_try > 0) {
+	  int new_i ;
+	  get_bit_address (s, next_block_to_try, &new_i, offset);
+
+	  /* block is not in this bitmap. reset i and continue
+	  ** we only reset i if new_i is in a later bitmap.
+	  */
+	  if (new_i > i) {
+	    i = (new_i - 1 ); /* i gets incremented by the for loop */
+	    continue ;
+	  }
+	} else {
+	  /* no suggestion was made, just try the next block */
+	  *offset = j+1 ;
+	}
+	goto retry ;
+      }
+    }
+  }
+  /* zero bit not found */
+  return 0;
+}
+
+                                /* return 0 if no free blocks, else return 1 */
+static int find_zero_bit_in_bitmap (struct super_block * s, unsigned long search_start, int * bmap_nr, int * offset, 
+				    int * repeat, int for_unformatted)
+{
+  int retry_count = 0 ;
+  /* get bit location (bitmap number and bit offset) of search_start block */
+  get_bit_address (s, search_start, bmap_nr, offset);
+
+    /* note that we search forward in the bitmap, benchmarks have shown that it is better to allocate in increasing
+       sequence, which is probably due to the disk spinning in the forward direction.. */
+    if (find_forward (s, bmap_nr, offset, repeat, for_unformatted) == 0) {
+      /* there wasn't a free block with number greater than our
+         starting point, so we are going to go to the beginning of the disk */
+
+retry:
+      search_start = 0; /* caller will reset search_start for itself also. */
+      get_bit_address (s, search_start, bmap_nr, offset);
+      if (find_forward (s, bmap_nr, offset, repeat, for_unformatted) == 0) {
+	if (for_unformatted) {
+	  if (retry_count == 0) {
+	    /* we've got a chance that flushing async commits will free up
+	    ** some space.  Sync then retry
+	    */
+	    flush_async_commits(s, repeat) ;
+	    retry_count++ ;
+	    goto retry ;
+	  } else if (retry_count > 0) {
+	    /* nothing more we can do.  Make the others wait, flush
+	    ** all log blocks to disk, and flush to their home locations.
+	    ** this will free up any blocks held by the journal
+	    */
+	    SB_JOURNAL(s)->j_must_wait = 1 ;
+	  }
+	}
+        return 0;
+      }
+    }
+  return 1;
+}
+
+/* get amount_needed free block numbers from scanning the bitmap of free/used blocks.
+   
+   Optimize layout by trying to find them starting from search_start
+   and moving in elevator_direction, until a free block or the disk
+   edge is reached, and then if the edge was reached, changing the
+   elevator direction, and looking backwards from search_start.
+
+   search_start is the block number of the current node if we are
+   creating a new node, and it is the block number of the left
+   semantic neighbor of the current node if we are relocating a node
+   (using the write next to algorithm).  
+
+   If no free blocks are found, and there are blocks on the preserve
+   list, run sync_buffers() to free them.
+   
+   Note that when we free the preserve list we free all members of the
+   free_blocknrs array that we have gotten so far, on the assumption
+   that the freeing was likely to have created a better choice of
+   blocknrs, since needing to free implies that there were few free
+   ones to choose from, and that in turn implies that they were likely
+   to be poor choices, but if schedule occurs because of lock then we
+   guess that the old values are likely enough to be good that we
+   should not bother to see if we get better ones and save on the CPU
+   consumption and code size.  I don't know if this is correct, but it
+   seems unlikely to really matter much.
+
+   return 0 if everything is ok
+   return NO_DISK_SPACE if out of disk space,
+   or SCHEDULE_OCCURED
+   
+   return block numbers found, in the array free_blocknrs.  assumes
+   that any non-zero entries already present in the array are valid.
+
+   if number of free blocks is less than RESERVED_FOR_PRESERVE_LIST, we try to
+   get_space_from_preserve_list, because we do not want to lose free
+   blocks, that are reserved for the preserve list
+
+   If number of free blocks + number of preserved blocks is less than
+   RESERVED_FOR_PRESERVE_LIST, than reiserfs_new_blocknrs will fail until it is used for
+   preserve list
+
+   reiserfsck has its own reiserfs_new_blocknrs, which can use RESERVED_FOR_PRESERVE_LIST blocks
+*/
+
+static int do_reiserfs_new_blocknrs (struct reiserfs_transaction_handle *th, struct super_block * s, unsigned long * free_blocknrs, 
+			   unsigned long search_start, int amount_needed, int for_preserve_list, int for_unformatted)
+{
+  int i, j;
+  int retval = CARRY_ON;	/* it is set to SCHEDULE_OCCURED when
+				   get_space_from_preserve_list ran,
+				   or NO_DISK_SPACE when .. */
+  unsigned long * block_list_start = free_blocknrs;
+  int init_amount_needed = amount_needed;
+
+/*
+  if (SB_FREE_BLOCKS (s) < RESERVED_FOR_PRESERVE_LIST) {
+    get_space_from_preserve_list (s);
+    retval = SCHEDULE_OCCURRED;
+  }
+*/
+
+  if (SB_FREE_BLOCKS (s) < RESERVED_FOR_PRESERVE_LIST && !for_preserve_list) {
+    /* there is some free space just to keep preserve list working */
+    return NO_DISK_SPACE;
+  }
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (!s)
+    reiserfs_panic (s, "vs-4020: reiserfs_new_blocknrs: trying to get new block from nonexistent device");
+
+  if (search_start == MAX_B_NUM)
+    reiserfs_panic (s, "vs-4025: reiserfs_new_blocknrs: we are optimizing location based on "
+		    "the bogus location of a temp buffer (%lu).", search_start);
+
+  if (amount_needed < 1 || amount_needed > MAX_PRESERVE_NODES) 
+    reiserfs_panic (s, "vs-4030: reiserfs_new_blocknrs: amount_needed parameter incorrect (%d)", amount_needed);
+#endif /* CONFIG_REISERFS_CHECK */
+
+  /* We continue the while loop if another process snatches our found
+   * free block from us after we find it but before we successfully
+   * mark it as in use, or if we need to use sync to free up some
+   * blocks on the preserve list.  */
+
+  while (amount_needed--) {
+    /* skip over any blocknrs already gotten last time. */
+    if (*(free_blocknrs) != 0) {
+#ifdef CONFIG_REISERFS_CHECK
+      if (is_reusable (s, *free_blocknrs, 1) == 0)
+	reiserfs_panic(s, "vs-4035: reiserfs_new_blocknrs: bad blocknr on free_blocknrs list");
+#endif /* CONFIG_REISERFS_CHECK */
+      free_blocknrs++;
+      continue;
+    }
+    /* look for zero bits in bitmap */
+    if (find_zero_bit_in_bitmap (s, search_start, &i, &j, &retval, for_unformatted) == 0) {
+      if (find_zero_bit_in_bitmap (s, search_start, &i, &j, &retval, for_unformatted) == 0) {
+	for ( ; block_list_start != free_blocknrs; block_list_start++) {
+	  reiserfs_free_block (th, s, *block_list_start);
+	  COMPLETE_BITMAP_DIRTING_AFTER_FREEING(s,*block_list_start / (s->s_blocksize * 8));
+	  *block_list_start = 0;
+	}
+	return NO_DISK_SPACE;
+      }
+    }
+    
+    /* i and j now contain the results of the search. i = bitmap block
+       number containing free block, j = offset in this block.  we
+       compute the blocknr which is our result, store it in
+       free_blocknrs, and increment the pointer so that on the next
+       loop we will insert into the next location in the array.  Also
+       in preparation for the next loop, search_start is changed so
+       that the next search will not rescan the same range but will
+       start where this search finished.  Note that while it is
+       possible that schedule has occurred and blocks have been freed
+       in that range, it is perhaps more important that the blocks
+       returned be near each other than that they be near their other
+       neighbors, and it also simplifies and speeds the code this way.  */
+
+    /* journal: we need to make sure the block we are giving out is not
+    ** a log block, horrible things would happen there.
+    */
+    search_start = (i * (s->s_blocksize << 3)) + j; 
+    if (search_start >= SB_JOURNAL_BLOCK(s) &&
+        search_start < (SB_JOURNAL_BLOCK(s) + JOURNAL_BLOCK_COUNT)) {
+      reiserfs_warning("bitmap-370, trying to allocate log block %lu\n",
+                        search_start) ;
+      search_start++ ;
+      continue ;
+    }
+       
+    *free_blocknrs = search_start ;
+
+#ifdef CONFIG_REISERFS_CHECK
+    if (buffer_locked (SB_AP_BITMAP (s)[i]) || is_reusable (s, search_start, 0) == 0)
+      reiserfs_panic (s, "vs-4040: reiserfs_new_blocknrs: bitmap block is locked or bad block number found");
+#endif
+
+    /* set bit in true bitmap, but do not set bit in cautious bitmap */
+    if (test_and_set_bit (j, SB_AP_BITMAP (s)[i]->b_data))
+      reiserfs_panic (s, "vs-4045: reiserfs_new_blocknrs: schedule did not occur and this block was free");
+    
+    journal_mark_dirty (th, s, SB_AP_BITMAP (s)[i]); 
+
+    /* it should be marked as suspected recipient when old items moved
+       to it. For now do it unconditionally */
+/*    mark_suspected_recipient (s, SB_AP_BITMAP (s)[i]);*/
+    free_blocknrs ++;
+  }
+
+  /* update free block count in super block */
+  SB_FREE_BLOCKS (s) = cpu_to_le32 (le32_to_cpu (SB_FREE_BLOCKS (s)) - init_amount_needed);
+  journal_mark_dirty (th, s, SB_BUFFER_WITH_SB (s));
+  s->s_dirt = 1;
+
+  return retval;
+}
+
+int reiserfs_new_blocknrs (struct reiserfs_transaction_handle *th, struct super_block * s, unsigned long * free_blocknrs,
+			    unsigned long search_start, int amount_needed, int for_preserve_list) {
+  return do_reiserfs_new_blocknrs(th, s, free_blocknrs, search_start, amount_needed, for_preserve_list, 0) ;
+}
+
+int reiserfs_new_unf_blocknrs(struct reiserfs_transaction_handle *th, struct super_block * s, unsigned long * free_blocknrs,
+				 unsigned long search_start, int amount_needed, int for_preserve_list) {
+  // unsigned long border = (SB_BLOCK_COUNT(th->t_super) / 10); 
+  // if ( search_start < border ) search_start=border;
+  return do_reiserfs_new_blocknrs(th, s, free_blocknrs, search_start, amount_needed, for_preserve_list, 1) ;
+}
diff -urN linux/fs/reiserfs/buffer.c /tmp/linux/fs/reiserfs/buffer.c
--- linux/fs/reiserfs/buffer.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/buffer.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,284 @@
+/*
+ *  Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+
+/*
+ * Contains code from
+ *
+ *  linux/include/linux/lock.h and linux/fs/buffer.c /linux/fs/minix/fsync.c
+ *
+ *  Copyright (C) 1991, 1992  Linus Torvalds
+ */
+
+
+/* this is to be included from fs/buffer.c. This requires adding few lines in kernel/ksyms.c */
+
+#ifndef __KERNEL__
+
+#include "nokernel.h"
+void mark_suspected_recipients_dirty(struct reiserfs_transaction_handle *th, kdev_t a) {}
+void fixup_reiserfs_buffers (kdev_t dev) {}
+void reiserfs_show_buffers (kdev_t dev){}
+#else
+#include <linux/reiserfs_fs.h>
+#endif
+
+#define CARRY_ON                0
+#define SCHEDULE_OCCURRED       1
+
+inline int  test_and_wait_on_buffer(
+              struct buffer_head * p_s_bh
+            ) {
+  if ( buffer_locked(p_s_bh) )  {
+    __wait_on_buffer(p_s_bh);
+    return SCHEDULE_OCCURRED;
+  }
+  return CARRY_ON;
+}
+ 
+
+/* This is pretty much the same as the corresponding original linux
+   function, it differs only in that it tracks whether schedule
+   occurred. */
+/* This function function calls find_buffer which looks for the buffer
+   desired on the appropriate hash queue, and then waits for it to unlock. */
+struct buffer_head  * reiserfs_get_hash_table(
+                        kdev_t  n_dev,
+                        int     n_block,
+                        int     n_size,
+                        int   * p_n_repeat
+                      ) {
+  struct buffer_head * p_s_bh;
+
+  for ( ; ; ) {
+    if ( ! (p_s_bh = find_buffer(n_dev, n_block, n_size)) )
+      return NULL;
+    p_s_bh->b_count++;
+    *p_n_repeat |= test_and_wait_on_buffer(p_s_bh);
+    if ( *p_n_repeat == CARRY_ON ||
+	 (p_s_bh->b_dev == n_dev && p_s_bh->b_blocknr == n_block && p_s_bh->b_size == n_size) )
+      return p_s_bh;
+    p_s_bh->b_count--;
+  }
+}
+
+
+#ifdef __KERNEL__
+#if 0 /* not needed anymore */
+struct super_block * reiserfs_get_super (kdev_t dev)
+{
+  struct super_block * s = NULL ;
+
+  
+  if (!dev)
+    return NULL;
+  s = sb_entry(super_blocks.next);
+  while (s != sb_entry(&super_blocks)) {
+    if (s && s->s_dev == dev) {
+      return s;
+    } else {
+      s = sb_entry(s->s_list.next);
+    }
+  }
+  return NULL;
+}
+#endif
+
+/*   
+** end_io for all the log blocks.
+**   
+** this used to do more, but right now it only decrements j_commit_left for the
+** journal list this log block belongs to
+*/   
+void finish_log_block_io(struct super_block *p_s_sb, struct buffer_head *bh, int uptodate) {
+  int j ;
+  int index ;
+  int found = 0 ;
+  int start ;
+  unsigned long startb, endb ;
+     
+  /* desc is at start, commit is at start + len + 1 */
+  start = SB_JOURNAL_LIST_INDEX(p_s_sb) ;
+  if (start < 5) {
+    start = JOURNAL_LIST_COUNT - 5 ;
+  } else {
+    start = start - 5; 
+  }
+  for (j = 0 ; j < JOURNAL_LIST_COUNT ; j++) {
+    index = (start + j) % JOURNAL_LIST_COUNT ;
+    startb = SB_JOURNAL_LIST(p_s_sb)[index].j_start + SB_JOURNAL_BLOCK(p_s_sb) ;
+    endb = SB_JOURNAL_BLOCK(p_s_sb) + ((SB_JOURNAL_LIST(p_s_sb)[index].j_start + SB_JOURNAL_LIST(p_s_sb)[index].j_len + 1) %
+           JOURNAL_BLOCK_COUNT) ;
+    if (SB_JOURNAL_LIST(p_s_sb)[index].j_len > 0 &&
+       ( 
+         ((SB_JOURNAL_LIST(p_s_sb)[index].j_start + SB_JOURNAL_LIST(p_s_sb)[index].j_len + 1) < JOURNAL_BLOCK_COUNT &&
+           startb <= bh->b_blocknr && endb >= bh->b_blocknr) ||
+         ((SB_JOURNAL_LIST(p_s_sb)[index].j_start + SB_JOURNAL_LIST(p_s_sb)[index].j_len + 1) >= JOURNAL_BLOCK_COUNT &&
+           (startb <= bh->b_blocknr || endb >= bh->b_blocknr))
+       )
+      ) {
+      atomic_dec(&(SB_JOURNAL_LIST(p_s_sb)[index].j_commit_left)) ;
+      found = 1 ;
+      break ;
+    }
+  }
+  if (found == 0) { 
+    printk("buffer-115: Unable to find journal list for block %lu\n", bh->b_blocknr) ;
+  } 
+  mark_buffer_uptodate(bh, uptodate);
+  unlock_buffer(bh);
+}
+
+
+/* no longer need, should just make journal.c use the default handler */
+void reiserfs_journal_end_io (struct buffer_head *bh, int uptodate)
+{
+  mark_buffer_uptodate(bh, uptodate);
+  unlock_buffer(bh);
+  return ;
+}
+
+/*
+** general end_io routine for all reiserfs blocks.
+** we check to make sure pinned blocks were not sent to disk, and
+** we clear the journal new bit in the buffer head
+*/
+void reiserfs_end_buffer_io_sync (struct buffer_head *bh, int uptodate)
+{
+
+  mark_buffer_notjournal_new(bh) ;
+  if (test_bit(BH_JDirty, &bh->b_state)) {
+    printk("clm-6000: Error, pinned buffer %lu has been sent to disk\n", 
+            bh->b_blocknr) ;
+  }
+  mark_buffer_uptodate(bh, uptodate);
+  unlock_buffer(bh);
+}
+
+/* This is pretty much the same as the corresponding original linux
+   function, it differs only in that it tracks whether schedule
+   occurred. */
+/* This function looks for a buffer which contains a given block.  If
+   the block is in cache it returns it, otherwise it returns a new
+   buffer which is not uptodate.  This is called by reiserfs_bread and
+   other functions. Note that get_new_buffer ought to be called this
+   and this ought to be called get_new_buffer, since this doesn't
+   actually get the block off of the disk. */
+struct buffer_head  * reiserfs_getblk(
+                        kdev_t    n_dev,
+                        int       n_block,
+                        int       n_size,
+                        int     * p_n_repeat
+                      ) {
+   struct buffer_head  * p_s_bh;
+  int                   n_isize;
+
+repeat:
+
+  p_s_bh = reiserfs_get_hash_table(n_dev, n_block, n_size, p_n_repeat);
+  if ( p_s_bh ) {
+    if ( ! buffer_dirty(p_s_bh) ) {
+      p_s_bh->b_flushtime = 0;
+    }
+    p_s_bh->b_end_io = reiserfs_end_buffer_io_sync;
+    return p_s_bh;
+  }
+
+  n_isize = BUFSIZE_INDEX(n_size);
+get_free:
+  p_s_bh = free_list[n_isize];
+  if (!p_s_bh)
+    goto refill;
+  remove_from_free_list(p_s_bh);
+
+  /* OK, FINALLY we know that this buffer is the only one of its kind,
+   * and that it's unused (b_count=0), unlocked, and clean.
+   */
+  init_buffer(p_s_bh, n_dev, n_block, reiserfs_end_buffer_io_sync, NULL);
+  p_s_bh->b_state=0;
+  insert_into_queues(p_s_bh);
+  return p_s_bh;
+
+  /*
+   * If we block while refilling the free list, somebody may
+   * create the buffer first ... search the hashes again.
+   */
+refill:
+  *p_n_repeat |= SCHEDULE_OCCURRED;
+  refill_freelist(n_size);
+  if (!find_buffer(n_dev,n_block,n_size))
+    goto get_free;
+  goto repeat;
+
+}
+
+void fixup_reiserfs_buffers (kdev_t dev)
+{
+  int i;
+  int nlist;
+  int slept;
+  struct buffer_head * bh;
+  
+ again:
+  slept = 0;
+  for(nlist = 0; nlist < NR_LIST; nlist++) {
+    bh = lru_list[nlist];
+    for (i = nr_buffers_type[nlist] ; i > 0 ; bh = bh->b_next_free, i--) {
+      if (bh->b_dev != dev)
+	continue;
+      if (buffer_locked(bh))
+      {
+	slept = 1;
+	__wait_on_buffer(bh);
+      }
+
+      /* set the end_io callback once the buffer is not under I/O,
+         nobody can start I/O from under us because we are protected
+         by the big kernel lock */
+      bh->b_end_io = end_buffer_io_sync;
+      if (bh->b_count > 0) {
+        printk("buffer-300: BAD, count is %d for buffer %lu\n", bh->b_count, bh->b_blocknr) ;
+      }
+      if (slept)
+        goto again;
+    }
+  }
+}
+
+/* call by unpreserve when cautous bitmap block is being dirtied */
+void reiserfs_refile_buffer (struct buffer_head * buf)
+{
+  if (BUF_DIRTY != buf->b_list)
+    file_buffer (buf, BUF_DIRTY);
+}
+
+void reiserfs_file_buffer(struct buffer_head *bh, int list) {
+  file_buffer(bh, list) ;
+}
+
+#else
+
+void reiserfs_end_buffer_io_sync (struct buffer_head *bh, int uptodate)
+{
+ ;
+}
+
+int reiserfs_add_handler(struct buffer_head *bh) {
+  bh->b_end_io =  reiserfs_end_buffer_io_sync  ;
+  return 0 ;
+}
+
+struct buffer_head * reiserfs_getblk (kdev_t dev, int block, int size, int * p_n_repeat) {
+  struct buffer_head *bh ;
+  bh = getblk(dev, block, size) ;
+  if (bh) {
+    bh->b_end_io = reiserfs_end_buffer_io_sync ;
+  }
+  return bh ;
+}
+
+
+#endif /* __KERNEL__ */
+
+
diff -urN linux/fs/reiserfs/buffer2.c /tmp/linux/fs/reiserfs/buffer2.c
--- linux/fs/reiserfs/buffer2.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/buffer2.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,340 @@
+/*
+ *  Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+
+/*
+ * Contains code from
+ *
+ *  linux/include/linux/lock.h and linux/fs/buffer.c /linux/fs/minix/fsync.c
+ *
+ *  Copyright (C) 1991, 1992  Linus Torvalds
+ */
+#ifdef __KERNEL__
+
+#include <linux/sched.h>
+#include <linux/locks.h>
+#include <linux/reiserfs_fs.h>
+
+
+
+/*
+ * wait_buffer_until_released
+ *  reiserfs_bread
+ *  reiserfs_sync_block
+ *  sync_unf_nodes
+ *  sync_file_item
+ *  sync_file_items
+ *  reiserfs_sync_file
+ */
+
+
+/* when we allocate a new block (get_new_buffer, get_empty_nodes,
+   get_nodes_for_preserving) and get buffer for it, it is possible
+   that it is held by someone else or even by this process. In this
+   function we wait until all other holders release buffer. To make
+   sure, that current process does not hold we did free all buffers in
+   tree balance structure (get_empty_nodes and
+   get_nodes_for_preserving) or in path structure only
+   (get_new_buffer) just before calling this */
+void wait_buffer_until_released (struct buffer_head * bh)
+{
+  int repeat_counter = 0;
+
+  while (bh->b_count > 1) {
+    if ( !(++repeat_counter % 200000) ) {
+      reiserfs_warning ("vs-3050: wait_buffer_until_released: nobody releases buffer (%b). Still waiting (%d) %cJDIRTY %cJWAIT\n",
+			bh, repeat_counter, buffer_journaled(bh) ? ' ' : '!',
+			buffer_journal_dirty(bh) ? ' ' : '!');
+    }
+    current->policy |= SCHED_YIELD;
+    schedule();
+  }
+}
+
+
+/* This is pretty much the same as the corresponding original linux
+   function, it differs only in that it tracks whether schedule
+   occurred. */
+/*
+ * reiserfs_bread() reads a specified block and returns the buffer that contains
+ * it. It returns NULL if the block was unreadable.
+ */
+/* It first tries to find the block in cache, and if it cannot do so
+   then it creates a new buffer and schedules I/O to read the
+   block. */
+
+struct buffer_head  * reiserfs_bread(
+                        kdev_t  n_dev,
+                        int     n_block,
+                        int     n_size,
+                        int   * p_n_repeat
+                      ) {
+  struct buffer_head * p_s_bh;
+
+  p_s_bh = reiserfs_getblk(n_dev, n_block, n_size, p_n_repeat);
+  if ( buffer_uptodate(p_s_bh) )
+    return p_s_bh;
+  ll_rw_block(READ, 1, &p_s_bh);
+  *p_n_repeat |= SCHEDULE_OCCURRED;
+  wait_on_buffer(p_s_bh);
+  if ( buffer_uptodate(p_s_bh) )
+    return p_s_bh;
+  printk("reiserfs_bread: unable to read dev = %d block = %d size = %d\n", n_dev, n_block, n_size);
+  brelse(p_s_bh);
+  return NULL;
+}
+
+
+
+
+
+/* Synchronize a block of reiserfs. */
+static int  reiserfs_sync_block(
+              struct buffer_head  * p_s_bh,   /* Pointer to the buffer header to sync.  */
+              int                   n_wait,   /* Wait parameter.                        */
+              struct path         * p_s_path  /* Pointer to the path contains buffer.
+                                                  NULL in case of an unformatted node.  */
+            ) {
+  if ( n_wait && buffer_req(p_s_bh) && ! buffer_uptodate(p_s_bh) )  {
+    /* Release buffer if it is unformatted node. If not it will be released by pathrelse(). */
+    if ( ! p_s_path )
+      brelse(p_s_bh);
+    return -1;
+  }
+  if ( n_wait || ! buffer_uptodate(p_s_bh) || ! buffer_dirty(p_s_bh) )  {
+    if ( ! p_s_path )
+      brelse(p_s_bh);
+    return 0;
+  }
+
+  /* unformatted nodes can go right to the disk, but they are
+  ** skipped if they have been logged recently.  Committing the
+  ** transaction will be good enough to call them synced
+  */
+  if (!p_s_path && !buffer_journaled(p_s_bh) && !buffer_journal_dirty(p_s_bh)) {
+    ll_rw_block(WRITE, 1, &p_s_bh); 
+  }
+  p_s_bh->b_count--;
+
+  /* Decrement path length if it is buffer at the path. */
+  if ( p_s_path ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( p_s_path->path_length < FIRST_PATH_ELEMENT_OFFSET )
+      reiserfs_panic(0, "PAP-16015: reiserfs_sync_block: path length is too small");
+#endif
+
+    p_s_path->path_length--;
+  }
+
+  return 0;
+}
+
+
+/* Sync unformatted nodes of the item *p_s_ih. */
+static int  sync_unf_nodes(
+              struct inode      * p_s_inode,        /* Pointer to the file inode.         */
+              struct item_head  * p_s_ih,           /* Pointer to the found item header.  */
+              int               * p_n_pos_in_item,  /* Position in the found item.        */
+              struct path       * p_s_path_to_item, /* Path to the found item.            */
+              int                 n_wait,           /* Wait parameter.                    */
+              unsigned long     * p_n_synced  /* Returned value. UNFM number were
+                                                        synced in this call.        */
+            ) {
+  struct key            s_item_key;
+  struct buffer_head  * p_s_unfm_bh;
+  unsigned long         n_unfm_pointer;
+  int                   n_repeat,
+                        n_counter,
+                        n_unfm_number_to_sync = I_UNFM_NUM(p_s_ih) - *p_n_pos_in_item;
+
+  /* Form key to search for the first unformatted node to be synced. */
+  copy_key(&s_item_key, &(p_s_ih->ih_key));
+  s_item_key.k_offset += *p_n_pos_in_item * p_s_inode->i_sb->s_blocksize;
+  if ( search_for_position_by_key(p_s_inode->i_sb, &s_item_key, p_s_path_to_item, p_n_pos_in_item, &n_repeat) == POSITION_NOT_FOUND )
+    return 1; /* Item to sync UNFM was not found. */
+  /* Remember found item header.  */
+  copy_item_head(p_s_ih, PATH_PITEM_HEAD(p_s_path_to_item));
+
+  /* Calculate number of the UNFM to be synced. */
+  if ( n_unfm_number_to_sync > I_UNFM_NUM(p_s_ih) - *p_n_pos_in_item/*vs*/)
+    /* do not sync more unformatted nodes, than number of synced unformatted node pointers. If there
+       is no so many unformatted nodes in found indirect item as we need, sync what we have. */
+    n_unfm_number_to_sync = I_UNFM_NUM(p_s_ih) - *p_n_pos_in_item/*vs*/;
+
+  for ( n_counter = 0; n_counter < n_unfm_number_to_sync; n_counter++ ) {
+    /* Found item is not at the path. */
+    if ( comp_items(p_s_ih, p_s_path_to_item) )
+      return 0;
+    /* Calculate block number of the UNFM. */
+    n_unfm_pointer = B_I_POS_UNFM_POINTER(PATH_PLAST_BUFFER(p_s_path_to_item),
+					  p_s_ih, *p_n_pos_in_item + n_counter);
+    /* It is hole. Nothing to sync. */
+    if ( ! n_unfm_pointer ) {
+      *p_n_synced += 1;
+      continue;
+    }
+    /* Get buffer contains UNFM. */
+    n_repeat = CARRY_ON;
+    p_s_unfm_bh = reiserfs_get_hash_table(p_s_inode->i_dev, n_unfm_pointer,
+					  p_s_inode->i_sb->s_blocksize, &n_repeat);
+    /* There is not needed buffer. */
+    if ( ! p_s_unfm_bh )  {
+      *p_n_synced += 1;
+      continue;
+    }
+    /* Check whether the found item at the path. */
+    if ( comp_items(p_s_ih, p_s_path_to_item) ) {
+      brelse(p_s_unfm_bh);
+      return 0;
+    }
+
+    /* Number of the UNFM is changed. */
+    if ( n_unfm_pointer != B_I_POS_UNFM_POINTER(PATH_PLAST_BUFFER(p_s_path_to_item), p_s_ih, *p_n_pos_in_item + n_counter) ) {
+      brelse(p_s_unfm_bh);
+      return 1;
+    }
+
+    /* Sync UNFM. */
+    if ( reiserfs_sync_block(p_s_unfm_bh, n_wait, NULL) )
+      return -1;
+
+    *p_n_synced += 1;
+
+  }
+  return 0;
+}
+
+
+/* Sync a reiserfs file item. */
+static int  sync_file_item(
+              struct inode  * p_s_inode,        /* Pointer to the file inode.             */
+              struct path   * p_s_path_to_item, /* Pointer to the path to the found item. */
+              int           * p_n_pos_in_item,  /* Position in the found item.            */
+              int             n_wait,           /* Sync parameter.                        */
+              unsigned long * p_n_synced  /* Returned value. Bytes number were
+                                                    synced in this call.            */
+            ) {
+  struct item_head      s_ih;
+  int                   n_ret_value;
+
+  *p_n_synced = 0;
+  /* Copy found item header. */
+  copy_item_head(&s_ih, PATH_PITEM_HEAD(p_s_path_to_item));
+
+  /* Sync found item. */
+  if ( (n_ret_value = reiserfs_sync_block(PATH_PLAST_BUFFER(p_s_path_to_item), n_wait, p_s_path_to_item)) )
+    return n_ret_value;
+
+  if ( I_IS_DIRECT_ITEM(&s_ih) )  {
+    /* s_ih.ih_item_len bytes was synced. */
+    *p_n_synced = s_ih.ih_item_len;
+    return 0;
+  }
+  /* Sync unformatted nodes pointed by the synced indirect item. */
+  n_ret_value = sync_unf_nodes(p_s_inode, &s_ih, p_n_pos_in_item,
+			       p_s_path_to_item, n_wait, p_n_synced);
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (!n_wait &&  ! *p_n_synced && ! n_ret_value )
+    reiserfs_warning("sync_file_item: no unformatted nodes were synced (ih %h) pass %d\n", &s_ih, n_wait);
+#endif
+
+  *p_n_synced *= p_s_inode->i_sb->s_blocksize;
+  return n_ret_value;
+}
+
+
+/* Sync a reiserfs file items. */
+static int sync_file_items(
+             struct inode  * p_s_inode,
+	     int             n_wait
+           ) {
+  struct key          s_item_key;
+  struct path         s_path_to_item;
+  int                 n_pos_in_item,
+                      n_repeat,
+                      n_ret_value = 0;
+  unsigned long       n_synced;
+
+  
+  init_path (&s_path_to_item);
+  /* Form key to search for the first file item. */
+  copy_key(&s_item_key, &(p_s_inode->u.reiserfs_i.i_key));
+  s_item_key.k_offset = 1;
+  if ( 1 == p_s_inode->u.reiserfs_i.i_first_direct_byte )
+    s_item_key.k_uniqueness = TYPE_DIRECT;
+  else
+    s_item_key.k_uniqueness = TYPE_INDIRECT;
+  /* While next file item is presented in the tree. */
+  while ( search_for_position_by_key(p_s_inode->i_sb, &s_item_key, &s_path_to_item, &n_pos_in_item, &n_repeat) == POSITION_FOUND )  {
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( I_IS_DIRECTORY_ITEM(PATH_PITEM_HEAD(&s_path_to_item)) ||
+	 I_IS_STAT_DATA_ITEM(PATH_PITEM_HEAD(&s_path_to_item)) )
+      reiserfs_panic (p_s_inode->i_sb, "PAP-16030: sync_file_items: unexpected item type");
+#endif
+
+    /* Synchronize the current item. */  
+    if ( (n_ret_value = sync_file_item(p_s_inode, &s_path_to_item, &n_pos_in_item,
+				       n_wait, &n_synced)) )
+      break;
+    /* Update key to search for the next file item. */
+    if ( (s_item_key.k_offset += n_synced) >= p_s_inode->u.reiserfs_i.i_first_direct_byte )
+      s_item_key.k_uniqueness = TYPE_DIRECT;  
+  }
+
+  decrement_counters_in_path(&s_path_to_item);
+  return n_ret_value;
+}
+
+
+
+/* Sync a reiserfs file. */
+int reiserfs_sync_file(
+      struct file   * p_s_filp,
+      struct dentry * p_s_dentry
+    ) {
+  struct inode * p_s_inode = p_s_dentry->d_inode;
+  struct reiserfs_transaction_handle th ;
+  int n_wait,
+      n_err = 0;
+  int windex ;
+  int jbegin_count = JOURNAL_PER_BALANCE_CNT * 2 ;
+  if ( S_ISDIR(p_s_inode->i_mode) )
+    reiserfs_panic (p_s_inode->i_sb,
+		    "PAP-16040: reiserfs_sync_file: attempt to sync directory using reiserfs_sync_file()");
+
+  if ( ! (S_ISREG(p_s_inode->i_mode) || S_ISLNK(p_s_inode->i_mode)) )
+    return -EINVAL;
+
+  /* note, since the transaction can't end while we have the
+  ** read_sync_counter incremented (deadlock), sync_file_items is not allowed
+  ** to do a polite transaction end.  I've fixed that by 
+  ** having it not log anything at all, it only puts unformatted
+  ** nodes on the disk, and only if they have not already been logged
+  */
+  increment_i_read_sync_counter(p_s_inode);
+  for ( n_wait = 0; n_wait < 2; n_wait++ )
+    n_err |= sync_file_items(p_s_inode, n_wait);
+  decrement_i_read_sync_counter(p_s_inode);
+
+  /* now we sync the inode, and commit the transaction.
+  ** this will commit any logged blocks involved with this file
+  */
+  if (reiserfs_inode_in_this_transaction(p_s_inode) || 
+      p_s_inode->i_state & I_DIRTY) {
+    journal_begin(&th, p_s_inode->i_sb, jbegin_count) ;
+    windex = push_journal_writer("reiserfs_sync_file") ;
+    n_err |= reiserfs_sync_inode(&th, p_s_inode);
+    pop_journal_writer(windex) ;
+    journal_end_sync(&th, th.t_super,jbegin_count) ;
+  } else {
+    reiserfs_commit_for_inode(p_s_inode) ;
+  }
+  return ( n_err < 0 ) ? -EIO : 0;
+}
+
+#endif /* __KERNEL__ */
+
diff -urN linux/fs/reiserfs/dir.c /tmp/linux/fs/reiserfs/dir.c
--- linux/fs/reiserfs/dir.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/dir.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,259 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+#ifdef __KERNEL__
+
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/fs.h>
+#include <linux/reiserfs_fs.h>
+#include <linux/stat.h>
+#include <asm/uaccess.h>
+
+#else
+
+#include "nokernel.h"
+
+#endif
+
+
+extern struct key  MIN_KEY;
+
+static ssize_t reiserfs_dir_read  (struct file * filp, char * buf, size_t count, loff_t * ppos)
+{
+  return -EISDIR;
+}
+
+/*static loff_t reiserfs_llseek(struct file *file, loff_t offset, int origin);*/
+static int reiserfs_readdir (struct file *, void *, filldir_t);
+int reiserfs_dir_fsync(struct file *filp, struct dentry *dentry)  ;
+
+static struct file_operations reiserfs_dir_operations = {
+	NULL,			/* lseek */
+	reiserfs_dir_read,	/* read */
+	NULL,			/* write */
+	reiserfs_readdir,	/* readdir */
+	NULL,			/* poll */
+	NULL,			/* ioctl */
+	NULL,			/* mmap */
+	NULL,			/* open */
+	NULL,			/* flush */
+	NULL,			/* release */
+	reiserfs_dir_fsync,	/* fsync */ 
+	NULL, 			/* fasync */
+	NULL,			/* check_media_change */
+	NULL,			/* revalidate */
+	NULL			/* lock */
+};
+
+/*
+ * directories can handle most operations...
+ */
+struct inode_operations reiserfs_dir_inode_operations = {
+	&reiserfs_dir_operations,	/* default_file_ops */
+	reiserfs_create,		/* create */
+	reiserfs_lookup,		/* lookup */
+	reiserfs_link,			/* link */
+	reiserfs_unlink,		/* unlink */
+	reiserfs_symlink,		/* symlink */
+	reiserfs_mkdir,			/* mkdir */
+	reiserfs_rmdir,			/* rmdir */
+	reiserfs_mknod,			/* mknod */
+	reiserfs_rename,		/* rename */
+	NULL,				/* readlink */
+	NULL,				/* follow_link */
+	NULL,				/* readpage */
+	NULL,				/* writepage */
+	NULL,				/* bmap */
+	NULL,				/* truncate */
+	NULL,				/* permission */
+	NULL,				/* smap */
+	NULL,				/* updatepage */
+	NULL				/* revalidate */
+};
+
+int reiserfs_dir_fsync(struct file *filp, struct dentry *dentry) {
+  struct reiserfs_transaction_handle th ;
+  if (!dentry || !dentry->d_inode) {
+     return file_fsync(filp, dentry) ;
+  }
+  /* ret = file_fsync(filp, dentry) ; we don't need this */
+
+  /* if any changes were made to the dir, this will catch them */
+  if (reiserfs_inode_in_this_transaction(dentry->d_inode)) {
+    journal_begin(&th, dentry->d_inode->i_sb, 1) ;
+    journal_end_sync(&th, dentry->d_inode->i_sb, 1) ;
+  } else {
+    reiserfs_commit_for_inode(dentry->d_inode) ;
+  }
+  return 0 ;
+}
+
+
+// FIXME: (I am not sure, how to fix it, though) f_pos for reiserfs
+// directory is not byte offset of the entry from the beginning of
+// directory.
+static int reiserfs_readdir (struct file * filp, void * dirent, filldir_t filldir)
+{
+    struct inode *inode = filp->f_dentry->d_inode;
+    struct key pos_key;	/* key of current position in the directory (key of directory entry) */
+    struct path path_to_entry;
+    struct buffer_head * bh;
+    int item_num, entry_num;
+    int repeat;
+    struct key * rkey;
+    struct item_head * ih, tmp_ih;
+    int search_res;
+    loff_t next_pos;
+    char * local_buf;
+    char small_buf[32] ;
+
+    init_path (&path_to_entry);
+
+    /* form key for search the next directory entry using f_pos field of file structure  */
+    copy_key (&pos_key, INODE_PKEY (inode));
+    pos_key.k_offset = (filp->f_pos) ? (filp->f_pos) : DOT_OFFSET;
+    pos_key.k_uniqueness = DIRENTRY_UNIQUENESS;
+    next_pos = pos_key.k_offset;
+
+    while (1) {
+    research:
+	/* search the directory item, containing entry with specified key */
+	search_res = search_by_entry_key (inode->i_sb, &pos_key, &path_to_entry, &entry_num, &repeat);
+
+	bh = PATH_PLAST_BUFFER (&path_to_entry);
+	item_num = PATH_LAST_POSITION (&path_to_entry);
+	ih = B_N_PITEM_HEAD (bh, item_num);
+	copy_item_head (&tmp_ih, ih);
+    
+#ifdef CONFIG_REISERFS_CHECK
+	/* we must have found item, that is item of this directory, */
+	if (COMP_SHORT_KEYS (&pos_key, B_N_PKEY (bh, item_num)))
+	    reiserfs_panic (inode->i_sb, "vs-9000: reiserfs_readdir: can not find directory item (%lu %lu)",
+			    pos_key.k_dir_id, pos_key.k_objectid);
+      
+	if (item_num > B_NR_ITEMS (bh) - 1)
+	    reiserfs_panic (inode->i_sb, "vs-9005: reiserfs_readdir: item_num == %d, item amount == %d",
+			    item_num, B_NR_ITEMS (bh));
+      
+	/* and entry must be not more than number of entries in the item */
+	if (I_ENTRY_COUNT (ih) < entry_num)
+	    reiserfs_panic (inode->i_sb, "vs-9010: reiserfs_readdir: entry number is too big %d (%d)",
+			    entry_num, I_ENTRY_COUNT (ih));
+#endif	/* CONFIG_REISERFS_CHECK */
+
+	if (search_res == POSITION_FOUND || entry_num < I_ENTRY_COUNT (ih)) {
+	    /* go through all entries in the directory item beginning from the entry, that has been found */
+	    struct reiserfs_de_head * deh = B_I_DEH (bh, ih) + entry_num;
+
+	    for (; entry_num < I_ENTRY_COUNT (ih); entry_num ++, deh ++) {
+		int d_reclen;
+		char * d_name;
+		off_t d_off;
+		ino_t d_ino;
+		if (!de_visible (deh))
+		    /* it is hidden entry */
+		    continue;
+
+		d_reclen = I_DEH_N_ENTRY_FILE_NAME_LENGTH (ih, deh, entry_num);
+		d_name = B_I_DEH_ENTRY_FILE_NAME (bh, ih, deh);
+		d_off = deh->deh_offset;
+		d_ino = deh->deh_objectid;
+		if (d_reclen <= 32) {
+		    local_buf = small_buf ;
+		} else {
+		    local_buf = kmalloc(d_reclen, GFP_KERNEL) ;
+		    if (!local_buf) {
+			pathrelse (&path_to_entry);
+			return -ENOMEM ;
+		    }
+		    if (comp_items (&tmp_ih, &path_to_entry)) {
+			kfree(local_buf) ;
+			goto research ;
+		    }
+		}
+		// Note, that we copy name to user space via temporary
+		// buffer (local_buf) because filldir will block if
+		// user space buffer is swapped out. At that time
+		// entry can move to somewhere else
+		memcpy (local_buf, d_name, d_reclen);
+		if (filldir (dirent, local_buf, d_reclen, d_off, d_ino) < 0) {
+		    pathrelse (&path_to_entry);
+		    filp->f_pos = next_pos;
+		    if (local_buf != small_buf) {
+			kfree (local_buf);
+		    }
+		    return 0;
+		}
+		if (local_buf != small_buf) {
+		    kfree (local_buf);
+		}
+
+		// next entry should be looked for with such offset
+		next_pos = deh->deh_offset + 1;
+		if (comp_items (&tmp_ih, &path_to_entry)) {
+		    // item we go through got changed
+		    reiserfs_warning ("vs-9020: reiserfs_readdir: "
+				      "things are moving under hands. Researching..\n");
+		    goto research;
+		}
+	    } /* for */
+	}
+	/* item we went through is last item of node. Using right
+	   delimiting key check is it directory end */
+	if (item_num == B_NR_ITEMS (bh) - 1) {
+	    rkey = get_rkey (&path_to_entry, inode->i_sb);
+	    if (! COMP_KEYS (rkey, &MIN_KEY)) {
+#ifdef CONFIG_REISERFS_CHECK
+		reiserfs_warning ("vs-9025: reiserfs_readdir :"
+				  "get_rkey failed. Researching..\n");
+#endif
+		/* set pos_key to key, that is the smallest and greater
+		   that key of the last entry in the item */
+		pos_key.k_offset = next_pos;
+		continue;
+	    }
+
+	    if ( COMP_SHORT_KEYS (rkey, &pos_key)) {
+		/* end of the directory */
+		pathrelse (&path_to_entry);
+		filp->f_pos = next_pos;
+		return 0;
+	    }
+	    /* directory continues in the right neighboring block */
+	    copy_key (&pos_key, rkey);
+	    continue;
+	}
+
+	/* directory item is not a last item of the node. End of the directory is achieved */
+	pathrelse (&path_to_entry);
+	filp->f_pos = next_pos;
+	return 0;
+    } /* while */
+
+    *(int *)0 = 0;
+    pathrelse (&path_to_entry);
+    return 0;
+}
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
diff -urN linux/fs/reiserfs/do_balan.c /tmp/linux/fs/reiserfs/do_balan.c
--- linux/fs/reiserfs/do_balan.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/do_balan.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,2003 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+/* Now we have all buffers that must be used in balancing of the tree 	*/
+/* Further calculations can not cause schedule(), and thus the buffer 	*/
+/* tree will be stable until the balancing will be finished 		*/
+/* balance the tree according to the analysis made before,		*/
+/* and using buffers obtained after all above.				*/
+
+
+/**
+ ** balance_leaf_when_delete
+ ** balance_leaf
+ ** do_balance
+ **
+ **/
+
+#ifdef __KERNEL__
+
+#include <asm/uaccess.h>
+#include <linux/sched.h>
+#include <linux/reiserfs_fs.h>
+
+#else
+
+#include "nokernel.h"
+
+#endif
+
+
+#ifdef CONFIG_REISERFS_CHECK
+
+struct tree_balance * cur_tb = NULL; /* detects whether more than one copy of tb exists as a means
+					of checking whether schedule is interrupting do_balance */
+struct tree_balance init_tb;	/* Sometimes used to store a snapshot of tb during debugging. */
+int init_item_pos, init_pos_in_item, init_mode;  /* Sometimes used to store a snapshot of tb during debugging. */
+
+struct buffer_head * buffers[MAX_HEIGHT];
+
+#endif /* CONFIG_REISERFS_CHECK */
+
+
+
+/* summary: 
+ if deleting something ( tb->insert_size[0] < 0 )
+   return(balance_leaf_when_delete()); (flag d handled here)
+ else
+   if lnum is larger than 0 we put items into the left node
+   if rnum is larger than 0 we put items into the right node
+   if snum1 is larger than 0 we put items into the new node s1
+   if snum2 is larger than 0 we put items into the new node s2 
+Note that all *num* count new items being created.
+
+It would be easier to read balance_leaf() if each of these summary
+lines was a separate procedure rather than being inlined.  I think
+that there are many passages here and in balance_leaf_when_delete() in
+which two calls to one procedure can replace two passages, and it
+might save cache space and improve software maintenance costs to do so.  
+
+Vladimir made the perceptive comment that we should offload most of
+the decision making in this function into fix_nodes/check_balance, and
+then create some sort of structure in tb that says what actions should
+be performed by do_balance.
+
+-Hans */
+
+
+
+/* Balance leaf node in case of delete or cut: insert_size[0] < 0
+ *
+ * lnum, rnum can have values >= -1
+ *	-1 means that the neighbor must be joined with S
+ *	 0 means that nothing should be done with the neighbor
+ *	>0 means to shift entirely or partly the specified number of items to the neighbor
+ */
+static int	balance_leaf_when_delete (
+					  struct reiserfs_transaction_handle *th,
+					  struct tree_balance * tb, 
+					  int pos_in_item, 
+					  int flag
+					  )
+{
+  struct buffer_head * tbS0 = PATH_PLAST_BUFFER (tb->tb_path);
+  int item_pos = PATH_LAST_POSITION (tb->tb_path);
+  struct buffer_info bi;
+  int n;
+  struct item_head * ih;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( tb->FR[0] && B_BLK_HEAD(tb->FR[0])->blk_level <= DISK_LEAF_NODE_LEVEL )
+    reiserfs_panic (tb->tb_sb,
+                    "balance_leaf_when_delete: 11999:level == %u\n", B_BLK_HEAD(tb->FR[0])->blk_level);
+  if ( tb->blknum[0] > 1 )
+    reiserfs_panic (tb->tb_sb,
+		    "PAP-12005: balance_leaf_when_delete: tb->blknum == %d, can not be > 1", tb->blknum[0]);
+	
+  if ( ! tb->blknum[0] && ! PATH_H_PPARENT(tb->tb_path, 0))
+    reiserfs_panic (tb->tb_sb, "PAP-12010: balance_leaf_when_delete: tree can not be empty");
+#endif
+
+  ih = B_N_PITEM_HEAD (tbS0, item_pos);
+
+  /* Delete or truncate the item */
+
+  switch (flag) {
+  case M_DELETE:   /* delete item in S[0] */
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( ! item_pos && (! tb->L[0] || COMP_KEYS(B_PRIGHT_DELIM_KEY(tb->L[0]), B_N_PKEY(tbS0, 0))) ) {
+      print_tb (flag, item_pos, pos_in_item, tb, "12006");
+      reiserfs_panic (tb->tb_sb, "PAP-12015: balance_leaf_when_delete: L0's rkey does not match to 1st key of S0: "
+		      "rkey in L %k, first key in S0 %k, rkey in CFL %k",
+		      tb->L[0] ? B_PRIGHT_DELIM_KEY(tb->L[0]) : 0, 
+		      B_N_PKEY(tbS0, 0),
+		      tb->CFL[0] ? B_N_PDELIM_KEY(tb->CFL[0],tb->lkey[0]) : 0);
+    }
+#endif
+
+    bi.bi_bh = tbS0;
+    bi.bi_parent = PATH_H_PPARENT (tb->tb_path, 0);
+    bi.bi_position = PATH_H_POSITION (tb->tb_path, 1);
+    leaf_delete_items (th, &bi, 0, item_pos, 1, -1);
+
+    if ( ! item_pos && tb->CFL[0] ) {
+      if ( B_NR_ITEMS(tbS0) ) {
+	replace_key(th, tb->CFL[0],tb->lkey[0],tbS0,0);
+	copy_key(B_PRIGHT_DELIM_KEY(tb->L[0]), B_N_PKEY(tbS0, 0));
+	/* reiserfs_mark_buffer_dirty (tb->L[0], 0); journal victim */
+	journal_mark_dirty (th, tb->tb_sb, tb->L[0]); 
+      }
+      else {
+	if ( ! PATH_H_POSITION (tb->tb_path, 1) )
+	  replace_key(th, tb->CFL[0],tb->lkey[0],PATH_H_PPARENT(tb->tb_path, 0),0);
+	copy_key(B_PRIGHT_DELIM_KEY(tb->L[0]), B_PRIGHT_DELIM_KEY(tbS0));
+	/* reiserfs_mark_buffer_dirty (tb->L[0], 0); journal victim */
+	journal_mark_dirty (th, tb->tb_sb, tb->L[0]);
+      }
+    } 
+
+#ifdef CONFIG_REISERFS_CHECK
+    if (! item_pos && (!tb->CFL[0] || !tb->L[0]))
+      reiserfs_panic (tb->tb_sb, "PAP-12020: balance_leaf_when_delete: tb->CFL[0]==%p, tb->L[0]==%p", tb->CFL[0], tb->L[0]);
+#endif
+    
+    break;
+
+  case M_CUT: {  /* cut item in S[0] */
+    bi.bi_bh = tbS0;
+    bi.bi_parent = PATH_H_PPARENT (tb->tb_path, 0);
+    bi.bi_position = PATH_H_POSITION (tb->tb_path, 1);
+    if (I_IS_DIRECTORY_ITEM (ih)) {
+
+#ifdef CONFIG_REISERFS_CHECK
+      if ( ! item_pos && ! pos_in_item && (! tb->L[0] || COMP_KEYS(B_PRIGHT_DELIM_KEY(tb->L[0]), 
+								   B_N_PKEY(tbS0, 0))) )
+	reiserfs_panic(tb->tb_sb, "PAP-12025: balance_leaf_when_delete: illegal right delimiting key");
+#endif
+
+      /* UFS unlink semantics are such that you can only delete one directory entry at a time. */
+      /* when we cut a directory tb->insert_size[0] means number of entries to be cut (always 1) */
+      tb->insert_size[0] = -1;
+      leaf_cut_from_buffer (th, &bi, item_pos, pos_in_item, -tb->insert_size[0]);
+
+#ifdef CONFIG_REISERFS_CHECK
+      if (! item_pos && ! pos_in_item && ! tb->CFL[0])
+	reiserfs_panic (tb->tb_sb, "PAP-12030: balance_leaf_when_delete: can not change delimiting key. CFL[0]=%p", tb->CFL[0]);
+#endif /* CONFIG_REISERFS_CHECK */
+
+      if ( ! item_pos && ! pos_in_item && tb->CFL[0] ) {
+	replace_key(th, tb->CFL[0],tb->lkey[0],tbS0,0);
+	copy_key(B_PRIGHT_DELIM_KEY(tb->L[0]), B_N_PKEY(tbS0, 0));
+	/* reiserfs_mark_buffer_dirty (tb->L[0], 0); journal victim */
+	journal_mark_dirty (th, tb->tb_sb, tb->L[0]);
+      }
+    } else {
+      leaf_cut_from_buffer (th, &bi, item_pos, pos_in_item, -tb->insert_size[0]);
+
+#ifdef CONFIG_REISERFS_CHECK
+      if (! ih->ih_item_len)
+	reiserfs_panic (tb->tb_sb, "PAP-12035: balance_leaf_when_delete: cut must leave non-zero dynamic length of item");
+#endif /* CONFIG_REISERFS_CHECK */
+    }
+    break;
+  }
+
+  default:
+    print_tb(flag, item_pos, pos_in_item, tb,"when_del");
+    reiserfs_panic (tb->tb_sb, "PAP-12040: balance_leaf_when_delete: unexpectable mode: %s(%d)",
+		    (flag == M_PASTE) ? "PASTE" : ((flag == M_INSERT) ? "INSERT" : "UNKNOWN"), flag);
+  }
+
+  /* the rule is that no shifting occurs unless by shifting a node can be freed */
+  n = B_NR_ITEMS(tbS0);
+  if ( tb->lnum[0] )     /* L[0] takes part in balancing */
+    {
+      if ( tb->lnum[0] == -1 )    /* L[0] must be joined with S[0] */
+	{
+	  if ( tb->rnum[0] == -1 )    /* R[0] must be also joined with S[0] */
+	    {			
+	      if ( tb->FR[0] == PATH_H_PPARENT(tb->tb_path, 0) )
+		{
+		  /* all contents of all the 3 buffers will be in L[0] */
+		  if ( PATH_H_POSITION (tb->tb_path, 1) == 0 && 1 < B_NR_ITEMS(tb->FR[0]) )
+		    replace_key(th, tb->CFL[0],tb->lkey[0],tb->FR[0],1);
+
+		  /* update right_delimiting_key field */
+		  copy_key (B_PRIGHT_DELIM_KEY (tb->L[0]), B_PRIGHT_DELIM_KEY (tb->R[0]));
+
+		  leaf_move_items (th, LEAF_FROM_S_TO_L, tb, n, -1, 0);
+		  leaf_move_items (th, LEAF_FROM_R_TO_L, tb, B_NR_ITEMS(tb->R[0]), -1, 0);
+
+		  reiserfs_invalidate_buffer (th, tb, tbS0, 1/*do_free_block*/);
+		  reiserfs_invalidate_buffer (th, tb, tb->R[0], 1/*do_free_block*/);
+/*		  preserve_invalidate(th, tb, tbS0, tb->L[0]); 
+		  preserve_invalidate(th, tb, tb->R[0], tb->L[0]);*/
+
+		  return 0;
+		}
+	      /* all contents of all the 3 buffers will be in R[0] */
+	      leaf_move_items(th, LEAF_FROM_S_TO_R, tb, n, -1, 0);
+	      leaf_move_items(th, LEAF_FROM_L_TO_R, tb, B_NR_ITEMS(tb->L[0]), -1, 0);
+
+	      /* right_delimiting_key is correct in R[0] */
+	      replace_key(th, tb->CFR[0],tb->rkey[0],tb->R[0],0);
+
+	      /* mark tb->R[0] as suspected recipient */
+	      reiserfs_invalidate_buffer (th, tb, tbS0, 1/*do_free_block*/);
+	      reiserfs_invalidate_buffer (th, tb, tb->L[0], 1/*do_free_block*/);
+/*	      preserve_invalidate(th, tb,tbS0, tb->R[0]);
+	      preserve_invalidate(th, tb,tb->L[0], tb->R[0]); */
+
+	      return -1;
+	    }
+
+#ifdef CONFIG_REISERFS_CHECK
+	  if ( tb->rnum[0] != 0 )
+	    reiserfs_panic (tb->tb_sb, "PAP-12045: balance_leaf_when_delete: rnum must be 0 (%d)", tb->rnum[0]);
+#endif /* CONFIG_REISERFS_CHECK */
+
+	  /* all contents of L[0] and S[0] will be in L[0] */
+	  leaf_shift_left(th, tb, n, -1);
+
+	  reiserfs_invalidate_buffer (th, tb, tbS0, 1/*do_free_block*/);
+	  /*preserve_invalidate(th, tb, tbS0, tb->L[0]);*/  /* preserved, shifting */
+
+	  return 0;
+	}
+      /* a part of contents of S[0] will be in L[0] and the rest part of S[0] will be in R[0] */
+
+#ifdef CONFIG_REISERFS_CHECK
+      if (( tb->lnum[0] + tb->rnum[0] < n ) || ( tb->lnum[0] + tb->rnum[0] > n+1 ))
+	reiserfs_panic (tb->tb_sb, "PAP-12050: balance_leaf_when_delete: rnum(%d) and lnum(%d) and item number in S[0] are not consistent",
+                        tb->rnum[0], tb->lnum[0], n);
+
+      if (( tb->lnum[0] + tb->rnum[0] == n ) && (tb->lbytes != -1 || tb->rbytes != -1))
+	reiserfs_panic (tb->tb_sb, "PAP-12055: balance_leaf_when_delete: bad rbytes (%d)/lbytes (%d) parameters when items are not split", 
+                        tb->rbytes, tb->lbytes);
+      if (( tb->lnum[0] + tb->rnum[0] == n + 1 ) && (tb->lbytes < 1 || tb->rbytes != -1))
+	reiserfs_panic (tb->tb_sb, "PAP-12060: balance_leaf_when_delete: bad rbytes (%d)/lbytes (%d) parameters when items are split", 
+                        tb->rbytes, tb->lbytes);
+#endif
+
+      leaf_shift_left (th, tb, tb->lnum[0], tb->lbytes);
+      leaf_shift_right(th, tb, tb->rnum[0], tb->rbytes);
+
+      reiserfs_invalidate_buffer (th, tb, tbS0, 1/*do_free_block*/);
+      /*preserve_invalidate (th, tb, tbS0, tb->L[0]);
+      if (!buffer_dirty (tb->R[0]) && !buffer_journaled(tb->R[0]))
+	printk ("clean buffer marked sr 1\n");
+      mark_suspected_recipient (tb->tb_sb, tb->R[0]);*/
+
+      return 0;
+    }
+
+  if ( tb->rnum[0] == -1 ) {
+    /* all contents of R[0] and S[0] will be in R[0] */
+    leaf_shift_right(th, tb, n, -1);
+    reiserfs_invalidate_buffer (th, tb, tbS0, 1/*do_free_block*/);
+/*    preserve_invalidate(th, tb, tbS0, tb->R[0]); */
+    return 0;
+  }
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( tb->rnum[0] )
+    reiserfs_panic (tb->tb_sb, "PAP-12065: balance_leaf_when_delete: bad rnum parameter must be 0 (%d)", tb->rnum[0]);
+#endif
+
+  return 0;
+}
+
+
+static int	balance_leaf(
+			     struct reiserfs_transaction_handle *th, 
+			     struct tree_balance * tb,		/* see reiserfs_fs.h */
+			     int pos_in_item,                /* position in item, in bytes for direct
+								and indirect items, in entries for
+								directories (for which it is an index
+								into the array of directory entry
+								headers.) */
+
+			     struct item_head * ih,		/* item header of inserted item */
+			     const char * body,		/* body  of inserted item or bytes to paste */
+			     int flag,			/* i - insert, d - delete, c - cut, p - paste
+							   (see comment to do_balance) */
+			     int mem_mode,	  /* memory mode; must be REISERFS_USER_MEM or REISERFS_KERNEL_MEM,
+						     depending on whether the body parameter is located in a
+						     user buffer in user memory or not. */
+			     int zeros_number,		/* will be commented later */
+				
+			     struct item_head * insert_key,  /* in our processing of one level we sometimes determine what
+								must be inserted into the next higher level.  This insertion
+								consists of a key or two keys and their corresponding
+								pointers */
+			     struct buffer_head ** insert_ptr /* inserted node-ptrs for the next level */
+			     )
+{
+  struct buffer_head * tbS0 = PATH_PLAST_BUFFER (tb->tb_path);
+/*  struct buffer_head * tbF0 = PATH_H_PPARENT (tb->tb_path, 0);
+  int S0_b_item_order = PATH_H_B_ITEM_ORDER (tb->tb_path, 0);*/
+  int item_pos = PATH_LAST_POSITION (tb->tb_path);	/*  index into the array of item headers in S[0] 
+							    of the affected item */
+  struct buffer_info bi;
+  struct buffer_head *S_new[2];  /* new nodes allocated to hold what could not fit into S */
+  int snum[2];			   /* number of items that will be placed into S_new (includes partially shifted items) */
+  int sbytes[2];                   /* if an item is partially shifted into S_new then 
+				      if it is a directory item 
+				      it is the number of entries from the item that are shifted into S_new
+				      else
+				      it is the number of bytes from the item that are shifted into S_new
+				      */
+  int n, i;
+  int ret_val;
+
+  /* Make balance in case insert_size[0] < 0 */
+  if ( tb->insert_size[0] < 0 )
+    return balance_leaf_when_delete (th, tb, pos_in_item, flag);
+  
+  /* for indirect item pos_in_item is measured in unformatted node
+     pointers. Recalculate to bytes */
+  if (flag != M_INSERT && I_IS_INDIRECT_ITEM (B_N_PITEM_HEAD (tbS0, item_pos)))
+    pos_in_item *= UNFM_P_SIZE;
+
+  if ( tb->lnum[0] > 0 ) {
+    /* Shift lnum[0] items from S[0] to the left neighbor L[0] */
+    if ( item_pos < tb->lnum[0] ) {
+      /* new item or it part falls to L[0], shift it too */
+      n = B_NR_ITEMS(tb->L[0]);
+
+      switch (flag) {
+      case M_INSERT:   /* insert item into L[0] */
+
+	if ( item_pos == tb->lnum[0] - 1 && tb->lbytes != -1 ) {
+	  /* part of new item falls into L[0] */
+	  int new_item_len;
+
+#ifdef CONFIG_REISERFS_CHECK
+	  if (!I_IS_DIRECT_ITEM (ih))
+	    reiserfs_panic (tb->tb_sb, "PAP-12075: balance_leaf: " 
+			    "only direct inserted item can be broken. k_uniqueness=%lu",
+			    ih->ih_key.k_uniqueness);
+#endif
+	  ret_val = leaf_shift_left (th, tb, tb->lnum[0]-1, -1);
+	  /* when reading the if conditions preceding the subsequent preserve_shifted
+	     lines understand that their goal is to determine if all that we are
+	     shifting is the new data being added */
+/*
+	  if (tb->lnum[0] - 1 > 0) {
+	    preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, tb->L[0]);
+	    tbS0 = PATH_PLAST_BUFFER (tb->tb_path);
+	  }
+*/
+	  /* Calculate item length to insert to S[0] */
+	  new_item_len = ih->ih_item_len - tb->lbytes;
+	  /* Calculate and check item length to insert to L[0] */
+	  ih->ih_item_len -= new_item_len;
+
+#ifdef CONFIG_REISERFS_CHECK
+	  if ( (int)(ih->ih_item_len) <= 0 )
+	    reiserfs_panic(tb->tb_sb, "PAP-12080: balance_leaf: "
+			   "there is nothing to insert into L[0]: ih_item_len=%d",
+			   (int)ih->ih_item_len);
+#endif
+
+	  /* Insert new item into L[0] */
+	  bi.bi_bh = tb->L[0];
+	  bi.bi_parent = tb->FL[0];
+	  bi.bi_position = get_left_neighbor_position (tb, 0);
+	  leaf_insert_into_buf (th, &bi, n + item_pos - ret_val, ih, body, mem_mode,
+				zeros_number > ih->ih_item_len ? ih->ih_item_len : zeros_number);
+/*
+	  if (tb->preserve_mode == PRESERVE_INDIRECT_TO_DIRECT){
+	    if (!buffer_dirty (bi.bi_bh) && !buffer_journaled(bi.bi_bh))
+	      printk ("clean buffer marked sr 2\n");
+	    mark_suspected_recipient (tb->tb_sb, bi.bi_bh);
+	  }
+*/  
+	  /* Calculate key component, item length and body to insert into S[0] */
+	  ih->ih_key.k_offset += tb->lbytes;
+	  ih->ih_item_len = new_item_len;
+          if ( tb->lbytes >  zeros_number ) {
+            body += (tb->lbytes - zeros_number);
+            zeros_number = 0;
+          }
+          else
+            zeros_number -= tb->lbytes;
+
+#ifdef CONFIG_REISERFS_CHECK
+	  if ( (int)(ih->ih_item_len) <= 0 )
+	    reiserfs_panic(tb->tb_sb, "PAP-12085: balance_leaf: "
+			   "there is nothing to insert into S[0]: ih_item_len=%d",
+			   (int)ih->ih_item_len);
+#endif
+	} else {
+	  /* new item in whole falls into L[0] */
+	  /* Shift lnum[0]-1 items to L[0] */
+	  ret_val = leaf_shift_left(th, tb, tb->lnum[0]-1, tb->lbytes);
+/*
+	  if (tb->lnum[0] > 1) {
+	    preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, tb->L[0]);
+	    tbS0 = PATH_PLAST_BUFFER (tb->tb_path);
+	  }
+*/
+	  /* Insert new item into L[0] */
+	  bi.bi_bh = tb->L[0];
+	  bi.bi_parent = tb->FL[0];
+	  bi.bi_position = get_left_neighbor_position (tb, 0);
+	  leaf_insert_into_buf (th, &bi, n + item_pos - ret_val, ih, body, mem_mode, zeros_number);
+/*
+	  if (tb->preserve_mode == PRESERVE_INDIRECT_TO_DIRECT){
+	    if (!buffer_dirty (bi.bi_bh) && !buffer_journaled(bi.bi_bh))
+	      printk ("clean buffer marked sr 3\n");
+	    mark_suspected_recipient (tb->tb_sb, bi.bi_bh);
+	  }
+*/
+	  tb->insert_size[0] = 0;
+          zeros_number = 0;
+	}
+	break;
+
+      case M_PASTE:   /* append item in L[0] */
+
+	if ( item_pos == tb->lnum[0] - 1 && tb->lbytes != -1 ) {
+	  /* we must shift the part of the appended item */
+	  if ( I_IS_DIRECTORY_ITEM (B_N_PITEM_HEAD (tbS0, item_pos))) {
+
+#ifdef CONFIG_REISERFS_CHECK
+	    if ( zeros_number )
+              reiserfs_panic(tb->tb_sb, "PAP-12090: balance_leaf: illegal parameter in case of a directory");
+#endif
+            
+	    /* directory item */
+	    if ( tb->lbytes > pos_in_item ) {
+	      /* new directory entry falls into L[0] */
+	      struct item_head * pasted;
+	      int l_pos_in_item = pos_in_item;
+							  
+	      /* Shift lnum[0] - 1 items in whole. Shift lbytes - 1 entries from given directory item */
+	      ret_val = leaf_shift_left(th, tb, tb->lnum[0], tb->lbytes - 1);
+/*	      preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, tb->L[0]);
+	      tbS0 = PATH_PLAST_BUFFER (tb->tb_path);*/
+	      if ( ret_val && ! item_pos ) {
+		pasted =  B_N_PITEM_HEAD(tb->L[0],B_NR_ITEMS(tb->L[0])-1);
+		l_pos_in_item += I_ENTRY_COUNT(pasted) - (tb->lbytes-1);
+	      }
+
+	      /* Append given directory entry to directory item */
+	      bi.bi_bh = tb->L[0];
+	      bi.bi_parent = tb->FL[0];
+	      bi.bi_position = get_left_neighbor_position (tb, 0);
+	      leaf_paste_in_buffer (th, &bi, n + item_pos - ret_val, l_pos_in_item,
+				    tb->insert_size[0], body, mem_mode, zeros_number);
+
+	      /* previous string prepared space for pasting new entry, following string pastes this entry */
+
+	      /* when we have merge directory item, pos_in_item has been changed too */
+
+	      /* paste new directory entry. 1 is entry number */
+	      leaf_paste_entries (bi.bi_bh, n + item_pos - ret_val, l_pos_in_item, 1,
+				  (struct reiserfs_de_head *)body, 
+				  body + DEH_SIZE, tb->insert_size[0]
+				  );
+	      tb->insert_size[0] = 0;
+	    } else {
+	      /* new directory item doesn't fall into L[0] */
+	      /* Shift lnum[0]-1 items in whole. Shift lbytes directory entries from directory item number lnum[0] */
+	      leaf_shift_left (th, tb, tb->lnum[0], tb->lbytes);
+/*	      preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, tb->L[0]);
+	      tbS0 = PATH_PLAST_BUFFER (tb->tb_path);*/
+	    }
+	    /* Calculate new position to append in item body */
+	    pos_in_item -= tb->lbytes;
+          }
+	  else {
+	    /* regular object */
+
+#ifdef CONFIG_REISERFS_CHECK
+	    if ( tb->lbytes  <= 0 )
+	      reiserfs_panic(tb->tb_sb, "PAP-12095: balance_leaf: " 
+			     "there is nothing to shift to L[0]. lbytes=%d",
+			     tb->lbytes);
+	    if ( pos_in_item != B_N_PITEM_HEAD(tbS0, item_pos)->ih_item_len )
+	      reiserfs_panic(tb->tb_sb, "PAP-12100: balance_leaf: " 
+			     "incorrect position to paste: item_len=%d, pos_in_item=%d",
+			     B_N_PITEM_HEAD(tbS0,item_pos)->ih_item_len, pos_in_item);
+#endif
+
+	    if ( tb->lbytes >= pos_in_item ) {
+	      /* appended item will be in L[0] in whole */
+	      int l_n;
+
+	      /* this bytes number must be appended to the last item of L[h] */
+	      l_n = tb->lbytes - pos_in_item;
+
+	      /* Calculate new insert_size[0] */
+	      tb->insert_size[0] -= l_n;
+
+#ifdef CONFIG_REISERFS_CHECK
+	      if ( tb->insert_size[0] <= 0 )
+		reiserfs_panic(tb->tb_sb, "PAP-12105: balance_leaf: " 
+			       "there is nothing to paste into L[0]. insert_size=%d",
+			       tb->insert_size[0]);
+#endif
+
+	      ret_val =  leaf_shift_left(th, tb,tb->lnum[0], 
+					 B_N_PITEM_HEAD(tbS0,item_pos)->ih_item_len);
+/*	      preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, tb->L[0]);
+	      tbS0 = PATH_PLAST_BUFFER (tb->tb_path);*/
+	      /* Append to body of item in L[0] */
+	      bi.bi_bh = tb->L[0];
+	      bi.bi_parent = tb->FL[0];
+	      bi.bi_position = get_left_neighbor_position (tb, 0);
+	      leaf_paste_in_buffer(th, 
+				   &bi,n + item_pos - ret_val,
+				   B_N_PITEM_HEAD(tb->L[0],n+item_pos-ret_val)->ih_item_len,
+				   l_n,body,mem_mode, zeros_number > l_n ? l_n : zeros_number
+				   );
+
+#ifdef CONFIG_REISERFS_CHECK
+	      if (l_n && I_IS_INDIRECT_ITEM(B_N_PITEM_HEAD(tb->L[0],
+							   n + item_pos - ret_val)))
+		reiserfs_panic(tb->tb_sb, "PAP-12110: balance_leaf: "
+			       "pasting more than 1 unformatted node pointer into indirect item");
+#endif
+
+	      /* 0-th item in S0 can be only of DIRECT type when l_n != 0*/
+	      B_N_PKEY (tbS0, 0)->k_offset += l_n;
+	      B_N_PDELIM_KEY(tb->CFL[0],tb->lkey[0])->k_offset += l_n;
+
+	      B_PRIGHT_DELIM_KEY(tb->L[0])->k_offset += l_n;
+
+#ifdef CONFIG_REISERFS_CHECK /* killed for journal */
+	      if (0 && (!buffer_dirty (tbS0) || !buffer_dirty (tb->CFL[0]) || !buffer_dirty (tb->L[0])))
+		reiserfs_panic(tb->tb_sb, "PAP-12115: balance_leaf: L, CLF and S must be dirty already");
+#endif
+
+	      /* Calculate new body, position in item and insert_size[0] */
+              if ( l_n > zeros_number ) {
+                body += (l_n - zeros_number);
+                zeros_number = 0;
+              }
+              else
+                zeros_number -= l_n;
+	      pos_in_item = 0;	
+
+#ifdef CONFIG_REISERFS_CHECK	
+	      if (COMP_SHORT_KEYS (B_N_PKEY(tbS0,0),
+				   B_N_PKEY(tb->L[0],B_NR_ITEMS(tb->L[0])-1)) ||
+		  !is_left_mergeable (B_N_PITEM_HEAD (tbS0, 0), tbS0->b_size) ||
+		  !is_left_mergeable((struct item_head *)B_N_PDELIM_KEY(tb->CFL[0],tb->lkey[0]), tbS0->b_size))
+		reiserfs_panic (tb->tb_sb, "PAP-12120: balance_leaf: "
+				"item must be merge-able with left neighboring item");
+#endif
+
+	    }
+	    else /* only part of the appended item will be in L[0] */
+	      {
+		/* Calculate position in item for append in S[0] */
+		pos_in_item -= tb->lbytes;
+
+#ifdef CONFIG_REISERFS_CHECK
+		if ( pos_in_item <= 0 )
+		  reiserfs_panic(tb->tb_sb, "PAP-12125: balance_leaf: "
+				 "no place for paste. pos_in_item=%d", pos_in_item);
+#endif
+
+		/* Shift lnum[0] - 1 items in whole. Shift lbytes - 1 byte from item number lnum[0] */
+		leaf_shift_left(th, tb,tb->lnum[0],tb->lbytes);
+/*		preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, tb->L[0]);
+		tbS0 = PATH_PLAST_BUFFER (tb->tb_path);*/
+	      }
+	  }
+	}
+	else /* appended item will be in L[0] in whole */
+	  {
+	    struct item_head * pasted;
+
+#ifdef REISERFS_FSCK
+	    if ( ! item_pos  && is_left_mergeable (tb->tb_sb, tb->tb_path) == 1 )
+#else
+	    if ( ! item_pos  && is_left_mergeable (B_N_PITEM_HEAD (tbS0, 0), tbS0->b_size) )
+#endif
+	      { /* if we paste into first item of S[0] and it is left mergable */
+		/* then increment pos_in_item by the size of the last item in L[0] */
+		pasted = B_N_PITEM_HEAD(tb->L[0],n-1);
+		if ( I_IS_DIRECTORY_ITEM(pasted) )
+		  pos_in_item += I_ENTRY_COUNT(pasted);
+		else
+		  pos_in_item += pasted->ih_item_len;
+	      }
+
+	    /* Shift lnum[0] - 1 items in whole. Shift lbytes - 1 byte from item number lnum[0] */
+	    ret_val = leaf_shift_left(th, tb,tb->lnum[0],tb->lbytes);
+/*	    preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, tb->L[0]);
+	    tbS0 = PATH_PLAST_BUFFER (tb->tb_path);*/
+	    /* Append to body of item in L[0] */
+	    bi.bi_bh = tb->L[0];
+	    bi.bi_parent = tb->FL[0];
+	    bi.bi_position = get_left_neighbor_position (tb, 0);
+	    leaf_paste_in_buffer (th, &bi, n + item_pos - ret_val, pos_in_item, tb->insert_size[0],
+				  body, mem_mode, zeros_number);
+
+	    /* if appended item is directory, paste entry */
+	    pasted = B_N_PITEM_HEAD (tb->L[0], n + item_pos - ret_val);
+	    if (I_IS_DIRECTORY_ITEM (pasted))
+	      leaf_paste_entries (
+				  bi.bi_bh, n + item_pos - ret_val, pos_in_item, 1, 
+				  (struct reiserfs_de_head *)body, body + DEH_SIZE, tb->insert_size[0]
+				  );
+	    /* if appended item is indirect item, put unformatted node into un list */
+	    if (I_IS_INDIRECT_ITEM (pasted))
+	      pasted->u.ih_free_space = ((struct unfm_nodeinfo*)body)->unfm_freespace;
+	    tb->insert_size[0] = 0;
+	    zeros_number = 0;
+	  }
+	break;
+      default:    /* cases d and t */
+	reiserfs_panic (tb->tb_sb, "PAP-12130: balance_leaf: lnum > 0: unexpectable mode: %s(%d)",
+			(flag == M_DELETE) ? "DELETE" : ((flag == M_CUT) ? "CUT" : "UNKNOWN"), flag);
+      }
+    } else { 
+      /* new item doesn't fall into L[0] */
+      leaf_shift_left(th, tb,tb->lnum[0],tb->lbytes);
+/*      preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, tb->L[0]);
+      tbS0 = PATH_PLAST_BUFFER (tb->tb_path);*/
+    }
+  }	/* tb->lnum[0] > 0 */
+
+  /* Calculate new item position */
+  item_pos -= ( tb->lnum[0] - (( tb->lbytes != -1 ) ? 1 : 0));
+
+  if ( tb->rnum[0] > 0 ) {
+    /* shift rnum[0] items from S[0] to the right neighbor R[0] */
+    n = B_NR_ITEMS(tbS0);
+    switch ( flag ) {
+
+    case M_INSERT:   /* insert item */
+      if ( n - tb->rnum[0] < item_pos )
+	{ /* new item or its part falls to R[0] */
+	  if ( item_pos == n - tb->rnum[0] + 1 && tb->rbytes != -1 )
+	    { /* part of new item falls into R[0] */
+	      int old_key_comp, old_len, r_zeros_number;
+	      const char * r_body;
+
+#ifdef CONFIG_REISERFS_CHECK
+	      if ( !I_IS_DIRECT_ITEM(ih) )
+		reiserfs_panic(tb->tb_sb, "PAP-12135: balance_leaf: "
+			       "only direct item can be split. Offset and uniqueness are: [%lu %lu]",
+			       ih->ih_key.k_offset, ih->ih_key.k_uniqueness);
+#endif
+
+	      leaf_shift_right(th, tb,tb->rnum[0]-1,-1);
+/*
+	      if (tb->rnum[0]>1) {
+		preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, tb->R[0]);
+		tbS0 = PATH_PLAST_BUFFER (tb->tb_path);
+	      }
+*/
+	      /* Remember key component and item length */
+	      old_key_comp = ih->ih_key.k_offset;
+	      old_len = ih->ih_item_len;
+
+	      /* Calculate key component and item length to insert into R[0] */
+	      ih->ih_key.k_offset += (old_len - tb->rbytes);
+	      ih->ih_item_len = tb->rbytes;
+	      /* Insert part of the item into R[0] */
+	      bi.bi_bh = tb->R[0];
+	      bi.bi_parent = tb->FR[0];
+	      bi.bi_position = get_right_neighbor_position (tb, 0);
+	      if ( ih->ih_key.k_offset - old_key_comp > zeros_number ) {
+		r_zeros_number = 0;
+		r_body = body + ih->ih_key.k_offset - old_key_comp - zeros_number;
+	      }
+	      else {
+		r_body = body;
+		r_zeros_number = zeros_number - (ih->ih_key.k_offset - old_key_comp);
+		zeros_number -= r_zeros_number;
+	      }
+
+	      leaf_insert_into_buf (th, &bi, 0, ih, r_body, mem_mode, r_zeros_number);
+/*	      if (tb->preserve_mode == PRESERVE_INDIRECT_TO_DIRECT){
+		if (!buffer_dirty (bi.bi_bh) && !buffer_journaled(bi.bi_bh))
+		  printk ("clean buffer marked sr 4\n");
+		mark_suspected_recipient (tb->tb_sb, bi.bi_bh);
+	      }
+*/
+
+	      /* Replace right delimiting key by first key in R[0] */
+	      replace_key(th, tb->CFR[0],tb->rkey[0],tb->R[0],0);
+
+	      /* update right delimiting key */
+	      copy_key(B_PRIGHT_DELIM_KEY(tbS0), &(ih->ih_key));
+	      journal_mark_dirty (th, tb->tb_sb, tbS0); /* probably not needed */
+
+	      /* Calculate key component and item length to insert into S[0] */
+	      ih->ih_key.k_offset = old_key_comp;
+	      ih->ih_item_len = old_len - tb->rbytes;
+
+	      tb->insert_size[0] -= tb->rbytes;
+
+	    }
+	  else /* whole new item falls into R[0] */
+	    {					  
+	      /* Shift rnum[0]-1 items to R[0] */
+	      ret_val = leaf_shift_right(th, tb,tb->rnum[0]-1,tb->rbytes);
+/*	      if (tb->rnum[0]>1) {
+		preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, tb->R[0]);
+		tbS0 = PATH_PLAST_BUFFER (tb->tb_path);
+	      }*/
+	      /* Insert new item into R[0] */
+	      bi.bi_bh = tb->R[0];
+	      bi.bi_parent = tb->FR[0];
+	      bi.bi_position = get_right_neighbor_position (tb, 0);
+	      leaf_insert_into_buf (th, &bi, item_pos - n + tb->rnum[0] - 1, ih, body, mem_mode, zeros_number);
+/*
+	      if (tb->preserve_mode == PRESERVE_INDIRECT_TO_DIRECT){
+		if (!buffer_dirty (bi.bi_bh) && !buffer_journaled(bi.bi_bh))
+		  printk ("clean buffer marked sr 5\n");
+		mark_suspected_recipient (tb->tb_sb, bi.bi_bh);
+	      }
+*/
+	      /* If we insert new item in the begin of R[0] change the right delimiting key */
+	      if ( item_pos - n + tb->rnum[0] - 1 == 0 ) {
+		replace_key(th, tb->CFR[0],tb->rkey[0],tb->R[0],0);
+
+		/* update right delimiting key */
+		copy_key(B_PRIGHT_DELIM_KEY(tbS0), &(ih->ih_key));
+		/* reiserfs_mark_buffer_dirty (tbS0, 0); journal victim */
+		journal_mark_dirty (th, tb->tb_sb, tbS0);
+	      }
+	      zeros_number = tb->insert_size[0] = 0;
+	    }
+	}
+      else /* new item or part of it doesn't fall into R[0] */
+	{
+	  leaf_shift_right(th, tb,tb->rnum[0],tb->rbytes);
+/*	  preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, tb->R[0]);
+	  tbS0 = PATH_PLAST_BUFFER (tb->tb_path);*/
+	}
+      break;
+
+    case M_PASTE:   /* append item */
+
+      if ( n - tb->rnum[0] <= item_pos )  /* pasted item or part of it falls to R[0] */
+	{
+	  if ( item_pos == n - tb->rnum[0] && tb->rbytes != -1 )
+	    { /* we must shift the part of the appended item */
+	      if ( I_IS_DIRECTORY_ITEM (B_N_PITEM_HEAD(tbS0, item_pos)))
+		{ /* we append to directory item */
+		  int entry_count;
+
+#ifdef CONFIG_REISERFS_CHECK
+		  if ( zeros_number )
+		    reiserfs_panic(tb->tb_sb, "PAP-12145: balance_leaf: illegal parametr in case of a directory");
+#endif
+
+		  entry_count = I_ENTRY_COUNT(B_N_PITEM_HEAD(tbS0, item_pos));
+		  if ( entry_count - tb->rbytes < pos_in_item )
+		    /* new directory entry falls into R[0] */
+		    {
+		      int paste_entry_position;
+
+#ifdef CONFIG_REISERFS_CHECK
+		      if ( tb->rbytes - 1 >= entry_count || ! tb->insert_size[0] )
+			reiserfs_panic(tb->tb_sb, "PAP-12150: balance_leaf: "
+				       "no enough of entries to shift to R[0]: rbytes=%d, entry_count=%d",
+				       tb->rbytes, entry_count);
+#endif
+
+		      /* Shift rnum[0]-1 items in whole. Shift rbytes-1 directory entries from directory item number rnum[0] */
+		      leaf_shift_right(th, tb,tb->rnum[0],tb->rbytes - 1);
+		      /* if we are shifting more than just the new entry */
+/*
+		      if (tb->rbytes > 1 || tb->rnum[0] > 1) {
+			preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, tb->R[0]);
+			tbS0 = PATH_PLAST_BUFFER (tb->tb_path);
+		      }
+*/
+		      /* Paste given directory entry to directory item */
+		      paste_entry_position = pos_in_item - entry_count + tb->rbytes - 1;
+
+		      bi.bi_bh = tb->R[0];
+		      bi.bi_parent = tb->FR[0];
+		      bi.bi_position = get_right_neighbor_position (tb, 0);
+		      leaf_paste_in_buffer (th, &bi, 0, paste_entry_position,
+					    tb->insert_size[0],body,mem_mode,zeros_number);
+		      /* paste entry */
+		      leaf_paste_entries (
+					  bi.bi_bh, 0, paste_entry_position, 1, (struct reiserfs_de_head *)body, 
+					  body + DEH_SIZE, tb->insert_size[0]
+					  );								
+						
+		      if ( paste_entry_position == 0 ) {
+			/* change delimiting keys */
+			replace_key(th, tb->CFR[0],tb->rkey[0],tb->R[0],0);
+			copy_key(B_PRIGHT_DELIM_KEY(tbS0), B_N_PKEY(tb->R[0], 0));
+			/* reiserfs_mark_buffer_dirty (tbS0, 0); journal victim */
+			journal_mark_dirty (th, tb->tb_sb, tbS0);
+		      }
+
+		      tb->insert_size[0] = 0;
+		      pos_in_item++;
+		    }
+		  else /* new directory entry doesn't fall into R[0] */
+		    {
+		      leaf_shift_right(th, tb,tb->rnum[0],tb->rbytes);
+/*
+		      preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, tb->R[0]);
+		      tbS0 = PATH_PLAST_BUFFER (tb->tb_path);
+*/
+		    }
+		}
+	      else /* regular object */
+		{
+		  int n_shift, n_rem, r_zeros_number;
+		  const char * r_body;
+
+		  /* Calculate number of bytes which must be shifted from appended item */
+		  if ( (n_shift = tb->rbytes - tb->insert_size[0]) < 0 )
+		    n_shift = 0;
+
+#ifdef CONFIG_REISERFS_CHECK
+		  if (pos_in_item != B_N_PITEM_HEAD (tbS0, item_pos)->ih_item_len)
+		    reiserfs_panic(tb->tb_sb,"PAP-12155: balance_leaf: invalid position to paste. ih_item_len=%d, pos_in_item=%d",
+				   pos_in_item, B_N_PITEM_HEAD(tbS0,item_pos)->ih_item_len);
+#endif
+
+		  leaf_shift_right(th, tb,tb->rnum[0],n_shift);
+		  /* if we are shifting an old part from the appended item or more than the appended item is going into R */
+/*
+		  if (n_shift || tb->rnum[0] > 1) {
+		    preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, tb->R[0]);
+		    tbS0 = PATH_PLAST_BUFFER (tb->tb_path);
+		  }
+*/
+		  /* Calculate number of bytes which must remain in body after appending to R[0] */
+		  if ( (n_rem = tb->insert_size[0] - tb->rbytes) < 0 )
+		    n_rem = 0;
+
+		  B_N_PKEY(tb->R[0],0)->k_offset += n_rem;
+		  B_N_PDELIM_KEY(tb->CFR[0],tb->rkey[0])->k_offset += n_rem;
+		  /* reiserfs_mark_buffer_dirty (tb->CFR[0], 0); */
+		  journal_mark_dirty (th, tb->tb_sb, tb->CFR[0]);
+
+		  B_PRIGHT_DELIM_KEY(tbS0)->k_offset += n_rem;
+		  /* reiserfs_mark_buffer_dirty (tbS0, 0); journal victim */
+		  journal_mark_dirty (th, tb->tb_sb, tbS0); 
+		  /* Append part of body into R[0] */
+		  bi.bi_bh = tb->R[0];
+		  bi.bi_parent = tb->FR[0];
+		  bi.bi_position = get_right_neighbor_position (tb, 0);
+		  if ( n_rem > zeros_number ) {
+		    r_zeros_number = 0;
+		    r_body = body + n_rem - zeros_number;
+		  }
+		  else {
+		    r_body = body;
+		    r_zeros_number = zeros_number - n_rem;
+		    zeros_number -= r_zeros_number;
+		  }
+
+		  leaf_paste_in_buffer(th, &bi, 0, n_shift, tb->insert_size[0] - n_rem, r_body, mem_mode, r_zeros_number);
+
+		  if (I_IS_INDIRECT_ITEM(B_N_PITEM_HEAD(tb->R[0],0))) {
+
+#ifdef CONFIG_REISERFS_CHECK
+		    if (n_rem)
+		      reiserfs_panic(tb->tb_sb, "PAP-12160: balance_leaf: paste more than one unformatted node pointer");
+#endif
+
+		    B_N_PITEM_HEAD(tb->R[0],0)->u.ih_free_space = ((struct unfm_nodeinfo*)body)->unfm_freespace;
+		  }
+
+		  tb->insert_size[0] = n_rem;
+		  if ( ! n_rem )
+		    pos_in_item ++;
+		}
+	    }
+	  else /* pasted item in whole falls into R[0] */
+	    {
+	      struct item_head * pasted;
+
+	      ret_val = leaf_shift_right(th, tb,tb->rnum[0],tb->rbytes);
+/*
+	      preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, tb->R[0]);
+	      tbS0 = PATH_PLAST_BUFFER (tb->tb_path);
+*/
+	      /* append item in R[0] */
+	      if ( pos_in_item >= 0 ) {
+		bi.bi_bh = tb->R[0];
+		bi.bi_parent = tb->FR[0];
+		bi.bi_position = get_right_neighbor_position (tb, 0);
+		leaf_paste_in_buffer(th, &bi,item_pos - n + tb->rnum[0], pos_in_item,
+				     tb->insert_size[0],body,mem_mode, zeros_number);
+	      }
+
+	      /* paste new entry, if item is directory item */
+	      pasted = B_N_PITEM_HEAD(tb->R[0], item_pos - n + tb->rnum[0]);
+	      if (I_IS_DIRECTORY_ITEM (pasted) && pos_in_item >= 0 ) {
+		leaf_paste_entries (
+				    bi.bi_bh, item_pos - n + tb->rnum[0], pos_in_item, 1, 
+				    (struct reiserfs_de_head *)body, body + DEH_SIZE, tb->insert_size[0]
+				    );
+		if ( ! pos_in_item ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+		  if ( item_pos - n + tb->rnum[0] )
+		    reiserfs_panic (tb->tb_sb, "PAP-12165: balance_leaf: " 
+				    "directory item must be first item of node when pasting is in 0th position");
+#endif
+
+		  /* update delimiting keys */
+		  replace_key(th, tb->CFR[0],tb->rkey[0],tb->R[0],0);
+		  copy_key(B_PRIGHT_DELIM_KEY(tbS0),B_N_PKEY(tb->R[0], 0));
+		  /* reiserfs_mark_buffer_dirty (tbS0, 0); */
+		  journal_mark_dirty (th, tb->tb_sb, tbS0);
+		}
+	      }
+
+	      if (I_IS_INDIRECT_ITEM (pasted))
+		pasted->u.ih_free_space = ((struct unfm_nodeinfo*)body)->unfm_freespace;
+	      zeros_number = tb->insert_size[0] = 0;
+	    }
+	}
+      else /* new item doesn't fall into R[0] */
+	{
+	  leaf_shift_right(th, tb,tb->rnum[0],tb->rbytes);
+/*
+	  preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, tb->R[0]);
+	  tbS0 = PATH_PLAST_BUFFER (tb->tb_path);
+*/
+	}
+      break;
+    default:    /* cases d and t */
+      reiserfs_panic (tb->tb_sb, "PAP-12175: balance_leaf: rnum > 0: unexpectable mode: %s(%d)",
+		      (flag == M_DELETE) ? "DELETE" : ((flag == M_CUT) ? "CUT" : "UNKNOWN"), flag);
+    }
+    
+  }	/* tb->rnum[0] > 0 */
+
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( tb->blknum[0] > 3 )  
+    reiserfs_panic (tb->tb_sb, "PAP-12180: balance_leaf: blknum can not be %d. It must be <= 3",  tb->blknum[0]);
+
+  if ( tb->blknum[0] < 0 )  
+    reiserfs_panic (tb->tb_sb, "PAP-12185: balance_leaf: blknum can not be %d. It must be >= 0",  tb->blknum[0]);
+#endif
+
+  /* if while adding to a node we discover that it is possible to split
+     it in two, and merge the left part into the left neighbor and the
+     right part into the right neighbor, eliminating the node */
+  if ( tb->blknum[0] == 0 ) { /* node S[0] is empty now */
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( ! tb->lnum[0] || ! tb->rnum[0] )
+      reiserfs_panic(tb->tb_sb, "PAP-12190: balance_leaf: lnum and rnum must not be zero");
+    if (COMP_KEYS (B_N_PKEY(tb->R[0], 0), B_PRIGHT_DELIM_KEY(tbS0)))
+      reiserfs_panic (tb->tb_sb, "vs-12192: balance_leaf: S[0] is being removed from the tree, it has incorrect right delimiting key");
+#endif
+
+    /* if insertion was done before 0-th position in R[0], right delimiting key of the tb->L[0]'s
+       and left delimiting key are not set correctly */
+    if (tb->L[0]) {
+      copy_key(B_PRIGHT_DELIM_KEY(tb->L[0]), B_PRIGHT_DELIM_KEY(tbS0));
+      /* reiserfs_mark_buffer_dirty (tb->L[0], 0); journal victim */
+      journal_mark_dirty (th, tb->tb_sb, tb->L[0]);
+    }
+
+    if (tb->CFL[0]) {
+      copy_key (B_N_PDELIM_KEY (tb->CFL[0], tb->lkey[0]), B_PRIGHT_DELIM_KEY(tbS0));
+      /* reiserfs_mark_buffer_dirty (tb->CFL[0], 0); journal victim */
+      journal_mark_dirty (th, tb->tb_sb, tb->CFL[0]);
+    }
+
+    /* we can free block because tbS0 must be preserved (lnum[0] > 0 && rnum[0] > 0) or
+       unwritten. If it was suspected recipient, L[0] and R[0] are suspected recipients already */
+    reiserfs_invalidate_buffer(th, tb,tbS0, 1);									
+    return 0;
+  }
+
+
+  /* Fill new nodes that appear in place of S[0] */
+
+  /* I am told that this copying is because we need an array to enable
+     the looping code. -Hans */
+  snum[0] = tb->s1num,
+    snum[1] = tb->s2num;
+  sbytes[0] = tb->s1bytes;
+  sbytes[1] = tb->s2bytes;
+  for( i = tb->blknum[0] - 2; i >= 0; i-- ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+    if (!snum[i])
+      reiserfs_panic(tb->tb_sb,"PAP-12200: balance_leaf: snum[%d] == %d. Must be > 0", i, snum[i]);
+#endif /* CONFIG_REISERFS_CHECK */
+
+    /* here we shift from S to S_new nodes */
+
+    S_new[i] = get_FEB(tb);
+
+    /* initialized block type and tree level */
+    B_BLK_HEAD(S_new[i])->blk_level = DISK_LEAF_NODE_LEVEL;
+
+
+    n = B_NR_ITEMS(tbS0);
+	
+    switch (flag) {
+    case M_INSERT:   /* insert item */
+
+      if ( n - snum[i] < item_pos )
+	{ /* new item or it's part falls to first new node S_new[i]*/
+	  if ( item_pos == n - snum[i] + 1 && sbytes[i] != -1 )
+	    { /* part of new item falls into S_new[i] */
+	      int old_key_comp, old_len, r_zeros_number;
+	      const char * r_body;
+
+#ifdef CONFIG_REISERFS_CHECK
+	      if ( !I_IS_DIRECT_ITEM(ih) )
+		/* The items which can be inserted are:
+		   Stat_data item, direct item, indirect item and directory item which consist of only two entries "." and "..".
+		   These items must not be broken except for a direct one. */
+		reiserfs_panic(tb->tb_sb, "PAP-12205: balance_leaf: "
+			       "non-direct item can not be broken when inserting");
+#endif
+
+	      /* Move snum[i]-1 items from S[0] to S_new[i] */
+	      leaf_move_items(th, LEAF_FROM_S_TO_SNEW, tb, snum[i] - 1, -1, S_new[i]);
+/*
+	      if (snum[i] > 1 ) {
+		preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, S_new[i]);
+		tbS0 = PATH_PLAST_BUFFER (tb->tb_path);
+	      }
+*/
+	      /* Remember key component and item length */
+	      old_key_comp = ih->ih_key.k_offset;
+	      old_len = ih->ih_item_len;
+
+	      /* Calculate key component and item length to insert into S_new[i] */
+	      ih->ih_key.k_offset += (old_len - sbytes[i]);
+
+	      ih->ih_item_len = sbytes[i];
+
+	      /* Insert part of the item into S_new[i] before 0-th item */
+	      bi.bi_bh = S_new[i];
+	      bi.bi_parent = 0;
+	      bi.bi_position = 0;
+
+	      if ( ih->ih_key.k_offset - old_key_comp > zeros_number ) {
+		r_zeros_number = 0;
+		r_body = body + (ih->ih_key.k_offset - old_key_comp) - zeros_number;
+	      }
+	      else {
+		r_body = body;
+		r_zeros_number = zeros_number - (ih->ih_key.k_offset - old_key_comp);
+		zeros_number -= r_zeros_number;
+	      }
+
+	      leaf_insert_into_buf (th, &bi, 0, ih, r_body, mem_mode, r_zeros_number);
+/*
+	      if (tb->preserve_mode == PRESERVE_INDIRECT_TO_DIRECT){
+		if (!buffer_dirty (bi.bi_bh) && !buffer_journaled(bi.bi_bh))
+		  printk ("clean buffer marked sr 5\n");
+		mark_suspected_recipient (tb->tb_sb, bi.bi_bh);
+	      }
+*/
+	      /* Calculate key component and item length to insert into S[i] */
+	      ih->ih_key.k_offset = old_key_comp;
+	      ih->ih_item_len = old_len - sbytes[i];
+	      tb->insert_size[0] -= sbytes[i];
+	    }
+	  else /* whole new item falls into S_new[i] */
+	    {
+	      /* Shift snum[0] - 1 items to S_new[i] (sbytes[i] of split item) */
+	      leaf_move_items(th, LEAF_FROM_S_TO_SNEW, tb, snum[i] - 1, sbytes[i], S_new[i]);
+/*
+	      if (snum[i] > 1 ) {
+		preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, S_new[i]);
+		tbS0 = PATH_PLAST_BUFFER (tb->tb_path);
+	      }
+*/
+	      /* Insert new item into S_new[i] */
+	      bi.bi_bh = S_new[i];
+	      bi.bi_parent = 0;
+	      bi.bi_position = 0;
+	      leaf_insert_into_buf (th, &bi, item_pos - n + snum[i] - 1, ih, body, mem_mode, zeros_number);
+/*
+	      if (tb->preserve_mode == PRESERVE_INDIRECT_TO_DIRECT){
+		if (!buffer_dirty (bi.bi_bh) && !buffer_journaled(bi.bi_bh))
+		  printk ("clean buffer marked sr 6\n");
+		mark_suspected_recipient (tb->tb_sb, bi.bi_bh);
+	      }
+*/
+	      zeros_number = tb->insert_size[0] = 0;
+	    }
+	}
+
+      else /* new item or it part don't falls into S_new[i] */
+	{
+	  leaf_move_items(th, LEAF_FROM_S_TO_SNEW, tb, snum[i], sbytes[i], S_new[i]);
+/*
+	  preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, S_new[i]);
+	  tbS0 = PATH_PLAST_BUFFER (tb->tb_path);
+*/
+	}
+      break;
+
+    case M_PASTE:   /* append item */
+
+      if ( n - snum[i] <= item_pos )  /* pasted item or part if it falls to S_new[i] */
+	{
+	  if ( item_pos == n - snum[i] && sbytes[i] != -1 )
+	    { /* we must shift part of the appended item */
+	      struct item_head * aux_ih;
+
+#ifdef CONFIG_REISERFS_CHECK
+	      if ( ih )
+		reiserfs_panic (tb->tb_sb, "PAP-12210: balance_leaf: ih must be 0");
+#endif /* CONFIG_REISERFS_CHECK */
+
+	      if ( I_IS_DIRECTORY_ITEM (aux_ih = B_N_PITEM_HEAD(tbS0,item_pos))) {
+		/* we append to directory item */
+
+		int entry_count;
+		
+		entry_count = I_ENTRY_COUNT(aux_ih);
+
+		if ( entry_count - sbytes[i] < pos_in_item  && pos_in_item <= entry_count ) {
+		  /* new directory entry falls into S_new[i] */
+		  
+#ifdef CONFIG_REISERFS_CHECK
+		  if ( ! tb->insert_size[0] )
+		    reiserfs_panic (tb->tb_sb, "PAP-12215: balance_leaif: insert_size is already 0");
+		  if ( sbytes[i] - 1 >= entry_count )
+		    reiserfs_panic (tb->tb_sb, "PAP-12220: balance_leaf: "
+				    "there are no so much entries (%d), only %d",
+				    sbytes[i] - 1, entry_count);
+#endif
+
+		  /* Shift snum[i]-1 items in whole. Shift sbytes[i] directory entries from directory item number snum[i] */
+		  leaf_move_items(th, LEAF_FROM_S_TO_SNEW, tb, snum[i], sbytes[i]-1, S_new[i]);
+		  /* if more than the affected item is shifted, or if more than
+		     one entry (from the affected item) is shifted */
+/*
+		  if (snum[i] > 1 || sbytes[i] > 1) {
+		    preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, S_new[i]);
+		    tbS0 = PATH_PLAST_BUFFER (tb->tb_path);
+		  }
+*/
+		  /* Paste given directory entry to directory item */
+		  bi.bi_bh = S_new[i];
+		  bi.bi_parent = 0;
+		  bi.bi_position = 0;
+		  leaf_paste_in_buffer (th, &bi, 0, pos_in_item - entry_count + sbytes[i] - 1,
+					tb->insert_size[0], body, mem_mode,zeros_number);
+		  /* paste new directory entry */
+		  leaf_paste_entries (
+				      bi.bi_bh, 0, pos_in_item - entry_count + sbytes[i] - 1,
+				      1, (struct reiserfs_de_head *)body, body + DEH_SIZE,
+				      tb->insert_size[0]
+				      );
+		  tb->insert_size[0] = 0;
+		  pos_in_item++;
+		} else { /* new directory entry doesn't fall into S_new[i] */
+		  leaf_move_items(th, LEAF_FROM_S_TO_SNEW, tb, snum[i], sbytes[i], S_new[i]);
+/*
+		  preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, S_new[i]);
+		  tbS0 = PATH_PLAST_BUFFER (tb->tb_path);
+*/
+		}
+	      }
+	      else /* regular object */
+		{
+		  int n_shift, n_rem, r_zeros_number;
+		  const char * r_body;
+
+#ifdef CONFIG_REISERFS_CHECK
+		  if ( pos_in_item != B_N_PITEM_HEAD(tbS0,item_pos)->ih_item_len ||
+		       tb->insert_size[0] <= 0 )
+		    reiserfs_panic (tb->tb_sb, "PAP-12225: balance_leaf: item too short or insert_size <= 0");
+#endif
+
+		  /* Calculate number of bytes which must be shifted from appended item */
+		  n_shift = sbytes[i] - tb->insert_size[0];
+		  if ( n_shift < 0 )
+		    n_shift = 0;
+		  leaf_move_items(th, LEAF_FROM_S_TO_SNEW, tb, snum[i], n_shift, S_new[i]);
+/*
+		  if (n_shift || snum[i] > 1) {
+		    preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, S_new[i]);
+		    tbS0 = PATH_PLAST_BUFFER (tb->tb_path);
+		  }
+*/
+		  /* Calculate number of bytes which must remain in body after append to S_new[i] */
+		  n_rem = tb->insert_size[0] - sbytes[i];
+		  if ( n_rem < 0 )
+		    n_rem = 0;
+		  /* Append part of body into S_new[0] */
+		  bi.bi_bh = S_new[i];
+		  bi.bi_parent = 0;
+		  bi.bi_position = 0;
+
+		  if ( n_rem > zeros_number ) {
+		    r_zeros_number = 0;
+		    r_body = body + n_rem - zeros_number;
+		  }
+		  else {
+		    r_body = body;
+		    r_zeros_number = zeros_number - n_rem;
+		    zeros_number -= r_zeros_number;
+		  }
+
+		  leaf_paste_in_buffer(th, &bi, 0, n_shift, tb->insert_size[0]-n_rem, r_body, mem_mode,r_zeros_number);
+		  if (I_IS_INDIRECT_ITEM(B_N_PITEM_HEAD(S_new[i],0))) {
+		    if (n_rem)
+		      reiserfs_panic (tb->tb_sb, "PAP-12230: balance_leaf: invalid action with indirect item");
+		    B_N_PITEM_HEAD(S_new[i],0)->u.ih_free_space = ((struct unfm_nodeinfo*)body)->unfm_freespace;
+		  }
+
+		  B_N_PKEY(S_new[i],0)->k_offset += n_rem;
+
+		  tb->insert_size[0] = n_rem;
+		  if ( ! n_rem )
+		    pos_in_item++;
+		}
+	    }
+	  else
+	    /* item falls wholly into S_new[i] */
+	    {
+	      int ret_val;
+	      struct item_head * pasted;
+
+#ifdef CONFIG_REISERFS_CHECK
+	      struct item_head * ih = B_N_PITEM_HEAD(tbS0,item_pos);
+
+	      if ( ! I_IS_DIRECTORY_ITEM(ih) && (pos_in_item != ih->ih_item_len ||
+						 tb->insert_size[0] <= 0) )
+		reiserfs_panic (tb->tb_sb, "PAP-12235: balance_leaf: pos_in_item must be equal to ih_item_len");
+#endif /* CONFIG_REISERFS_CHECK */
+
+	      ret_val = leaf_move_items(th, LEAF_FROM_S_TO_SNEW, tb, snum[i], sbytes[i], S_new[i]);
+	      /* we must preserve that which we are pasting onto the end of and shifting */
+/*
+	      preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, S_new[i]);
+	      tbS0 = PATH_PLAST_BUFFER (tb->tb_path);
+*/
+#ifdef CONFIG_REISERFS_CHECK
+	      if ( ret_val )
+		reiserfs_panic (tb->tb_sb, "PAP-12240: balance_leaf: "
+				"unexpected value returned by leaf_move_items (%d)",
+				ret_val);
+#endif /* CONFIG_REISERFS_CHECK */
+
+	      /* paste into item */
+	      bi.bi_bh = S_new[i];
+	      bi.bi_parent = 0;
+	      bi.bi_position = 0;
+	      leaf_paste_in_buffer(th, &bi, item_pos - n + snum[i], pos_in_item, tb->insert_size[0], body, mem_mode, zeros_number);
+
+	      pasted = B_N_PITEM_HEAD(S_new[i], item_pos - n + snum[i]);
+	      if (I_IS_DIRECTORY_ITEM (pasted))
+		{
+		  leaf_paste_entries (
+				      bi.bi_bh, item_pos - n + snum[i], pos_in_item, 1, 
+				      (struct reiserfs_de_head *)body, body + DEH_SIZE, tb->insert_size[0]
+				      );
+		}
+
+	      /* if we paste to indirect item update ih_free_space */
+	      if (I_IS_INDIRECT_ITEM (pasted))
+		pasted->u.ih_free_space = ((struct unfm_nodeinfo*)body)->unfm_freespace;
+	      zeros_number = tb->insert_size[0] = 0;
+	    }
+	}
+
+      else /* pasted item doesn't fall into S_new[i] */
+	{
+	  leaf_move_items(th, LEAF_FROM_S_TO_SNEW, tb, snum[i], sbytes[i], S_new[i]);
+/*
+	  preserve_shifted(tb, &(PATH_PLAST_BUFFER (tb->tb_path)), tbF0, S0_b_item_order, S_new[i]);
+	  tbS0 = PATH_PLAST_BUFFER (tb->tb_path);
+*/
+	}
+      break;
+    default:    /* cases d and t */
+      reiserfs_panic (tb->tb_sb, "PAP-12245: balance_leaf: blknum > 2: unexpectable mode: %s(%d)",
+		      (flag == M_DELETE) ? "DELETE" : ((flag == M_CUT) ? "CUT" : "UNKNOWN"), flag);
+    }
+
+    memcpy (insert_key + i,B_N_PKEY(S_new[i],0),KEY_SIZE);
+    insert_ptr[i] = S_new[i];
+
+#ifdef CONFIG_REISERFS_CHECK
+    if (S_new[i]->b_count != 1) {
+      if (buffer_journaled(S_new[i]) || buffer_journal_dirty(S_new[i])) {
+        ;
+      } else {
+	reiserfs_panic (tb->tb_sb, "PAP-12247: balance_leaf: S_new[%d]->b_count=%u blocknr = %lu\n", i, S_new[i]->b_count,
+	                S_new[i]->b_blocknr);
+      }
+    }
+#endif
+
+    /* update right_delimiting_key fields */
+    copy_key (B_PRIGHT_DELIM_KEY (S_new[i]), B_PRIGHT_DELIM_KEY (tbS0));
+    copy_key (B_PRIGHT_DELIM_KEY (tbS0), B_N_PKEY (S_new[i], 0));
+    /* reiserfs_mark_buffer_dirty (tbS0, 0); journal victim */
+    journal_mark_dirty (th, tb->tb_sb, tbS0);
+
+    /*brelse(S_new[i]);*/
+  }
+
+  /* if the affected item was not wholly shifted then we perform all necessary operations on that part or whole of the
+     affected item which remains in S */
+  if ( 0 <= item_pos && item_pos < tb->s0num )
+    { /* if we must insert or append into buffer S[0] */
+
+      switch (flag)
+	{
+	case M_INSERT:   /* insert item into S[0] */
+	  bi.bi_bh = tbS0;
+	  bi.bi_parent = PATH_H_PPARENT (tb->tb_path, 0);
+	  bi.bi_position = PATH_H_POSITION (tb->tb_path, 1);
+	  leaf_insert_into_buf (th, &bi, item_pos, ih, body, mem_mode, zeros_number);
+/*
+	  if (tb->preserve_mode == PRESERVE_INDIRECT_TO_DIRECT){
+	    if (!buffer_dirty (bi.bi_bh) && !buffer_journaled(bi.bi_bh))
+	      printk ("clean buffer marked sr 7\n");
+	    mark_suspected_recipient (tb->tb_sb, bi.bi_bh);
+	  }
+*/
+
+	  /* If we insert the first key change the delimiting key */
+	  if( item_pos == 0 ) {
+	    if (tb->CFL[0]) /* can be 0 in reiserfsck */
+	      replace_key(th, tb->CFL[0], tb->lkey[0],tbS0,0);
+
+#ifdef CONFIG_REISERFS_CHECK
+	    if ( ! tb->CFL[0] || ! tb->L[0] || (B_NR_ITEMS (tbS0) > 1 && 
+						COMP_KEYS(B_PRIGHT_DELIM_KEY(tb->L[0]), B_N_PKEY(tbS0, 1))) )
+	      reiserfs_panic(tb->tb_sb, "PAP-12250: balance_leaf: invalid right delimiting key");
+	    if (!buffer_dirty (tb->L[0]) && !buffer_journaled(tb->L[0]))
+	      reiserfs_panic (tb->tb_sb, "PAP-12255: balance_leaf: tb->L[0] must be dirty");
+#endif
+	    if (tb->L[0]) /* can be 0 in reiserfsck */
+	      copy_key (B_PRIGHT_DELIM_KEY (tb->L[0]), &(ih->ih_key));   
+	  }
+	  break;
+
+	case M_PASTE: {  /* append item in S[0] */
+	  struct item_head * pasted;
+
+	  pasted = B_N_PITEM_HEAD (tbS0, item_pos);
+	  /* when directory, may be new entry already pasted */
+	  if (I_IS_DIRECTORY_ITEM (pasted)) {
+	    if ( pos_in_item >= 0 && pos_in_item <= I_ENTRY_COUNT (pasted) ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+	      if ( ! tb->insert_size[0] )
+		reiserfs_panic (tb->tb_sb, "PAP-12260: balance_leaf: insert_size is 0 already");
+#endif /* CONFIG_REISERFS_CHECK */
+
+	      /* prepare space */
+	      bi.bi_bh = tbS0;
+	      bi.bi_parent = PATH_H_PPARENT (tb->tb_path, 0);
+	      bi.bi_position = PATH_H_POSITION (tb->tb_path, 1);
+	      leaf_paste_in_buffer(th, &bi, item_pos, pos_in_item, tb->insert_size[0], body, mem_mode, zeros_number);
+
+
+#ifdef CONFIG_REISERFS_CHECK
+	      if ( ! item_pos && ! pos_in_item  && (! tb->L[0] || COMP_KEYS(B_PRIGHT_DELIM_KEY(tb->L[0]), 
+									    B_N_PKEY(tbS0, 0))) )
+		reiserfs_panic(tb->tb_sb, "PAP-12265: balance_leaf: invalid right delimiting key");
+#endif
+
+	      /* paste entry */
+	      leaf_paste_entries (
+				  bi.bi_bh, item_pos, pos_in_item, 1, (struct reiserfs_de_head *)body,
+				  body + DEH_SIZE, tb->insert_size[0]
+				  );
+	      if ( ! item_pos && ! pos_in_item ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+		if (!tb->CFL[0] || !tb->L[0])
+		  reiserfs_panic (tb->tb_sb, "PAP-12270: balance_leaf: CFL[0]/L[0] must be specified");
+#endif /* CONFIG_REISERFS_CHECK */
+
+		if (tb->CFL[0]) {
+		  replace_key(th, tb->CFL[0], tb->lkey[0],tbS0,0);
+
+		  /* update right delimiting key */
+		  copy_key (B_PRIGHT_DELIM_KEY (tb->L[0]), B_N_PKEY(tbS0, 0));   
+		  /* probably not needed as something has been shifted to tb->L[0] already */
+		  /* reiserfs_mark_buffer_dirty (tb->L[0], 0); journal victim */
+		  journal_mark_dirty (th, tb->tb_sb, tb->L[0]);
+		}
+	      }
+	      tb->insert_size[0] = 0;
+	    }
+	  } else { /* regular object */
+	    if ( pos_in_item == pasted->ih_item_len ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+	      if ( tb->insert_size[0] <= 0 )
+		reiserfs_panic (tb->tb_sb,
+				"PAP-12275: balance_leaf: insert size must not be %d", tb->insert_size[0]);
+#endif /* CONFIG_REISERFS_CHECK */
+
+	      bi.bi_bh = tbS0;
+	      bi.bi_parent = PATH_H_PPARENT (tb->tb_path, 0);
+	      bi.bi_position = PATH_H_POSITION (tb->tb_path, 1);
+	      leaf_paste_in_buffer (th, &bi, item_pos, pos_in_item, tb->insert_size[0], body, mem_mode, zeros_number);
+
+	      if (I_IS_INDIRECT_ITEM (pasted)) {
+
+#ifdef CONFIG_REISERFS_CHECK
+		if ( tb->insert_size[0] != UNFM_P_SIZE )
+		  reiserfs_panic (tb->tb_sb,
+				  "PAP-12280: balance_leaf: insert_size for indirect item must be %d, not %d",
+				  UNFM_P_SIZE, tb->insert_size[0]);
+#endif /* CONFIG_REISERFS_CHECK */
+
+		pasted->u.ih_free_space = ((struct unfm_nodeinfo*)body)->unfm_freespace;
+	      }
+	      tb->insert_size[0] = 0;
+	    }
+
+#ifdef CONFIG_REISERFS_CHECK
+	    else {
+	      if ( tb->insert_size[0] ) {
+		print_tb (init_mode, init_item_pos, init_pos_in_item, &init_tb, "12285");
+		reiserfs_panic (tb->tb_sb, "PAP-12285: balance_leaf: insert_size must be 0 (%d)", tb->insert_size[0]);
+	      }
+	    }
+#endif /* CONFIG_REISERFS_CHECK */
+	    
+	  }
+	} /* case M_PASTE: */
+	}
+    }
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( flag == M_PASTE && tb->insert_size[0] ) {
+    print_tb (M_PASTE, item_pos, pos_in_item, tb, "balance");
+    reiserfs_panic (tb->tb_sb, "PAP-12290: balance_leaf: insert_size is still not 0 (%d)", tb->insert_size[0]);
+  }
+#endif /* CONFIG_REISERFS_CHECK */
+
+  return 0;
+} /* Leaf level of the tree is balanced (end of balance_leaf) */
+
+
+
+/* Make empty node */
+void make_empty_node (struct buffer_info * bi)
+{
+  struct block_head * blkh;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (bi->bi_bh == NULL)
+    reiserfs_panic (0, "PAP-12295: make_empty_node: pointer to the buffer is NULL");
+#endif
+
+  (blkh = B_BLK_HEAD(bi->bi_bh))->blk_nr_item = 0;
+  blkh->blk_free_space = MAX_CHILD_SIZE(bi->bi_bh); 
+
+  if (bi->bi_parent)
+    B_N_CHILD (bi->bi_parent, bi->bi_position)->dc_size = 0; 
+}
+
+
+/* Get first empty buffer */
+struct buffer_head * get_FEB (struct tree_balance * tb)
+{
+  int i;
+  struct buffer_head * first_b;
+  struct buffer_info bi;
+
+  for (i = 0; i < MAX_FEB_SIZE; i ++)
+    if (tb->FEB[i] != 0)
+      break;
+
+  if (i == MAX_FEB_SIZE)
+    reiserfs_panic(tb->tb_sb, "vs-12300: get_FEB: FEB list is empty");
+
+  bi.bi_bh = first_b = tb->FEB[i];
+  bi.bi_parent = 0;
+  bi.bi_position = 0;
+  make_empty_node (&bi);
+  set_bit(BH_Uptodate, &first_b->b_state);
+
+/*  mark_buffer_unwritten (first_b);*/
+
+  tb->FEB[i] = 0;
+  tb->used[i] = first_b;
+#ifdef REISERFS_FSCK
+  mark_block_formatted (first_b->b_blocknr);
+#endif
+
+  return(first_b);
+}
+
+
+/* Replace n_dest'th key in buffer dest by n_src'th key of buffer src.*/
+void	replace_key (
+		     struct reiserfs_transaction_handle *th, 
+		     struct buffer_head * dest, 
+		     int n_dest,
+		     struct buffer_head * src,
+		     int n_src
+		     )
+{
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (dest == NULL || src == NULL)
+    reiserfs_panic (0, "vs-12305: replace_key: sourse or destination buffer is 0 (src=%p, dest=%p)", src, dest);
+
+  if ( ! B_IS_KEYS_LEVEL (dest) )
+    reiserfs_panic (0, "vs-12310: replace_key: invalid level (%d) for destination buffer. Must be > %d",
+		    B_BLK_HEAD(dest)->blk_level, DISK_LEAF_NODE_LEVEL);
+
+  if (n_dest < 0 || n_src < 0)
+    reiserfs_panic (0, "vs-12315: replace_key: src(%d) or dest(%d) key number less than 0", n_src, n_dest);
+
+  if (n_dest >= B_NR_ITEMS(dest) || n_src >= B_NR_ITEMS(src))
+    reiserfs_panic (0, "vs-12320: replace_key: src(%d(%d)) or dest(%d(%d)) key number is too big",
+		    n_src, B_NR_ITEMS(src), n_dest, B_NR_ITEMS(dest));
+#endif	/* CONFIG_REISERFS_CHECK */
+   
+  if (B_IS_ITEMS_LEVEL (src))
+    /* source buffer contains leaf node */
+    memcpy (B_N_PDELIM_KEY(dest,n_dest), B_N_PITEM_HEAD(src,n_src), KEY_SIZE);
+  else
+    memcpy (B_N_PDELIM_KEY(dest,n_dest), B_N_PDELIM_KEY(src,n_src), KEY_SIZE);
+
+  /* not preserved, preserves are in balance_leaf() and  balance_leaf_when_delete() */
+  /* reiserfs_mark_buffer_dirty(dest, 0); journal victim */
+  journal_mark_dirty(th, th->t_super, dest);
+}
+
+
+void reiserfs_invalidate_buffer (struct reiserfs_transaction_handle *th,
+                                 struct tree_balance * tb, struct buffer_head * bh, int do_free_block)
+{
+
+  int i ;
+  B_BLK_HEAD (bh)->blk_level = FREE_LEVEL;
+  clear_bit(BH_Dirty, &bh->b_state);
+  //reiserfs_mark_buffer_clean (bh);
+  /*refile_buffer(bh);*/
+
+
+#ifdef CONFIG_REISERFS_CHECK
+  B_NR_ITEMS (bh) = 0;
+#endif
+
+  if (do_free_block) {
+#ifdef REISERFS_FSCK
+    struct buffer_head * to_be_forgotten;
+
+    to_be_forgotten = find_buffer (bh->b_dev, bh->b_blocknr, bh->b_size);
+    if (to_be_forgotten) {
+      to_be_forgotten->b_count ++;
+      bforget (to_be_forgotten);
+    }
+    unmark_block_formatted (bh->b_blocknr);
+#endif
+    for (i = 0 ; i < (sizeof(tb->thrown)/sizeof(tb->thrown[0])) ; i++) {
+      if (tb->thrown[i] == NULL) {
+        bh->b_count++ ; /* brelsed in free_thrown */
+	tb->thrown[i] = bh ;
+	return ;
+      }
+    }
+    reiserfs_warning("clm-5000: store_thrown, too many thrown buffers\n") ;
+  }
+}
+
+static void free_thrown(struct reiserfs_transaction_handle *th, 
+                   struct tree_balance *tb) {
+  
+  int i ;
+  unsigned long blocknr ;
+  for (i = 0 ; i < (sizeof(tb->thrown)/sizeof(tb->thrown[0])) ; i++) {
+    if (tb->thrown[i]) {
+      blocknr = tb->thrown[i]->b_blocknr ;
+      brelse(tb->thrown[i]) ; /* inc'd by reiserfs_invalidate_buffer */
+      reiserfs_free_block(th, tb->tb_sb, blocknr) ;
+    }
+  }
+}
+
+int get_left_neighbor_position (
+				struct tree_balance * tb, 
+				int h
+				)
+{
+  int Sh_position = PATH_H_POSITION (tb->tb_path, h + 1);
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (PATH_H_PPARENT (tb->tb_path, h) == 0 || tb->FL[h] == 0)
+    reiserfs_panic (tb->tb_sb, "vs-12325: get_left_neighbor_position: FL[%d](%p) or F[%d](%p) does not exist", 
+		    h, tb->FL[h], h, PATH_H_PPARENT (tb->tb_path, h));
+#endif
+
+  if (Sh_position == 0)
+    return B_NR_ITEMS (tb->FL[h]);
+  else
+    return Sh_position - 1;
+}
+
+
+int get_right_neighbor_position (struct tree_balance * tb, int h)
+{
+  int Sh_position = PATH_H_POSITION (tb->tb_path, h + 1);
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (PATH_H_PPARENT (tb->tb_path, h) == 0 || tb->FR[h] == 0)
+    reiserfs_panic (tb->tb_sb, "vs-12330: get_right_neighbor_position: F[%d](%p) or FR[%d](%p) does not exist", 
+		    h, PATH_H_PPARENT (tb->tb_path, h), h, tb->FR[h]);
+#endif
+
+  if (Sh_position == B_NR_ITEMS (PATH_H_PPARENT (tb->tb_path, h)))
+    return 0;
+  else
+    return Sh_position + 1;
+}
+
+
+#ifdef CONFIG_REISERFS_CHECK
+
+int is_reusable (struct super_block * s, unsigned long block, int bit_value);
+static void check_internal_node (struct super_block * s, struct buffer_head * bh, char * mes)
+{
+  struct disk_child * dc;
+  int i;
+
+  if (!bh)
+    reiserfs_panic (s, "PAP-12336: check_internal_node: bh == 0");
+
+  if (!bh || !B_IS_IN_TREE (bh))
+    return;
+ 
+  if (!buffer_dirty (bh) && !buffer_journaled(bh) ) {
+    reiserfs_panic (s, "PAP-12337: check_internal_node: buffer (%b) must be dirty", bh);
+  }
+
+  dc = B_N_CHILD (bh, 0);
+
+  for (i = 0; i <= B_NR_ITEMS (bh); i ++, dc ++) {
+    if (!is_reusable (s, dc->dc_block_number, 1) ) {
+      print_tb (init_mode, init_item_pos, init_pos_in_item, &init_tb, mes);
+      reiserfs_panic (s, "PAP-12338: check_internal_node: invalid child pointer %y in %b", dc, bh);
+    }
+  }
+}
+
+
+static int locked_or_not_in_tree (struct buffer_head * bh, char * which)
+{
+  if ( buffer_locked (bh) || !B_IS_IN_TREE (bh) ) {
+    reiserfs_warning ("vs-12339: locked_or_not_in_tree: %s (%b)\n", which, bh);
+    return 1;
+  }
+  return 0;
+}
+
+
+static int check_before_balancing (struct tree_balance * tb)
+{
+  int retval = 0;	
+  int i;
+
+  for (i = 0; i < MAX_HEIGHT; i ++)
+    buffers[i] = 0;
+
+  if ( cur_tb ) {
+    reiserfs_panic (tb->tb_sb, "vs-12335: check_before_balancing: "
+		    "suspect that schedule occurred based on cur_tb not being null at this point in code. "
+		    "do_balance cannot properly handle schedule occuring while it runs.");
+  }
+  
+  /* double check that buffers that we will modify are unlocked. (fix_nodes should already have
+     prepped all of these for us). */
+  if ( tb->lnum[0] ) {
+    retval |= locked_or_not_in_tree (tb->L[0], "L[0]");
+    retval |= locked_or_not_in_tree (tb->FL[0], "FL[0]");
+    retval |= locked_or_not_in_tree (tb->CFL[0], "CFL[0]");
+    check_leaf (tb->L[0]);
+  }
+  if ( tb->rnum[0] ) {
+    retval |= locked_or_not_in_tree (tb->R[0], "R[0]");
+    retval |= locked_or_not_in_tree (tb->FR[0], "FR[0]");
+    retval |= locked_or_not_in_tree (tb->CFR[0], "CFR[0]");
+    check_leaf (tb->R[0]);
+  }
+  retval |= locked_or_not_in_tree (PATH_PLAST_BUFFER (tb->tb_path), "S[0]");
+  check_leaf (PATH_PLAST_BUFFER (tb->tb_path));
+  return retval;
+}
+
+#if 0
+static int check_before_balancing (struct tree_balance * tb)
+{
+  int needed_buffer_locked = 0;	
+  int i;
+
+  for (i = 0; i < MAX_HEIGHT; i ++)
+    buffers[i] = 0;
+
+  if ( cur_tb ) {
+    reiserfs_panic (tb->tb_sb, "vs-12335: check_before_balancing: "
+		    "suspect that schedule occurred based on cur_tb not being null at this point in code. "
+		    "do_balance cannot properly handle schedule occuring while it runs.");
+  }
+  
+  /* double check that buffers that we will modify are unlocked. (fix_nodes should already have
+     prepped all of these for us). */
+  if ( tb->lnum[0] ) {
+    if ( buffer_locked(tb->L[0]) || !B_IS_IN_TREE (tb->L[0])) {
+      reiserfs_warning ("vs-12336: check_before_balancing: L[0] locked %b\n", tb->L[0]);
+      needed_buffer_locked = 1;
+    }
+    if ( buffer_locked(tb->FL[0]) || !B_IS_IN_TREE (tb->FL[0]))	{
+      needed_buffer_locked = 1;
+      reiserfs_warning ("vs-12337: check_before_balancing: FL[0] locked %b\n", tb->FL[0]);
+    }
+    if ( buffer_locked(tb->CFL[0]) || !B_IS_IN_TREE (tb->CFL[0])) {
+      needed_buffer_locked = 1;
+      print_buffer_head(tb->CFL[0],"CFL[0]");
+    }
+  }
+  if ( tb->rnum[0] ) {
+    if ( buffer_locked(tb->R[0]) || !B_IS_IN_TREE (tb->R[0])) {
+      needed_buffer_locked = 1;
+      print_buffer_head(tb->R[0],"R[0]");
+    }
+    if ( buffer_locked(tb->FR[0]) || !B_IS_IN_TREE (tb->FR[0]))	{
+      needed_buffer_locked = 1;
+      print_buffer_head(tb->FR[0],"FR[0]");
+    }
+    if ( buffer_locked(tb->CFR[0]) || !B_IS_IN_TREE (tb->CFR[0])) {
+      needed_buffer_locked = 1;
+      print_buffer_head(tb->CFR[0],"CFR[0]");
+    }
+  }
+  if ( buffer_locked(PATH_PLAST_BUFFER (tb->tb_path)) || !B_IS_IN_TREE (PATH_PLAST_BUFFER (tb->tb_path)))
+    {
+      needed_buffer_locked = 1;
+      print_buffer_head(PATH_PLAST_BUFFER (tb->tb_path), "S[0]");
+    }
+  if ( needed_buffer_locked )
+    printk ("There are %d locked buffers\n", needed_buffer_locked);
+
+  return needed_buffer_locked;
+}
+#endif
+
+
+void check_after_balance_leaf (struct tree_balance * tb)
+{
+  if (tb->lnum[0]) {
+    if (B_BLK_HEAD (tb->L[0])->blk_free_space != 
+	MAX_CHILD_SIZE (tb->L[0]) - B_N_CHILD (tb->FL[0], get_left_neighbor_position (tb, 0))->dc_size) {
+      print_tb (init_mode, init_item_pos, init_pos_in_item, &init_tb, "12221");
+      reiserfs_panic (tb->tb_sb, "PAP-12355: check_after_balance_leaf: shift to left was incorrect");
+    }
+  }
+  if (tb->rnum[0]) {
+    if (B_BLK_HEAD (tb->R[0])->blk_free_space != 
+	MAX_CHILD_SIZE (tb->R[0]) - B_N_CHILD (tb->FR[0], get_right_neighbor_position (tb, 0))->dc_size) {
+      print_tb (init_mode, init_item_pos, init_pos_in_item, &init_tb, "12222");
+      reiserfs_panic (tb->tb_sb, "PAP-12360: check_after_balance_leaf: shift to right was incorrect");
+    }
+  }
+  if (PATH_H_PBUFFER(tb->tb_path,1) && 
+      B_BLK_HEAD (PATH_H_PBUFFER(tb->tb_path,0))->blk_free_space != 
+      MAX_CHILD_SIZE (PATH_H_PBUFFER(tb->tb_path,0)) -
+      B_N_CHILD (PATH_H_PBUFFER(tb->tb_path,1), 
+		 PATH_H_POSITION (tb->tb_path, 1))->dc_size) {
+    print_tb (init_mode, init_item_pos, init_pos_in_item, &init_tb, "12223");
+    reiserfs_panic (tb->tb_sb, "PAP-12365: check_after_balance_leaf: S is incorrect");
+  }
+}
+
+void check_after_balancing (struct tree_balance * tb)
+{
+  int h;
+
+  /* check all internal nodes */
+  for (h = 1; tb->insert_size[h]; h ++) {
+    check_internal_node (tb->tb_sb, PATH_H_PBUFFER (tb->tb_path, h), "BAD BUFFER ON PATH");
+    if (tb->lnum[h])
+      check_internal_node (tb->tb_sb, tb->L[h], "BAD L");
+    if (tb->rnum[h])
+      check_internal_node (tb->tb_sb, tb->R[h], "BAD R");
+    /* check new buffers if any */
+    if (buffers[h])
+      check_internal_node (tb->tb_sb, buffers[h], "BAD S_NEW");
+  }
+
+}
+
+#endif
+
+
+void check_leaf_level (struct tree_balance * tb)
+{
+  check_leaf (tb->L[0]);
+  check_leaf (tb->R[0]);
+  check_leaf (PATH_PLAST_BUFFER (tb->tb_path));
+}
+
+
+void check_internal_level (struct tree_balance * tb)
+{
+
+}
+
+
+/* Now we have all of the buffers that must be used in balancing of the tree.  We rely on the
+   assumption that schedule() will not occur while do_balance works. ( Only interrupt handlers are
+   acceptable.)  We balance the tree according to the analysis made before this, using buffers already
+   obtained.  For SMP support it will someday be necessary to add ordered locking of tb. */
+
+/* Some interesting rules of balancing:
+
+   we delete a maximum of two nodes per level per balancing: we never delete R, when we delete two
+   of three nodes L, S, R then we move them into R.
+
+   we only delete L if we are deleting two nodes, if we delete only one node we delete S
+
+   if we shift leaves then we shift as much as we can: this is a deliberate policy of extremism in
+   node packing which results in higher average utilization after repeated random balance
+   operations at the cost of more memory copies and more balancing as a result of small insertions
+   to full nodes.
+
+   if we shift internal nodes we try to evenly balance the node utilization, with consequent less
+   balancing at the cost of lower utilization.
+
+   one could argue that the policy for directories in leaves should be that of internal nodes, but
+   we will wait until another day to evaluate this....  It would be nice to someday measure and
+   prove these assumptions as to what is optimal....
+
+*/
+
+void	do_balance (
+		    struct reiserfs_transaction_handle *th, 
+		    struct tree_balance * tb,		/* tree_balance structure 		*/
+		    int pos_in_item,                /* position in item, in bytes for direct
+						       and indirect items, in entries for
+						       directories (for which it is an index
+						       into the array of directory entry
+						       headers.) */
+		    struct item_head * ih,			/* item header of inserted item 	*/
+		    const char * body,					/* body  of inserted item or bytes to paste */
+		    int flag,		   /* i - insert, d - delete
+					      c - cut, p - paste
+						      
+					      Cut means delete part of an item (includes removing
+					      an entry from a directory).
+						      
+					      Delete means delete whole item.
+						      
+					      Insert means add a new item into the tree.
+						      						      
+					      Paste means to append to the end of an existing file
+					      or to insert a directory entry.
+					      */
+		    int mem_mode,		/* memory mode; must be REISERFS_USER_MEM or REISERFS_KERNEL_MEM,
+						   depending on whether the body parameter is located in a
+						   user buffer in user memory or not. */
+		    int zeros_num
+		    )
+{
+  int child_pos,					/* position of a child node in its parent */
+    h;								/* level of the tree being processed */
+  struct item_head insert_key[2]; /* in our processing of one level we sometimes determine what
+				     must be inserted into the next higher level.  This insertion
+				     consists of a key or two keys and their corresponding
+				     pointers */
+  struct buffer_head *insert_ptr[2]; /* inserted node-ptrs for the next
+					level */
+
+
+#ifdef CONFIG_REISERFS_CHECK
+  memcpy(&init_tb, tb, sizeof(struct tree_balance));
+  init_item_pos = PATH_LAST_POSITION (tb->tb_path);
+  init_pos_in_item = pos_in_item;
+  init_mode = flag;
+
+  /* do not delete, just comment it out */
+  /*print_tb(flag, PATH_LAST_POSITION(tb->tb_path), pos_in_item, tb, "check");*/
+
+  if (check_before_balancing (tb))
+    reiserfs_panic (tb->tb_sb, "PAP-12340: do_balance: locked buffers in TB");
+
+#ifdef CONFIG_REISERFS_CHECK_ONE_PROCESS
+  if ( (PATH_PLAST_BUFFER(tb->tb_path)->b_count > 1 || (tb->L[0] && tb->L[0]->b_count > 1) ||
+       (tb->R[0] && tb->R[0]->b_count > 1)) ) {
+    print_tb(0, 0, 0, tb, "first three parameters are invalid");
+    reiserfs_panic (tb->tb_sb, "PAP-12345: do_balance: counter too big");
+  }
+#endif
+
+  cur_tb = tb;
+#endif /* CONFIG_REISERFS_CHECK */
+
+  /* if we have no real work to do  */
+  if ( ! tb->insert_size[0] ) {
+#ifdef CONFIG_REISERFS_CHECK
+    cur_tb = NULL;
+    if (flag != M_CUT)
+      reiserfs_panic (tb->tb_sb, "PAP-12350: do_balance: insert_size == 0, mode == %c", flag);
+#endif
+    unfix_nodes(th, tb);
+    return;
+  }
+
+#ifdef REISERFS_FSCK
+  if (flag == M_INTERNAL) {
+    insert_ptr[0] = (struct buffer_head *)body;
+    /* we must prepare insert_key */
+
+    if (PATH_H_B_ITEM_ORDER (tb->tb_path, 0)/*LAST_POSITION (tb->tb_path)*//*item_pos*/ == -1) {
+      /* get delimiting key from buffer in tree */
+      copy_key (&insert_key[0].ih_key, B_N_PKEY (PATH_PLAST_BUFFER (tb->tb_path), 0));
+      /*insert_ptr[0]->b_item_order = 0;*/
+    } else {
+      /* get delimiting key from new buffer */
+      copy_key (&insert_key[0].ih_key, B_N_PKEY((struct buffer_head *)body,0));
+      /*insert_ptr[0]->b_item_order = item_pos;*/
+    }
+      
+    /* and insert_ptr instead of balance_leaf */
+    child_pos = PATH_H_B_ITEM_ORDER (tb->tb_path, 0)/*item_pos*/;
+  } else
+#endif
+
+  /* balance leaf returns 0 except if combining L R and S into one node.  see balance_internal()
+     for explanation of this line of code.*/
+  child_pos = PATH_H_B_ITEM_ORDER (tb->tb_path, 0) +
+    balance_leaf (th, tb, pos_in_item, ih, body, flag, mem_mode, zeros_num, insert_key, insert_ptr);
+
+#ifdef CONFIG_REISERFS_CHECK
+  check_after_balance_leaf (tb);
+#endif
+
+  /* Balance internal level of the tree. */
+  for ( h = 1; h < MAX_HEIGHT && tb->insert_size[h]; h++ )
+    child_pos = balance_internal (th, tb, h, child_pos, insert_key, insert_ptr);
+
+/*&&&&&&&&&&&&&&&&&*/
+  check_leaf_level (tb);
+  check_internal_level (tb);
+/*&&&&&&&&&&&&&&&&&*/
+
+#ifdef CONFIG_REISERFS_CHECK
+  cur_tb = NULL;
+  check_after_balancing (tb);
+#endif
+
+  /* Release all (except for S[0]) non NULL buffers fixed by fix_nodes() */
+  free_thrown(th, tb) ;
+  unfix_nodes(th, tb);
+
+#ifdef CONFIG_REISERFS_CHECK
+  tb->tb_sb->u.reiserfs_sb.s_do_balance ++;
+#endif
+
+}
+
+
+
+
+
+
+
+
+
+
+
+
diff -urN linux/fs/reiserfs/file.c /tmp/linux/fs/reiserfs/file.c
--- linux/fs/reiserfs/file.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/file.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,2238 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+/* Contains the routines related to read and write. */
+
+/*
+ * direct_to_indirect
+ * get_new_buffer
+ * reiserfs_file_write
+ * reiserfs_file_read
+*/
+
+#ifdef __KERNEL__
+
+#include <asm/uaccess.h>
+#include <asm/system.h>
+
+#include <linux/sched.h>
+
+#include <linux/reiserfs_fs.h>
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/fcntl.h>
+#include <linux/locks.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+
+#else
+
+#include "nokernel.h"
+
+#endif
+
+static ssize_t reiserfs_file_read  (struct file *, char *, size_t, loff_t *);
+static ssize_t reiserfs_file_write (struct file *, const char *, size_t, loff_t *);
+
+int reiserfs_bmap (struct inode * inode, int block);
+int reiserfs_readpage (struct file * file, struct page * page);
+
+
+static struct file_operations reiserfs_file_operations = {
+                NULL,        		/* lseek */
+                reiserfs_file_read,     /* read */
+                reiserfs_file_write,    /* write */
+                NULL,                   /* readdir */
+                NULL,                   /* poll */
+                reiserfs_ioctl,         /* ioctl */
+                generic_file_mmap,     	/* mmap */
+                NULL,                   /* open */
+		NULL,			/* flush */
+                reiserfs_file_release,  /* release */
+                reiserfs_sync_file,   	/* fsync */
+                NULL,		        /* fasync */
+                NULL, 	                /* check_media_change */
+                NULL,		        /* revalidate*/
+                NULL	       	        /* lock */
+};
+
+
+struct  inode_operations reiserfs_file_inode_operations = {
+          &reiserfs_file_operations,  /* default file operations */
+          NULL,                       /* create */
+          NULL,                       /* lookup */
+          NULL,                       /* link */
+          NULL,                       /* unlink */
+          NULL,                       /* symlink */
+          NULL,                       /* mkdir */
+          NULL,                       /* rmdir */
+          NULL,                       /* mknod */
+          NULL,                       /* rename */
+          NULL,                       /* readlink */
+	  NULL,			      /* follow_link */
+          reiserfs_readpage,          /* readpage */
+          NULL,		              /* writepage */
+          reiserfs_bmap,              /* bmap */
+          reiserfs_truncate_file,     /* truncate */
+          NULL,                       /* permission */
+          NULL,		              /* smap */ 
+          NULL,		              /* updatepage */
+          NULL		              /* revalidate */
+};
+
+/*
+** this will put an updated inode in the current transaction, and
+** then restart it.  Make sure you release any paths you are holding
+** before calling this.
+**
+** th is your handle
+** p_s_inode is a pointer to the inode to update
+** n_pos_in_file is file_write's concept of its current spot in the file.
+** 
+** after running this, the inode's size will be the greater of
+** p_s_inode->i_size and (n_pos_in_file -1)
+*/
+static void 
+update_inode_and_restart_transaction(struct reiserfs_transaction_handle *th,
+                               struct inode *p_s_inode,
+			       unsigned long n_pos_in_file) {
+  int orig_count = th->t_blocks_allocated ;
+  struct super_block *s = th->t_super ;
+  if ((n_pos_in_file - 1) > p_s_inode->i_size) { 
+    p_s_inode->i_size = (n_pos_in_file - 1) ;
+    p_s_inode->i_ctime = CURRENT_TIME ;
+  }
+  p_s_inode->i_mtime = CURRENT_TIME ;
+  if_in_ram_update_sd(th, p_s_inode) ; 
+  journal_end(th, s, orig_count) ;
+  journal_begin(th, s, orig_count) ;
+  reiserfs_update_inode_transaction(p_s_inode) ;
+}
+
+
+
+/* Converts direct items to an unformatted node. Returns 1 if conversion has been done, 0 if it has
+   been done somewhere else, -1 if no disk space for conversion */
+/*static*/ int  direct_to_indirect(
+	      struct reiserfs_transaction_handle *th,
+              struct super_block  * p_s_sb,               /* Pointer to the super block.        */
+              struct inode        * p_s_inode,            /* Pointer to the file inode.         */
+              struct path         * p_s_search_path,      /* Path to the item to be converted.  */
+              int                   n_item_zeros_to_add,  /* Number of zeros to be inserted
+                                                              (file hole related).              */
+              const char          * p_c_buf,              /* Buffer with user data being
+                                                              written.                          */
+              int                   n_item_bytes_to_write,/* The number of bytes to be written
+                                                              from buf to the new unformatted
+                                                              node.                             */
+              struct buffer_head  * p_s_un_bh             /* The new unformatted node pointer.  */
+            ) {
+  struct key		s_key_to_search;  /* Key to search for the last byte of the converted
+                                                item.                                           */
+  struct item_head   * 	p_s_ih,           /* Pointer to the item header of the converted item.*/
+			s_item_to_insert;
+  unsigned long		n_unp;            /* Unformatted node block number.                   */
+  int			n_blk_size,       /* Block size.                                      */
+			n_len_to_move,    /* Length of tail to be converted.                  */
+			n_pos_in_item,    /* Found position in the item                       */
+			n_repeat,	  /* used for tracking whether schedule occurred */
+    			n_retval;	  /* returned value for reiserfs_insert_item and clones */
+  char		     *	p_c_to;	    /* End of the unformatted node.                     */
+  struct unfm_nodeinfo	s_node_to_write;  /* Handle on an unformatted node that will be
+                                                inserted in the tree.                           */
+
+  p_s_sb->u.reiserfs_sb.s_direct2indirect ++;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( ! p_s_un_bh )
+    reiserfs_panic (p_s_sb, "PAP-14010: direct_to_indirect: pointer to the unfm buffer is NULL");
+#endif
+
+  n_blk_size = p_s_un_bh->b_size;
+  n_unp = p_s_un_bh->b_blocknr;
+  /* Set the key to search for the last byte of the converted item and
+      key to search for append or insert pointer to the new unformatted node. */
+  copy_key(&s_key_to_search,(struct key *)PATH_PITEM_HEAD(p_s_search_path));
+  copy_item_head(&s_item_to_insert,PATH_PITEM_HEAD(p_s_search_path));
+
+  s_key_to_search.k_offset += s_item_to_insert.ih_item_len - 1;
+  s_item_to_insert.ih_key.k_offset -= (s_item_to_insert.ih_key.k_offset - 1) % n_blk_size;
+  s_item_to_insert.ih_key.k_uniqueness = TYPE_INDIRECT;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( s_item_to_insert.ih_key.k_offset != p_s_inode->u.reiserfs_i.i_first_direct_byte )
+    reiserfs_panic(p_s_sb, "PAP-14020: direct_to_indirect: illegal first direct byte position");
+#endif
+
+  /* Calculate length of tail to be converted (tail may be stored in two
+     direct items in different nodes). */
+  n_len_to_move = s_key_to_search.k_offset % n_blk_size;
+  /* Calculate address to copy from user buffer. */
+  p_c_to = p_s_un_bh->b_data + n_len_to_move;
+   /* let only one process append at a time */
+  lock_inode_to_convert(p_s_inode);
+  if (p_s_inode->u.reiserfs_i.i_first_direct_byte == NO_BYTES_IN_DIRECT_ITEM) {
+    /* somewhere else mmap or bmap did this conversion */
+    /*brelse(p_s_un_bh);*/
+    unlock_inode_after_convert (p_s_inode);
+    return 0;
+  }
+
+  if ( search_for_position_by_key (p_s_sb, (struct key *)&s_item_to_insert, p_s_search_path, &n_pos_in_item, &n_repeat) == POSITION_FOUND )
+    reiserfs_panic (p_s_sb, "PAP-14030: direct_to_indirect: pasted or inserted byte exists in the tree");
+  p_s_ih = PATH_PITEM_HEAD(p_s_search_path);
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( n_blk_size - n_len_to_move - n_item_zeros_to_add - n_item_bytes_to_write < 0 )
+    reiserfs_panic (p_s_sb, "PAP-14040: direct_to_indirect: illegal ih_free_space");
+#endif
+
+  if (p_c_buf)
+    s_node_to_write.unfm_freespace = n_blk_size - n_len_to_move - n_item_zeros_to_add - n_item_bytes_to_write;
+  else
+    s_node_to_write.unfm_freespace = n_blk_size - n_len_to_move;
+
+  /* Insert indirect item. */
+  if ( I_IS_STAT_DATA_ITEM(p_s_ih) )  {
+    s_item_to_insert.u.ih_free_space = s_node_to_write.unfm_freespace;
+    s_item_to_insert.ih_item_len      = UNFM_P_SIZE;
+    PATH_LAST_POSITION(p_s_search_path)++;
+    n_retval = reiserfs_insert_item (th, p_s_sb, p_s_search_path, &s_item_to_insert, (char *)&n_unp, REISERFS_KERNEL_MEM, 0/*zero number*/, NOTHING_SPECIAL);
+  }
+  /* Paste into last object indirect item. */
+  else  {
+    s_node_to_write.unfm_nodenum    = n_unp;
+    /*s_node_to_write.unfm_freespace = n_repeat;*/
+    n_retval = reiserfs_paste_into_item(th, p_s_sb, p_s_search_path, &n_pos_in_item, &(s_item_to_insert.ih_key),
+					(char *)&s_node_to_write, UNFM_P_SIZE, REISERFS_KERNEL_MEM, 0);
+  }
+  if ( n_retval < 0 ) {
+    /*brelse(p_s_un_bh);*/
+    unlock_inode_after_convert(p_s_inode);
+    return -1; /* Can not convert direct item. */
+  }
+
+  /* i_blocks counts only unformatted nodes */
+  /*  p_s_inode->i_blocks += p_s_sb->s_blocksize / 512;*/
+
+  /* copy data from user space to a new unformatted node */
+  if (p_c_buf) {
+    memset(p_c_to, '\0', n_item_zeros_to_add);
+    copy_from_user(p_c_to + n_item_zeros_to_add, p_c_buf, n_item_bytes_to_write);
+
+    update_vm_cache (p_s_inode, s_item_to_insert.ih_key.k_offset - 1 + n_len_to_move + n_item_zeros_to_add, 
+		     p_c_to + n_item_zeros_to_add, n_item_bytes_to_write);
+
+    memset(p_c_to + n_item_zeros_to_add + n_item_bytes_to_write, '\0',
+	   s_node_to_write.unfm_freespace);
+  } else {
+    /* this works when we convert tail stored in direct item(s) to unformatted node without appending from user buffer */
+    memset(p_c_to, 0, s_node_to_write.unfm_freespace);
+  }
+  /* non-atomic mark_buffer_dirty is allowed here */
+  clear_bit(BH_Dirty, &p_s_un_bh->b_state) ;
+  wait_on_buffer(p_s_un_bh) ;
+  journal_mark_dirty(th, p_s_sb, p_s_un_bh); /* Destination buffer, preserve not needed. */
+
+  /* Move bytes from the direct items to the new unformatted node. 
+  **
+  ** note, this logs the unformatted node, so O_SYNC does not have to 
+  ** be concerned with flushing it
+  **
+  */
+  while ( n_len_to_move )  {
+    if ( search_for_position_by_key (p_s_sb, &s_key_to_search, p_s_search_path, &n_pos_in_item, &n_repeat) == POSITION_NOT_FOUND )
+      reiserfs_panic (p_s_sb, "PAP-14050: direct_to_indirect: indirect item does not exist");
+    n_retval = reiserfs_delete_item (th, p_s_inode, p_s_search_path, &n_pos_in_item, &s_key_to_search, 
+                                     p_s_un_bh, PRESERVE_DIRECT_TO_INDIRECT);
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( n_retval <= 0 || n_retval > n_len_to_move)
+      reiserfs_panic(p_s_sb, "PAP-14060: direct_to_indirect: illegal case");
+#endif
+
+    n_len_to_move -= n_retval;
+    s_key_to_search.k_offset -= n_retval;
+  }
+  brelse (p_s_un_bh);
+
+  p_s_inode->u.reiserfs_i.i_first_direct_byte = NO_BYTES_IN_DIRECT_ITEM;
+  unlock_inode_after_convert(p_s_inode);
+  return 1;
+}
+
+
+#ifdef NEW_GET_NEW_BUFFER
+
+/* returns one buffer with a blocknr near blocknr. */
+static int get_new_buffer_near_blocknr(
+		   struct reiserfs_transaction_handle *th,
+                   struct super_block *  p_s_sb,
+                   int blocknr,
+                   struct buffer_head ** pp_s_new_bh,
+                   struct path         * p_s_path 
+                   ) {
+  unsigned      long n_new_blocknumber = 0;
+  int           n_ret_value,
+                n_repeat = CARRY_ON;
+
+#ifdef CONFIG_REISERFS_CHECK
+  int repeat_counter = 0;
+  
+  if (!blocknr)
+    printk ("blocknr passed to get_new_buffer_near_blocknr was 0");
+#endif
+
+
+  if ( (n_ret_value = reiserfs_new_unf_blocknrs (th, p_s_sb, &n_new_blocknumber,
+                                             blocknr, 1)) == NO_DISK_SPACE )
+    return NO_DISK_SPACE;
+  
+  *pp_s_new_bh = reiserfs_getblk(p_s_sb->s_dev, n_new_blocknumber, p_s_sb->s_blocksize, &n_repeat);
+  if ( buffer_uptodate(*pp_s_new_bh) ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( buffer_dirty(*pp_s_new_bh) || (*pp_s_new_bh)->b_dev == NODEV ) {
+      reiserfs_panic(p_s_sb, "PAP-14080: get_new_buffer: invalid uptodate buffer %b for the new block", *pp_s_new_bh);
+    }
+#endif
+
+    /* Free path buffers to prevent deadlock. */
+    /* It is possible that this process has the buffer, which this function is getting, already in
+       its path, and is responsible for double incrementing the value of b_count.  If we recalculate
+       the path after schedule we can avoid risking an endless loop.  This problematic situation is
+       possible in a multiple processing environment.  Suppose process 1 has acquired a path P; then
+       process 2 balanced and remove block A from the tree.  Process 1 continues and runs
+       get_new_buffer, that returns buffer with block A. If node A was on the path P, then it will
+       have b_count == 2. If we now will simply wait in while ( (*pp_s_new_bh)->b_count > 1 ) we get
+       into an endless loop, as nobody will release this buffer and the current process holds buffer
+       twice. That is why we do decrement_counters_in_path(p_s_path) before waiting until b_count
+       becomes 1. (it there were other processes holding node A, then eventually we will get a
+       moment, when all of them released a buffer). */
+    if ( (*pp_s_new_bh)->b_count > 1  ) {
+      decrement_counters_in_path(p_s_path);
+      n_ret_value |= SCHEDULE_OCCURRED;
+    }
+
+    while ( (*pp_s_new_bh)->b_count > 1 ) {
+
+#ifdef REISERFS_INFO
+      printk("get_new_buffer() calls schedule to decrement b_count\n");
+#endif
+
+#ifdef CONFIG_REISERFS_CHECK
+      if ( ! (++repeat_counter % 10000) )
+	printk("get_new_buffer(%u): counter(%d) too big", current->pid, repeat_counter);
+#endif
+
+      current->policy |= SCHED_YIELD;
+      schedule();
+    }
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( buffer_dirty(*pp_s_new_bh) || (*pp_s_new_bh)->b_dev == NODEV ) {
+      print_buffer_head(*pp_s_new_bh,"get_new_buffer");
+      reiserfs_panic(p_s_sb, "PAP-14090: get_new_buffer: invalid uptodate buffer %b for the new block(case 2)", *pp_s_new_bh);
+    }
+#endif
+
+  }
+  else {
+    ;
+
+#ifdef CONFIG_REISERFS_CHECK
+    /* journal victim */
+    if ( 0 && (*pp_s_new_bh)->b_count != 1 && !buffer_journaled(*pp_s_new_bh)) {
+      reiserfs_panic(p_s_sb,"PAP-14100: get_new_buffer: not uptodate buffer %b for the new block has b_count more than one",
+		     *pp_s_new_bh);
+    }
+#endif
+
+  }
+  /* marking buffer here gives it the chance to die in the journal if we free it before commit */
+  if (*pp_s_new_bh) {
+    mark_buffer_journal_new(*pp_s_new_bh) ;
+  }
+  return (n_ret_value | n_repeat);
+}
+
+
+/* returns the block number of the last unformatted node, assumes p_s_key_to_search.k_offset is a byte in the tail of
+   the file, Useful for when you want to append to a file, and convert a direct item into an unformatted node near the
+   last unformatted node of the file.  Putting the unformatted node near the direct item is potentially very bad to do.
+   If there is no unformatted node in the file, then we return the block number of the direct item.  */
+inline int get_last_unformatted_node_blocknr_of_file(  struct key * p_s_key_to_search, struct super_block * p_s_sb,
+                                                       struct buffer_head * p_s_bh, int * p_n_repeat, 
+                                                       struct path * p_unf_search_path, struct inode * p_s_inode)
+
+{
+  struct key unf_key_to_search;
+  struct item_head * p_s_ih;
+  int n_pos_in_item;
+  struct buffer_head * p_indirect_item_bh;
+
+      copy_key(&unf_key_to_search,p_s_key_to_search);
+      unf_key_to_search.k_uniqueness = TYPE_INDIRECT;
+      unf_key_to_search.k_offset = p_s_inode->u.reiserfs_i.i_first_direct_byte -1;
+
+        /* p_s_key_to_search->k_offset -  MAX_ITEM_LEN(p_s_sb->s_blocksize); */
+      if (search_for_position_by_key (p_s_sb, &unf_key_to_search, p_unf_search_path, &n_pos_in_item, p_n_repeat) == POSITION_FOUND)
+        {
+          p_s_ih = B_N_PITEM_HEAD(p_indirect_item_bh = PATH_PLAST_BUFFER(p_unf_search_path), PATH_LAST_POSITION(p_unf_search_path));
+          return (B_I_POS_UNFM_POINTER(p_indirect_item_bh, p_s_ih, n_pos_in_item));
+        }
+     /*  else */
+      printk("reiser-1800: search for unformatted node failed, p_s_key_to_search->k_offset = %u,  unf_key_to_search.k_offset = %u, MAX_ITEM_LEN(p_s_sb->s_blocksize) = %ld, debug this\n", p_s_key_to_search->k_offset, unf_key_to_search.k_offset,  MAX_ITEM_LEN(p_s_sb->s_blocksize) );
+      print_buffer_head(PATH_PLAST_BUFFER(p_unf_search_path), "the buffer holding the item before the key we failed to find");
+      print_block_head(PATH_PLAST_BUFFER(p_unf_search_path), "the block head");
+      return 0;                         /* keeps the compiler quiet */
+}
+
+
+                                /* hasn't been out of disk space tested  */
+static int get_buffer_near_last_unf (struct reiserfs_transaction_handle *th,
+				     struct super_block * p_s_sb, struct key * p_s_key_to_search,
+                                                 struct inode *  p_s_inode,  struct buffer_head * p_s_bh, 
+                                                 struct buffer_head ** pp_s_un_bh, struct path * p_s_search_path)
+{
+  int unf_blocknr = 0, /* blocknr from which we start search for a free block for an unformatted node, if 0
+                          then we didn't find an unformatted node though we might have found a file hole */
+    n_repeat = CARRY_ON;        /* did schedule occur?  if so, then don't carry on, and recalc things */
+  struct key unf_key_to_search;
+  struct path unf_search_path;
+
+  copy_key(&unf_key_to_search,p_s_key_to_search);
+  unf_key_to_search.k_uniqueness = TYPE_INDIRECT;
+  
+  if (
+      (p_s_inode->u.reiserfs_i.i_first_direct_byte > 4095) /* i_first_direct_byte gets used for all sorts of
+                                                              crap other than what the name indicates, thus
+                                                              testing to see if it is 0 is not enough */
+      && (p_s_inode->u.reiserfs_i.i_first_direct_byte < MAX_KEY_OFFSET) /* if there is no direct item then
+                                                                           i_first_direct_byte = MAX_KEY_OFFSET */
+      )
+    {
+                                /* actually, we don't want the last unformatted node, we want the last unformatted node
+                                   which is before the current file offset */
+      unf_key_to_search.k_offset = ((p_s_inode->u.reiserfs_i.i_first_direct_byte -1) < unf_key_to_search.k_offset) ? p_s_inode->u.reiserfs_i.i_first_direct_byte -1 :  unf_key_to_search.k_offset;
+
+      while (unf_key_to_search.k_offset > -1)
+        {
+                                /* This is our poorly documented way of initializing paths. -Hans */
+          init_path (&unf_search_path);
+                                /* get the blocknr from which we start the search for a free block. */
+          unf_blocknr = get_last_unformatted_node_blocknr_of_file(  p_s_key_to_search, /* assumes this points to the file tail */
+                                                                    p_s_sb,     /* lets us figure out the block size */
+                                                                    p_s_bh, /* if there is no unformatted node in the file,
+                                                                               then it returns p_s_bh->b_blocknr */
+                                                                    &n_repeat, /* tells us if schedule occurred */
+                                                                    &unf_search_path,
+                                                                    p_s_inode
+                                                                    );
+/*        printk("in while loop: unf_blocknr = %d,  *pp_s_un_bh = %p\n", unf_blocknr, *pp_s_un_bh); */
+          if (unf_blocknr) 
+            break;
+          else                  /* release the path and search again, this could be really slow for huge
+                                   holes.....better to spend the coding time adding compression though.... -Hans */
+            {
+                                /* Vladimir, is it a problem that I don't brelse these buffers ?-Hans */
+              decrement_counters_in_path(&unf_search_path);
+              unf_key_to_search.k_offset -= 4096;
+            }
+        }
+      if (unf_blocknr) {
+        n_repeat |= get_new_buffer_near_blocknr(th, p_s_sb, unf_blocknr, pp_s_un_bh, p_s_search_path);
+      }
+      else {                    /* all unformatted nodes are holes */
+        n_repeat |= get_new_buffer_near_blocknr(th, p_s_sb, p_s_bh->b_blocknr, pp_s_un_bh, p_s_search_path); 
+      }
+    }
+  else {                        /* file has no unformatted nodes */
+    n_repeat |= get_new_buffer_near_blocknr(th, p_s_sb, p_s_bh->b_blocknr, pp_s_un_bh, p_s_search_path);
+/*     printk("in else: unf_blocknr = %d,  *pp_s_un_bh = %p\n", unf_blocknr, *pp_s_un_bh); */
+/*     print_path (0,  p_s_search_path); */
+  }
+
+  return n_repeat;
+}
+
+#endif /* NEW_GET_NEW_BUFFER */
+
+
+#ifdef OLD_GET_NEW_BUFFER
+/*
+** BIG CHANGE HERE!  If this fails to get a block, it will update the inode
+** release the path, stop the transaction, start a new transaction, and
+** try again.
+*/
+/*static*/ int get_new_buffer(
+		   struct reiserfs_transaction_handle *th,
+		   struct super_block *	 p_s_sb,
+		   struct buffer_head *  p_s_bh,
+		   struct buffer_head ** pp_s_new_bh,
+		   struct path	       * p_s_path,
+		   struct inode        * p_s_inode,     /* for update of inode*/
+		   unsigned long         n_pos_in_file /* from file_write */
+		   ) {
+  unsigned	long n_new_blocknumber = 0;
+  int		n_repeat, n_repeat1, disk_full_check;
+
+
+  if ( (n_repeat = reiserfs_new_unf_blocknrs (th, p_s_sb, &n_new_blocknumber, p_s_bh->b_blocknr, 1, 0/*not for preserve list*/)) == NO_DISK_SPACE ) {
+
+    /* We might not be out of space.  The journal might be able to reclaim
+    ** some, but not while our transaction is open.  So, we update our inode
+    ** and restart the transaction hoping there will be more space when we
+    ** get back.  
+    */
+    n_repeat |= SCHEDULE_OCCURRED;
+    decrement_counters_in_path(p_s_path);
+    update_inode_and_restart_transaction(th, p_s_inode, n_pos_in_file) ;
+    if ( (disk_full_check = reiserfs_new_unf_blocknrs (th, p_s_sb, 
+                                                       &n_new_blocknumber, 
+						       p_s_bh->b_blocknr, 
+						       1, 
+						       0)) == NO_DISK_SPACE ) {
+
+      return NO_DISK_SPACE;
+    }
+  } 
+  n_repeat1 = CARRY_ON;
+  *pp_s_new_bh = reiserfs_getblk(p_s_sb->s_dev, n_new_blocknumber, p_s_sb->s_blocksize, &n_repeat1);
+  n_repeat |= n_repeat1;
+  if ((*pp_s_new_bh)->b_count > 1) {
+    /* Free path buffers to prevent deadlock which can occur in the
+       situation like : this process holds p_s_path; Block
+       (*pp_s_new_bh)->b_blocknr is on the path p_s_path, but it is
+       not necessary, that *pp_s_new_bh is in the tree; process 2
+       could remove it from the tree and freed block
+       (*pp_s_new_bh)->b_blocknr. Reiserfs_new_blocknrs in above
+       returns block (*pp_s_new_bh)->b_blocknr. Reiserfs_getblk gets
+       buffer for it, and it has b_count > 1. If we now will simply
+       wait in while ( (*pp_s_new_bh)->b_count > 1 ) we get into an
+       endless loop, as nobody will release this buffer and the
+       current process holds buffer twice. That is why we do
+       decrement_counters_in_path(p_s_path) before waiting until
+       b_count becomes 1. (it there were other processes holding node
+       pp_s_new_bh, then eventually we will get a moment, when all of
+       them released a buffer). */
+    decrement_counters_in_path(p_s_path);
+    n_repeat |= SCHEDULE_OCCURRED;
+    wait_buffer_until_released (*pp_s_new_bh);
+  }
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (0 && !buffer_journaled(*pp_s_new_bh) && ((*pp_s_new_bh)->b_count != 1 || buffer_dirty (*pp_s_new_bh))) {
+    reiserfs_panic(p_s_sb,"PAP-14100: get_new_buffer: not free or dirty buffer %b for the new block",
+		   *pp_s_new_bh);
+  }
+#endif
+  if (*pp_s_new_bh) {
+    mark_buffer_journal_new(*pp_s_new_bh) ;
+  }
+
+  return n_repeat;
+}
+
+#endif /* OLD_GET_NEW_BUFFER */
+
+
+#ifdef GET_MANY_BLOCKNRS
+                                /* code not yet functional */
+get_next_blocknr (
+                  unsigned long *       p_blocknr_array,          /* we get a whole bunch of blocknrs all at once for
+                                                                     the write.  This is better than getting them one at
+                                                                     a time.  */
+                  unsigned long **      p_blocknr_index,        /* pointer to current offset into the array. */
+                  unsigned long        blocknr_array_length
+)
+{
+  unsigned long return_value;
+
+  if (*p_blocknr_index < p_blocknr_array + blocknr_array_length) {
+    return_value = **p_blocknr_index;
+    **p_blocknr_index = 0;
+    *p_blocknr_index++;
+    return (return_value);
+  }
+  else
+    {
+      kfree (p_blocknr_array);
+    }
+}
+#endif /* GET_MANY_BLOCKNRS */
+
+/* returns 0 if everything went well
+**         -1 if any of the blocks failed to sync
+** send with bh == NULL to flush the list, otherwise, the list 
+** is only flushed when *buffer_count == size
+**
+** n_blocks_flushed is increased to total the first x number of blocks that
+** were properly flushed.  So if you try to write blocks 3, 5, and 7,
+** and block 5 fails, n_block_count will be increased by 1, regardless of
+** what happens to block 7.
+**
+** if -1 is returned, and n_written was zero when passed in, n_written 
+** will be set to -EIO on return.  n_tail_bytes_written is set to 
+** zero when this returns -1
+*/
+static int do_osync(struct reiserfs_transaction_handle *th, 
+                    struct file *f, struct buffer_head *bh,
+		    struct buffer_head **buffer_list,
+		    int size, int *buffer_count, int *n_blocks_flushed,
+		    int *n_written, int *n_tail_bytes_written) {
+  int i ;
+  int write_error = 0;
+  int tmp_blocks_flushed = 0 ;
+
+  if (bh) {
+    if (buffer_dirty(bh)) {
+      buffer_list[(*buffer_count)] = bh ;
+      (*buffer_count)++ ;
+    } else {
+      /* the block was logged */
+      SB_JOURNAL(th->t_super)->j_next_async_flush = 1 ;
+      brelse(bh) ;
+      *n_blocks_flushed += 1 ;
+    }
+  }
+  if (*buffer_count == size || (bh == NULL && *buffer_count > 0)) {
+    ll_rw_block(WRITE, *buffer_count, buffer_list) ;
+    for(i = 0 ; i < *buffer_count ; i++) {
+      wait_on_buffer(buffer_list[i]) ;
+      if (write_error == 0) {
+        if (!buffer_uptodate(buffer_list[i])) {
+	  *n_tail_bytes_written = 0 ;
+	  write_error = -1 ;
+        } else {
+	  tmp_blocks_flushed++ ;
+	}
+      }
+      brelse(buffer_list[i]) ;
+    }
+    *buffer_count = 0 ;
+  }
+  *n_blocks_flushed += tmp_blocks_flushed; 
+  if (write_error < 0 && *n_written <= 0) {
+    *n_written = -EIO ;
+  }
+  return write_error ;
+}
+
+/* stolen from ext2 */
+static inline void remove_suid(struct inode *inode)
+{
+	unsigned int mode;
+
+	/* set S_IGID if S_IXGRP is set, and always set S_ISUID */
+	mode = (inode->i_mode & S_IXGRP)*(S_ISGID/S_IXGRP) | S_ISUID;
+
+	/* was any of the uid bits set? */
+	mode &= inode->i_mode;
+	if (mode && !capable(CAP_FSETID)) {
+		/* note, we don't make the inode dirty so
+		** you must log it after calling this
+		*/
+		inode->i_mode &= ~mode;
+	}
+}
+
+/* Summary:
+
+We have a file consisting of some number of indirect items and direct
+items.  We translate filp->f_pos into where we are supposed to start
+the write in terms of the tree data structures.  We then write one
+node at a time, decrementing the count of bytes that we are to write,
+after we write each node, by the appropriate amount.  We loop until
+count is zero.  We have a complex if statement that handles each
+possible case in writing a node, taking into account holes,
+conversions from direct to indirect, overwrites, etc.  This if
+statement is horrible, and should be rewritten. -Hans
+
+reiserfs_paste_into_item, direct_to_indirect, and reiserfs_insert_into_item perform
+all balancing that results from the write.
+
+We return either the number of bytes successfully written or an error.
+Note that if we successfully write some bytes and have an error, we
+return the number of bytes written, not the error.  This allows the
+user to record that the bytes were written, retry, and then get the
+errorid on the retry, or if the error goes away, to never know.  Note
+that ENOSPC errors are complicated by the preserve list (see
+preserve.c). - Hans
+
+*/
+
+/*
+
+We don't need to worry about schedule for direct items because they
+search for one buffer and use it.  Schedule is a problem only when
+searching for two things, and the second search invalidates the first.
+Insertion of direct items is also not a concern, for the same reason.
+It is only {insertion of a pointer into an indirect item coupled with
+getting a buffer to hold the unformatted node, and converting direct
+to indirect items} that require that upon schedule we decide whether
+to continue, free resources gotten, and retry.
+
+When schedule occurs, we look at the indirect item that we found, and
+make sure it is still there.  If something else is there that has the
+same string of bytes as the item we are looking for, we will have a
+bug?  This can happen for preserved nodes?
+
+*/
+
+/* Write n_count bytes from user buffer p_c_buf into a system buffer. */
+static ssize_t reiserfs_file_write(
+                                   struct file   * p_s_filp, /* current position in the file, man write() for details. */
+                                   const char    * p_c_buf,  /* The user space buffer from which we copy. */
+                                   size_t          n_count,   /* The number of bytes to write: once the function
+                                                                 starts up, count is the number of bytes remaining
+                                                                 to be written. */
+                                   loff_t        * p_n_pos
+            ) {
+  struct inode * p_s_inode = p_s_filp->f_dentry->d_inode;
+  struct super_block  * p_s_sb;
+  struct key            s_key_to_search;      	/* Key of item we are searching for that we are
+                                                  going to write into. */
+  /*loff_t*/unsigned long	        n_pos_in_file;        	/* Position (in bytes from beginning of object)
+                                                  where you must write. */
+  unsigned long         n_unp,                  /* Unformatted node number. */
+                        n_append_startpoint;    /* Offset of the byte after the last byte in the file. */
+  struct path           s_search_path;        /* path from root of tree to some leaf, we keep this
+						 data around so that it is faster to go up and down
+						 the tree */
+  ssize_t               n_written;              /* Bytes written, or errorid. */
+  ssize_t               n_tail_bytes_written; /* Bytes written to direct item */
+  size_t        n_orig_count = n_count ;           /* original number of bytes wanted */
+  int                   n_pos_in_item,
+                        n_search_res,
+                        n_pos_in_buffer,
+                        n_item_bytes_to_write,  /* The number of bytes from the user buffer to be
+			                                              written in this pass of the loop to the
+                                                    current item. */
+                        n_item_zeros_to_add,    /* Number of zeros to be added to this item.  */
+                        n_repeat,	              /* If schedule occured. */
+                        n_item_size,	          /* Number of bytes in the current item. */
+                        n_offset_in_node,       /* Write offset in the unformatted node. */
+                        n_zero_bytes,           /* Number of zero bytes which must be created
+                                                    (if we write past the end of the file). */
+                        n_blk_size,             /* Block size in bytes. */
+                        n_bytes_in_item,        /* Item bytes number. */
+                        n_cur_len,              /* Length of current item. */
+                        retval,                 /* What direct_to_indirect returns */
+                        n_max_squeeze_in;       /* Maximum amount that could be added to the direct item
+                                                    and still fit into it or an unformatted node that it
+                                                    is converted into */
+  struct buffer_head  * p_s_bh,                 /* Buffer contains found item. */
+                      * p_s_un_bh;              /* Buffer contains unformatted node. */
+  struct item_head      s_ih,
+                        s_item_to_insert,       /* Header of mythical item: it contains the key to
+                                                    search for. */
+                      * p_s_ih;                 /* Found item header. */
+  struct unfm_nodeinfo	s_node_to_write;      /* Handle on an unformatted node that will be passed
+                                                    to do_balance the last byte of the file. */
+#ifdef GET_MANY_BLOCKNRS
+  unsigned long *       p_blocknr_array,          /* we get a whole bunch of blocknrs all at once for the write.  This is
+                                                     better than getting them one at a time.  */
+                        p_blocknr_index;        /* current offset into the array. */
+  unsigned long        blocknr_array_length;
+
+#endif /* GET_MANY_BLOCKNRS */
+  char * local_buf = 0;
+  int j;
+  int jbegin_count = JOURNAL_PER_BALANCE_CNT * 3 ;
+  int windex ;
+  struct reiserfs_transaction_handle th ;
+  unsigned long	limit = current->rlim[RLIMIT_FSIZE].rlim_cur;
+  unsigned long pos = *p_n_pos;
+
+  struct buffer_head *buffer_list[REISERFS_NBUF] ;
+  int buffer_count = 0 ;
+  int n_blocks_flushed = 0 ; /* tracks i/o errors during O_SYNC */
+  
+/*
+  if (!p_s_inode->i_op || !p_s_inode->i_op->updatepage)
+      return -EIO;
+  */
+  if (p_s_filp->f_error) {
+      int error = p_s_filp->f_error;
+      p_s_filp->f_error = 0;
+      return error;
+  }
+
+  /* Calculate position in the file. */
+  if ( p_s_filp->f_flags & O_APPEND ) {
+    n_pos_in_file = p_s_inode->i_size + 1;
+    pos = p_s_inode->i_size;
+  } else
+    n_pos_in_file = *p_n_pos + 1;
+
+  if (pos >= limit)
+      return -EFBIG;
+
+  if (n_count > limit - pos)
+      n_count = limit - pos;
+
+  n_written = 0;
+  n_tail_bytes_written = 0;
+  p_s_sb = p_s_inode->i_sb;
+
+  remove_suid(p_s_inode) ;
+  journal_begin(&th, p_s_sb, jbegin_count) ;
+
+  if (p_s_filp->f_flags & O_SYNC) {
+    SB_JOURNAL(p_s_sb)->j_next_async_flush = 1 ;
+  }
+  windex = push_journal_writer("file-write") ;
+
+  reiserfs_update_inode_transaction(p_s_inode) ;
+
+  /* Set key_to_search to search for first item of object */
+  copy_key(&s_key_to_search, INODE_PKEY (p_s_inode));
+
+
+  /*  printk ("File_write: objectid = %lu offset = %lu, count = %lu\n", s_key_to_search.k_objectid, n_pos_in_file - 1, n_count);*/
+
+  /* Set key_to_search to search for item we write into */
+  s_key_to_search.k_offset = n_pos_in_file;
+  if ( n_pos_in_file >= p_s_inode->u.reiserfs_i.i_first_direct_byte )
+    s_key_to_search.k_uniqueness = TYPE_DIRECT;
+  else
+    s_key_to_search.k_uniqueness = TYPE_INDIRECT;
+
+  n_written = 0;
+  n_blk_size = p_s_sb->s_blocksize;
+  p_s_un_bh = NULL;
+  init_path (&s_search_path);
+
+  local_buf = reiserfs_kmalloc (n_blk_size, GFP_BUFFER, p_s_sb);
+  if (local_buf == 0) {
+    pop_journal_writer(windex) ;
+    journal_end(&th, p_s_sb, jbegin_count) ;
+    return -ENOMEM;
+  }
+
+  while ( n_count ) {
+    if (current->need_resched)
+      schedule();
+
+
+    /* Search for the item which contains 'n_pos_in_file-th' object byte.  If we are writing past the
+       end of the file then item_num and bh are set to the last item in the file.
+       Note: the repeat of this expensive search plus do_balance for every iteration
+       of this loop is a performance loss source for large files.  We should calculate how
+       many unformatted nodes we are going to add and insert their
+       pointers into the indirect item all at once. Though, if we used extents.... -Hans */
+    n_search_res = search_for_position_by_key (p_s_sb, &s_key_to_search, &s_search_path, &n_pos_in_item, &n_repeat);
+
+    /* The item which contains 'pos-th' object byte.  If we are writing past the end
+       of the file p_s_ih is set to the last item in the file. */
+    p_s_ih = B_N_PITEM_HEAD(p_s_bh = PATH_PLAST_BUFFER(&s_search_path),
+			    n_pos_in_buffer = PATH_LAST_POSITION(&s_search_path));
+    /* Remember the found item header. */
+    copy_item_head(&s_ih, p_s_ih);
+
+    /* Item which contains 'n_pos_in_file-th' byte is found. */
+    if ( n_search_res == POSITION_FOUND )  {
+      n_item_zeros_to_add = 0;
+      if ( I_IS_DIRECT_ITEM(p_s_ih) ) {
+	n_item_bytes_to_write = p_s_ih->ih_item_len - n_pos_in_item;
+	if ( n_item_bytes_to_write > n_count )
+	  n_item_bytes_to_write = n_count;
+	/* copy data from user space to intermediate buffer */
+	copy_from_user (local_buf, p_c_buf, n_item_bytes_to_write);
+
+	/* make sure, that direct item is still on its old place */
+	if (comp_items (&s_ih, &s_search_path)) {
+	  printk ("reiserfs_file_write: item has been moved while we were in copy_from_user (overwriting the direct item)\n");
+	  continue;
+	}
+	/* Overwrite to end of this direct item or end of count bytes from user buffer to direct item. */
+	memcpy (B_I_PITEM(p_s_bh, p_s_ih) + n_pos_in_item, local_buf, n_item_bytes_to_write);
+	/* mark buffer dirty atomically. It will be refiled in pathrelse */
+	journal_mark_dirty(&th, p_s_sb, p_s_bh) ;
+
+	n_tail_bytes_written += n_item_bytes_to_write ;
+	update_vm_cache(p_s_inode, n_pos_in_file - 1,
+			B_I_PITEM(p_s_bh, p_s_ih) + n_pos_in_item, n_item_bytes_to_write);
+
+      }
+      else  { /* Indirect item. */
+	/* Set n_unp to blocknr containing the byte we start the write at. */
+        n_unp = B_I_POS_UNFM_POINTER(p_s_bh, p_s_ih, n_pos_in_item);
+	/* The number of bytes in the unformatted node that are to be overwritten
+	   (calculation adjusts for the case in which not all bytes are used in the
+	   unformatted node). */
+	n_item_bytes_to_write = (n_item_size = I_POS_UNFM_SIZE(p_s_ih, n_pos_in_item, n_blk_size))
+	  			- ((int)n_pos_in_file - 1) % n_blk_size;
+	if ( n_item_bytes_to_write > n_count )
+	  n_item_bytes_to_write = n_count;
+
+#ifdef CONFIG_REISERFS_CHECK
+	if ( n_item_bytes_to_write <= 0 ) {
+	  printk("n_item_size = %d n_pos_in_file = %lu\n", n_item_size, n_pos_in_file);
+	  reiserfs_panic(p_s_sb, "PAP-14110: reiserfs_file_write: n_item_bytes_to_write <= 0");
+	}
+#endif
+
+	/* If not a hole in the file. */
+	if ( n_unp )  {
+	  /* Get the block we are to write into. */
+	  n_repeat = CARRY_ON;
+          p_s_un_bh = reiserfs_getblk(p_s_bh->b_dev, n_unp, n_blk_size, &n_repeat);
+
+	  /* If we are not overwriting the entire node and we have not yet read the buffer from disk
+	     then we must read the block into memory before writing. */
+	  if ( n_item_bytes_to_write != n_item_size && ! buffer_uptodate(p_s_un_bh) ) {
+	    ll_rw_block(READ, 1, &p_s_un_bh);
+	    wait_on_buffer(p_s_un_bh);
+	    /* Check for I/O error. */
+	    if ( ! buffer_uptodate(p_s_un_bh) ) {
+	      brelse(p_s_un_bh);
+              p_s_un_bh = NULL;
+	      pathrelse(&s_search_path);
+	      if ( ! n_written )  {
+		n_written = -EIO;
+	      }
+	      break;
+	    }
+          }
+
+	  /* Do the write. */
+	  copy_from_user(p_s_un_bh->b_data + ((int)n_pos_in_file - 1) % n_blk_size,
+                                                        p_c_buf, n_item_bytes_to_write);
+	  update_vm_cache(p_s_inode, n_pos_in_file - 1,
+			  p_s_un_bh->b_data + ((int)n_pos_in_file - 1) % n_blk_size, n_item_bytes_to_write);
+	  mark_buffer_uptodate (p_s_un_bh, 1);
+	  journal_mark_dirty_nolog(&th, p_s_sb, p_s_un_bh) ;
+
+	  if (p_s_filp->f_flags & O_SYNC) {
+	    if (do_osync(&th, p_s_filp, p_s_un_bh, buffer_list, 
+	                       REISERFS_NBUF, &buffer_count, 
+			       &n_blocks_flushed, &n_written,
+			       &n_tail_bytes_written) < 0) {
+	      pathrelse(&s_search_path);
+	      n_pos_in_file += n_item_bytes_to_write;
+	      break ;
+	    } 
+	  } else {
+	    brelse (p_s_un_bh);
+	  }
+	  p_s_un_bh = NULL;
+        }
+	else  { /* If writing to a hole. */
+          /* Get a new buffer for the unformatted node. */
+          if ( ! p_s_un_bh )  {
+#ifdef OLD_GET_NEW_BUFFER
+	    n_repeat = get_new_buffer(&th, p_s_sb, p_s_bh, &p_s_un_bh, &s_search_path, p_s_inode, n_pos_in_file);
+#else
+	    /* if there is an unformatted node in the file, then get our unformatted node from near
+	       the last one of them else get it from near the direct item.  */
+	    n_repeat = get_buffer_near_last_unf ( &th, p_s_sb, &s_key_to_search,
+						  p_s_inode, p_s_bh, 
+						  &p_s_un_bh, &s_search_path);
+#endif /* NEW_GET_NEW_BUFFER */
+						  
+            if ( ! p_s_un_bh )  {
+              /* No disk space for new block. */
+              if ( ! n_written )
+                n_written = -ENOSPC;
+	      pathrelse(&s_search_path);
+	      break;
+            }
+	    memset (p_s_un_bh->b_data, 0, n_blk_size);
+
+	    if ( n_repeat != CARRY_ON && (s_search_path.path_length == ILLEGAL_PATH_ELEMENT_OFFSET || 
+					  comp_items(&s_ih, &s_search_path)) ) {
+	      /* other processes are allowed */
+	      j = p_s_un_bh->b_blocknr / (p_s_sb->s_blocksize * 8);
+	      COMPLETE_BITMAP_DIRTING_AFTER_ALLOCATING (p_s_sb, j);
+	      continue;
+	    }
+          }
+
+#ifdef CONFIG_REISERFS_CHECK
+	  if (B_I_POS_UNFM_POINTER (p_s_bh, p_s_ih, n_pos_in_item) != 0)
+	    reiserfs_panic (p_s_sb, "vs-14120: reiserfs_file_write: unformatted node pointer %lu, must 0",
+			    B_I_POS_UNFM_POINTER(p_s_bh,p_s_ih, n_pos_in_item));
+#endif
+	  /* Initialize pointer to the unformatted node in the its parent. */
+	  B_I_POS_UNFM_POINTER(p_s_bh,p_s_ih, n_pos_in_item) = p_s_un_bh->b_blocknr;
+          /* reiserfs_mark_buffer_dirty(p_s_bh, 1); journal victim */
+          journal_mark_dirty(&th, p_s_sb, p_s_bh); 
+
+	  /* Put zeros before the place where the write starts. */
+	  n_offset_in_node = ((int)n_pos_in_file - 1) % n_blk_size;
+	  //memset(p_s_un_bh->b_data, '\0', n_offset_in_node = ((int)n_pos_in_file - 1) % n_blk_size);
+	  /* Do the write at the appropriate spot. */
+	  copy_from_user(p_s_un_bh->b_data + n_offset_in_node, p_c_buf, n_item_bytes_to_write);
+
+	  update_vm_cache(p_s_inode, n_pos_in_file - 1, 
+			  p_s_un_bh->b_data + n_offset_in_node, n_item_bytes_to_write);
+
+	  /* it is not a hole now */
+	  p_s_inode->i_blocks += p_s_sb->s_blocksize / 512;
+
+	  /* Put zeros after the place where the write finishes. */
+	  //memset(p_s_un_bh->b_data + n_offset_in_node + n_item_bytes_to_write, '\0',
+	  //	 n_blk_size - n_offset_in_node - n_item_bytes_to_write);
+
+	  mark_buffer_uptodate(p_s_un_bh, 1);
+	  /* non-atomic mark_buffer_dirty is allowed here */
+	  /* mark_buffer_dirty(p_s_un_bh, 0);   */
+	  journal_mark_dirty_nolog(&th, p_s_sb, p_s_un_bh) ;
+	  if (p_s_filp->f_flags & O_SYNC) {
+	    if (do_osync(&th, p_s_filp, p_s_un_bh, buffer_list, 
+	                       REISERFS_NBUF, &buffer_count, 
+			       &n_blocks_flushed, &n_written,
+			       &n_tail_bytes_written) < 0) {
+	      pathrelse(&s_search_path);
+	      n_pos_in_file += n_item_bytes_to_write;
+	      break ;
+	    } 
+	  } else {
+	    brelse (p_s_un_bh);
+	  }
+	  p_s_un_bh = NULL;
+        }	/* replace hole with unformatted node */
+      }		/* overwrite indirect item */
+      pathrelse(&s_search_path);
+    } 		/* position we are going to write to is found */
+
+    /* Item containing n_pos_in_file-th byte not found (writing past current end of file). */
+    else  {
+      /* Form item key to insert. */
+      copy_key(&(s_item_to_insert.ih_key),&s_key_to_search);
+      /* We calculate offset of the byte after the last byte in the file. */
+      n_append_startpoint = ( I_IS_STAT_DATA_ITEM(p_s_ih) ) ? 1 :
+				p_s_ih->ih_key.k_offset + I_BYTES_NUMBER(p_s_ih, n_blk_size);
+      /* Calculate number of zero bytes which must be created. */
+
+      n_zero_bytes = n_pos_in_file - n_append_startpoint;
+
+#ifdef CONFIG_REISERFS_CHECK
+      if ( COMP_SHORT_KEYS(&(p_s_ih->ih_key), &s_key_to_search) || n_zero_bytes < 0 ) {
+	printk("n_pos_in_file  = %lu n_append_startpoint = %lu\n",n_pos_in_file, n_append_startpoint);
+        reiserfs_panic(p_s_sb, "PAP-14130: reiserfs_file_write: item found %h, key to search %k", p_s_ih, &s_key_to_search);
+      }
+#endif
+#ifdef GET_MANY_BLOCKNRS
+                                /* Allocate space for blocknr_array if we don't have one. */
+      if (!p_blocknr_array) {
+        /* Calculate maximum amount that could be added to the direct item and still
+             fit into it or an unformatted node that it is converted into */
+        n_max_squeeze_in = n_blk_size - (n_append_startpoint - 1) % n_blk_size;
+        blocknr_array_length = (count > n_max_squeeze_in) ?((count - n_max_squeeze_in) / n_blk_size): 0;
+        if (blocknr_array_length) {
+          p_blocknr_array = kmalloc (sizeof(unsigned long) * blocknr_array_length, GFP_ATOMIC);
+          if (!p_blocknr_array) {
+            reiserfs_panic("reiserfs-1830: reiserfs_file_write: kmalloc failed");
+	  }
+        }
+      }
+#endif /*  GET_MANY_BLOCKNRS */
+
+      /* Last file item is the stat_data. */
+      if ( I_IS_STAT_DATA_ITEM(p_s_ih) ) {
+
+        /* Calculate zeros to insert before write into unformatted node. */
+	n_item_zeros_to_add = ( n_zero_bytes < n_blk_size ) ? n_zero_bytes : n_blk_size;
+	/* Calculate the amount to write into unformatted node. */
+	n_item_bytes_to_write = ( n_item_zeros_to_add < n_blk_size ) ?
+	  ( n_blk_size - n_item_zeros_to_add) : 0;
+	if ( n_item_bytes_to_write > n_count )
+          n_item_bytes_to_write = n_count;
+
+	s_item_to_insert.ih_key.k_offset = n_append_startpoint;
+	/* Insert pointer to the unformatted node. */
+	if ( dont_have_tails (p_s_sb) ||
+	     (n_item_zeros_to_add + n_item_bytes_to_write > MAX_DIRECT_ITEM_LEN(n_blk_size)) ||
+	     (n_orig_count >= MIN_PACK_ON_CLOSE) ||
+	     STORE_TAIL_IN_UNFM (0, n_item_zeros_to_add + n_item_bytes_to_write, n_blk_size) )  {
+
+	  if (!STORE_TAIL_IN_UNFM (0, n_item_zeros_to_add + n_item_bytes_to_write, n_blk_size)) {
+	    p_s_inode->u.reiserfs_i.i_pack_on_close = 1 ;
+	  } else {
+	    p_s_inode->u.reiserfs_i.i_pack_on_close = 0 ;
+	  }
+          if ( n_item_bytes_to_write &&  ! p_s_un_bh ) {
+#ifdef OLD_GET_NEW_BUFFER
+            /* Get new buffer for an unformatted node. */
+            n_repeat = get_new_buffer(&th, p_s_sb, p_s_bh, &p_s_un_bh, &s_search_path, p_s_inode, n_pos_in_file);
+#else
+	    /* if there is an unformatted node in the file, then get
+	       our unformatted node from near the last one of them
+	       else get it from near the direct item.  */
+	    n_repeat = get_buffer_near_last_unf (&th,  p_s_sb, &s_key_to_search,
+						  p_s_inode, p_s_bh,
+						  &p_s_un_bh, &s_search_path);
+#endif /* NEW_GET_NEW_BUFFER */
+	    if ( ! p_s_un_bh )  {
+	      /* No disk space for new block. */
+	      if ( ! n_written )
+                n_written = -ENOSPC;
+	      pathrelse(&s_search_path);
+	      break;
+            }
+	    memset (p_s_un_bh->b_data, 0, n_blk_size);
+	    if ( n_repeat != CARRY_ON && (s_search_path.path_length == ILLEGAL_PATH_ELEMENT_OFFSET || comp_items(&s_ih, &s_search_path)) ) {
+	      /* compelte what was not finished in reiserfs_new_blocknrs */
+	      j = p_s_un_bh->b_blocknr / (p_s_sb->s_blocksize * 8);
+	      COMPLETE_BITMAP_DIRTING_AFTER_ALLOCATING (p_s_sb, j);
+	      continue;
+	    }
+          }
+	  n_unp = p_s_un_bh ? p_s_un_bh->b_blocknr : 0;
+
+          /* Form indirect item header. */
+	  s_item_to_insert.ih_key.k_uniqueness = TYPE_INDIRECT;
+	  s_item_to_insert.u.ih_free_space = n_blk_size - n_item_zeros_to_add - n_item_bytes_to_write;
+	  s_item_to_insert.ih_item_len = UNFM_P_SIZE;
+          /* reiserfs_insert_item() inserts before given position in the node, so we must
+	     increment to point to the next item after searched one. */
+	  PATH_LAST_POSITION(&s_search_path)++;
+          /* Insert indirect item. */
+	  if ( reiserfs_insert_item (&th, p_s_sb, &s_search_path, &s_item_to_insert, (char *)&n_unp, REISERFS_KERNEL_MEM, 0, NOTHING_SPECIAL) < 0 )  {
+            if ( ! n_written )
+	      n_written = -ENOSPC;
+	    if ( p_s_un_bh ) {
+	      reiserfs_free_block (&th, p_s_sb, p_s_un_bh->b_blocknr);
+
+	      j = p_s_un_bh->b_blocknr / (p_s_sb->s_blocksize * 8);
+	      COMPLETE_BITMAP_DIRTING_AFTER_FREEING(p_s_sb,j);
+
+              bforget(p_s_un_bh);
+	      p_s_un_bh = NULL;
+	    }
+	    break; /* No disk space. */
+          }
+          if ( p_s_un_bh ) {
+	    /* pointer to an unformatted node is in tree. Copy data */
+            //memset(p_s_un_bh->b_data, '\0', n_item_zeros_to_add);
+	    copy_from_user(p_s_un_bh->b_data + n_item_zeros_to_add, p_c_buf, n_item_bytes_to_write);
+	    //memset(p_s_un_bh->b_data + n_item_zeros_to_add + n_item_bytes_to_write, '\0',
+	    //   n_blk_size - n_item_zeros_to_add - n_item_bytes_to_write);
+
+	    update_vm_cache(p_s_inode, n_pos_in_file - 1,
+			    p_s_un_bh->b_data + n_item_zeros_to_add, n_item_bytes_to_write);
+            mark_buffer_uptodate(p_s_un_bh, 1);
+	    journal_mark_dirty_nolog(&th, p_s_sb, p_s_un_bh) ;
+	    if (p_s_filp->f_flags & O_SYNC) {
+	      if (do_osync(&th, p_s_filp, p_s_un_bh, buffer_list, 
+				 REISERFS_NBUF, &buffer_count, 
+				 &n_blocks_flushed, &n_written,
+				 &n_tail_bytes_written) < 0) {
+		pathrelse(&s_search_path);
+		n_pos_in_file += n_item_bytes_to_write;
+		break ;
+	      } 
+	    } else {
+	      brelse(p_s_un_bh);
+	    }
+	    p_s_un_bh = NULL;
+	    /* i_blocks counts only unformatted nodes */
+	    p_s_inode->i_blocks += p_s_sb->s_blocksize / 512;
+          }
+        }
+        /* Insert direct item. */
+        else  {
+	  p_s_inode->u.reiserfs_i.i_pack_on_close = 0 ;
+
+#ifdef CONFIG_REISERFS_CHECK
+	  if ( p_s_inode->u.reiserfs_i.i_first_direct_byte != NO_BYTES_IN_DIRECT_ITEM )
+	    reiserfs_panic(p_s_sb, "PAP-14140: reiserfs_file_write: file must have no direct items");
+#endif
+
+	  /* copy data from user space to intermediate buffer */
+	  copy_from_user (local_buf, p_c_buf, n_item_bytes_to_write);
+
+	  /* make sure, that direct item is still on its old place */
+	  if (comp_items (&s_ih, &s_search_path)) {
+	    printk ("reiserfs_file_write: item has been moved while we were in copy_from_user (inserting direct item after stat data)\n");
+	    continue;
+	  }
+
+          /* Form direct item header. */
+	  s_item_to_insert.ih_key.k_uniqueness = TYPE_DIRECT;
+	  s_item_to_insert.u.ih_free_space = MAX_US_INT;
+	  s_item_to_insert.ih_item_len = n_item_zeros_to_add + n_item_bytes_to_write;
+          /* reiserfs_insert_item() inserts before given position in the node, so we must
+	     increment to point to the next item after searched one. */
+	  PATH_LAST_POSITION(&s_search_path)++;
+          /* Insert direct item. */
+	  if ( reiserfs_insert_item (&th, p_s_sb, &s_search_path, &s_item_to_insert,
+				     local_buf, REISERFS_KERNEL_MEM, n_item_zeros_to_add, NOTHING_SPECIAL) < 0 )  {
+	    if ( ! n_written )
+	      n_written = -ENOSPC;
+	    break; /* No disk space. */
+          }
+#ifdef CONFIG_REISERFS_CHECK
+	  if (n_pos_in_file != n_append_startpoint + n_item_zeros_to_add)
+	    reiserfs_panic (p_s_sb, "vs-14145: reiserfs_file_write: wrong positions in file");
+#endif
+	  n_tail_bytes_written += n_item_bytes_to_write ;
+	  update_vm_cache (p_s_inode, n_pos_in_file - 1, p_c_buf, n_item_bytes_to_write);
+
+	  p_s_inode->u.reiserfs_i.i_first_direct_byte = n_append_startpoint;
+
+	  /* calculate direct item as whole block */
+	  p_s_inode->i_blocks += p_s_sb->s_blocksize / 512;
+        }
+      } // if ( I_IS_STAT_DATA_ITEM(p_s_ih) )
+
+      else  {
+        /* Last file item is the direct one. */
+	if ( I_IS_DIRECT_ITEM(p_s_ih) ) {
+	  /* n_cur_len is not always equal to p_s_ih->ih_item_len, if you
+	     write past the end of the file, cur_len can be the length the
+	     item would be if it extended to where we start the write. */
+          n_cur_len =  (n_append_startpoint - 1) % n_blk_size;
+#ifndef GET_MANY_BLOCKNRS
+          /* Calculate maximum amount that could be added to the direct item
+             and still fit into it or an unformatted node that it is converted
+             into */
+          n_max_squeeze_in = n_blk_size - (n_append_startpoint - 1) % n_blk_size;
+#endif
+          /* Calculate whether write requires converting direct item into an unformatted node. */
+
+          if ( dont_have_tails (p_s_sb) ||
+	       STORE_TAIL_IN_UNFM(n_append_startpoint - 1, n_cur_len + n_zero_bytes + n_count, n_blk_size) )  {
+
+	    p_s_inode->u.reiserfs_i.i_pack_on_close = 0 ; /* no sense in packing here, we're already doing direct->
+							  ** indirect conversion.  This was the case we are trying to
+							  ** avoid, it really slows down the journal
+	                                                  */
+
+            /* Calculate number of zeros to be added to this item.  */
+            n_item_zeros_to_add = ( n_zero_bytes > n_max_squeeze_in ) ? n_max_squeeze_in : n_zero_bytes;
+            /* Item_bytes_to_write is the number of bytes from the user buffer to be
+               written after the zeros to the new indirect item to be created.  */
+            n_item_bytes_to_write = ( n_item_zeros_to_add < n_max_squeeze_in ) ?
+              (n_max_squeeze_in - n_item_zeros_to_add) : 0;
+            if ( n_item_bytes_to_write > n_count )
+              n_item_bytes_to_write = n_count;
+
+            /* Get a new buffer for storing the unformatted node. */
+            if ( ! p_s_un_bh )  {
+#ifdef OLD_GET_NEW_BUFFER
+	      /* Minor design issue: putting the unformatted node in the place where the
+		 formatted node used to be would result in more optimal layout, but then we
+		 could not preserve the old formatted node.  This means that slowly grown
+		 files (e.g. logs) use every other block of the available blocks.  At least
+		 it is much better in layout than what some other fs`s do to slowly growing
+		 files that are interspersed with other writes. A better solution will wait
+		 until later. */
+	      n_repeat = get_new_buffer(&th, p_s_sb, p_s_bh, &p_s_un_bh, &s_search_path, p_s_inode, n_pos_in_file);
+#else
+              /* This code gets the last non-hole blocknr of the last unformatted node without reading from disk that unformatted
+                 node, but possibly reading from disk the node of the indirect item pointing to it. For large holes it
+                 is inefficient, but better to spend the time writing code to compress holes than to fix that..  */
+              /* if there is an unformatted node in the file, then get our unformatted node from near the last one of
+                 them else get it from near the direct item.  */
+              n_repeat = get_buffer_near_last_unf (&th, p_s_sb, &s_key_to_search,
+                                                    p_s_inode, p_s_bh, 
+                                                    &p_s_un_bh, &s_search_path);
+#endif
+              if ( ! p_s_un_bh )  {
+                /* No disk space for new block. */
+                if ( ! n_written )
+                  n_written = -ENOSPC;
+                pathrelse(&s_search_path);
+                break;
+              }
+	      memset (p_s_un_bh->b_data, 0, n_blk_size);
+              if ( n_repeat != CARRY_ON && (s_search_path.path_length == ILLEGAL_PATH_ELEMENT_OFFSET ||
+					    comp_items(&s_ih, &s_search_path)) ) {
+		/* compelte what was not finished in reiserfs_new_blocknrs */
+		j = p_s_un_bh->b_blocknr / (p_s_sb->s_blocksize * 8);
+		COMPLETE_BITMAP_DIRTING_AFTER_ALLOCATING (p_s_sb, j);
+		continue;
+	      }
+            }
+	    mark_buffer_uptodate(p_s_un_bh, 1);
+
+	    /* bitmap block containing set bit */
+	    j = p_s_un_bh->b_blocknr / (p_s_sb->s_blocksize * 8);
+	    /* Perform the conversion. */
+	    retval = direct_to_indirect (&th, p_s_sb, p_s_inode, &s_search_path, n_item_zeros_to_add,
+					 p_c_buf, n_item_bytes_to_write, p_s_un_bh);
+	    if (retval <= 0) {
+	      /* conversion is done by another process (in bmap or mmap) or there is no disk space to
+                 perform coversion */
+	      reiserfs_free_block (&th, p_s_sb, p_s_un_bh->b_blocknr);
+	      bforget (p_s_un_bh);
+
+	      COMPLETE_BITMAP_DIRTING_AFTER_FREEING(p_s_sb,j);
+
+	      p_s_un_bh = NULL;
+	      if (retval < 0) {
+		if ( ! n_written )
+		  n_written = -ENOSPC;
+		break;/* No disk space */
+	      }
+
+	      /* direct2indirect returned 0. Conversion has been done
+                 by other process */
+	      continue;
+	    }
+	    /* complete what was not finished in reiserfs_new_blocknrs */
+	    COMPLETE_BITMAP_DIRTING_AFTER_ALLOCATING (p_s_sb, j);
+
+	    /* ok, conversion is done. Unformatted node brelsed in direct_to_indirect */
+	    p_s_un_bh = NULL;
+	    n_tail_bytes_written += n_item_bytes_to_write ; 
+	  }
+	  /* If it is possible to perform write without converting to an unformatted node then
+	     append to the direct item. */
+	  else  {
+	    p_s_inode->u.reiserfs_i.i_pack_on_close = 0 ;
+
+	    n_item_bytes_to_write = n_count;
+	    n_item_zeros_to_add   = n_zero_bytes;
+	    if ( n_append_startpoint)
+	      s_item_to_insert.ih_key.k_offset =  n_append_startpoint;
+            n_bytes_in_item = p_s_ih->ih_item_len;
+
+	    /* copy data from user space to intermediate buffer */
+	    copy_from_user (local_buf, p_c_buf, n_item_bytes_to_write);
+	    /* make sure, that direct item is still on its old place */
+	    if (comp_items (&s_ih, &s_search_path)) {
+	      printk ("reiserfs_file_write: item has been moved while we were in copy_from_user (appending to the direct item)\n");
+	      continue;
+	    }
+
+	    if ( reiserfs_paste_into_item(&th, p_s_sb, &s_search_path, &n_bytes_in_item, &(s_item_to_insert.ih_key),
+					  local_buf, n_count + n_zero_bytes, REISERFS_KERNEL_MEM, n_zero_bytes) < 0 ) {
+	      if ( ! n_written )
+		n_written = -ENOSPC;
+	      break;
+            }
+
+	    n_tail_bytes_written += n_item_bytes_to_write ;
+	    update_vm_cache (p_s_inode, n_pos_in_file - 1, p_c_buf, n_item_bytes_to_write);
+
+          }
+        } // if ( I_IS_DIRECT_ITEM(p_s_ih) ) {
+
+        else  { /* last item is indirect item */
+
+#ifdef CONFIG_REISERFS_CHECK
+	  if ( COMP_SHORT_KEYS (&(p_s_ih->ih_key), INODE_PKEY (p_s_inode)) || !I_IS_INDIRECT_ITEM (p_s_ih) || n_pos_in_item != I_UNFM_NUM(p_s_ih) )
+	    reiserfs_panic(p_s_sb,
+			   "PAP-14150: reiserfs_file_write: item of another file, not indirect item or illegal position in the indirect item");
+#endif
+
+	  /* Blocknr from last entry in last item in file. */
+	  n_unp = B_I_POS_UNFM_POINTER(p_s_bh,p_s_ih, n_pos_in_item - 1);
+	  if ( p_s_ih->u.ih_free_space )  { /* Unformatted node has free space. */
+	    /* Set n_pos_in_item to point to last entry of last indirect item. */
+	    n_pos_in_item--;
+	    /* See comments above, it is the same except that we paste into unused space at the
+	       end of the unformatted node. */
+	    n_max_squeeze_in = p_s_ih->u.ih_free_space;
+	    n_item_zeros_to_add = ( n_zero_bytes < n_max_squeeze_in ) ? n_zero_bytes : n_max_squeeze_in;
+	    n_item_bytes_to_write = ( n_item_zeros_to_add < n_max_squeeze_in ) ?
+	      ( n_max_squeeze_in - n_item_zeros_to_add) : 0;
+	    if ( n_item_bytes_to_write > n_count )
+	      n_item_bytes_to_write = n_count;
+	    if ( n_unp )  {
+	      if ( ! p_s_un_bh ) {
+		n_repeat = CARRY_ON;
+                p_s_un_bh = reiserfs_getblk(p_s_bh->b_dev, n_unp, n_blk_size, &n_repeat);
+		if ( ! buffer_uptodate(p_s_un_bh) ) {
+		  n_repeat |= SCHEDULE_OCCURRED;
+		  ll_rw_block(READ, 1, &p_s_un_bh);
+		  wait_on_buffer(p_s_un_bh);
+		  if ( ! buffer_uptodate(p_s_un_bh) ) {
+		    pathrelse(&s_search_path);
+		    brelse(p_s_un_bh);
+		    if ( ! n_written )  {
+		      
+#ifdef REISERFS_INFO
+		      printk ("REISERFS: reiserfs_file_write() returned EIO2\n");
+#endif
+		      
+		      n_written = -EIO;
+		    }
+		    break;
+		  }
+		}
+		
+		if ( n_repeat != CARRY_ON && comp_items(&s_ih, &s_search_path) )
+		  continue;
+	      }
+
+	      /* set free space of the indirect item */
+	      p_s_ih->u.ih_free_space -= (n_item_zeros_to_add + n_item_bytes_to_write);
+	      /* reiserfs_mark_buffer_dirty(p_s_bh, 1); */
+	      journal_mark_dirty(&th,  p_s_sb, p_s_bh);
+
+	      /* copy user data to the unformatted node */
+	      memset(p_s_un_bh->b_data + n_blk_size - n_max_squeeze_in, '\0', n_item_zeros_to_add);
+	      copy_from_user(p_s_un_bh->b_data + n_blk_size - n_max_squeeze_in + n_item_zeros_to_add,
+			     p_c_buf, n_item_bytes_to_write);
+	      update_vm_cache(p_s_inode, n_pos_in_file - 1,
+			      p_s_un_bh->b_data + n_blk_size - n_max_squeeze_in + n_item_zeros_to_add,
+			      n_item_bytes_to_write);
+	      /* unformatted node is uptodate already */
+	      /* non-atomic mark_buffer_dirty is allowed here */
+	      /* mark_buffer_dirty(p_s_un_bh, 0);  */
+	      journal_mark_dirty_nolog(&th, p_s_sb, p_s_un_bh) ;
+	      if (p_s_filp->f_flags & O_SYNC) {
+		if (do_osync(&th, p_s_filp, p_s_un_bh, buffer_list, 
+				   REISERFS_NBUF, &buffer_count, 
+				   &n_blocks_flushed, &n_written,
+				   &n_tail_bytes_written) < 0) {
+		  pathrelse(&s_search_path);
+		  n_pos_in_file += n_item_bytes_to_write;
+		  break ;
+		} 
+	      } else {
+		brelse(p_s_un_bh);
+	      }
+	      p_s_un_bh = NULL;
+            }
+            else
+              /* If last entry of last item is a hole (an undesirable feature, that can occur after
+                 truncate). */
+              if ( n_item_bytes_to_write ) {/* If writing to this item rather than to somewhere past it. */
+                if ( ! p_s_un_bh )  {
+#ifdef OLD_GET_NEW_BUFFER
+		  n_repeat = get_new_buffer(&th, p_s_sb, p_s_bh, &p_s_un_bh, &s_search_path, p_s_inode, n_pos_in_file);
+#else
+		  n_repeat = get_buffer_near_last_unf (&th,p_s_sb, &s_key_to_search, p_s_inode, p_s_bh, &p_s_un_bh, &s_search_path);
+#endif
+                  if ( ! p_s_un_bh )  {
+                    /* No disk space for new block. */
+                    if ( ! n_written )
+                      n_written = -ENOSPC;
+                    pathrelse(&s_search_path);
+                    break;
+                  }
+		  memset (p_s_un_bh->b_data, 0, n_blk_size);
+		  if ( n_repeat != CARRY_ON && (s_search_path.path_length == ILLEGAL_PATH_ELEMENT_OFFSET || comp_items(&s_ih, &s_search_path)) ) {
+		    /* compelte what was not finished in reiserfs_new_blocknrs */
+		    j = p_s_un_bh->b_blocknr / (p_s_sb->s_blocksize * 8);
+		    COMPLETE_BITMAP_DIRTING_AFTER_ALLOCATING (p_s_sb, j);
+		    continue;
+		  }
+                }
+
+#ifdef CONFIG_REISERFS_CHECK
+		if (B_I_POS_UNFM_POINTER (p_s_bh, p_s_ih, n_pos_in_item) != 0)
+		  reiserfs_panic (p_s_sb, "vs-14160: reiserfs_file_write: unformatted node pointer %lu, must 0 (hole at the end of file)",
+				  B_I_POS_UNFM_POINTER(p_s_bh,p_s_ih, n_pos_in_item));
+#endif
+
+		/* set pointer to the unformatted node and free space of the indirect item */
+		B_I_POS_UNFM_POINTER(p_s_bh, p_s_ih, n_pos_in_item) = p_s_un_bh->b_blocknr;
+		n_cur_len = n_blk_size - p_s_ih->u.ih_free_space + n_item_zeros_to_add;
+		p_s_ih->u.ih_free_space -= (n_item_zeros_to_add + n_item_bytes_to_write);
+		/* reiserfs_mark_buffer_dirty(p_s_bh, 1); journal victim */
+		journal_mark_dirty(&th, p_s_sb, p_s_bh); 
+
+		/* copy user data to the unformatted node */
+                //memset(p_s_un_bh->b_data, '\0',
+		//     n_cur_len = n_blk_size - p_s_ih->u.ih_free_space + n_item_zeros_to_add);
+		copy_from_user(p_s_un_bh->b_data + n_cur_len, p_c_buf, n_item_bytes_to_write);
+
+		update_vm_cache(p_s_inode, n_pos_in_file - 1,
+				p_s_un_bh->b_data + n_cur_len, n_item_bytes_to_write);
+
+		p_s_inode->i_blocks += p_s_sb->s_blocksize / 512;
+
+		//memset(p_s_un_bh->b_data + n_cur_len + n_item_bytes_to_write, '\0',
+		//     n_blk_size - n_cur_len - n_item_bytes_to_write);
+                mark_buffer_uptodate(p_s_un_bh, 1);
+		journal_mark_dirty_nolog(&th, p_s_sb, p_s_un_bh) ;
+		if (p_s_filp->f_flags & O_SYNC) {
+		  if (do_osync(&th, p_s_filp, p_s_un_bh, buffer_list, 
+				     REISERFS_NBUF, &buffer_count, 
+				     &n_blocks_flushed, &n_written,
+				     &n_tail_bytes_written) < 0) {
+		    pathrelse(&s_search_path);
+		    n_pos_in_file += n_item_bytes_to_write;
+		    break ;
+		  } 
+		} else {
+		  brelse(p_s_un_bh);
+		}
+		p_s_un_bh = NULL;
+              } else {
+		// last pointer in indirect item is 0. That indirect item has
+		// ih_free_space != 0. But we have to write past this free
+		// space
+		p_s_ih->u.ih_free_space -= (n_item_zeros_to_add + n_item_bytes_to_write);
+		journal_mark_dirty(&th,  p_s_sb, p_s_bh);
+	      }
+            pathrelse(&s_search_path);
+          } /* appending to the free space */
+
+	  else  { /* Unformatted node doesn't have free space. */
+	    /* This is where we could see a performance improvement by writing a little bit of code to:
+	       1) calculate number of unformatted nodes to add at a time
+	       entries_can_add_to_indirect_item = (end_of_node - end_of_item)/ indirect item entry size
+	       if ( entries_can_add_to_indirect_item > 0) 
+	       entries_can_add_to_indirect_item = min (entries_can_add_to_indirect_item, disk space free,
+	       count) 
+	       else
+	       entries_can_add_to_indirect_item = min (max_indirect_item_size, disk space free, count)
+	       2) construct new indirect item,
+	       3) fill new indirect item with new blocknrs using reiserfs_new_block_nrs 
+	       3) for each new blocknr, get_new_buffer, and write to that buffer
+	       4) replace old indirect item with new indirect item
+	       5) let this loop continue its work
+	       
+	       What do you think Volodya? -Hans
+	       */
+	    /* If we need to create an unformatted node. */
+            if ( dont_have_tails (p_s_sb) ||
+		 (n_orig_count >= MIN_PACK_ON_CLOSE) ||
+		 STORE_TAIL_IN_UNFM(n_append_startpoint - 1, n_zero_bytes + n_count, n_blk_size) ) {
+
+	      if (!STORE_TAIL_IN_UNFM(n_append_startpoint - 1, n_zero_bytes + n_count, n_blk_size) ) {
+		p_s_inode->u.reiserfs_i.i_pack_on_close = 1 ;
+	      } else {
+		p_s_inode->u.reiserfs_i.i_pack_on_close = 0 ;
+	      }
+              /* Calculate zeros to insert before write into unformatted node. */
+              n_item_zeros_to_add = ( n_zero_bytes < n_blk_size ) ? n_zero_bytes : n_blk_size;
+              /* Calculate the amount to write into unformatted node. */
+              n_item_bytes_to_write = ( n_item_zeros_to_add < n_blk_size ) ?
+                ( n_blk_size - n_item_zeros_to_add) : 0;
+              if ( n_item_bytes_to_write > n_count )
+                n_item_bytes_to_write = n_count;
+              /* If not making a hole. */
+              if ( n_item_bytes_to_write )  {
+                if ( ! p_s_un_bh )  {
+#ifdef OLD_GET_NEW_BUFFER		  
+		  n_repeat = get_new_buffer(&th, p_s_sb, p_s_bh, &p_s_un_bh, &s_search_path, p_s_inode, n_pos_in_file);
+#else
+                  if (n_unp) {
+                    n_repeat = get_new_buffer_near_blocknr(&th, p_s_sb, n_unp, &p_s_un_bh, &s_search_path);
+                  }
+                  else {
+		    n_repeat = get_buffer_near_last_unf (&th,p_s_sb,&s_key_to_search,p_s_inode, p_s_bh, &p_s_un_bh, &s_search_path);
+		  }
+#endif /* NEW_GET_NEW_BUFFER */  
+                  if ( ! p_s_un_bh )  {
+                    /* No disk space for new block. */
+                    if ( ! n_written )
+                      n_written = -ENOSPC;
+                    pathrelse(&s_search_path);
+                    break;
+                  }
+		  memset (p_s_un_bh->b_data, 0, n_blk_size);
+		  if ( n_repeat != CARRY_ON && (s_search_path.path_length == ILLEGAL_PATH_ELEMENT_OFFSET || comp_items(&s_ih, &s_search_path)) ) {
+		    /* compelte what was not finished in reiserfs_new_blocknrs */
+		    j = p_s_un_bh->b_blocknr / (p_s_sb->s_blocksize * 8);
+		    COMPLETE_BITMAP_DIRTING_AFTER_ALLOCATING (p_s_sb, j);
+		    continue;
+		  }
+                }
+
+		s_node_to_write.unfm_nodenum = p_s_un_bh->b_blocknr;
+		s_node_to_write.unfm_freespace = n_blk_size - n_item_zeros_to_add - n_item_bytes_to_write;
+              }
+              else  { /* If making a hole. */
+
+#ifdef CONFIG_REISERFS_CHECK
+		if ( p_s_un_bh ) {
+		  reiserfs_panic(p_s_sb, "PAP-14170: reiserfs_file_write: pointer to the unformatted node buffer must be equals NULL");
+		}
+#endif
+		s_node_to_write.unfm_nodenum = 0;
+		s_node_to_write.unfm_freespace = 0;
+              }
+
+#ifdef CONFIG_REISERFS_CHECK
+	      if ( n_append_startpoint < n_blk_size + 1 )
+		reiserfs_panic (p_s_sb, "PAP-14180: reiserfs_file_write: offset is 0");
+	      if ( p_s_ih->ih_item_len % UNFM_P_SIZE ) {
+		reiserfs_panic (p_s_sb, "PAP-14190: reiserfs_file_write: item %h length is incorrect", p_s_ih);
+	      }
+#endif
+              s_item_to_insert.ih_key.k_offset = n_append_startpoint;
+	      n_bytes_in_item = p_s_ih->ih_item_len / UNFM_P_SIZE;
+
+	      /* Paste entry for p_s_un_bh into last indirect item. */
+	      if ( reiserfs_paste_into_item(&th, p_s_sb, &s_search_path, &n_bytes_in_item, &(s_item_to_insert.ih_key),
+					    (char *)&s_node_to_write, UNFM_P_SIZE, REISERFS_KERNEL_MEM, 0) < 0 ) {
+		/* If no disk space for balancing required to insert entry for new unformatted
+		   node into last indirect item. */
+		if ( ! n_written )
+		  n_written = -ENOSPC;
+                if ( p_s_un_bh )  {
+		  reiserfs_free_block(&th, p_s_sb, p_s_un_bh->b_blocknr);
+		  j = p_s_un_bh->b_blocknr / (p_s_sb->s_blocksize * 8);
+		  COMPLETE_BITMAP_DIRTING_AFTER_FREEING(p_s_sb,j);
+
+                  bforget(p_s_un_bh);
+                  p_s_un_bh = NULL;
+                }
+                break;
+              }
+              if ( p_s_un_bh )	{ /* If not a hole. */
+		/* copy user data to the unformatted node */
+                //memset(p_s_un_bh->b_data, '\0', n_item_zeros_to_add);
+		copy_from_user(p_s_un_bh->b_data + n_item_zeros_to_add, p_c_buf, n_item_bytes_to_write);
+		//memset(p_s_un_bh->b_data + n_item_zeros_to_add + n_item_bytes_to_write, '\0', s_node_to_write.unfm_freespace);
+
+		update_vm_cache(p_s_inode, n_pos_in_file - 1,
+				p_s_un_bh->b_data + n_item_zeros_to_add, n_item_bytes_to_write);
+                mark_buffer_uptodate(p_s_un_bh,1);
+		/* non-atomic mark_buffer_dirty is allowed here */
+		/* mark_buffer_dirty(p_s_un_bh, 0); */
+		journal_mark_dirty_nolog(&th, p_s_sb, p_s_un_bh) ;
+		if (p_s_filp->f_flags & O_SYNC) {
+		  if (do_osync(&th, p_s_filp, p_s_un_bh, buffer_list, 
+				     REISERFS_NBUF, &buffer_count, 
+				     &n_blocks_flushed, &n_written,
+				     &n_tail_bytes_written) < 0) {
+		    pathrelse(&s_search_path);
+		    n_pos_in_file += n_item_bytes_to_write;
+		    break ;
+		  } 
+		} else {
+		  brelse(p_s_un_bh);
+		}
+                p_s_un_bh = NULL;
+
+		/* i_blocks counts only unformatted nodes */
+		p_s_inode->i_blocks += p_s_sb->s_blocksize / 512;
+	      }
+            }
+            else  { /* Insert direct item. */
+	      p_s_inode->u.reiserfs_i.i_pack_on_close = 0 ;
+
+	      n_item_bytes_to_write = n_count;
+	      n_item_zeros_to_add = n_zero_bytes;
+	      /* Create direct item header. */
+	      s_item_to_insert.ih_key.k_offset = n_append_startpoint;
+              /* Mark item as not mergeable. */
+	      s_item_to_insert.ih_key.k_uniqueness = TYPE_DIRECT;
+              /* Mark item as direct. */
+	      s_item_to_insert.u.ih_free_space = MAX_US_INT;
+	      s_item_to_insert.ih_item_len = n_zero_bytes + n_item_bytes_to_write;
+
+	      /* copy data from user space to intermediate buffer */
+	      copy_from_user (local_buf, p_c_buf, n_item_bytes_to_write);
+	      /* make sure, that direct item is still on its old place */
+	      if (comp_items (&s_ih, &s_search_path)) {
+		printk ("reiserfs_file_write: item has been moved while we were in copy_from_user (inserting the direct item after the last indirect)\n");
+		continue;
+	      }
+
+              /* reiserfs_insert_item() inserts before given position in the node, so we must
+		 increment to point to the next item after searched one. */
+	      PATH_LAST_POSITION(&s_search_path)++;
+	      if ( reiserfs_insert_item (&th, p_s_sb, &s_search_path, &s_item_to_insert,
+					 local_buf, REISERFS_KERNEL_MEM, n_zero_bytes, NOTHING_SPECIAL) < 0 )  {
+		if ( ! n_written )
+		  n_written = -ENOSPC;
+		break; /* No disk space. */
+              }
+	      
+	      n_tail_bytes_written += n_item_bytes_to_write ;
+	      update_vm_cache(p_s_inode, n_pos_in_file - 1, p_c_buf, n_item_bytes_to_write);
+
+	      /* calculate direct item as whole block */
+	      p_s_inode->i_blocks += p_s_sb->s_blocksize / 512;
+
+#ifdef CONFIG_REISERFS_CHECK
+	      if ( p_s_inode->u.reiserfs_i.i_first_direct_byte != NO_BYTES_IN_DIRECT_ITEM ||
+		   n_append_startpoint + n_zero_bytes != n_pos_in_file)
+		reiserfs_panic(p_s_sb, "PAP-14200: reiserfs_file_write: file must have no direct items");
+#endif
+
+	      p_s_inode->u.reiserfs_i.i_first_direct_byte = n_append_startpoint;
+            }
+          }
+	}
+      }
+    }
+
+    n_count       -= n_item_bytes_to_write;
+    p_c_buf       += n_item_bytes_to_write;
+    n_pos_in_file += n_item_bytes_to_write;
+    n_written     += n_item_bytes_to_write;
+
+    if ( (s_key_to_search.k_offset += n_item_bytes_to_write) >= p_s_inode->u.reiserfs_i.i_first_direct_byte )
+      s_key_to_search.k_uniqueness = TYPE_DIRECT;
+    else
+      s_key_to_search.k_uniqueness = TYPE_INDIRECT;
+
+    /* here we do a polite test to see if the journal needs a little more room.
+    ** if so, we write our inode to make sure it stays with this transaction, and give
+    ** the journal_end/begin pair the chance to end the current transaction
+    ** don't bother ending if we're already done writing
+    */
+    if (n_count > 0 && journal_transaction_should_end(&th, jbegin_count)) {
+      pathrelse(&s_search_path);
+
+      /* we want to release the buffer heads held in the buffer_list before
+      ** trying to end the transaction.
+      */
+      if ((p_s_filp->f_flags & O_SYNC) && 
+          (do_osync(&th, p_s_filp, NULL, buffer_list, 
+			   REISERFS_NBUF, &buffer_count, 
+			   &n_blocks_flushed, &n_written, 
+			   &n_tail_bytes_written) < 0)) {
+	break ;
+      }
+      update_inode_and_restart_transaction(&th, p_s_inode, n_pos_in_file) ;
+      if (p_s_filp->f_flags & O_SYNC) {
+	SB_JOURNAL(p_s_sb)->j_next_async_flush = 1 ;
+      }
+    }
+  }
+
+  if ( --n_pos_in_file > p_s_inode->i_size )  {
+    p_s_inode->i_size   = n_pos_in_file;
+    p_s_inode->i_ctime  = CURRENT_TIME;
+  }
+
+  p_s_inode->i_mtime  = CURRENT_TIME;
+  *p_n_pos = n_pos_in_file;
+
+  if (p_s_filp->f_flags & O_SYNC) {
+    do_osync(&th, p_s_filp, NULL, buffer_list, 
+	     REISERFS_NBUF, &buffer_count, &n_blocks_flushed, &n_written, 
+	     &n_tail_bytes_written) ;
+    /* adjust n_written to reflect actual number of bytes properly flushed
+    ** to disk.  We need to include the number of bytes written to the file
+    ** tail, as those blocks are not counted by do_osync.  do_osync 
+    ** sets n_tail_bytes_written to 0 on any failure, which means we might
+    ** adjust n_written too low here.  
+    ** 
+    ** So, we error on the side of caution, the only other way is to call 
+    ** do_osync before direct_to_indirect, which might schedule and force 
+    ** researching everything again.
+    */
+    if (n_written > 0 && (n_tail_bytes_written + n_blocks_flushed * 
+        p_s_sb->s_blocksize) < n_written) {
+      n_written = n_blocks_flushed * p_s_sb->s_blocksize + n_tail_bytes_written;
+      if (!n_written) {
+        n_written = -EIO ;
+      }
+    }
+  }
+
+  if_in_ram_update_sd (&th, p_s_inode); 
+
+  reiserfs_kfree (local_buf, n_blk_size, p_s_inode->i_sb);
+  pop_journal_writer(windex) ;
+  journal_end(&th, p_s_sb, jbegin_count) ;
+  return n_written;
+}
+
+
+
+/* Please note that the benchmarking of the right numbers for
+   RESIERFS_NBUF, etc., was insufficiently investigated.
+
+   Hans */
+
+
+
+/* Wait for and then release the read-ahead blocks. We need a brelse that does not wait. */
+#define RELEASE_READ_AHEAD_BLOCKS       while ( p_s_bhe != p_s_bhb ) {\
+                                                brelse(p_s_bhe->bi_buf);\
+                                                if ( ++p_s_bhe == &a_p_s_range_bufs_ids[REISERFS_NBUF] )\
+                                                        p_s_bhe = a_p_s_range_bufs_ids;\
+                                        }
+#define INCREASE_P_S_BHE		if ( ++p_s_bhe == &a_p_s_range_bufs_ids[REISERFS_NBUF] )\
+              					p_s_bhe = a_p_s_range_bufs_ids
+
+
+/* if Hans understands correctly this works by oscillating between a
+   request and a fulfill loop.  (It would be nice if Anatoly edited the
+   code to make it clearer as to its design objectives and algorithm,
+   as that might make it easier to see ways to simplify it).
+
+   The request loop assembles a list of not more than NBUF buffers
+   (buflist) which are within the range and which are cache children
+   (their parents are in cache, they are not).  It then requests I/O
+   on that list, and then the fulfill loop starts processing the list.
+
+   The fulfill loop goes through the list, and completes as much of
+   the read as it can.  If the fulfill loop processes a buffer whose
+   children contain data that needs to be read, then it oscillates
+   back to the request loop, which will then request not only those
+   new cache children which were children of the buffer that the
+   fulfill loop stopped on, but all cache children in the range.
+
+   The request loop works by calling get_buffer_by_range(), which
+   returns a buffer which either contains the node containing the
+   readkey, or which is prepared for requesting I/O on to get the
+   cache child corresponding to the readkey.  The readkey is
+   incremented as a result of each get_buffer_by_range so that it
+   holds the key of the next byte after the buffer that was returned
+   in the range.
+   
+   The reason for this algorithm is to allow one to read as much in
+   parallel as possible, and this is done by ensuring that there is an
+   outstanding request for all of the first NBUF cache children that
+   are in the range, and submitting new requests that contain more
+   cache children as soon as possible.  Question: what happens if
+   there are NBUF outstanding requests, the first of them completes,
+   it contains an indirect item with lots of buffers which must be
+   read, but bhreq is full?  Is the indirect item requested one buffer
+   at a time?  Or will all of the nodes on the request list get
+   processed, and bhreq gets freed up? Will we get a pathological
+   behaviour in which the disk head starts to move towards the other
+   nodes on the request list, but keeps getting dragged back to handle
+   one more node from the indirect item, each request for which is
+   separated by a rotation? Can we test to see if this ever
+   happens?  Maybe we can printk some blocknumbers? -Hans */
+
+static ssize_t  reiserfs_file_read(		/*  Read from file system into user buffer.		*/
+              struct file     *	p_s_filp,   	/*  Object table entry for this inode. ( p_s_filp->f_pos
+						    will provide us with the offset from which we 
+						    start the read, and we will update it to reflect
+						    how much we have read as we perform the read.)	*/
+              char 	      *	p_c_buf,	/*  Address of the user buffer.				*/
+              size_t		n_count,		/*  Count of bytes copied into user buffer.		*/
+	      loff_t	      * p_n_pos
+            ) {
+  ssize_t		n_read;			/*  Number of bytes which have been read.	*/
+#if 0
+  struct inode * p_s_inode = p_s_filp->f_dentry->d_inode;
+  struct super_block  *	p_s_sb;			/* Pointer to the super block.			*/
+  struct key            s_range_begin,		/*  Minimal range key to request.       	*/
+                        s_range_end,		/*  Maximal range key to request.       	*/
+                        s_readkey;		/*  Current read key,
+					    	    (the key version of offset )		*/
+  unsigned int		n_pos_in_file,		/* Current offset in the file.			*/
+			n_file_size,
+			n_left;			/*  Number of bytes remaining to read.		*/
+
+
+ 
+  int                   n_offset_in_item,       /*  Offset in unformatted node or direct item.  */
+                        n_chars,                /*  Number of bytes to copy.                    */
+#ifdef READ_LOCK_REISERFS
+                        this_syscall_has_the_read_lock = 0, /* flag to indicate whether this read syscall is the one that
+                                                        locked the FS, if so then don't worry about the FS being read
+                                                        locked */
+#endif /* READ_LOCK_REISERFS */
+                        n_blocksize;            /* Buffer size.                                 */
+
+  char                * p_c_addr = NULL;        /*  Address in a system buffer.                 */
+
+
+  struct buffer_head  * a_p_s_bhreq[REISERFS_NBUF],      /*  Array of the not uptodate buffers
+                                                    from  the read range.                       */
+                      * p_s_bh;
+  struct buffer_and_id a_p_s_range_bufs_ids[REISERFS_NBUF];      /*  Array of all buffers and ids from
+                                                            the read range.                     */
+
+                                /* it seems that bhb is used in the
+                                   request preparation to point to
+                                   where to insert the next buffer
+                                   onto bhreq, and bhe is used in the
+                                   post-request processing to go
+                                   through the array to do things with
+                                   every buffer that has completed its
+                                   requested I/O and is now uptodate
+                                   and unlocked.  The case in which
+                                   bhe = bhb represents the case in
+                                   which either all requests have
+                                   completed, or bhreq is completely
+                                   filled with requests uncompleted.
+				   
+				   Note that both bhb and bhe are
+				   allowed to wrap around the end of
+				   buflist.  This is necessary for
+				   when the read is larger than bhreq
+				   can hold.  -Hans */
+  struct buffer_and_id	      *	p_s_bhb,	/*  We need two variables to go through		*/
+    			      *	p_s_bhe;	/*  array a_p_s_range_bufs_ids.                 */
+  int                   	n_bhrequest,	/* offset in the array a_p_s_bhreq.		*/
+                        	n_uptodate;
+  char * local_buf;		/* copy_to_user can cause schedule. Therefore we can not copy bytes
+				   directly from direct item to user buffer. Local_buf is used as
+				   intermediate buffer */
+
+#ifdef CONFIG_REISERFS_CHECK
+  int				n_repeat_counter = 0;
+#endif
+#endif
+
+
+  n_read = generic_file_read (p_s_filp, p_c_buf, n_count, p_n_pos);
+  return n_read;
+
+#if 0
+  if ( ! p_s_inode ) {
+    printk("reiserfs_file_read: pointer to the inode = NULL\n");
+    return -EINVAL;
+  }
+
+  if ( ! S_ISREG(p_s_inode->i_mode) && ! S_ISLNK(p_s_inode->i_mode) ) {
+    printk("reiserfs_file_read: mode = %07o\n",p_s_inode->i_mode);
+    return -EINVAL;
+  }
+
+  /* Calculate position in the file. */
+  n_pos_in_file = *p_n_pos + 1 ;
+
+  /* Calculate object size. */
+  n_file_size = p_s_inode->i_size;
+
+  /* Using position in the file, file size, and the given number of bytes to read
+     calculate the number of bytes, that should be actually read;
+     put it in variable n_left. */
+  if ( n_pos_in_file > n_file_size || n_count <= 0 ) /* Nothing to read. */
+    return 0;
+
+  increment_i_read_sync_counter(p_s_inode);
+
+  n_left = n_file_size - n_pos_in_file + 1;
+  if ( n_left > n_count )
+    n_left = n_count;
+  n_read = 0;
+
+  p_s_sb = p_s_inode->i_sb;
+
+  /* Initialize read range. */
+  copy_key(&s_range_begin, &(p_s_inode->u.reiserfs_i.i_key));
+  s_range_begin.k_offset = n_pos_in_file;
+  if ( INODE_OFFSET_IN_DIRECT(p_s_inode, n_pos_in_file) )
+    s_range_begin.k_uniqueness = TYPE_DIRECT;
+  else
+    s_range_begin.k_uniqueness = TYPE_INDIRECT;
+
+  copy_key(&s_range_end, &(p_s_inode->u.reiserfs_i.i_key));
+  s_range_end.k_offset = n_pos_in_file + n_left - 1;
+  if ( INODE_OFFSET_IN_DIRECT(p_s_inode, s_range_end.k_offset) )
+    s_range_end.k_uniqueness = TYPE_DIRECT;
+  else
+    s_range_end.k_uniqueness = TYPE_INDIRECT;
+
+#ifdef REISERFS_OBJECT_READ_AHEAD
+  s_range_end.k_offset = n_file_size;
+  s_range_end.k_uniqueness = TYPE_DIRECT;
+#endif
+
+#ifdef PACKING_LOCALITY_READ_AHEAD
+  s_range_end.k_objectid = MAX_KEY_OBJECTID;
+  s_range_end.k_offset = MAX_KEY_OFFSET;
+  s_range_end.k_uniqueness = MAX_KEY_UNIQUENESS;
+#endif
+
+  /* Set current key to read . */
+  copy_key(&s_readkey, &s_range_begin);
+
+  p_s_bhb = p_s_bhe = a_p_s_range_bufs_ids;
+
+  n_blocksize = p_s_sb->s_blocksize;
+
+  local_buf = reiserfs_kmalloc (n_blocksize, GFP_KERNEL, p_s_sb);
+  if (local_buf == 0) {
+    return -ENOMEM;
+  }
+
+  /* Here is the loop to cause us to oscillate between requesting and fulfilling */
+  do {
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( ! (++n_repeat_counter % 50000) ) {
+      reiserfs_panic(p_s_sb, "PAP-14205: reiserfs_fileread: counter(%d) too big. Range begin %k",
+		     n_repeat_counter, &s_range_begin);
+    }
+#endif
+
+    n_bhrequest = 0;
+
+    /* This says no, there are not any buffers that we are waiting for I/O to complete for. -Hans */
+    n_uptodate = 1;
+
+    if ( p_s_bhb == p_s_bhe && COMP_KEYS(&s_range_begin, &s_range_end) == 1 ) {
+
+#ifdef REISERFS_INFO
+      printk("reiserfs_fileread: request key is not in the range and request is empty but there are bytes to read \n");
+#endif
+
+      copy_key(&s_range_begin, &s_readkey);
+    }
+
+    /* Request loop (well, actually the prepare to request loop) */
+    /* This while loop assembles a request array (bhreq) which contains either a single buffer which does not require
+       I/O to fetch and which we will proceed to read into the user buffer immediately or enough cache children in the
+       range to fill bhreq so that we can get REISERFS_NBUF buffers all at once.  -Hans */
+    while ( COMP_KEYS(&s_range_begin, &s_range_end) != 1 )  { /* While current key is in the range. */
+
+      /* Calculate next buffer from range. This returns either a
+         buffer that was in cache, or a buffer all set for us to
+         request I/O on. */
+      get_buffer_by_range(p_s_sb, &s_range_begin, &s_range_end, &(p_s_bhb->bi_buf), &(p_s_bhb->bi_id));
+
+      /* Buffer is not uptodate (in other words, it wasn't in cache
+         and we need to read it). Put it in the request array. */
+      if ( p_s_bhb->bi_buf && ! buffer_uptodate(p_s_bhb->bi_buf) ) {
+        n_uptodate = 0;
+        a_p_s_bhreq[n_bhrequest++] = p_s_bhb->bi_buf;
+      }
+
+      /* Increment, and possibly wraparound to beginning of list */
+      if ( ++p_s_bhb == &a_p_s_range_bufs_ids[REISERFS_NBUF] )
+        p_s_bhb = a_p_s_range_bufs_ids;
+
+      /* If all of the buffers we have processed so far in this pass
+         of the loop are uptodate (in which case there is only one
+         such buffer) then go ahead and copy its contents to user
+         space now rather than assembling more buffers. */
+
+      if ( n_uptodate )
+        break;
+      if ( p_s_bhb == p_s_bhe )
+        break;
+    }
+
+#ifdef CONFIG_REISERFS_CHECK
+    /* Check whether buffers from the request have valid device. */
+    for ( n_chars = 0; n_chars < n_bhrequest; n_chars++ )
+      if ( a_p_s_bhreq[n_chars]->b_dev == NODEV )
+	reiserfs_panic(p_s_sb, "PAP-14210: reiserfs_file_read: device is NODEV");
+#endif
+
+    /* Now request them all. */
+    if ( n_bhrequest ) {
+#ifdef READ_LOCK_REISERFS
+                                /* So why read_lock the FS?  Because serial reads are more efficient than parallel
+                                   reads, substantially so say the benchmarks.  now you might ask, why not wait on the
+                                   lock?  The reason is that I have an untested hope that it will cause a series of
+                                   large reads from the same process succeeding in its lock once to tend to get
+                                   priority.  It is deliberately unfair.  Don't go moving that disk head.... -Hans */
+      while (!try_ulong_lock(&(p_s_sb->u.reiserfs_sb.read_lock), 0) && !this_syscall_has_the_read_lock)
+        {
+/*        printk("blocked for lock %lu:", p_s_sb->u.reiserfs_sb.read_lock); */
+          schedule();
+          /*  don't know if schedule can invalidate what we are reading, or if the read picks up all the pieces properly
+              when this is done. -Hans */
+        }
+      this_syscall_has_the_read_lock = 1;
+#endif /* READ_LOCK_REISERFS */
+      ll_rw_block(READ, n_bhrequest, a_p_s_bhreq);
+    }
+
+    /* fulfillment loop */
+    /* Finish off all I/O that has actually completed. */
+    do {
+      /* Check to see if read error occured. In this case we break read function.  */
+      if ( (p_s_bh = p_s_bhe->bi_buf) ) {
+        wait_on_buffer(p_s_bh);
+        if ( ! buffer_uptodate(p_s_bh) ) {
+          brelse(p_s_bh);
+	  INCREASE_P_S_BHE;
+          n_left = 0;
+	  printk ("reiserfs_file_read: I/O error (block %lu, dev 0%o, size %ld)\n", p_s_bh->b_blocknr, p_s_bh->b_dev, p_s_bh->b_size);
+          break;
+        }
+      }
+
+      /* If buffer is not in tree, or is key level, then repeat buffer calculating.  Buffer is not in tree means that
+        some balancing occured while we were waiting for the needed buffer or getting next buffer from range. This
+        balancing removed needed buffer from the tree.  It is possible for all tree levels. It is not possible just for
+        unformatted nodes.  If after waiting we have internal node buffer we can not read from it(it does not contain
+        data).  In both cases we call get_buffer_from range once more to get bytes for read. -Anatoly.  */
+
+      /* ok, so if we now have a formatted node, check to confirm that
+         we got the right one, then copy our data to user space,
+         checking as we copy to see if we need to descend into any
+         unformatted node children.  If we so need, then we better
+         oscillate back to the request loop to read those children
+         into memory (and while in that request loop we might as well
+         try to read everything else in the range that we can....)
+         -Hans */
+      if ( p_s_bh && p_s_bhe->bi_id == MAX_KEY_OBJECTID ) {
+	if ( ! B_IS_IN_TREE(p_s_bh) || ! B_IS_ITEMS_LEVEL(p_s_bh) ) {
+	  /* We are repeating the read starting from the current s_readkey */   
+          brelse(p_s_bh);
+          INCREASE_P_S_BHE;
+          RELEASE_READ_AHEAD_BLOCKS;
+          copy_key(&s_range_begin, &s_readkey);
+          p_s_bhb = p_s_bhe = a_p_s_range_bufs_ids;
+          break;
+        }
+
+        /* Needed byte should be in the leaf we were waiting for */
+        if ( COMP_KEYS(B_N_PKEY(p_s_bh, 0), &s_readkey) < 1 &&
+	     COMP_KEYS(B_PRIGHT_DELIM_KEY(p_s_bh), &s_readkey) == 1 ) {
+
+          int 			n_search_res,
+				n_item_pos;
+	  struct item_head    *	p_s_ih;
+
+          /* Find item contains needed byte. */
+          n_search_res = bin_search(&s_readkey, B_N_PITEM_HEAD(p_s_bh, 0), B_NR_ITEMS(p_s_bh), IH_SIZE, &n_item_pos);
+          p_s_ih = B_N_PITEM_HEAD(p_s_bh, n_item_pos);
+          /* We are looking for an item contains needed byte of the needed object. In case of n_search_res = 0 it can
+             not be *p_s_ih. Probably *(p_s_ih--) it is. We are checking it. */
+          if ( n_search_res == ITEM_NOT_FOUND )
+            p_s_ih--;
+
+	  /* error checking: if ih does not contain the byte corresponding to readkey -Hans */
+	  if ( ! I_K_KEY_IN_ITEM(p_s_ih, &s_readkey, n_blocksize) ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+            printk ("reiserfs_file_read: can not read bytes (file was truncated or deleted)\n");
+#endif
+
+            brelse(p_s_bh);
+	    INCREASE_P_S_BHE;
+            n_left = 0;
+            break;
+          }
+
+          /* If needed byte is located in an unformatted node then oscillate
+             back to request loop so that it will be gotten for us,
+             but only do so after waiting for completion of all
+             read_ahead blocks. */
+          if ( I_IS_INDIRECT_ITEM(p_s_ih) ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+	    if ( s_readkey.k_uniqueness != TYPE_INDIRECT )
+	      reiserfs_panic(p_s_sb, "PAP-14240: reiserfs_file_read: invalid uniqueness in the read key");
+#endif
+
+            brelse(p_s_bh);
+            INCREASE_P_S_BHE;
+                                /* waits for as well as releases */
+            RELEASE_READ_AHEAD_BLOCKS;
+            copy_key(&s_range_begin, &s_readkey);
+            p_s_bhb = p_s_bhe = a_p_s_range_bufs_ids;
+            break;
+          }
+
+#ifdef CONFIG_REISERFS_CHECK
+	  if ( s_readkey.k_uniqueness != TYPE_DIRECT )
+	    reiserfs_panic(p_s_sb, "PAP-14250: reiserfs_file_read: invalid uniqueness in the read key");
+#endif
+
+	  /* So, there are bytes to copy from the direct item. */
+          n_offset_in_item = n_pos_in_file - p_s_ih->ih_key.k_offset;
+          n_chars = p_s_ih->ih_item_len - n_offset_in_item;
+          p_c_addr  = B_I_PITEM(p_s_bh, p_s_ih) + n_offset_in_item;
+	}
+        else {
+
+#ifdef CONFIG_REISERFS_CHECK
+          printk("reiserfs_file_read: key is not in the buffer\n");
+#endif
+
+          brelse(p_s_bh);
+	  INCREASE_P_S_BHE;
+          RELEASE_READ_AHEAD_BLOCKS;
+
+          copy_key(&s_range_begin, &s_readkey);
+          p_s_bhb = p_s_bhe = a_p_s_range_bufs_ids;
+          break;
+        }
+      }
+
+      else {
+	/* We waited for an unformatted node. Read from it. */
+	if ( p_s_bh ||  p_s_bhe->bi_id != MAX_KEY_OBJECTID ) {
+
+	  if ( p_s_bhe->bi_id != s_readkey.k_objectid ) {
+	    
+#ifdef CONFIG_REISERFS_CHECK
+	    /*printk("reiserfs_file_read: can not read bytes(3) (file was truncated or deleted)\n");*/
+#endif
+            
+            brelse(p_s_bh);
+            INCREASE_P_S_BHE;
+            n_left = 0;
+            
+            break;
+          }
+          
+#ifdef CONFIG_REISERFS_CHECK
+          if ( s_readkey.k_uniqueness != TYPE_INDIRECT ) {
+            print_block(p_s_bh, 0, -1, -1);
+            printk("size = %ld, first_direct = %d\n", p_s_inode->i_size, p_s_inode->u.reiserfs_i.i_first_direct_byte);
+            printk ("p_s_bhe->bi_id==%lu\n", p_s_bhe->bi_id);
+            reiserfs_panic(p_s_sb, "PAP-14270: reiserfs_file_read: invalid uniqueness in the read key %k. TYPE_INDIRECT expected",
+			   &s_readkey);
+          }
+#endif
+
+	}
+
+        /* Calculate offset in the unformatted node. */
+        n_offset_in_item = (n_pos_in_file - 1) % n_blocksize;
+        n_chars = n_blocksize - n_offset_in_item;
+        if ( p_s_bh )
+          p_c_addr = n_offset_in_item + p_s_bh->b_data;
+      }
+
+      if ( n_chars > n_left )
+        n_chars = n_left;
+      *p_n_pos += n_chars ; /* p_s_filp->f_pos += n_chars; */
+      n_left -= n_chars;
+      n_read += n_chars;
+      n_pos_in_file += n_chars;
+      /* Here is one place where we reset readkey so that the next
+         buffer is gotten on the next loop iteration.. */
+      s_readkey.k_offset = n_pos_in_file;
+      if ( n_pos_in_file >= p_s_inode->u.reiserfs_i.i_first_direct_byte )
+	s_readkey.k_uniqueness = TYPE_DIRECT;
+
+#ifdef CONFIG_REISERFS_CHECK
+      if ( n_chars < 0 || n_chars > n_blocksize )
+	reiserfs_panic(p_s_sb, "PAP-14280: reiserfs_file_read: illegal bytes number to read");
+#endif
+
+      if (p_s_bhe->bi_id != MAX_KEY_OBJECTID) {
+	/* when copying bytes from an unformatted node, we do not need intermediate buffer */
+	if ( p_s_bh ) {
+	  copy_to_user(p_c_buf, p_c_addr, n_chars);
+	  brelse(p_s_bh);
+	  p_c_buf += n_chars;
+	} else {
+	  while ( n_chars-- > 0 )
+	    if ( put_user(0, p_c_buf++) )
+	      reiserfs_panic(p_s_sb, "PAP-14290: reiserfs_file_read: put_user failed");
+	}
+      } else {
+	/* Copy bytes from direct item into intermediate buffer. */
+	if ( p_s_bh ) {
+	  memcpy (local_buf, p_c_addr, n_chars);
+	  brelse(p_s_bh);
+	} else {
+	  memset (local_buf, 0, n_chars);
+	}
+
+	/* copy bytes from intermediate buffer to the user buffer */
+	copy_to_user(p_c_buf, local_buf, n_chars);
+	p_c_buf += n_chars;
+      }
+
+      INCREASE_P_S_BHE;
+    } while ( n_left > 0 && p_s_bhe != p_s_bhb && (! p_s_bhe->bi_buf || ! buffer_locked(p_s_bhe->bi_buf)) ) ;
+  } while ( n_left > 0 );
+
+  RELEASE_READ_AHEAD_BLOCKS;
+
+  decrement_i_read_sync_counter(p_s_inode);
+
+#ifdef READ_LOCK_REISERFS
+  unlock_ulong_lock(&(p_s_sb->u.reiserfs_sb.read_lock), 0,  &(p_s_sb->u.reiserfs_sb.read_wait));
+ /*  printk("unlocked lock %lu:", p_s_sb->u.reiserfs_sb.read_lock); */
+#endif /* READ_LOCK_REISERFS */
+
+  reiserfs_kfree (local_buf, n_blocksize, p_s_sb);
+  if ( ! n_read ) {
+    return -EIO;
+  }
+  UPDATE_ATIME(p_s_inode);
+
+  return n_read;
+
+#endif
+}
+
+
diff -urN linux/fs/reiserfs/fix_node.c /tmp/linux/fs/reiserfs/fix_node.c
--- linux/fs/reiserfs/fix_node.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/fix_node.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,2983 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+/**
+ ** old_item_num
+ ** old_entry_num
+ ** set_entry_sizes
+ ** create_virtual_node
+ ** check_left
+ ** check_right
+ ** directory_part_size
+ ** get_num_ver
+ ** item_length
+ ** set_parameters
+ ** is_leaf_removable
+ ** are_leaves_removable
+ ** get_empty_nodes
+ ** get_lfree
+ ** get_rfree
+ ** is_left_neighbor_in_cache
+ ** decrement_key
+ ** get_far_parent
+ ** get_parents
+ ** can_node_be_removed
+ ** ip_check_balance
+ ** dc_check_balance_internal
+ ** dc_check_balance_leaf
+ ** dc_check_balance
+ ** check_balance
+ ** get_direct_parent
+ ** get_neighbors
+ ** fix_nodes
+ ** 
+ ** 
+ **/
+
+
+#ifdef __KERNEL__
+
+#include <linux/sched.h>
+#include <linux/string.h>
+#include <linux/reiserfs_fs.h>
+
+#else
+
+#include "nokernel.h"
+
+#endif
+
+
+#define ROUND_UP(x,n) (((x)+(n)-1u) & ~((n)-1u))
+
+
+/* To make any changes in the tree we find a node, that contains item
+   to be changed/deleted or position in the node we insert a new item
+   to. We call this node S. To do balancing we need to decide what we
+   will shift to left/right neighbor, or to a new node, where new item
+   will be etc. To make this analysis simpler we build virtual
+   node. Virtual node is an array of items, that will replace items of
+   node S. (For instance if we are going to delete an item, virtual
+   node does not contain it). Virtual node keeps information about
+   item sizes and types, mergeability of first and last items, sizes
+   of all entries in directory item. We use this array of items when
+   calculating what we can shift to neighbors and how many nodes we
+   have to have if we do not any shiftings, if we shift to left/right
+   neighbor or to both. */
+
+
+/* taking item number in virtual node, returns number of item, that it has in source buffer */
+static inline int old_item_num (int new_num, int affected_item_num, int mode)
+{
+  if (mode == M_PASTE || mode == M_CUT || new_num < affected_item_num)
+    return new_num;
+
+  if (mode == M_INSERT) {
+
+#ifdef CONFIG_REISERFS_CHECK
+    if (new_num == 0)
+      reiserfs_panic (0,"vs-8005: old_item_num: for INSERT mode and item number of inserted item");
+#endif
+
+    return new_num - 1;
+  }
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (mode != M_DELETE)
+      reiserfs_panic (0, "vs-8010: old_item_num: mode must be M_DELETE (mode = \'%c\'", mode);
+#endif
+
+  /* delete mode */
+  return new_num + 1;
+}
+
+
+/*
+ * function returns old entry number in directory item in real node
+ * using new entry number in virtual item in virtual node */
+static inline int old_entry_num (int new_num, int affected_item_num, int new_entry_num, int pos_in_item, int mode)
+{
+  if ( mode == M_INSERT || mode == M_DELETE)
+    return new_entry_num;
+
+  if (new_num != affected_item_num) {
+    /* cut or paste is applied to another item */
+    return new_entry_num;
+  }
+
+  if (new_entry_num < pos_in_item)
+    return new_entry_num;
+
+  if (mode == M_CUT)
+    return new_entry_num + 1;
+
+#ifdef CONFIG_REISERFS_CHECK  
+  if (mode != M_PASTE)
+      reiserfs_panic (0, "vs-8015: old_entry_num: mode must be M_PASTE (mode = \'%c\'", mode);
+#endif
+
+  return new_entry_num - 1;
+}
+
+
+
+/*
+ * Create an array of sizes of directory entries for virtual item
+ */
+static void set_entry_sizes (struct tree_balance * tb,
+			     int old_num, int new_num,
+			     struct buffer_head * bh,
+			     struct item_head * ih
+			     )
+{
+  struct virtual_node * vn = tb->tb_vn;
+  int i;
+  struct reiserfs_de_head * deh;
+  struct virtual_item * vi;
+  
+  deh = B_I_DEH (bh, ih);
+
+  /* seek to given virtual item in array of virtual items */
+  vi = vn->vn_vi + new_num;
+
+  /* virtual directory item have this amount of entry after */
+  vi->vi_entry_count = I_ENTRY_COUNT (ih) + 
+    ((old_num == vn->vn_affected_item_num) ? ((vn->vn_mode == M_CUT) ? -1 :
+					      (vn->vn_mode == M_PASTE ? 1 : 0)) : 0);
+
+#ifdef CONFIG_REISERFS_CHECK
+  /* check whether we have enough space for array of entry sizes */
+  if (tb->vn_buf + tb->vn_buf_size - vn->vn_free_ptr < vi->vi_entry_count * sizeof (__u16))
+    reiserfs_panic (tb->tb_sb, "vs-8020: set_entry_sizes: "
+		    "no enough space for %d entries of virtual item", vi->vi_entry_count);
+#endif
+
+  vi->vi_entry_sizes = (__u16 *)vn->vn_free_ptr;
+  vn->vn_free_ptr += vi->vi_entry_count * sizeof (__u16);
+
+  /* set sizes of old entries */
+  for (i = 0; i < vi->vi_entry_count; i ++) {
+    int j;
+    
+    j = old_entry_num (old_num, vn->vn_affected_item_num, i, vn->vn_pos_in_item, vn->vn_mode);
+    vi->vi_entry_sizes[i] = I_DEH_N_ENTRY_LENGTH (ih, &(deh[j]), j) + DEH_SIZE;
+  }
+  
+  /* set size of pasted entry */
+  if (old_num == vn->vn_affected_item_num && vn->vn_mode == M_PASTE)
+    vi->vi_entry_sizes[vn->vn_pos_in_item] = tb->insert_size[0];
+
+
+#ifdef CONFIG_REISERFS_CHECK
+  /* compare total size of entries with item length */
+  {
+    int k, l;
+    
+    l = 0;
+    for (k = 0; k < vi->vi_entry_count; k ++)
+      l += vi->vi_entry_sizes[k];
+    
+    if (l + IH_SIZE != vi->vi_item_len + 
+	((old_num == vn->vn_affected_item_num && (vn->vn_mode == M_PASTE || vn->vn_mode == M_CUT)) ? tb->insert_size[0] : 0) ) {
+      reiserfs_panic (0, "vs-8025: set_entry_sizes: (mode==%c, old_num==%d, aff_num==%d, insert_size==%d), invalid length of directory item",
+		      vn->vn_mode, old_num, vn->vn_affected_item_num, tb->insert_size[0]);
+    }
+  }
+#endif
+
+}
+
+
+static void create_virtual_node (struct tree_balance * tb, int h)
+{
+  struct item_head * ih;
+  struct virtual_node * vn = tb->tb_vn;
+  int new_num;
+  struct buffer_head * Sh;	/* this comes from tb->S[h] */
+
+  Sh = PATH_H_PBUFFER (tb->tb_path, h);
+
+  /* size of changed node */
+  vn->vn_size = MAX_CHILD_SIZE (Sh) - B_BLK_HEAD (Sh)->blk_free_space + tb->insert_size[h];
+
+  /* for internal nodes array if virtual items is not created */
+  if (h) {
+    vn->vn_nr_item = (vn->vn_size - DC_SIZE) / (DC_SIZE + KEY_SIZE);
+    return;
+  }
+
+  /* number of items in virtual node  */
+  vn->vn_nr_item = B_NR_ITEMS (Sh) + ((vn->vn_mode == M_INSERT)? 1 : 0) - ((vn->vn_mode == M_DELETE)? 1 : 0);
+
+  /* first virtual item */
+  vn->vn_vi = (struct virtual_item *)(tb->tb_vn + 1);
+  memset (vn->vn_vi, 0, vn->vn_nr_item * sizeof (struct virtual_item));
+  vn->vn_free_ptr += vn->vn_nr_item * sizeof (struct virtual_item);
+
+
+  /* first item in the node */
+  ih = B_N_PITEM_HEAD (Sh, 0);
+
+  /* define the mergeability for 0-th item (if it is not being deleted) */
+#ifdef REISERFS_FSCK
+  if (is_left_mergeable (tb->tb_sb, tb->tb_path) == 1 && (vn->vn_mode != M_DELETE || vn->vn_affected_item_num))
+#else
+  if (is_left_mergeable (ih, Sh->b_size) && (vn->vn_mode != M_DELETE || vn->vn_affected_item_num))
+#endif
+    vn->vn_vi[0].vi_type |= VI_TYPE_LEFT_MERGEABLE;
+
+  /* go through all items those remain in the virtual node (except for the new (inserted) one) */
+  for (new_num = 0; new_num < vn->vn_nr_item; new_num ++) {
+    int j;
+    
+    if (vn->vn_affected_item_num == new_num && vn->vn_mode == M_INSERT)
+      continue;
+    
+    /* get item number in source node */
+    j = old_item_num (new_num, vn->vn_affected_item_num, vn->vn_mode);
+    
+    vn->vn_vi[new_num].vi_item_len += ih[j].ih_item_len + IH_SIZE;
+    
+    if (I_IS_STAT_DATA_ITEM (ih + j)) {
+      vn->vn_vi[new_num].vi_type |= VI_TYPE_STAT_DATA;
+
+#ifdef CONFIG_REISERFS_CHECK
+      if (new_num == vn->vn_affected_item_num && (vn->vn_mode == M_CUT || vn->vn_mode == M_PASTE))
+	reiserfs_panic (0, "vs-8035: create_virtual_node: stat data cannot be affected item");
+#endif
+
+      continue;
+    }
+
+    /* set type of item */
+    if (I_IS_DIRECT_ITEM (ih + j))
+      vn->vn_vi[new_num].vi_type |= VI_TYPE_DIRECT;
+    
+    if (I_IS_INDIRECT_ITEM (ih + j))
+      vn->vn_vi[new_num].vi_type |= VI_TYPE_INDIRECT;
+
+    if (I_IS_DIRECTORY_ITEM (ih + j)) {
+      set_entry_sizes (tb, j, new_num, Sh, ih + j);
+      vn->vn_vi[new_num].vi_type |= VI_TYPE_DIRECTORY;
+      if (ih[j].ih_key.k_offset == DOT_OFFSET)
+	vn->vn_vi[new_num].vi_type |= VI_TYPE_FIRST_DIRECTORY_ITEM;
+    }
+    
+    if (new_num != vn->vn_affected_item_num)
+      /* this is not being changed */
+      continue;
+    
+    if (vn->vn_mode == M_PASTE || vn->vn_mode == M_CUT)
+      vn->vn_vi[new_num].vi_item_len += tb->insert_size[0];
+  }
+  
+  
+  /* virtual inserted item is not defined yet */
+  if (vn->vn_mode == M_INSERT) {
+      
+#ifdef CONFIG_REISERFS_CHECK
+    if (vn->vn_ins_ih == 0)
+      reiserfs_panic (0, "vs-8040: create_virtual_node: item header of inserted item is not specified");
+#endif
+
+    vn->vn_vi[vn->vn_affected_item_num].vi_item_len = tb->insert_size[0];
+    
+    switch (vn->vn_ins_ih->ih_key.k_uniqueness) {
+    case TYPE_STAT_DATA:
+      vn->vn_vi[vn->vn_affected_item_num].vi_type |= VI_TYPE_STAT_DATA;
+      break;
+    case TYPE_DIRECT:
+      vn->vn_vi[vn->vn_affected_item_num].vi_type |= VI_TYPE_DIRECT;
+      break;
+    case TYPE_INDIRECT:
+      vn->vn_vi[vn->vn_affected_item_num].vi_type |= VI_TYPE_INDIRECT;
+      break;
+    default:
+      /* inseted item is directory (it must be item with "." and "..") */
+      vn->vn_vi[vn->vn_affected_item_num].vi_type |= 
+	(VI_TYPE_DIRECTORY | VI_TYPE_FIRST_DIRECTORY_ITEM | VI_TYPE_INSERTED_DIRECTORY_ITEM);
+      
+      /* this directory item can not be split, so do not set sizes of entries */
+      break;
+    }
+  }
+  
+  /* set right merge flag we take right delimiting key and check whether it is a mergeable item */
+  if (tb->CFR[0]) {
+    ih = (struct item_head *)B_N_PDELIM_KEY (tb->CFR[0], tb->rkey[0]);
+#ifdef REISERFS_FSCK
+    if (is_right_mergeable (tb->tb_sb, tb->tb_path) == 1 && (vn->vn_mode != M_DELETE ||
+							     vn->vn_affected_item_num != B_NR_ITEMS (Sh) - 1))
+#else
+    if (is_left_mergeable (ih, Sh->b_size) && (vn->vn_mode != M_DELETE ||
+					       vn->vn_affected_item_num != B_NR_ITEMS (Sh) - 1))
+#endif
+      vn->vn_vi[vn->vn_nr_item-1].vi_type |= VI_TYPE_RIGHT_MERGEABLE;
+
+#ifdef CONFIG_REISERFS_CHECK
+    if (is_left_mergeable (ih, Sh->b_size) &&
+	!(vn->vn_mode != M_DELETE || vn->vn_affected_item_num != B_NR_ITEMS (Sh) - 1) ) {
+      /* we delete last item and it could be merged with right neighbor's first item */
+      if (!(B_NR_ITEMS (Sh) == 1 && I_IS_DIRECTORY_ITEM (B_N_PITEM_HEAD (Sh, 0)) &&
+	    I_ENTRY_COUNT (B_N_PITEM_HEAD (Sh, 0)) == 1)) {
+	/* node contains more than 1 item, or item is not directory item, or this item contains more than 1 entry */
+	print_block (Sh, 0, -1, -1);
+	reiserfs_panic (tb->tb_sb, "vs-8045: create_virtual_node: rdkey %k, affected item==%d (mode==%c) Must be %c", 
+			&(ih->ih_key), vn->vn_affected_item_num, vn->vn_mode, M_DELETE);
+      } else
+	/* we can delete directory item, that has only one directory entry in it */
+	;
+    }
+#endif
+    
+  }
+}
+
+
+/* using virtual node check, how many items can be shifted to left
+   neighbor */
+static  int check_left (struct tree_balance * tb, int h, int cur_free)
+{
+  int i;
+  struct virtual_node * vn = tb->tb_vn;
+  int d_size, ih_size, bytes = -1;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (cur_free < 0)
+    reiserfs_panic (0, "vs-8050: check_left: cur_free (%d) < 0", cur_free);
+#endif
+
+  /* internal level */
+  if (h > 0) {	
+    if (!cur_free ) {
+      tb->lnum[h] = 0; 
+      return 0;
+    }
+    tb->lnum[h] = cur_free / (DC_SIZE + KEY_SIZE);
+    return -1;
+  }
+
+  /* leaf level */
+
+  if (!cur_free || !vn->vn_nr_item) {
+    /* no free space */
+    tb->lnum[h] = 0;
+    tb->lbytes = -1;
+    return 0;
+  }
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (!PATH_H_PPARENT (tb->tb_path, 0))
+    reiserfs_panic (0, "vs-8055: check_left: parent does not exist or invalid");
+#endif
+
+  if ((unsigned int)cur_free >= (vn->vn_size - ((vn->vn_vi[0].vi_type & VI_TYPE_LEFT_MERGEABLE) ? IH_SIZE : 0))) {
+    /* all contents of S[0] fits into L[0] */
+
+#ifdef CONFIG_REISERFS_CHECK
+    if (vn->vn_mode == M_INSERT || vn->vn_mode == M_PASTE)
+      reiserfs_panic (0, "vs-8055: check_left: invalid mode or balance condition failed");
+#endif
+
+    tb->lnum[0] = vn->vn_nr_item;
+    tb->lbytes = -1;
+    return -1;
+  }
+  
+
+  d_size = 0, ih_size = IH_SIZE;
+
+  /* first item may be merge with last item in left neighbor */
+  if (vn->vn_vi[0].vi_type & VI_TYPE_LEFT_MERGEABLE)
+    d_size = -((int)IH_SIZE), ih_size = 0;
+
+  tb->lnum[0] = 0;
+  for (i = 0; i < vn->vn_nr_item; i ++, ih_size = IH_SIZE, d_size = 0) {
+    d_size += vn->vn_vi[i].vi_item_len;
+    if (cur_free >= d_size) {	
+      /* the item can be shifted entirely */
+      cur_free -= d_size;
+      tb->lnum[0] ++;
+      continue;
+    }
+      
+    /* the item cannot be shifted entirely, try to split it */
+    /* check whether L[0] can hold ih and at least one byte of the item body */
+    if (cur_free <= ih_size) {
+      /* cannot shift even a part of the current item */
+      tb->lbytes = -1;
+      return -1;
+    }
+    cur_free -= ih_size;
+    
+    if (vn->vn_vi[i].vi_type & VI_TYPE_STAT_DATA ||
+	vn->vn_vi[i].vi_type & VI_TYPE_INSERTED_DIRECTORY_ITEM)	{
+      /* virtual item is a stat_data or empty directory body ("." and ".."), that is not split able */
+      tb->lbytes = -1;
+      return -1;
+    }
+    
+    if (vn->vn_vi[i].vi_type & VI_TYPE_DIRECT)
+      /* body of a direct item can be split at any byte */
+      tb->lbytes = bytes = cur_free;
+    
+    if (vn->vn_vi[i].vi_type & VI_TYPE_INDIRECT)
+      /* body of a indirect item can be split at unformatted pointer bound */
+      tb->lbytes = bytes = cur_free - cur_free % UNFM_P_SIZE;
+    
+    /* item is of directory type */     
+    if (vn->vn_vi[i].vi_type & VI_TYPE_DIRECTORY) {
+      /* directory entries are the solid granules of the directory
+	 item, they cannot be split in the middle */
+      
+      /* calculate number of dir entries that can be shifted, and
+	 their total size */
+      int j;
+      struct virtual_item * vi;
+      
+      tb->lbytes = 0;
+      bytes = 0;
+      vi = &vn->vn_vi[i];
+      
+      for (j = 0; j < vi->vi_entry_count; j ++) {
+	if (vi->vi_entry_sizes[j] > cur_free)
+	  /* j-th entry doesn't fit into L[0] */
+	  break;
+		  
+	bytes += vi->vi_entry_sizes[j];
+	cur_free -= vi->vi_entry_sizes[j];
+	tb->lbytes ++;
+      }
+      /* "." can not be cut from first directory item */
+      if ((vn->vn_vi[i].vi_type & VI_TYPE_FIRST_DIRECTORY_ITEM) && tb->lbytes < 2)
+	tb->lbytes = 0;
+    }
+    
+
+    if (tb->lbytes <= 0) {
+      /* nothing can flow from the item */
+      tb->lbytes = -1;
+      return -1;
+    }
+    
+    /* something can flow from the item */
+    tb->lnum[0] ++;
+
+#ifdef CONFIG_REISERFS_CHECK
+    if (bytes == -1)
+      reiserfs_panic (tb->tb_sb, "vs-8060: check_left: bytes is not initialized");
+#endif      
+
+    return bytes;	/* part of split item in bytes */
+  }
+  
+
+  reiserfs_panic (0, "vs: 8065: check_left: all items fit in the left neighbor");
+  return 0;
+}
+	
+
+/* using virtual node check, how many items can be shifted to right
+   neighbor */
+static int check_right (struct tree_balance * tb, int h, int cur_free)
+{
+  int i;
+  struct virtual_node * vn = tb->tb_vn;
+  int d_size, ih_size, bytes = -1;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (cur_free < 0)
+    reiserfs_panic (tb->tb_sb, "vs-8070: check_right: cur_free < 0");
+#endif
+    
+  /* internal level */
+  if (h > 0) {
+    if (!cur_free) {
+      tb->rnum[h] = 0; 
+      return 0;
+    }
+    tb->rnum[h] = cur_free / (DC_SIZE + KEY_SIZE);
+    return -1;
+  }
+
+  /* leaf level */
+
+  if (!cur_free || !vn->vn_nr_item) {
+    /* no free space  */
+    tb->rnum[h] = 0;
+    tb->rbytes = -1;
+    return 0;
+  }
+  
+#ifdef CONFIG_REISERFS_CHECK
+  if (!PATH_H_PPARENT (tb->tb_path, 0))
+    reiserfs_panic (tb->tb_sb, "vs-8075: check_right: parent does not exist or invalid");
+#endif
+  
+  if ((unsigned int)cur_free >= (vn->vn_size - ((vn->vn_vi[vn->vn_nr_item-1].vi_type & VI_TYPE_RIGHT_MERGEABLE) ? IH_SIZE : 0)))
+    {
+      /* all contents of S[0] fits into R[0] */
+
+#ifdef CONFIG_REISERFS_CHECK
+      if (vn->vn_mode == M_INSERT || vn->vn_mode == M_PASTE)
+	reiserfs_panic (tb->tb_sb, "vs-8080: check_right: invalid mode or balance condition failed");
+#endif
+
+      tb->rnum[h] = vn->vn_nr_item;
+      tb->rbytes = -1;
+      return -1;
+    }
+
+  d_size = 0, ih_size = IH_SIZE;
+
+  /* last item may be merge with first item in right neighbor */
+  if (vn->vn_vi[vn->vn_nr_item - 1].vi_type & VI_TYPE_RIGHT_MERGEABLE)
+    d_size = -(int)IH_SIZE, ih_size = 0;
+
+  tb->rnum[0] = 0;
+  for (i = vn->vn_nr_item - 1; i >= 0; i --, d_size = 0, ih_size = IH_SIZE)
+    {
+      d_size += vn->vn_vi[i].vi_item_len;
+      if (cur_free >= d_size)
+	{	
+	  /* the item can be shifted entirely */
+	  cur_free -= d_size;
+	  tb->rnum[0] ++;
+	  continue;
+	}
+
+      /* the item cannot be shifted entirely, try to split it */
+      if (vn->vn_vi[i].vi_type & VI_TYPE_STAT_DATA || vn->vn_vi[i].vi_type & VI_TYPE_INSERTED_DIRECTORY_ITEM)
+	{
+	  /* virtual item is a stat_data or empty directory body ("." and "..), that is not split able */
+	  tb->rbytes = -1;
+	  return -1;
+	}
+      
+      /* check whether R[0] can hold ih and at least one byte of the item body */
+      if ( cur_free <= ih_size )
+	/* cannot shift even a part of the current item */
+	{
+	  tb->rbytes = -1;
+	  return -1;
+	}
+      
+      /* R[0] can hold the header of the item and at least one byte of its body */
+      cur_free -= ih_size;	/* cur_free is still > 0 */
+
+      /* item is of direct type */
+      if (vn->vn_vi[i].vi_type & VI_TYPE_DIRECT)
+	/* body of a direct item can be split at any byte */
+	tb->rbytes = bytes = cur_free;
+	
+      /* item is of indirect type */
+      if (vn->vn_vi[i].vi_type & VI_TYPE_INDIRECT)
+	/* an unformatted node pointer (having size long) is a solid granule of the item */
+	tb->rbytes = bytes = cur_free - cur_free % UNFM_P_SIZE;
+
+      /* item is of directory type */
+      if (vn->vn_vi[i].vi_type & VI_TYPE_DIRECTORY)
+	{
+	  int j;
+	  struct virtual_item * vi;
+	  
+	  tb->rbytes = 0;
+	  bytes = 0;
+	  vi = &vn->vn_vi[i];
+	  
+	  for (j = vi->vi_entry_count - 1; j >= 0; j --)
+	    {
+	      if (vi->vi_entry_sizes[j] > cur_free)
+		/* j-th entry doesn't fit into L[0] */
+		break;
+	      
+	      bytes += vi->vi_entry_sizes[j];
+	      cur_free -= vi->vi_entry_sizes[j];
+	      tb->rbytes ++;
+	    }
+
+	  /* ".." can not be cut from first directory item */
+	  if ((vn->vn_vi[i].vi_type & VI_TYPE_FIRST_DIRECTORY_ITEM) && tb->rbytes > vi->vi_entry_count - 2) {
+
+#ifdef CONFIG_REISERFS_CHECK
+	    if (tb->rbytes > vi->vi_entry_count - 1) {
+	      reiserfs_panic (tb->tb_sb, "vs-8085: check_right: all entries can be shifted to right neighbor");
+	    }
+#endif
+
+	    tb->rbytes = vi->vi_entry_count - 2;
+	  }
+	}
+	
+	if ( tb->rbytes <= 0 )
+	  {
+	    /* nothing can flow from the item */
+	    tb->rbytes = -1;
+	    return -1;
+	  }
+
+
+	/* something can flow from the item */
+	tb->rnum[0] ++;
+#ifdef CONFIG_REISERFS_CHECK
+	if (bytes == -1)
+	  reiserfs_panic (tb->tb_sb, "vs-8090: check_right: bytes is not initialized");
+#endif      
+	return bytes;	/* part of split item in bytes */
+    }
+
+  reiserfs_panic (tb->tb_sb, "vs-8095: check_right: all items fit in the left neighbor");
+  return 0;
+}
+
+
+/* sum of entry sizes between from-th and to-th entries including both edges */
+static int directory_part_size (struct virtual_item * vi, int from, int to)
+{
+  int i, retval;
+
+  retval = 0;
+  for (i = from; i <= to; i ++)
+    retval += vi->vi_entry_sizes[i];
+
+  return retval;
+}
+
+
+/*
+ * from - number of items, which are shifted to left neighbor entirely
+ * to - number of item, which are shifted to right neighbor entirely
+ * from_bytes - number of bytes of boundary item (or directory entries) which are shifted to left neighbor
+ * to_bytes - number of bytes of boundary item (or directory entries) which are shifted to right neighbor */
+static int get_num_ver (int mode, struct tree_balance * tb, int h,
+			int from, int from_bytes,
+			int to,   int to_bytes,
+			short * snum012, int flow
+			)
+{
+  int i;
+  int bytes;
+  struct virtual_node * vn = tb->tb_vn;
+  struct virtual_item * vi;
+
+  int total_node_size, max_node_size, current_item_size;
+  int needed_nodes;
+  int start_item, 	/* position of item we start filling node from */
+    end_item,	/* position of item we finish filling node by */
+    start_bytes,/* number of first bytes (entries for directory) of start_item-th item 
+		   we do not include into node that is being filled */
+    end_bytes;	/* number of last bytes (entries for directory) of end_item-th item 
+		   we do node include into node that is being filled */
+  int splitted_item_positions[2];	/* these are positions in virtual item of items, 
+					   that are splitted between S[0] and S1new and S1new and S2new */
+
+
+#ifdef CONFIG_REISERFS_CHECK
+  /* We only create additional nodes if we are in insert or paste mode
+     or we are in replace mode at the internal level. If h is 0 and
+     the mode is M_REPLACE then in fix_nodes we change the mode to
+     paste or insert before we get here in the code.  */
+  if ( tb->insert_size[h] < 0  || (mode != M_INSERT && mode != M_PASTE))
+    reiserfs_panic (0, "vs-8100: get_num_ver: insert_size < 0 in overflow");
+#endif
+
+  max_node_size = MAX_CHILD_SIZE (PATH_H_PBUFFER (tb->tb_path, h));
+
+  /* snum012 [0-2] - number of items, that lay
+     to S[0], first new node and second new node */
+  snum012[3] = -1;	/* s1bytes */
+  snum012[4] = -1;	/* s2bytes */
+
+
+  /* internal level */
+  if (h > 0) {
+    i = ((to - from) * (KEY_SIZE + DC_SIZE) + DC_SIZE);
+    if (i == max_node_size)
+      return 1;
+    return (i / max_node_size + 1);
+  }
+
+
+  /* leaf level */
+  needed_nodes = 1;
+  total_node_size = 0;
+
+  start_item = from;
+  start_bytes = from_bytes;
+  end_item = vn->vn_nr_item - to - 1;
+  end_bytes = to_bytes;
+
+  /* go through all items begining from the start_item-th item and ending by
+     the end_item-th item. If start_bytes != -1 we skip first start_bytes
+     item units (entries in case of directory). If end_bytes != -1 we skip
+     end_bytes units of the end_item-th item. */
+  for (i = start_item; i <= end_item; i ++) {
+
+#ifdef CONFIG_REISERFS_CHECK
+    if (needed_nodes > 3)
+      reiserfs_panic (0, "vs-8105: get_num_ver: too many nodes are needed");
+#endif
+
+    /* get size of current item */
+    current_item_size = (vi = &vn->vn_vi[i])->vi_item_len;
+
+    /* do not take in calculation head part (from_bytes) of from-th item */
+    if (i == start_item && start_bytes != -1) {
+      if (vi->vi_type & VI_TYPE_DIRECTORY)
+	current_item_size -= directory_part_size (vi, 0, start_bytes - 1);
+      else
+	current_item_size -= start_bytes;
+    }
+      
+    /* do not take in calculation tail part of (to-1)-th item */
+    if (i == end_item && end_bytes != -1) {
+      if (vi->vi_type & VI_TYPE_DIRECTORY)
+	/* first entry, that is not included */
+	current_item_size -= directory_part_size (vi, vi->vi_entry_count - end_bytes, vi->vi_entry_count - 1);
+      else
+	current_item_size -= end_bytes;
+    }
+
+    /* if item fits into current node entirely */
+    if (total_node_size + current_item_size <= max_node_size) {
+      snum012[needed_nodes - 1] ++;
+      total_node_size += current_item_size;
+      continue;
+    }
+
+    if (current_item_size > max_node_size) {
+      /* virtual item length is longer, than max size of item in a node. It is impossible for direct item */
+#ifdef CONFIG_REISERFS_CHECK
+      if (vi->vi_type & VI_TYPE_DIRECT)
+	reiserfs_panic (0, "vs-8110: get_num_ver: direct item length is %d. It can not be longer than %d", 
+			current_item_size, max_node_size);
+#endif
+      /* we will try to split it */
+      flow = 1;
+    }
+
+    if (!flow) {
+      /* as we do not split items, take new node and continue */
+      needed_nodes ++; i --; total_node_size = 0;
+      continue;
+    }
+
+    if (total_node_size + (int)IH_SIZE >= max_node_size) {
+      /* even minimal item does not fit into current node, take new node and continue */
+      needed_nodes ++, i--, total_node_size = 0;
+      continue;
+    }
+    if (vi->vi_type & VI_TYPE_STAT_DATA) {
+
+      /* stat data can not be split */
+      needed_nodes ++, i--, total_node_size = 0;
+      continue;
+    }
+
+    /* body of a direct item can be split at any byte */
+    /* bytes is free space in filled node */
+    bytes = max_node_size - total_node_size - IH_SIZE;
+
+    /* item is of indirect type */
+    if (vi->vi_type & VI_TYPE_INDIRECT)
+      /* an unformatted node pointer (having size long) is a solid granule of the item */
+      /* bytes of unformatted node pointers fits into free space of filled node */
+      bytes -= (bytes) % UNFM_P_SIZE;
+
+    /* S1bytes or S2bytes. It depends from needed_nodes */
+    snum012[needed_nodes - 1 + 3] = bytes;
+
+    /* item is of directory type */
+    if (vi->vi_type & VI_TYPE_DIRECTORY) {
+      /* calculate, how many entries can be put into current node */
+      int j;
+      int end_entry;
+
+      snum012[needed_nodes - 1 + 3] = 0;
+
+      total_node_size += IH_SIZE;
+      if (start_bytes == -1 || i != start_item)
+	start_bytes = 0;
+
+      end_entry = vi->vi_entry_count - ((i == end_item && end_bytes != -1) ? end_bytes : 0);
+      for (j = start_bytes; j < end_entry; j ++) {
+	/* j-th entry doesn't fit into current node */
+	if (total_node_size + vi->vi_entry_sizes[j] > max_node_size)
+	  break;
+	snum012[needed_nodes - 1 + 3] ++;
+	bytes += vi->vi_entry_sizes[j];
+	total_node_size += vi->vi_entry_sizes[j];
+      }
+      /* "." can not be cut from first directory item */
+      if (start_bytes == 0 && (vn->vn_vi[i].vi_type & VI_TYPE_FIRST_DIRECTORY_ITEM) && 
+	  snum012[needed_nodes - 1 + 3] < 2)
+	snum012[needed_nodes - 1 + 3] = 0;
+
+
+#ifdef CONFIG_REISERFS_CHECK
+      if (vi->vi_entry_count && 
+	  vi->vi_entry_count - ((i == end_item && end_bytes != -1) ? end_bytes : 0)
+	  - (start_bytes) <= snum012[needed_nodes - 1 + 3])
+	reiserfs_panic (0, "vs-8115: get_num_ver: required part of directory fits into current node");
+#endif
+    }
+
+    if (snum012[needed_nodes-1+3] <= 0 ) {
+      /* nothing fits into current node, take new node and continue */
+      needed_nodes ++, i--, total_node_size = 0;
+      continue;
+    }
+
+    /* something fits into the current node */
+    if (vi->vi_type & VI_TYPE_DIRECTORY)
+      start_bytes += snum012[needed_nodes - 1 + 3];
+    else
+      start_bytes = bytes;
+
+    snum012[needed_nodes - 1] ++;
+    splitted_item_positions[needed_nodes - 1] = i;
+
+    needed_nodes ++;
+    /* continue from the same item with start_bytes != -1 */
+    start_item = i;
+    i --;
+    total_node_size = 0;
+  }
+
+
+  /* snum012[3] and snum012[4] contain how many bytes (entries) of
+     split item can be in S[0] and S1new. s1bytes and s2bytes are how
+     many bytes (entries) can be in S1new and S2new. Recalculate it */
+  
+  if (snum012[4] > 0) {	/* s2bytes */
+    /* get number of item that is split between S1new and S2new */
+    int split_item_num;
+    int bytes_to_r, bytes_to_l;
+    
+    split_item_num = splitted_item_positions[1];
+    bytes_to_l = ((from == split_item_num && from_bytes != -1) ? from_bytes : 0);
+    bytes_to_r = ((end_item == split_item_num && end_bytes != -1) ? end_bytes : 0);
+    if (vn->vn_vi[split_item_num].vi_type & VI_TYPE_DIRECTORY) {
+      int entries_to_S2new;
+      
+      /* calculate number of entries fit into S2new */
+      entries_to_S2new =  vn->vn_vi[split_item_num].vi_entry_count - snum012[4] - bytes_to_r - bytes_to_l;
+      if (snum012[3] != -1 && snum012[1] == 1) {
+	/* directory split into 3 nodes */
+	int entries_to_S1new;
+
+	entries_to_S2new -= snum012[3];
+	entries_to_S1new = snum012[4];
+	snum012[3] = entries_to_S1new;
+	snum012[4] = entries_to_S2new;
+	return needed_nodes;
+      }
+      snum012[4] = entries_to_S2new;
+    } else {
+      /* item is not of directory type */
+      int bytes_to_S2new;
+      
+      bytes_to_S2new = vn->vn_vi[split_item_num].vi_item_len - IH_SIZE - snum012[4] - bytes_to_r - bytes_to_l;
+      snum012[4] = bytes_to_S2new;
+    }
+  }
+
+  /* now we know S2bytes, calculate S1bytes */
+  if (snum012[3] > 0) {	/* s1bytes */
+    /* get number of item that is split between S0 and S1new */
+    int split_item_num;
+    int bytes_to_r, bytes_to_l;
+    
+    split_item_num = splitted_item_positions[0];
+    bytes_to_l = ((from == split_item_num && from_bytes != -1) ? from_bytes : 0);
+    bytes_to_r = ((end_item == split_item_num && end_bytes != -1) ? end_bytes : 0);
+    if (vn->vn_vi[split_item_num].vi_type & VI_TYPE_DIRECTORY) {
+      /* entries, who go to S1new node */
+      snum012[3] =  vn->vn_vi[split_item_num].vi_entry_count - snum012[3] - bytes_to_r - bytes_to_l;
+    } else
+      /* bytes, who go to S1new node (not including HI_SIZE) */
+      snum012[3] = vn->vn_vi[split_item_num].vi_item_len - IH_SIZE - snum012[3] - bytes_to_r - bytes_to_l;
+  }
+
+  return needed_nodes;
+}
+
+
+#ifdef CONFIG_REISERFS_CHECK
+extern struct tree_balance * cur_tb;
+#endif
+
+
+/* size of item_num-th item in bytes when regular and in entries when
+   item is directory */
+static int item_length (struct tree_balance * tb, int item_num)
+{
+  struct virtual_node * vn = tb->tb_vn;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (item_num >= vn->vn_nr_item)
+    reiserfs_panic (tb->tb_sb, "vs-8120: item_length: invalid index of item: index = %d (item number = %d)", item_num, vn->vn_nr_item);
+#endif
+
+  if (vn->vn_vi[item_num].vi_type & VI_TYPE_DIRECTORY)
+    return vn->vn_vi[item_num].vi_entry_count;
+
+  return vn->vn_vi[item_num].vi_item_len - IH_SIZE;
+}
+
+
+/* Set parameters for balancing.
+ * Performs write of results of analysis of balancing into structure tb,
+ * where it will later be used by the functions that actually do the balancing. 
+ * Parameters:
+ *	tb	tree_balance structure;
+ *	h	current level of the node;
+ *	lnum	number of items from S[h] that must be shifted to L[h];
+ *	rnum	number of items from S[h] that must be shifted to R[h];
+ *	blk_num	number of blocks that S[h] will be splitted into;
+ *	s012	number of items that fall into splitted nodes.
+ *	lbytes	number of bytes which flow to the left neighbor from the item that is not
+ *		not shifted entirely
+ *	rbytes	number of bytes which flow to the right neighbor from the item that is not
+ *		not shifted entirely
+ *	s1bytes	number of bytes which flow to the first  new node when S[0] splits (this number is contained in s012 array)
+ */
+
+static void set_parameters (struct tree_balance * tb, int h, int lnum,
+			    int rnum, int blk_num, short * s012, int lb, int rb)
+{
+
+  tb->lnum[h] = lnum;
+  tb->rnum[h] = rnum;
+  tb->blknum[h] = blk_num;
+
+  if (h == 0)
+    {  /* only for leaf level */
+      if (s012 != NULL)
+	{
+	  tb->s0num = * s012 ++,
+	  tb->s1num = * s012 ++,
+	  tb->s2num = * s012 ++;
+	  tb->s1bytes = * s012 ++;
+	  tb->s2bytes = * s012;
+	}
+      tb->lbytes = lb;
+      tb->rbytes = rb;
+    }
+}
+
+static void decrement_key(
+			  struct key * p_s_key
+			  ) {
+  unsigned long * p_n_key_field = (unsigned long *)p_s_key + REISERFS_FULL_KEY_LEN - 1;
+  int		  n_counter;
+
+  for( n_counter = 0; n_counter < REISERFS_FULL_KEY_LEN; n_counter++, p_n_key_field-- )
+    if ( *p_n_key_field ) {
+      (*p_n_key_field)--;
+      break;
+    }
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( n_counter == REISERFS_FULL_KEY_LEN )
+    reiserfs_panic(NULL, "PAP-8175: decrement_key: zero key");
+#endif
+
+}
+
+
+#ifndef REISERFS_FSCK
+
+inline int is_left_mergeable (struct item_head * ih, unsigned long bsize)
+{
+  if (I_IS_DIRECT_ITEM (ih))
+    return (ih->ih_key.k_offset % bsize != 1);
+
+  if (I_IS_INDIRECT_ITEM (ih))
+    return (ih->ih_key.k_offset != 1);
+
+  if (I_IS_DIRECTORY_ITEM (ih))
+   return ((ih)->ih_key.k_offset != DOT_OFFSET);
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( ! I_IS_STAT_DATA_ITEM (ih))
+    reiserfs_panic (0, "vs-16060: is_left_mergeable: item [%h] must be a stat data", ih);
+#endif
+
+  return 0;
+}
+
+#else
+
+int are_items_mergeable (struct item_head * left, struct item_head * right, int bsize)
+{
+  if (comp_keys (&left->ih_key, &right->ih_key) != -1) {
+    reiserfs_panic (0, "vs-16070: are_items_mergeable: left %k, right %k", &(left->ih_key), &(right->ih_key));
+  }
+
+  if (comp_short_keys (&left->ih_key, &right->ih_key))
+    return 0;
+
+  if (I_IS_DIRECTORY_ITEM (left)) {
+    return 1;
+  }
+
+  if ((I_IS_DIRECT_ITEM (left) && I_IS_DIRECT_ITEM (right)) || 
+      (I_IS_INDIRECT_ITEM (left) && I_IS_INDIRECT_ITEM (right)))
+    return (left->ih_key.k_offset + I_BYTES_NUMBER (left, bsize) == right->ih_key.k_offset) ? 1 : 0;
+
+  return 0;
+}
+
+/* get left neighbor of the leaf node */
+static struct buffer_head * get_left_neighbor (struct super_block * s, struct path * path)
+{
+  struct key key;
+  struct path path_to_left_neighbor;
+  struct buffer_head * bh;
+  int repeat;
+
+  copy_key (&key, B_N_PKEY (PATH_PLAST_BUFFER (path), 0));
+  decrement_key (&key);
+
+  init_path (&path_to_left_neighbor);
+  search_by_key (s, &key, &path_to_left_neighbor, &repeat, DISK_LEAF_NODE_LEVEL, READ_BLOCKS);
+  if (PATH_LAST_POSITION (&path_to_left_neighbor) == 0) {
+    pathrelse (&path_to_left_neighbor);
+    return 0;
+  }
+  bh = PATH_PLAST_BUFFER (&path_to_left_neighbor);
+  bh->b_count ++;
+  pathrelse (&path_to_left_neighbor);
+  return bh;
+}
+
+extern struct key  MIN_KEY;
+static struct buffer_head * get_right_neighbor (struct super_block * s, struct path * path)
+{
+  struct key key;
+  struct key * rkey;
+  int repeat;
+  struct path path_to_right_neighbor;
+  struct buffer_head * bh;
+
+  rkey = get_rkey (path, s);
+  if (comp_keys (rkey, &MIN_KEY) == 0)
+    reiserfs_panic (s, "vs-16080: get_right_neighbor: get_rkey returned min key (path has changed)");
+  copy_key (&key, rkey);
+
+  
+  init_path (&path_to_right_neighbor);
+  search_by_key (s, &key, &path_to_right_neighbor, &repeat, DISK_LEAF_NODE_LEVEL, READ_BLOCKS);
+  if (PATH_PLAST_BUFFER (&path_to_right_neighbor) == PATH_PLAST_BUFFER (path)) {
+    pathrelse (&path_to_right_neighbor);
+    return 0;
+  }
+  bh = PATH_PLAST_BUFFER (&path_to_right_neighbor);
+  bh->b_count ++;
+  pathrelse (&path_to_right_neighbor);
+  return bh;
+}
+
+
+int is_left_mergeable (struct super_block * s, struct path * path)
+{
+  struct item_head * right;
+  struct buffer_head * bh;
+  int retval;
+  
+  right = B_N_PITEM_HEAD (PATH_PLAST_BUFFER (path), 0);
+
+  bh = get_left_neighbor (s, path);
+  if (bh == 0) {
+    return 0;
+  }
+  retval = are_items_mergeable (B_N_PITEM_HEAD (bh, B_NR_ITEMS (bh) - 1), right, bh->b_size);
+  brelse (bh);
+  return retval;
+}
+
+
+int is_right_mergeable (struct super_block * s, struct path * path)
+{
+  struct item_head * left;
+  struct buffer_head * bh;
+  int retval;
+  
+  left = B_N_PITEM_HEAD (PATH_PLAST_BUFFER (path), B_NR_ITEMS (PATH_PLAST_BUFFER (path)) - 1);
+
+  bh = get_right_neighbor (s, path);
+  if (bh == 0) {
+    return 0;
+  }
+  retval = are_items_mergeable (left, B_N_PITEM_HEAD (bh, 0), bh->b_size);
+  brelse (bh);
+  return retval;
+}
+
+#endif /* REISERFS_FSCK */
+
+
+
+/* check, does node disappear if we shift tb->lnum[0] items to left
+   neighbor and tb->rnum[0] to the right one. */
+static int is_leaf_removable (struct tree_balance * tb)
+{
+  struct virtual_node * vn = tb->tb_vn;
+  int to_left, to_right;
+  int size;
+  int remain_items;
+
+  /* number of items, that will be shifted to left (right) neighbor
+     entirely */
+  to_left = tb->lnum[0] - ((tb->lbytes != -1) ? 1 : 0);
+  to_right = tb->rnum[0] - ((tb->rbytes != -1) ? 1 : 0);
+  remain_items = vn->vn_nr_item;
+
+  /* how many items remain in S[0] after shiftings to neighbors */
+  remain_items -= (to_left + to_right);
+
+  if (remain_items < 1) {
+    /* all content of node can be shifted to neighbors */
+    set_parameters (tb, 0, to_left, vn->vn_nr_item - to_left, 0, NULL, -1, -1);    
+    return 1;
+  }
+  
+  if (remain_items > 1 || tb->lbytes == -1 || tb->rbytes == -1)
+    /* S[0] is not removable */
+    return 0;
+
+  /* check, whether we can divide 1 remaining item between neighbors */
+
+  /* get size of remaining item (in directory entry count if directory) */
+  size = item_length (tb, to_left);
+
+  if (tb->lbytes + tb->rbytes >= size) {
+    set_parameters (tb, 0, to_left + 1, to_right + 1, 0, NULL, tb->lbytes, -1);
+    return 1;
+  }
+
+  return 0;
+}
+
+
+/* check whether L, S, R can be joined in one node */
+static int are_leaves_removable (struct tree_balance * tb, int lfree, int rfree)
+{
+    struct virtual_node * vn = tb->tb_vn;
+    int ih_size;
+    struct buffer_head *S0;
+
+    S0 = PATH_H_PBUFFER (tb->tb_path, 0);
+
+    ih_size = 0;
+    if (vn->vn_nr_item) {
+	if (vn->vn_vi[0].vi_type & VI_TYPE_LEFT_MERGEABLE)
+	    ih_size += IH_SIZE;
+    
+	if (vn->vn_vi[vn->vn_nr_item-1].vi_type & VI_TYPE_RIGHT_MERGEABLE)
+	    ih_size += IH_SIZE;
+    } else {
+	/* there was only one item and it will be deleted */
+	struct item_head * ih;
+    
+#ifdef CONFIG_REISERFS_CHECK
+	if (B_NR_ITEMS (S0) != 1)
+	    reiserfs_panic (0, "vs-8125: are_leaves_removable: item number must be 1: it is %d", B_NR_ITEMS(S0));
+#endif
+
+	ih = B_N_PITEM_HEAD (S0, 0);
+	if (tb->CFR[0] && !COMP_SHORT_KEYS (&(ih->ih_key), B_N_PDELIM_KEY (tb->CFR[0], tb->rkey[0])))
+	    if (I_IS_DIRECTORY_ITEM(ih)) {
+#ifndef REISERFS_FSCK
+	
+		/* Directory must be in correct state here: that is
+		   somewhere at the left side should exist first
+		   directory item. But the item being deleted can not
+		   be that first one because its right neighbor is
+		   item of the same directory. (But first item always
+		   gets deleted in last turn). So, neighbors of
+		   deleted item can be merged, so we can save ih_size */
+		ih_size = IH_SIZE;
+
+#ifdef CONFIG_REISERFS_CHECK
+		/* we might check that left neighbor exists and is of
+                   the same directory */
+		if (ih->ih_key.k_offset == DOT_OFFSET)
+		    reiserfs_panic (tb->tb_sb, "vs-8130: are_leaves_removable: "
+				    "first directory item can not be removed until directory is not empty");
+#endif
+	
+	
+#else	/* REISERFS_FSCK */
+
+		/* we can delete any directory item in fsck (if it is unreachable) */
+		if (ih->ih_key.k_offset != DOT_OFFSET) {
+		    /* must get left neighbor here to make sure, that
+                       left neighbor is of the same directory */
+		    struct buffer_head * left;
+		    
+		    left = get_left_neighbor (tb->tb_sb, tb->tb_path);
+		    if (left) {
+			struct item_head * last;
+
+			if (B_NR_ITEMS (left) == 0)
+			    reiserfs_panic (tb->tb_sb, "vs-8135: are_leaves_removable: "
+					    "empty node in the tree");
+			last = B_N_PITEM_HEAD (left, B_NR_ITEMS (left) - 1);
+			if (!comp_short_keys (&last->ih_key, &ih->ih_key))
+			    ih_size = IH_SIZE;
+			brelse (left);
+		    }
+		}
+#endif
+	    }
+    
+    }
+
+    if (MAX_CHILD_SIZE (S0) + vn->vn_size <= rfree + lfree + ih_size) {
+	set_parameters (tb, 0, -1, -1, -1, NULL, -1, -1);
+	return 1;  
+    }
+    return 0;
+  
+}
+
+
+
+/* when we do not split item, lnum and rnum are numbers of entire items */
+#define SET_PAR_SHIFT_LEFT \
+if (h)\
+{\
+   int to_l;\
+   \
+   to_l = (MAX_NR_KEY(Sh)+1 - lpar + vn->vn_nr_item + 1) / 2 -\
+	      (MAX_NR_KEY(Sh) + 1 - lpar);\
+	      \
+	      set_parameters (tb, h, to_l, 0, lnver, NULL, -1, -1);\
+}\
+else \
+{\
+   if (lset==LEFT_SHIFT_FLOW)\
+     set_parameters (tb, h, lpar, 0, lnver, snum012+lset,\
+		     tb->lbytes, -1);\
+   else\
+     set_parameters (tb, h, lpar - (tb->lbytes!=-1), 0, lnver, snum012+lset,\
+		     -1, -1);\
+}
+
+
+#define SET_PAR_SHIFT_RIGHT \
+if (h)\
+{\
+   int to_r;\
+   \
+   to_r = (MAX_NR_KEY(Sh)+1 - rpar + vn->vn_nr_item + 1) / 2 - (MAX_NR_KEY(Sh) + 1 - rpar);\
+   \
+   set_parameters (tb, h, 0, to_r, rnver, NULL, -1, -1);\
+}\
+else \
+{\
+   if (rset==RIGHT_SHIFT_FLOW)\
+     set_parameters (tb, h, 0, rpar, rnver, snum012+rset,\
+		  -1, tb->rbytes);\
+   else\
+     set_parameters (tb, h, 0, rpar - (tb->rbytes!=-1), rnver, snum012+rset,\
+		  -1, -1);\
+}
+
+
+void free_buffers_in_tb (
+		       struct tree_balance * p_s_tb
+		       ) {
+  int n_counter;
+
+  decrement_counters_in_path(p_s_tb->tb_path);
+  
+  for ( n_counter = 0; n_counter < MAX_HEIGHT; n_counter++ ) {
+    decrement_bcount(p_s_tb->L[n_counter]);
+    p_s_tb->L[n_counter] = NULL;
+    decrement_bcount(p_s_tb->R[n_counter]);
+    p_s_tb->R[n_counter] = NULL;
+    decrement_bcount(p_s_tb->FL[n_counter]);
+    p_s_tb->FL[n_counter] = NULL;
+    decrement_bcount(p_s_tb->FR[n_counter]);
+    p_s_tb->FR[n_counter] = NULL;
+    decrement_bcount(p_s_tb->CFL[n_counter]);
+    p_s_tb->CFL[n_counter] = NULL;
+    decrement_bcount(p_s_tb->CFR[n_counter]);
+    p_s_tb->CFR[n_counter] = NULL;
+  }
+}
+
+
+/* Get new buffers for storing new nodes that are created while balancing.
+ * Returns:	SCHEDULE_OCCURED - schedule occured while the function worked;
+ *	        CARRY_ON - schedule didn't occur while the function worked;
+ *	        NO_DISK_SPACE - no disk space.
+ */
+static int  get_empty_nodes(
+	      struct reiserfs_transaction_handle *th,
+              struct tree_balance * p_s_tb,
+              int n_h
+            ) {
+  struct buffer_head  * p_s_new_bh,
+    		      *	p_s_Sh = PATH_H_PBUFFER (p_s_tb->tb_path, n_h);
+  unsigned long	      *	p_n_blocknr,
+    			a_n_blocknrs[MAX_AMOUNT_NEEDED] = {0, };
+  int       		n_counter,
+   			n_number_of_freeblk,
+                	n_amount_needed,/* number of needed empty blocks */
+   			n_repeat1,
+			n_repeat;
+ struct super_block *	p_s_sb = p_s_tb->tb_sb;
+
+
+#ifdef REISERFS_FSCK
+   if (n_h == 0 && p_s_tb->insert_size[n_h] == 0x7fff)
+     return CARRY_ON;
+#endif
+
+  /* number_of_freeblk is the number of empty blocks which have been
+     acquired for use by the balancing algorithm minus the number of
+     empty blocks used in the previous levels of the analysis,
+     number_of_freeblk = tb->cur_blknum can be non-zero if a schedule occurs
+     after empty blocks are acquired, and the balancing analysis is
+     then restarted, amount_needed is the number needed by this level
+     (n_h) of the balancing analysis.
+			    
+     Note that for systems with many processes writing, it would be
+     more layout optimal to calculate the total number needed by all
+     levels and then to run reiserfs_new_blocks to get all of them at once.  */
+
+  /* Initiate number_of_freeblk to the amount acquired prior to the restart of
+     the analysis or 0 if not restarted, then subtract the amount needed
+     by all of the levels of the tree below n_h. */
+  /* blknum includes S[n_h], so we subtract 1 in this calculation */
+  for ( n_counter = 0, n_number_of_freeblk = p_s_tb->cur_blknum; n_counter < n_h; n_counter++ )
+    n_number_of_freeblk -= ( p_s_tb->blknum[n_counter] ) ? (p_s_tb->blknum[n_counter] - 1) : 0;
+
+  /* Allocate missing empty blocks. */
+  /* if p_s_Sh == 0  then we are getting a new root */
+  n_amount_needed = ( p_s_Sh ) ? (p_s_tb->blknum[n_h] - 1) : 1;
+  /*  Amount_needed = the amount that we need more than the amount that we have. */
+  if ( n_amount_needed > n_number_of_freeblk )
+    n_amount_needed -= n_number_of_freeblk;
+  else /* If we have enough already then there is nothing to do. */
+    return CARRY_ON;
+
+  if ( (n_repeat = reiserfs_new_blocknrs (th, p_s_tb->tb_sb, a_n_blocknrs,
+					  PATH_PLAST_BUFFER(p_s_tb->tb_path)->b_blocknr, n_amount_needed, 0/*not for preserve list*/)) != CARRY_ON ) {
+    if (n_repeat != NO_DISK_SPACE) {
+      int locali ;
+      for (locali = 0 ; locali < n_amount_needed ; locali++) {
+        reiserfs_free_block(th, th->t_super, a_n_blocknrs[locali]) ;
+      }
+    }
+    return n_repeat; /* Out of disk space or schedule() occured. */ 
+  }
+
+
+  /* for each blocknumber we just got, get a buffer and stick it on FEB */
+  for ( p_n_blocknr = a_n_blocknrs, n_counter = 0; n_counter < n_amount_needed;
+	p_n_blocknr++, n_counter++ ) { 
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( ! *p_n_blocknr )
+      reiserfs_panic(p_s_sb, "PAP-8135: get_empty_nodes: reiserfs_new_blocknrs failed when got new blocks");
+#endif
+
+    n_repeat1 = CARRY_ON;
+    p_s_new_bh = reiserfs_getblk(p_s_sb->s_dev, *p_n_blocknr, p_s_sb->s_blocksize, &n_repeat1);
+    n_repeat |= n_repeat1;
+    if (p_s_new_bh->b_count > 1) {
+      n_repeat |= SCHEDULE_OCCURRED;
+      free_buffers_in_tb (p_s_tb);
+      wait_buffer_until_released (p_s_new_bh);
+    }
+#ifdef CONFIG_REISERFS_CHECK_NOCHECK
+    if ((p_s_new_bh->b_count != 1 && !buffer_journaled(p_s_new_bh)) 
+         || (buffer_dirty (p_s_new_bh) && !buffer_journal_dirty(p_s_new_bh))) {
+      reiserfs_panic(p_s_sb,"PAP-8140: get_empty_nodes: not free or dirty buffer %b for the new block",
+		     p_s_new_bh);
+    }
+#endif
+    mark_buffer_journal_new(p_s_new_bh) ;
+    
+    /* Put empty buffers into the array. */
+    p_s_tb->FEB[p_s_tb->cur_blknum++] = p_s_new_bh;
+
+    /* in the reiserfs_new_blocknrs we have atomically dirtied bitmap
+       block of true bitmap, containing bit, that corresponds to
+       p_s_new_bh->b_blocknr. Tree balance contains 1 bit per each
+       bitmap block. Set there bit corresponding to dirtied bitmap */
+    set_bit (*p_n_blocknr / (p_s_new_bh->b_size * 8), DIRTY_BITMAP_MAP (p_s_tb));
+  }
+
+  return n_repeat;
+}
+
+
+/* Get free space of the left neighbor,
+ * which is stored in the parent node of the left neighbor.
+ */
+static int get_lfree (struct tree_balance * tb, int h)
+{
+  struct buffer_head * l, * f;
+  int order;
+
+  if ((f = PATH_H_PPARENT (tb->tb_path, h)) == 0 || (l = tb->FL[h]) == 0)
+    return 0;
+
+  if (f == l)
+    order = PATH_H_B_ITEM_ORDER (tb->tb_path, h) - 1;
+  else {
+      order = B_BLK_HEAD(l)->blk_nr_item;
+      f = l;
+  }
+
+  return (MAX_CHILD_SIZE(f) - B_N_CHILD(f,order)->dc_size);
+}
+
+
+/* Get free space of the right neighbor,
+ * which is stored in the parent node of the right neighbor.
+ */
+static int get_rfree (struct tree_balance * tb, int h)
+{
+  struct buffer_head * r, * f;
+  int order;
+
+  if ((f = PATH_H_PPARENT (tb->tb_path, h)) == 0 || (r = tb->FR[h]) == 0)
+    return 0;
+
+  if (f == r)
+      order = PATH_H_B_ITEM_ORDER (tb->tb_path, h) + 1;
+  else {
+      order = 0;
+      f = r;
+  }
+
+  return (MAX_CHILD_SIZE(f) - B_N_CHILD(f,order)->dc_size);
+
+}
+
+
+/* Check whether left neighbor is in memory. */
+static int  is_left_neighbor_in_cache(
+              struct tree_balance * p_s_tb,
+              int                   n_h
+            ) {
+  struct buffer_head  * p_s_father;
+  struct super_block  * p_s_sb = p_s_tb->tb_sb;
+  unsigned long         n_left_neighbor_blocknr;
+  int                   n_left_neighbor_position;
+
+  if ( ! p_s_tb->FL[n_h] ) /* Father of the left neighbor does not exist. */
+    return 0;
+
+  /* Calculate father of the node to be balanced. */
+  p_s_father = PATH_H_PBUFFER(p_s_tb->tb_path, n_h + 1);
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( ! p_s_father || ! B_IS_IN_TREE (p_s_father) || ! B_IS_IN_TREE (p_s_tb->FL[n_h]) ||
+       ! buffer_uptodate (p_s_father) || ! buffer_uptodate (p_s_tb->FL[n_h]) ) {
+    reiserfs_panic (p_s_sb, "vs-8165: is_left_neighbor_in_cache: F[h] (%b) or FL[h] (%b) is invalid",
+		    p_s_father, p_s_tb->FL[n_h]);
+  }
+#endif
+
+
+  /* Get position of the pointer to the left neighbor into the left father. */
+  n_left_neighbor_position = ( p_s_father == p_s_tb->FL[n_h] ) ?
+                      p_s_tb->lkey[n_h] : B_BLK_HEAD(p_s_tb->FL[n_h])->blk_nr_item;
+  /* Get left neighbor block number. */
+  n_left_neighbor_blocknr = B_N_CHILD_NUM(p_s_tb->FL[n_h], n_left_neighbor_position);
+  /* Look for the left neighbor in the cache. */
+  if ( (p_s_father = find_buffer(p_s_sb->s_dev, n_left_neighbor_blocknr, p_s_sb->s_blocksize)) ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( buffer_uptodate (p_s_father) && ! B_IS_IN_TREE(p_s_father) ) {
+      reiserfs_panic(p_s_sb, "vs-8170: is_left_neighbor_in_cache: left neighbor (%b %z) is not in the tree",
+		     p_s_father, p_s_father);
+    }
+#endif
+
+    return 1;
+  }
+
+  return 0;
+}
+
+
+#define LEFT_PARENTS  'l'
+#define RIGHT_PARENTS 'r'
+
+
+
+
+void init_path (struct path * path)
+{
+  path->path_length = ILLEGAL_PATH_ELEMENT_OFFSET;
+}
+
+
+/* Calculate far left/right parent of the left/right neighbor of the current node, that
+ * is calculate the left/right (FL[h]/FR[h]) neighbor of the parent F[h].
+ * Calculate left/right common parent of the current node and L[h]/R[h].
+ * Calculate left/right delimiting key position.
+ * Returns:	PATH_INCORRECT   - path in the tree is not correct;
+ 		SCHEDULE_OCCURRED - schedule occured while the function worked;
+ *	        CARRY_ON         - schedule didn't occur while the function worked;
+ */
+static int  get_far_parent(
+              struct tree_balance *   p_s_tb,
+              int                     n_h,
+              struct buffer_head  **  pp_s_father,
+              struct buffer_head  **  pp_s_com_father,
+              char                    c_lr_par
+            ) {
+  struct buffer_head  * p_s_parent;
+  struct path         	s_path_to_neighbor_father,
+    		      * p_s_path = p_s_tb->tb_path;
+  struct key		s_lr_father_key;
+  int                   n_counter,
+                        n_position = MAX_INT,
+    			n_repeat,
+                        n_first_last_position = 0,
+                        n_path_offset = PATH_H_PATH_OFFSET(p_s_path, n_h);
+
+  /* Starting from F[n_h] go upwards in the tree, and look for the common
+      ancestor of F[n_h], and its neighbor l/r, that should be obtained. */
+
+  n_counter = n_path_offset;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( n_counter < FIRST_PATH_ELEMENT_OFFSET )
+    reiserfs_panic(p_s_tb->tb_sb, "PAP-8180: get_far_parent: invalid path length");
+#endif
+
+  
+  for ( ; n_counter > FIRST_PATH_ELEMENT_OFFSET; n_counter--  )  {
+    /* Check whether parent of the current buffer in the path is really parent in the tree. */
+    if ( ! B_IS_IN_TREE(p_s_parent = PATH_OFFSET_PBUFFER(p_s_path, n_counter - 1)) )
+      return PATH_INCORRECT;
+    /* Check whether position in the parent is correct. */
+    if ( (n_position = PATH_OFFSET_POSITION(p_s_path, n_counter - 1)) > B_NR_ITEMS(p_s_parent) )
+      return PATH_INCORRECT;
+    /* Check whether parent at the path really points to the child. */
+    if ( B_N_CHILD_NUM(p_s_parent, n_position) !=
+                                          PATH_OFFSET_PBUFFER(p_s_path, n_counter)->b_blocknr )
+      return PATH_INCORRECT;
+    /* Return delimiting key if position in the parent is not equal to first/last one. */
+    if ( c_lr_par == RIGHT_PARENTS )
+      n_first_last_position = B_BLK_HEAD(p_s_parent)->blk_nr_item;
+    if ( n_position != n_first_last_position ) {
+      (*pp_s_com_father = p_s_parent)->b_count++;
+      break;
+    }
+  }
+
+  /* Hopefully we are in the root of the tree. */
+  if ( n_counter == FIRST_PATH_ELEMENT_OFFSET ) {
+    /* Check whether first buffer in the path is the root of the tree. */
+    if ( PATH_OFFSET_PBUFFER(p_s_tb->tb_path, FIRST_PATH_ELEMENT_OFFSET)->b_blocknr ==
+                                            p_s_tb->tb_sb->u.reiserfs_sb.s_rs->s_root_block ) {
+      *pp_s_father = *pp_s_com_father = NULL;
+      return CARRY_ON;
+    }
+    return PATH_INCORRECT;
+  }
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( B_BLK_HEAD(*pp_s_com_father)->blk_level <= DISK_LEAF_NODE_LEVEL ) {
+    reiserfs_panic(p_s_tb->tb_sb, "PAP-8185: get_far_parent: (%b %z) level too small", *pp_s_com_father, *pp_s_com_father);
+  }
+#endif
+
+  /* Check whether the common parent is locked. */
+  if ( test_and_wait_on_buffer(*pp_s_com_father) == SCHEDULE_OCCURRED ) {
+    decrement_bcount(*pp_s_com_father);
+    return SCHEDULE_OCCURRED; /* schedule() occured */
+  }
+
+  /* So, we got common parent of the current node and its left/right neighbor.
+     Now we are geting the parent of the left/right neighbor. */
+
+  /* Form key to get parent of the left/right neighbor. */
+  copy_key(&s_lr_father_key, B_N_PDELIM_KEY(*pp_s_com_father, ( c_lr_par == LEFT_PARENTS ) ?
+     (p_s_tb->lkey[n_h - 1] = n_position - 1) : (p_s_tb->rkey[n_h - 1] = n_position)));
+
+  if ( c_lr_par == LEFT_PARENTS )
+    decrement_key(&s_lr_father_key);
+
+  init_path (&s_path_to_neighbor_father);
+
+  search_by_key(p_s_tb->tb_sb, &s_lr_father_key, &s_path_to_neighbor_father, &n_repeat, n_h + 1, READ_BLOCKS);
+
+  if ( n_repeat != CARRY_ON ) {
+    decrement_counters_in_path(&s_path_to_neighbor_father);
+    decrement_bcount(*pp_s_com_father);
+    return n_repeat;
+  }
+
+  *pp_s_father = PATH_PLAST_BUFFER(&s_path_to_neighbor_father);
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( B_BLK_HEAD(*pp_s_father)->blk_level != n_h + 1 ) {
+    reiserfs_panic(p_s_tb->tb_sb, "PAP-8190: get_far_parent: (%b %z) level too small", *pp_s_father, *pp_s_father);
+  }
+  
+  if ( s_path_to_neighbor_father.path_length < FIRST_PATH_ELEMENT_OFFSET )
+    reiserfs_panic(0, "PAP-8192: get_far_parent: path length is too small");
+
+#endif
+
+  s_path_to_neighbor_father.path_length--;
+  decrement_counters_in_path(&s_path_to_neighbor_father);
+  return CARRY_ON;
+}
+
+
+/* Get parents of neighbors of node in the path(S[n_path_offset]) and common parents of
+ * S[n_path_offset] and L[n_path_offset]/R[n_path_offset]: F[n_path_offset], FL[n_path_offset],
+ * FR[n_path_offset], CFL[n_path_offset], CFR[n_path_offset].
+ * Calculate numbers of left and right delimiting keys position: lkey[n_path_offset], rkey[n_path_offset].
+ * Returns:	SCHEDULE_OCCURRED - schedule occured while the function worked;
+ *	        CARRY_ON - schedule didn't occur while the function worked;
+ */
+static int  get_parents(
+              struct tree_balance * p_s_tb,
+              int                   n_h
+            ) {
+  struct path         * p_s_path = p_s_tb->tb_path;
+  int                   n_position,
+                        n_ret_value,
+                        n_path_offset = PATH_H_PATH_OFFSET(p_s_tb->tb_path, n_h);
+  struct buffer_head  * p_s_curf,
+                      * p_s_curcf;
+
+  /* Current node is the root of the tree or will be root of the tree */
+  if ( n_path_offset <= FIRST_PATH_ELEMENT_OFFSET ) {
+  /* The root can not have parents.
+      Release nodes which previously were obtained as parents of the current node neighbors. */
+    decrement_bcount(p_s_tb->FL[n_h]);
+    decrement_bcount(p_s_tb->CFL[n_h]);
+    decrement_bcount(p_s_tb->FR[n_h]);
+    decrement_bcount(p_s_tb->CFR[n_h]);
+    p_s_tb->FL[n_h] = p_s_tb->CFL[n_h] = p_s_tb->FR[n_h] = p_s_tb->CFR[n_h] = NULL;
+    return CARRY_ON;
+  }
+  
+  /* Get parent FL[n_path_offset] of L[n_path_offset]. */
+  if ( (n_position = PATH_OFFSET_POSITION(p_s_path, n_path_offset - 1)) )  {
+    /* Current node is not the first child of its parent. */
+    (p_s_curf = p_s_curcf = PATH_OFFSET_PBUFFER(p_s_path, n_path_offset - 1))->b_count += 2;
+    p_s_tb->lkey[n_h] = n_position - 1;
+  }
+  else  {
+  /* Calculate current parent of L[n_path_offset], which is the left neighbor of the current node.
+     Calculate current common parent of L[n_path_offset] and the current node. Note that
+     CFL[n_path_offset] not equal FL[n_path_offset] and CFL[n_path_offset] not equal F[n_path_offset].
+     Calculate lkey[n_path_offset]. */
+    if ( (n_ret_value = get_far_parent(p_s_tb, n_h + 1, &p_s_curf,
+				       &p_s_curcf, LEFT_PARENTS)) != CARRY_ON )
+      return n_ret_value; /*schedule() occured or path is not correct*/
+  }
+
+ decrement_bcount(p_s_tb->FL[n_h]);	
+ p_s_tb->FL[n_h] = p_s_curf; /* New initialization of FL[n_h]. */
+ 
+ decrement_bcount(p_s_tb->CFL[n_h]);
+ p_s_tb->CFL[n_h] = p_s_curcf; /* New initialization of CFL[n_h]. */
+
+#ifdef CONFIG_REISERFS_CHECK
+ if ((p_s_curf && !B_IS_IN_TREE (p_s_curf)) || (p_s_curcf && !B_IS_IN_TREE (p_s_curcf))) {
+   reiserfs_panic (p_s_tb->tb_sb, "PAP-8195: get_parents: FL (%b) or CFL (%b) is invalid", p_s_curf, p_s_curcf);
+ }
+#endif
+
+/* Get parent FR[n_h] of R[n_h]. */
+
+/* Current node is the last child of F[n_h]. FR[n_h] != F[n_h]. */
+ if ( n_position == B_BLK_HEAD(PATH_H_PBUFFER(p_s_path, n_h + 1))->blk_nr_item ) {
+/* Calculate current parent of R[n_h], which is the right neighbor of F[n_h].
+   Calculate current common parent of R[n_h] and current node. Note that CFR[n_h]
+   not equal FR[n_path_offset] and CFR[n_h] not equal F[n_h]. */
+   if ( (n_ret_value = get_far_parent(p_s_tb, n_h + 1, &p_s_curf,  &p_s_curcf, RIGHT_PARENTS)) != CARRY_ON )
+     return n_ret_value; /*schedule() occured while get_far_parent() worked.*/
+ }
+ else {
+/* Current node is not the last child of its parent F[n_h]. */
+   (p_s_curf = p_s_curcf = PATH_OFFSET_PBUFFER(p_s_path, n_path_offset - 1))->b_count += 2;
+   p_s_tb->rkey[n_h] = n_position;
+ }	
+
+ decrement_bcount(p_s_tb->FR[n_h]);
+ p_s_tb->FR[n_h] = p_s_curf; /* New initialization of FR[n_path_offset]. */
+
+ decrement_bcount(p_s_tb->CFR[n_h]);
+ p_s_tb->CFR[n_h] = p_s_curcf; /* New initialization of CFR[n_path_offset]. */
+
+#ifdef CONFIG_REISERFS_CHECK
+ if (n_h == 0 && p_s_tb->CFR[n_h] && COMP_KEYS (B_PRIGHT_DELIM_KEY (PATH_H_PBUFFER(p_s_path, n_h)), 
+						B_N_PDELIM_KEY (p_s_tb->CFR[n_h], p_s_tb->rkey[n_h]))) {
+   reiserfs_panic (p_s_tb->tb_sb, "PAP-8200: get_parents: rdkey in S0 %k, rdkey in CFR0 %k do not match",
+		   B_PRIGHT_DELIM_KEY (PATH_H_PBUFFER(p_s_path, n_h)), B_N_PDELIM_KEY (p_s_tb->CFR[n_h], p_s_tb->rkey[n_h]));
+ }
+ if ((p_s_curf && !B_IS_IN_TREE (p_s_curf)) || (p_s_curcf && !B_IS_IN_TREE (p_s_curcf))) {
+   reiserfs_panic (p_s_tb->tb_sb, "PAP-8205: get_parents: FR (%b) or CFR (%b) is invalid", p_s_curf, p_s_curcf);
+ }
+#endif
+
+ return CARRY_ON; /* schedule not occured while get_parents() worked. */
+}
+
+
+/* it is possible to remove node as result of shiftings to
+   neighbors even when we insert or paste item. */
+static inline int can_node_be_removed (int mode, int lfree, int sfree, int rfree, struct tree_balance * tb, int h)
+{
+  struct buffer_head * Sh = PATH_H_PBUFFER (tb->tb_path, h);
+  int levbytes = tb->insert_size[h];
+  struct item_head * ih;
+  struct item_head * r_ih = NULL;
+  
+  ih = B_N_PITEM_HEAD (Sh, 0);
+  if ( tb->CFR[h] )
+    r_ih = (struct item_head *)B_N_PDELIM_KEY(tb->CFR[h],tb->rkey[h]);
+  
+  if (
+      lfree + rfree + sfree < MAX_CHILD_SIZE(Sh) + levbytes
+      /* shifting may merge items which might save space */
+#ifdef REISERFS_FSCK
+      - (( ! h && is_left_mergeable (tb->tb_sb, tb->tb_path) == 1 ) ? IH_SIZE : 0)
+      - (( ! h && r_ih && is_right_mergeable (tb->tb_sb, tb->tb_path) == 1 ) ? IH_SIZE : 0)
+#else
+      - (( ! h && is_left_mergeable (ih, Sh->b_size) ) ? IH_SIZE : 0)
+      - (( ! h && r_ih && is_left_mergeable (r_ih, Sh->b_size) ) ? IH_SIZE : 0)
+#endif
+      + (( h ) ? KEY_SIZE : 0))
+    {
+      /* node can not be removed */
+      if (sfree >= levbytes ) /* new item fits into node S[h] without any shifting */
+	{
+	  if ( ! h )
+	    tb->s0num = B_NR_ITEMS(Sh) + ((mode == M_INSERT ) ? 1 : 0);
+	  set_parameters (tb, h, 0, 0, 1, NULL, -1, -1);
+	  return NO_BALANCING_NEEDED;
+	}
+    }
+  return !NO_BALANCING_NEEDED;
+}
+
+
+
+/* Check whether current node S[h] is balanced when increasing its size by
+ * Inserting or Pasting.
+ * Calculate parameters for balancing for current level h.
+ * Parameters:
+ *	tb	tree_balance structure;
+ *	h	current level of the node;
+ *	inum	item number in S[h];
+ *	mode	i - insert, p - paste;
+ * Returns:	1 - schedule occured; 
+ *	        0 - balancing for higher levels needed;
+ *	       -1 - no balancing for higher levels needed;
+ *	       -2 - no disk space.
+ */
+/* ip means Inserting or Pasting */
+static int ip_check_balance (struct reiserfs_transaction_handle *th, struct tree_balance * tb, int h)
+{
+  struct virtual_node * vn = tb->tb_vn;
+  int levbytes,  /* Number of bytes that must be inserted into (value
+		    is negative if bytes are deleted) buffer which
+		    contains node being balanced.  The mnemonic is
+		    that the attempted change in node space used level
+		    is levbytes bytes. */
+    n_ret_value;
+
+  int lfree, sfree, rfree /* free space in L, S and R */;
+
+  /* nver is short for number of vertixes, and lnver is the number if
+     we shift to the left, rnver is the number if we shift to the
+     right, and lrnver is the number if we shift in both directions.
+     The goal is to minimize first the number of vertixes, and second,
+     the number of vertixes whose contents are changed by shifting,
+     and third the number of uncached vertixes whose contents are
+     changed by shifting and must be read from disk.  */
+  int nver, lnver, rnver, lrnver;
+
+  /* used at leaf level only, S0 = S[0] is the node being balanced,
+     sInum [ I = 0,1,2 ] is the number of items that will
+     remain in node SI after balancing.  S1 and S2 are new
+     nodes that might be created. */
+  
+  /* we perform 8 calls to get_num_ver().  For each call we calculate five parameters.
+     where 4th parameter is s1bytes and 5th - s2bytes
+  */
+  short snum012[40] = {0,};	/* s0num, s1num, s2num for 8 cases 
+				   0,1 - do not shift and do not shift but bottle
+				   2 - shift only whole item to left
+				   3 - shift to left and bottle as much as possible
+				   4,5 - shift to right	(whole items and as much as possible
+				   6,7 - shift to both directions (whole items and as much as possible)
+				   */
+
+  /* Sh is the node whose balance is currently being checked */
+  struct buffer_head * Sh;
+  
+#ifdef REISERFS_FSCK
+  /* special mode for insert pointer to the most low internal node */
+  if (h == 0 && vn->vn_mode == M_INTERNAL) {
+    /* blk_num == 2 is to get pointer inserted to the next level */
+    set_parameters (tb, h, 0, 0, 2, NULL, -1, -1);
+    return 0;
+  }
+#endif
+
+  Sh = PATH_H_PBUFFER (tb->tb_path, h);
+  levbytes = tb->insert_size[h];
+  
+     /* Calculate balance parameters for creating new root. */
+  if ( ! Sh )  {
+    if ( ! h )
+      reiserfs_panic (tb->tb_sb, "vs-8210: ip_check_balance: S[0] can not be 0");
+    switch ( n_ret_value = get_empty_nodes (th, tb, h) )  {
+    case CARRY_ON:
+      set_parameters (tb, h, 0, 0, 1, NULL, -1, -1);
+      return NO_BALANCING_NEEDED; /* no balancing for higher levels needed */
+
+    case NO_DISK_SPACE:
+    case SCHEDULE_OCCURRED:
+      return n_ret_value;
+    default:   
+      reiserfs_panic(tb->tb_sb, "vs-8215: ip_check_balance: incorrect return value of get_empty_nodes");
+    }
+  }
+  
+  if ( (n_ret_value = get_parents (tb, h)) != CARRY_ON ) /* get parents of S[h] neighbors. */
+    return n_ret_value;
+  
+     sfree = B_BLK_HEAD(Sh)->blk_free_space;
+
+     /* get free space of neighbors */
+     rfree = get_rfree (tb, h);
+     lfree = get_lfree (tb, h);
+
+     if (can_node_be_removed (vn->vn_mode, lfree, sfree, rfree, tb, h) == NO_BALANCING_NEEDED)
+       /* and new item fits into node S[h] without any shifting */
+       return NO_BALANCING_NEEDED;
+     
+     create_virtual_node (tb, h);
+
+     /*	
+	determine maximal number of items we can shift to the left neighbor (in tb structure)
+	and the maximal number of bytes that can flow to the left neighbor
+	from the left most liquid item that cannot be shifted from S[0] entirely (returned value)
+	*/
+     check_left (tb, h, lfree);
+
+     /*
+        determine maximal number of items we can shift to the right neighbor (in tb structure)
+	and the maximal number of bytes that can flow to the right neighbor
+	from the right most liquid item that cannot be shifted from S[0] entirely (returned value)
+	*/
+     check_right (tb, h, rfree);
+
+
+     /* all contents of internal node S[h] can be moved into its
+        neighbors, S[h] will be removed after balancing */
+     if (h && (tb->rnum[h] + tb->lnum[h] >= vn->vn_nr_item + 1)) {
+       int to_r; 
+       
+       /* Since we are working on internal nodes, and our internal
+	  nodes have fixed size entries, then we can balance by the
+	  number of items rather than the space they consume.  In this
+	  routine we set the left node equal to the right node,
+	  allowing a difference of less than or equal to 1 child
+	  pointer. */
+       to_r = ((MAX_NR_KEY(Sh)<<1)+2-tb->lnum[h]-tb->rnum[h]+vn->vn_nr_item+1)/2 - 
+	 (MAX_NR_KEY(Sh) + 1 - tb->rnum[h]);
+       set_parameters (tb, h, vn->vn_nr_item + 1 - to_r, to_r, 0, NULL, -1, -1);
+       return CARRY_ON;
+     }
+
+#ifdef CONFIG_REISERFS_CHECK
+     /* this checks balance condition, that any two neighboring nodes can not fit in one node */
+     if ( h && ( tb->lnum[h] >= vn->vn_nr_item + 1 || tb->rnum[h] >= vn->vn_nr_item + 1) )
+       reiserfs_panic (tb->tb_sb, "vs-8220: ip_check_balance: tree is not balanced on internal level");
+
+     if ( ! h && ((tb->lnum[h] >= vn->vn_nr_item && (tb->lbytes == -1)) ||
+		  (tb->rnum[h] >= vn->vn_nr_item && (tb->rbytes == -1)) ))
+       reiserfs_panic(tb->tb_sb, "vs-8225: ip_check_balance: tree is not balanced on leaf level");
+#endif
+
+     /* all contents of S[0] can be moved into its neighbors
+	S[0] will be removed after balancing. */
+     if (!h && is_leaf_removable (tb))
+       return CARRY_ON;
+
+
+     /* why do we perform this check here rather than earlier??
+        Answer: we can win 1 node in some cases above. Moreover we
+        checked it above, when we checked, that S[0] is not removable
+        in principle */
+     if (sfree >= levbytes) { /* new item fits into node S[h] without any shifting */
+       if ( ! h )
+	 tb->s0num = vn->vn_nr_item;
+       set_parameters (tb, h, 0, 0, 1, NULL, -1, -1);
+       return NO_BALANCING_NEEDED;
+     }
+
+
+     {
+       int lpar, rpar, nset, lset, rset, lrset;
+     /* 
+      * regular overflowing of the node
+      */
+
+     /* get_num_ver works in 2 modes (FLOW & NO_FLOW) 
+	lpar, rpar - number of items we can shift to left/right neighbor (including splitting item)
+	nset, lset, rset, lrset - shows, whether flowing items give better packing 
+	*/
+#define FLOW 1
+#define NO_FLOW 0	/* do not any splitting */
+
+     /* we choose one the following */
+#define NOTHING_SHIFT_NO_FLOW	0
+#define NOTHING_SHIFT_FLOW	5
+#define LEFT_SHIFT_NO_FLOW	10
+#define LEFT_SHIFT_FLOW		15
+#define RIGHT_SHIFT_NO_FLOW	20
+#define RIGHT_SHIFT_FLOW	25
+#define LR_SHIFT_NO_FLOW	30
+#define LR_SHIFT_FLOW		35
+
+
+       lpar = tb->lnum[h];
+       rpar = tb->rnum[h];
+
+
+     /* calculate number of blocks S[h] must be split into when
+	nothing is shifted to the neighbors,
+	as well as number of items in each part of the split node (s012 numbers),
+	and number of bytes (s1bytes) of the shared drop which flow to S1 if any */
+     nset = NOTHING_SHIFT_NO_FLOW;
+     nver = get_num_ver (vn->vn_mode, tb, h,
+			 0, -1, h?vn->vn_nr_item:0, -1, 
+			 snum012, NO_FLOW);
+
+     if (!h)
+       {
+	 int nver1;
+
+	 /* note, that in this case we try to bottle between S[0] and S1 (S1 - the first new node) */
+	 nver1 = get_num_ver (vn->vn_mode, tb, h, 
+			      0, -1, 0, -1, 
+			      snum012 + NOTHING_SHIFT_FLOW, FLOW);
+	 if (nver > nver1)
+	   nset = NOTHING_SHIFT_FLOW, nver = nver1;
+       }
+       
+ 
+     /* calculate number of blocks S[h] must be split into when
+	l_shift_num first items and l_shift_bytes of the right most
+	liquid item to be shifted are shifted to the left neighbor,
+	as well as number of items in each part of the splitted node (s012 numbers),
+	and number of bytes (s1bytes) of the shared drop which flow to S1 if any
+	*/
+     lset = LEFT_SHIFT_NO_FLOW;
+     lnver = get_num_ver (vn->vn_mode, tb, h, 
+			  lpar - (( h || tb->lbytes == -1 ) ? 0 : 1), -1, h ? vn->vn_nr_item:0, -1,
+			  snum012 + LEFT_SHIFT_NO_FLOW, NO_FLOW);
+     if (!h)
+       {
+	 int lnver1;
+
+	 lnver1 = get_num_ver (vn->vn_mode, tb, h, 
+			       lpar - ((tb->lbytes != -1) ? 1 : 0), tb->lbytes, 0, -1,
+			       snum012 + LEFT_SHIFT_FLOW, FLOW);
+	 if (lnver > lnver1)
+	   lset = LEFT_SHIFT_FLOW, lnver = lnver1;
+       }
+
+
+     /* calculate number of blocks S[h] must be split into when
+	r_shift_num first items and r_shift_bytes of the left most
+	liquid item to be shifted are shifted to the right neighbor,
+	as well as number of items in each part of the splitted node (s012 numbers),
+	and number of bytes (s1bytes) of the shared drop which flow to S1 if any
+	*/
+     rset = RIGHT_SHIFT_NO_FLOW;
+     rnver = get_num_ver (vn->vn_mode, tb, h, 
+			  0, -1, h ? (vn->vn_nr_item-rpar) : (rpar - (( tb->rbytes != -1 ) ? 1 : 0)), -1, 
+			  snum012 + RIGHT_SHIFT_NO_FLOW, NO_FLOW);
+     if (!h)
+       {
+	 int rnver1;
+
+	 rnver1 = get_num_ver (vn->vn_mode, tb, h, 
+			       0, -1, (rpar - ((tb->rbytes != -1) ? 1 : 0)), tb->rbytes, 
+			       snum012 + RIGHT_SHIFT_FLOW, FLOW);
+
+	 if (rnver > rnver1)
+	   rset = RIGHT_SHIFT_FLOW, rnver = rnver1;
+       }
+
+
+     /* calculate number of blocks S[h] must be split into when
+	items are shifted in both directions,
+	as well as number of items in each part of the splitted node (s012 numbers),
+	and number of bytes (s1bytes) of the shared drop which flow to S1 if any
+	*/
+     lrset = LR_SHIFT_NO_FLOW;
+     lrnver = get_num_ver (vn->vn_mode, tb, h, 
+			   lpar - ((h || tb->lbytes == -1) ? 0 : 1), -1, h ? (vn->vn_nr_item-rpar):(rpar - ((tb->rbytes != -1) ? 1 : 0)), -1,
+			   snum012 + LR_SHIFT_NO_FLOW, NO_FLOW);
+     if (!h)
+       {
+	 int lrnver1;
+
+	 lrnver1 = get_num_ver (vn->vn_mode, tb, h, 
+				lpar - ((tb->lbytes != -1) ? 1 : 0), tb->lbytes, (rpar - ((tb->rbytes != -1) ? 1 : 0)), tb->rbytes,
+				snum012 + LR_SHIFT_FLOW, FLOW);
+	 if (lrnver > lrnver1)
+	   lrset = LR_SHIFT_FLOW, lrnver = lrnver1;
+       }
+
+
+
+     /* Our general shifting strategy is:
+	1) to minimized number of new nodes;
+	2) to minimized number of neighbors involved in shifting;
+	3) to minimized number of disk reads; */
+
+     /* we can win TWO or ONE nodes by shifting in both directions */
+     if (lrnver < lnver && lrnver < rnver)
+       {
+#ifdef CONFIG_REISERFS_CHECK
+	 if (h && (tb->lnum[h] != 1 || tb->rnum[h] != 1 || lrnver != 1 || rnver != 2 || lnver != 2 || h != 1))
+	   reiserfs_panic (0, "vs-8230: check_balance: bad h");
+#endif
+	 if (lrset == LR_SHIFT_FLOW)
+	   set_parameters (tb, h, tb->lnum[h], tb->rnum[h], lrnver, snum012 + lrset,
+			   tb->lbytes, tb->rbytes);
+	 else
+ 	   set_parameters (tb, h, tb->lnum[h] - ((tb->lbytes == -1) ? 0 : 1), 
+			   tb->rnum[h] - ((tb->rbytes == -1) ? 0 : 1), lrnver, snum012 + lrset, -1, -1);
+
+	 return CARRY_ON;
+       }
+
+     /* if shifting doesn't lead to better packing then don't shift */
+     if (nver == lrnver)
+       {
+	 set_parameters (tb, h, 0, 0, nver, snum012 + nset, -1, -1);
+	 return CARRY_ON;
+       }
+
+
+     /* now we know that for better packing shifting in only one
+	direction either to the left or to the right is required */
+
+     /*  if shifting to the left is better than shifting to the right */
+     if (lnver < rnver)
+       {
+	 SET_PAR_SHIFT_LEFT;
+	 return CARRY_ON;
+       }
+
+     /* if shifting to the right is better than shifting to the left */
+     if (lnver > rnver)
+       {
+	 SET_PAR_SHIFT_RIGHT;
+	 return CARRY_ON;
+       }
+
+
+     /* now shifting in either direction gives the same number
+	of nodes and we can make use of the cached neighbors */
+     if (is_left_neighbor_in_cache (tb,h))
+       {
+	 SET_PAR_SHIFT_LEFT;
+	 return CARRY_ON;
+       }
+
+     /* shift to the right independently on whether the right neighbor in cache or not */
+     SET_PAR_SHIFT_RIGHT;
+     return CARRY_ON;
+     }
+}
+
+
+/* Check whether current node S[h] is balanced when Decreasing its size by
+ * Deleting or Cutting for INTERNAL node of S+tree.
+ * Calculate parameters for balancing for current level h.
+ * Parameters:
+ *	tb	tree_balance structure;
+ *	h	current level of the node;
+ *	inum	item number in S[h];
+ *	mode	i - insert, p - paste;
+ * Returns:	1 - schedule occured; 
+ *	        0 - balancing for higher levels needed;
+ *	       -1 - no balancing for higher levels needed;
+ *	       -2 - no disk space.
+ *
+ * Note: Items of internal nodes have fixed size, so the balance condition for
+ * the internal part of S+tree is as for the B-trees.
+ */
+static int dc_check_balance_internal (struct tree_balance * tb, int h)
+{
+  struct virtual_node * vn = tb->tb_vn;
+
+  /* Sh is the node whose balance is currently being checked,
+     and Fh is its father.  */
+  struct buffer_head * Sh, * Fh;
+  int maxsize,
+      n_ret_value;
+  int lfree, rfree /* free space in L and R */;
+
+  Sh = PATH_H_PBUFFER (tb->tb_path, h); 
+  Fh = PATH_H_PPARENT (tb->tb_path, h); 
+
+  maxsize = MAX_CHILD_SIZE(Sh); 
+
+/*   using tb->insert_size[h], which is negative in this case, create_virtual_node calculates: */
+/*   new_nr_item = number of items node would have if operation is */
+/* 	performed without balancing (new_nr_item); */
+  create_virtual_node (tb, h);
+
+  if ( ! Fh )
+    {   /* S[h] is the root. */
+      if ( vn->vn_nr_item > 0 )
+	{
+	  set_parameters (tb, h, 0, 0, 1, NULL, -1, -1);
+	  return NO_BALANCING_NEEDED; /* no balancing for higher levels needed */
+	}
+      /* new_nr_item == 0.
+       * Current root will be deleted resulting in
+       * decrementing the tree height. */
+      set_parameters (tb, h, 0, 0, 0, NULL, -1, -1);
+      return CARRY_ON;
+    }
+
+  if ( (n_ret_value = get_parents(tb,h)) != CARRY_ON )
+    return n_ret_value;
+
+
+  /* get free space of neighbors */
+  rfree = get_rfree (tb, h);
+  lfree = get_lfree (tb, h);
+		
+  /* determine maximal number of items we can fit into neighbors */
+  check_left (tb, h, lfree);
+  check_right (tb, h, rfree);
+
+
+  if ( vn->vn_nr_item >= MIN_NR_KEY(Sh) )
+    { /* Balance condition for the internal node is valid.
+       * In this case we balance only if it leads to better packing. */ 
+      if ( vn->vn_nr_item == MIN_NR_KEY(Sh) )
+	{ /* Here we join S[h] with one of its neighbors,
+	   * which is impossible with greater values of new_nr_item. */
+	  if ( tb->lnum[h] >= vn->vn_nr_item + 1 )
+	    {
+	      /* All contents of S[h] can be moved to L[h]. */
+	      int n;
+	      int order_L;
+	      
+	      order_L = ((n=PATH_H_B_ITEM_ORDER(tb->tb_path, h))==0) ? B_NR_ITEMS(tb->FL[h]) : n - 1;
+	      n = B_N_CHILD(tb->FL[h],order_L)->dc_size / (DC_SIZE + KEY_SIZE);
+	      set_parameters (tb, h, -n-1, 0, 0, NULL, -1, -1);
+	      return CARRY_ON;
+	    }
+
+	  if ( tb->rnum[h] >= vn->vn_nr_item + 1 )
+	    {
+	      /* All contents of S[h] can be moved to R[h]. */
+	      int n;
+	      int order_R;
+	    
+	      order_R = ((n=PATH_H_B_ITEM_ORDER(tb->tb_path, h))==B_NR_ITEMS(Fh)) ? 0 : n + 1;
+	      n = B_N_CHILD(tb->FR[h],order_R)->dc_size / (DC_SIZE + KEY_SIZE);
+	      set_parameters (tb, h, 0, -n-1, 0, NULL, -1, -1);
+	      return CARRY_ON;   
+	    }
+	}
+
+      if (tb->rnum[h] + tb->lnum[h] >= vn->vn_nr_item + 1)
+	{
+	  /* All contents of S[h] can be moved to the neighbors (L[h] & R[h]). */
+	  int to_r;
+
+	  to_r = ((MAX_NR_KEY(Sh)<<1)+2-tb->lnum[h]-tb->rnum[h]+vn->vn_nr_item+1)/2 - 
+	    (MAX_NR_KEY(Sh) + 1 - tb->rnum[h]);
+	  set_parameters (tb, h, vn->vn_nr_item + 1 - to_r, to_r, 0, NULL, -1, -1);
+	  return CARRY_ON;
+	}
+
+      /* Balancing does not lead to better packing. */
+      set_parameters (tb, h, 0, 0, 1, NULL, -1, -1);
+      return NO_BALANCING_NEEDED;
+    }
+
+  /* Current node contain insufficient number of items. Balancing is required. */	
+  /* Check whether we can merge S[h] with left neighbor. */
+  if (tb->lnum[h] >= vn->vn_nr_item + 1)
+    if (is_left_neighbor_in_cache (tb,h) || tb->rnum[h] < vn->vn_nr_item + 1 || !tb->FR[h])
+      {
+	int n;
+	int order_L;
+	      
+	order_L = ((n=PATH_H_B_ITEM_ORDER(tb->tb_path, h))==0) ? B_NR_ITEMS(tb->FL[h]) : n - 1;
+	n = B_N_CHILD(tb->FL[h],order_L)->dc_size / (DC_SIZE + KEY_SIZE);
+	set_parameters (tb, h, -n-1, 0, 0, NULL, -1, -1);
+	return CARRY_ON;
+      }
+
+  /* Check whether we can merge S[h] with right neighbor. */
+  if (tb->rnum[h] >= vn->vn_nr_item + 1)
+    {
+      int n;
+      int order_R;
+	    
+      order_R = ((n=PATH_H_B_ITEM_ORDER(tb->tb_path, h))==B_NR_ITEMS(Fh)) ? 0 : (n + 1);
+      n = B_N_CHILD(tb->FR[h],order_R)->dc_size / (DC_SIZE + KEY_SIZE);
+      set_parameters (tb, h, 0, -n-1, 0, NULL, -1, -1);
+      return CARRY_ON;   
+    }
+
+  /* All contents of S[h] can be moved to the neighbors (L[h] & R[h]). */
+  if (tb->rnum[h] + tb->lnum[h] >= vn->vn_nr_item + 1)
+    {
+      int to_r;
+	    
+      to_r = ((MAX_NR_KEY(Sh)<<1)+2-tb->lnum[h]-tb->rnum[h]+vn->vn_nr_item+1)/2 - 
+	(MAX_NR_KEY(Sh) + 1 - tb->rnum[h]);
+      set_parameters (tb, h, vn->vn_nr_item + 1 - to_r, to_r, 0, NULL, -1, -1);
+      return CARRY_ON;
+    }
+
+  /* For internal nodes try to borrow item from a neighbor */
+#ifdef CONFIG_REISERFS_CHECK
+  if (!tb->FL[h] && !tb->FR[h])
+    reiserfs_panic (0, "vs-8235: dc_check_balance_internal: trying to borrow for root");
+#endif
+
+  /* Borrow one or two items from caching neighbor */
+  if (is_left_neighbor_in_cache (tb,h) || !tb->FR[h])
+    {
+      int from_l;
+		
+      from_l = (MAX_NR_KEY(Sh) + 1 - tb->lnum[h] + vn->vn_nr_item + 1) / 2 -  (vn->vn_nr_item + 1);
+      set_parameters (tb, h, -from_l, 0, 1, NULL, -1, -1);
+      return CARRY_ON;
+    }
+
+  set_parameters (tb, h, 0, -((MAX_NR_KEY(Sh)+1-tb->rnum[h]+vn->vn_nr_item+1)/2-(vn->vn_nr_item+1)), 1, 
+		  NULL, -1, -1);
+  return CARRY_ON;
+}
+
+
+/* Check whether current node S[h] is balanced when Decreasing its size by
+ * Deleting or Truncating for LEAF node of S+tree.
+ * Calculate parameters for balancing for current level h.
+ * Parameters:
+ *	tb	tree_balance structure;
+ *	h	current level of the node;
+ *	inum	item number in S[h];
+ *	mode	i - insert, p - paste;
+ * Returns:	1 - schedule occured; 
+ *	        0 - balancing for higher levels needed;
+ *	       -1 - no balancing for higher levels needed;
+ *	       -2 - no disk space.
+ */
+static int dc_check_balance_leaf (struct tree_balance * tb, int h)
+{
+  struct virtual_node * vn = tb->tb_vn;
+
+  /* Number of bytes that must be deleted from
+     (value is negative if bytes are deleted) buffer which
+     contains node being balanced.  The mnemonic is that the
+     attempted change in node space used level is levbytes bytes. */
+  int levbytes;
+  /* the maximal item size */
+  int maxsize,
+      n_ret_value;
+  /* S0 is the node whose balance is currently being checked,
+     and F0 is its father.  */
+  struct buffer_head * S0, * F0;
+  int lfree, rfree /* free space in L and R */;
+
+  S0 = PATH_H_PBUFFER (tb->tb_path, 0);
+  F0 = PATH_H_PPARENT (tb->tb_path, 0);
+
+  levbytes = tb->insert_size[h];
+
+  maxsize = MAX_CHILD_SIZE(S0); 	/* maximal possible size of an item */
+
+  if ( ! F0 )
+    {  /* S[0] is the root now. */
+
+#ifdef CONFIG_REISERFS_CHECK
+      if ( -levbytes >= maxsize - B_BLK_HEAD(S0)->blk_free_space )
+	reiserfs_panic (tb->tb_sb, "vs-8240: dc_check_balance_leaf: attempt to create empty buffer tree");
+#endif
+
+      set_parameters (tb, h, 0, 0, 1, NULL, -1, -1);
+      return NO_BALANCING_NEEDED;
+    }
+
+  if ( (n_ret_value = get_parents(tb,h)) != CARRY_ON )
+    return n_ret_value;
+
+  /* get free space of neighbors */
+  rfree = get_rfree (tb, h);
+  lfree = get_lfree (tb, h);		
+
+  create_virtual_node (tb, h);
+
+  /* if 3 leaves can be merge to one, set parameters and return */
+  if (are_leaves_removable (tb, lfree, rfree))
+    return CARRY_ON;
+
+  /* determine maximal number of items we can shift to the left/right  neighbor
+     and the maximal number of bytes that can flow to the left/right neighbor
+     from the left/right most liquid item that cannot be shifted from S[0] entirely
+     */
+  check_left (tb, h, lfree);
+  check_right (tb, h, rfree);   
+
+  /* check whether we can merge S with left neighbor. */
+  if (tb->lnum[0] >= vn->vn_nr_item && tb->lbytes == -1)
+    if (is_left_neighbor_in_cache (tb,h) ||
+	((tb->rnum[0] - ((tb->rbytes == -1) ? 0 : 1)) < vn->vn_nr_item) || /* S can not be merged with R */
+	!tb->FR[h]) {
+      
+#ifdef CONFIG_REISERFS_CHECK
+      if (!tb->FL[h])
+	reiserfs_panic (0, "vs-8245: dc_check_balance_leaf: FL[h] must exist");
+#endif
+
+      /* set parameter to merge S[0] with its left neighbor */
+      set_parameters (tb, h, -1, 0, 0, NULL, -1, -1);
+      return CARRY_ON;
+    }
+
+  /* check whether we can merge S[0] with right neighbor. */
+  if (tb->rnum[0] >= vn->vn_nr_item && tb->rbytes == -1) {
+    set_parameters (tb, h, 0, -1, 0, NULL, -1, -1);
+    return CARRY_ON;
+  }
+  
+  /* All contents of S[0] can be moved to the neighbors (L[0] & R[0]). Set parameters and return */
+  if (is_leaf_removable (tb))
+    return CARRY_ON;
+  
+  /* Balancing is not required. */
+  tb->s0num = vn->vn_nr_item;
+  set_parameters (tb, h, 0, 0, 1, NULL, -1, -1);
+  return NO_BALANCING_NEEDED;
+}
+
+
+
+/* Check whether current node S[h] is balanced when Decreasing its size by
+ * Deleting or Cutting.
+ * Calculate parameters for balancing for current level h.
+ * Parameters:
+ *	tb	tree_balance structure;
+ *	h	current level of the node;
+ *	inum	item number in S[h];
+ *	mode	d - delete, c - cut.
+ * Returns:	1 - schedule occured; 
+ *	        0 - balancing for higher levels needed;
+ *	       -1 - no balancing for higher levels needed;
+ *	       -2 - no disk space.
+ */
+static int dc_check_balance (struct tree_balance * tb, int h)
+{
+
+#ifdef CONFIG_REISERFS_CHECK
+ if ( ! (PATH_H_PBUFFER (tb->tb_path, h)) )
+   reiserfs_panic(tb->tb_sb, "vs-8250: dc_check_balance: S is not initialized");
+#endif
+
+ if ( h )
+   return dc_check_balance_internal (tb, h);
+ else
+   return dc_check_balance_leaf (tb, h);
+}
+
+
+
+/* Check whether current node S[h] is balanced.
+ * Calculate parameters for balancing for current level h.
+ * Parameters:
+ *
+ *	tb	tree_balance structure:
+ *
+ *              tb is a large structure that must be read about in the header file
+ *              at the same time as this procedure if the reader is to successfully
+ *              understand this procedure
+ *
+ *	h	current level of the node;
+ *	inum	item number in S[h];
+ *	mode	i - insert, p - paste, d - delete, c - cut.
+ * Returns:	1 - schedule occured; 
+ *	        0 - balancing for higher levels needed;
+ *	       -1 - no balancing for higher levels needed;
+ *	       -2 - no disk space.
+ */
+static int check_balance (struct reiserfs_transaction_handle *th, 
+   			  int mode, 
+			  struct tree_balance * tb,
+			  int h, 
+			  int inum,
+			  int pos_in_item,
+			  struct item_head * ins_ih
+			  )
+{
+  struct virtual_node * vn;
+
+  vn = tb->tb_vn = (struct virtual_node *)(tb->vn_buf + ROUND_UP(SB_BMAP_NR (tb->tb_sb) * 2 / 8 + 1, 4));
+  vn->vn_free_ptr = (char *)(tb->tb_vn + 1);
+  vn->vn_mode = mode;
+  vn->vn_affected_item_num = inum;
+  vn->vn_pos_in_item = pos_in_item;
+  vn->vn_ins_ih = ins_ih;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (mode == M_INSERT && !vn->vn_ins_ih)
+    reiserfs_panic (0, "vs-8255: check_balance: ins_ih can not be 0 in insert mode");
+#endif
+
+ if ( tb->insert_size[h] > 0 )
+   /* Calculate balance parameters when size of node is increasing. */
+   return ip_check_balance (th, tb, h);
+
+ /* Calculate balance parameters when  size of node is decreasing. */
+ return dc_check_balance (tb, h);
+}
+
+
+
+/* Check whether parent at the path is the really parent of the current node.*/
+static int  get_direct_parent(
+              struct tree_balance * p_s_tb,
+              int                   n_h
+            ) {
+  struct buffer_head  * p_s_bh;
+  struct path         * p_s_path      = p_s_tb->tb_path;
+  int                   n_position,
+    			n_path_offset = PATH_H_PATH_OFFSET(p_s_tb->tb_path, n_h);
+
+  /* We are in the root or in the new root. */
+  if ( n_path_offset <= FIRST_PATH_ELEMENT_OFFSET ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( n_path_offset < FIRST_PATH_ELEMENT_OFFSET - 1 )
+      reiserfs_panic(p_s_tb->tb_sb, "PAP-8260: get_direct_parent: illegal offset in the path");
+#endif
+
+    if ( PATH_OFFSET_PBUFFER(p_s_path, FIRST_PATH_ELEMENT_OFFSET)->b_blocknr ==
+                                      p_s_tb->tb_sb->u.reiserfs_sb.s_rs->s_root_block ) {
+      /* Root is not changed. */
+      PATH_OFFSET_PBUFFER(p_s_path, n_path_offset - 1) = NULL;
+	    PATH_OFFSET_POSITION(p_s_path, n_path_offset - 1) = 0;
+      return CARRY_ON;
+    }
+    return PATH_INCORRECT; /* Root is changed and we must recalculate the path. */
+  }
+
+  if ( ! B_IS_IN_TREE(p_s_bh = PATH_OFFSET_PBUFFER(p_s_path, n_path_offset - 1)) )
+    return PATH_INCORRECT; /* Parent in the path is not in the tree. */
+
+  if ( (n_position = PATH_OFFSET_POSITION(p_s_path, n_path_offset - 1)) > B_NR_ITEMS(p_s_bh) )
+    return PATH_INCORRECT;
+
+  if ( B_N_CHILD_NUM(p_s_bh, n_position) != PATH_OFFSET_PBUFFER(p_s_path, n_path_offset)->b_blocknr )
+     /* Parent in the path is not parent of the current node in the tree. */
+    return PATH_INCORRECT;
+
+  if ( test_and_wait_on_buffer(p_s_bh) == SCHEDULE_OCCURRED ) /* Buffer was locked. */
+    return SCHEDULE_OCCURRED;
+
+  return CARRY_ON; /* Parent in the path is unlocked and really parent of the current node.  */
+}
+
+
+/* Using lnum[n_h] and rnum[n_h] we should determine what neighbors
+ * of S[n_h] we
+ * need in order to balance S[n_h], and get them if necessary.
+ * Returns:	SCHEDULE_OCCURRED - schedule occured while the function worked;
+ *	        CARRY_ON - schedule didn't occur while the function worked;
+ */
+static int  get_neighbors(
+	            struct tree_balance * p_s_tb,
+	            int 		  n_h
+	          ) {
+  int		 	n_child_position,
+    			n_repeat,
+          		n_path_offset = PATH_H_PATH_OFFSET(p_s_tb->tb_path, n_h + 1);
+  unsigned long		n_son_number;
+  struct super_block  *	p_s_sb = p_s_tb->tb_sb;
+  struct buffer_head  * p_s_bh;
+  struct virtual_node * vn = p_s_tb->tb_vn;/*(struct virtual_node *)(p_s_tb->vn_buf);*/
+
+  if ( p_s_tb->lnum[n_h] ||
+      ( ! n_h && ! vn->vn_affected_item_num && (vn->vn_mode == M_DELETE ||
+						(vn->vn_mode == M_CUT && ! vn->vn_pos_in_item)
+#ifdef REISERFS_FSCK
+						|| (vn->vn_mode == M_PASTE && ! vn->vn_pos_in_item)
+#endif
+						)) ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+   if ( ! p_s_tb->lnum[n_h] && vn->vn_mode == M_CUT &&
+	! (vn->vn_vi[0].vi_type & VI_TYPE_DIRECTORY) )
+     reiserfs_panic (p_s_tb->tb_sb, "PAP-8265: get_neighbors: item must be directory item");
+#endif
+
+   /* We need left neighbor to balance S[n_h]. */
+   p_s_bh = PATH_OFFSET_PBUFFER(p_s_tb->tb_path, n_path_offset);
+
+#ifdef CONFIG_REISERFS_CHECK
+   if ( p_s_bh == p_s_tb->FL[n_h] && ! PATH_OFFSET_POSITION(p_s_tb->tb_path, n_path_offset) )
+       reiserfs_panic (p_s_tb->tb_sb, "PAP-8270: get_neighbors: invalid position in the parent");
+#endif
+
+   n_child_position = ( p_s_bh == p_s_tb->FL[n_h] ) ? p_s_tb->lkey[n_h] : B_BLK_HEAD(p_s_tb->FL[n_h])->blk_nr_item;
+   n_son_number = B_N_CHILD_NUM(p_s_tb->FL[n_h], n_child_position);
+   n_repeat = CARRY_ON;
+   p_s_bh = reiserfs_bread(p_s_sb->s_dev, n_son_number, p_s_sb->s_blocksize, &n_repeat);
+   if (!p_s_bh)
+     return IO_ERROR;
+   if ( n_repeat != CARRY_ON ) {
+     decrement_bcount(p_s_bh);
+
+#ifdef REISERFS_INFO
+   if ( ! p_s_tb->lnum[n_h] )
+     printk("Schedule occured in case when we need left neighbor just to update right delimiting key\n");
+#endif
+
+     return SCHEDULE_OCCURRED;
+   }
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( ! B_IS_IN_TREE(p_s_tb->FL[n_h]) || n_child_position > B_NR_ITEMS(p_s_tb->FL[n_h]) ||
+	 B_N_CHILD_NUM(p_s_tb->FL[n_h], n_child_position) != p_s_bh->b_blocknr )
+      reiserfs_panic (p_s_tb->tb_sb, "PAP-8275: get_neighbors: invalid parent");
+    if ( ! B_IS_IN_TREE(p_s_bh) )
+      reiserfs_panic (p_s_tb->tb_sb, "PAP-8280: get_neighbors: invalid child");
+
+    if ( ! n_h && COMP_KEYS(B_PRIGHT_DELIM_KEY(p_s_bh), B_N_PDELIM_KEY(p_s_tb->CFL[0], p_s_tb->lkey[0])) ) {
+      print_block(p_s_bh, 0, -1, -1);
+      print_block(p_s_tb->CFL[0], 0, -1, -1);
+      reiserfs_panic (p_s_tb->tb_sb, "PAP-8285: get_neighbors: invalid rdkey");
+    }
+    if (! n_h && B_BLK_HEAD (p_s_bh)->blk_free_space != MAX_CHILD_SIZE (p_s_bh) - B_N_CHILD (p_s_tb->FL[0],n_child_position)->dc_size) {
+      reiserfs_panic (p_s_tb->tb_sb, "PAP-8290: get_neighbors: invalid child size of left neighbor");
+    }
+#endif
+
+   decrement_bcount(p_s_tb->L[n_h]);
+   p_s_tb->L[n_h] = p_s_bh;
+ }
+
+ if ( p_s_tb->rnum[n_h] ) { /* We need right neighbor to balance S[n_path_offset]. */
+   p_s_bh = PATH_OFFSET_PBUFFER(p_s_tb->tb_path, n_path_offset);
+
+#ifdef CONFIG_REISERFS_CHECK
+   if ( p_s_bh == p_s_tb->FR[n_h] && PATH_OFFSET_POSITION(p_s_tb->tb_path, n_path_offset) >= B_NR_ITEMS(p_s_bh) )
+       reiserfs_panic (p_s_tb->tb_sb, "PAP-8295: get_neighbors: invalid position in the parent");
+#endif
+
+   n_child_position = ( p_s_bh == p_s_tb->FR[n_h] ) ? p_s_tb->rkey[n_h] + 1 : 0;
+   n_son_number = B_N_CHILD_NUM(p_s_tb->FR[n_h], n_child_position);
+   n_repeat = CARRY_ON;
+   p_s_bh = reiserfs_bread(p_s_sb->s_dev, n_son_number, p_s_sb->s_blocksize, &n_repeat);
+   if (!p_s_bh)
+     return IO_ERROR;
+   if ( n_repeat != CARRY_ON ) {
+     decrement_bcount(p_s_bh);
+     return SCHEDULE_OCCURRED;
+   }
+   decrement_bcount(p_s_tb->R[n_h]);
+   p_s_tb->R[n_h] = p_s_bh;
+
+#ifdef CONFIG_REISERFS_CHECK
+    if (! n_h && B_BLK_HEAD (p_s_bh)->blk_free_space != MAX_CHILD_SIZE (p_s_bh) - B_N_CHILD (p_s_tb->FR[0],n_child_position)->dc_size) {
+      reiserfs_panic (p_s_tb->tb_sb, "PAP-8300: get_neighbors: invalid child size of right neighbor (%d != %d - %d)",
+		      B_BLK_HEAD (p_s_bh)->blk_free_space, MAX_CHILD_SIZE (p_s_bh), B_N_CHILD (p_s_tb->FR[0],n_child_position)->dc_size);
+    }
+#endif
+
+ }
+ return CARRY_ON;
+}
+
+
+void * reiserfs_kmalloc (size_t size, int flags, struct super_block * s)
+{
+  void * vp;
+  static size_t malloced;
+
+
+  vp = kmalloc (size, flags);
+  if (vp) {
+    s->u.reiserfs_sb.s_kmallocs += size;
+    if (s->u.reiserfs_sb.s_kmallocs > malloced + 200000) {
+#ifdef CONFIG_REISERFS_CHECK    
+      reiserfs_warning ("vs-8301: reiserfs_kmalloc: allocated memory %d\n", s->u.reiserfs_sb.s_kmallocs);
+#endif
+      malloced = s->u.reiserfs_sb.s_kmallocs;
+    }
+  }
+/*printk ("malloc : size %d, allocated %d\n", size, s->u.reiserfs_sb.s_kmallocs);*/
+  return vp;
+}
+
+void reiserfs_kfree (/*const */void * vp, size_t size, struct super_block * s)
+{
+  kfree (vp);
+  
+  s->u.reiserfs_sb.s_kmallocs -= size;
+  if (s->u.reiserfs_sb.s_kmallocs < 0)
+    reiserfs_warning ("vs-8302: reiserfs_kfree: allocated memory %d\n", s->u.reiserfs_sb.s_kmallocs);
+
+}
+
+
+int get_virtual_node_size (struct super_block * sb, struct buffer_head * bh)
+{
+  int size = sizeof (struct virtual_item); /* for new item in case of insert */
+  int i, nr_items;
+  struct item_head * ih;
+
+
+  size = sizeof (struct virtual_node) + sizeof (struct virtual_item);
+  ih = B_N_PITEM_HEAD (bh, 0);
+  nr_items = B_NR_ITEMS (bh);
+  for (i = 0; i < nr_items; i ++, ih ++) {
+    /* each item occupies some space in virtual node */
+    size += sizeof (struct virtual_item);
+    if (I_IS_DIRECTORY_ITEM (ih))
+      /* each entry and new one occupeis 2 byte in the virtual node */
+      size += (I_ENTRY_COUNT (ih) + 1) * sizeof (__u16);
+  }
+  
+  /* 1 bit for each bitmap block to note whether bitmap block was
+     dirtied in the operation */
+  size += (SB_BMAP_NR (sb) * 2 / 8 + 4);
+  return size;
+}
+
+int get_mem_for_virtual_node (struct tree_balance * tb)
+{
+  int retval = CARRY_ON;
+  int size;
+  char * buf;
+
+  size = get_virtual_node_size (tb->tb_sb, PATH_PLAST_BUFFER(tb->tb_path));
+
+  if (size > tb->vn_buf_size) {
+    /* we have to allocate more memory for virtual node */
+    if (tb->vn_buf) {
+      /* free memory allocated before */
+      reiserfs_kfree (tb->vn_buf, tb->vn_buf_size, tb->tb_sb);
+      /* this is not needed if kfree is atomic */
+      retval |= SCHEDULE_OCCURRED;
+    }
+
+    tb->vn_buf_size = size;
+    /* get memory for virtual item */
+    buf = reiserfs_kmalloc(size, GFP_ATOMIC, tb->tb_sb);
+    if ( ! buf ) {
+      buf = reiserfs_kmalloc(size, GFP_BUFFER, tb->tb_sb);
+      if ( !buf ) {
+	    tb->vn_buf_size = 0;
+	    reiserfs_warning ("vs-8345: get_mem_for_virtual_node: kmalloc failed.  There were %d allocations\n",
+			  tb->tb_sb->u.reiserfs_sb.s_kmallocs);
+      }
+      retval |= SCHEDULE_OCCURRED;
+      schedule() ;
+    }
+    tb->vn_buf = buf;
+  }
+
+  return retval;
+}
+
+/* Prepare for balancing, that is
+ *	get all necessary parents, and neighbors;
+ *	analyze what and where should be moved;
+ *	get sufficient number of new nodes;
+ * Balancing will start only after all resources will be collected at a time.
+ * 
+ * When ported to SMP kernels, only at the last moment after all needed nodes
+ * are collected in cache, will the resources be locked using the usual
+ * textbook ordered lock acquisition algorithms.  Note that ensuring that
+ * this code neither write locks what it does not need to write lock nor locks out of order
+ * will be a pain in the butt that could have been avoided.  Grumble grumble. -Hans
+ * 
+ * fix is meant in the sense of render unchanging
+ * 
+ * Latency might be improved by first gathering a list of what buffers are needed
+ * and then getting as many of them in parallel as possible? -Hans
+ *
+ * Parameters:
+ *	op_mode	i - insert, d - delete, c - cut (truncate), p - paste (append)
+ *	tb	tree_balance structure;
+ *	inum	item number in S[h];
+ *      pos_in_item - comment this if you can
+ *      ins_ih & ins_sd are used when inserting
+ * Returns:	1 - schedule occurred while the function worked;
+ *	        0 - schedule didn't occur while the function worked;
+ *             -1 - if no_disk_space 
+ */
+
+
+int fix_nodes (
+	    struct reiserfs_transaction_handle *th,
+	    int				n_op_mode,
+	    struct tree_balance * 	p_s_tb,
+	    int 		  	n_pos_in_item, 
+	    struct item_head    * 	p_s_ins_ih
+    ) {
+  int	n_ret_value,
+    	n_h,
+    	n_item_num = PATH_LAST_POSITION(p_s_tb->tb_path);
+  struct buffer_head  * p_s_tbS0 = PATH_PLAST_BUFFER(p_s_tb->tb_path);
+
+  /* if it possible in indirect_to_direct conversion */
+  if (buffer_locked (p_s_tbS0)) {
+    return SCHEDULE_OCCURRED;
+  }
+
+  /*
+if ( p_s_tbS0->b_count > 1)
+  {
+    reiserfs_warning ("Fix_nodes: S0: %b\n", p_s_tbS0);
+    if (p_s_tbS0->b_blocknr == 100)
+      *(int *)0 = 0;
+  }
+  */
+#ifdef CONFIG_REISERFS_CHECK_ONE_PROCESS
+  if ( p_s_tbS0->b_count > 1 || (p_s_tb->L[0] && p_s_tb->L[0]->b_count > 1) ||
+       (p_s_tb->R[0] && p_s_tb->R[0]->b_count > 1) ) {
+    printk ("mode=%c, insert_size=%d\n", n_op_mode, p_s_tb->insert_size[0]);
+    print_tb(0, 0, 0, p_s_tb, "first three parameters are invalid");
+    reiserfs_panic (p_s_tb->tb_sb, "PAP-8310: fix_nodes: all buffers must be hold once in one thread processing");
+  }
+#endif
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( cur_tb ) {
+    print_tb (n_op_mode, n_item_num, n_pos_in_item, cur_tb,"fix_nodes");
+    reiserfs_panic(p_s_tb->tb_sb,"PAP-8305: fix_nodes:  there is pending do_balance");
+  }
+
+  if (!buffer_uptodate (p_s_tbS0) || !B_IS_IN_TREE (p_s_tbS0)) {
+    reiserfs_panic (p_s_tb->tb_sb, "PAP-8320: fix_nodes: S[0] (%b %z) is not uptodate "
+		    "at the beginning of fix_nodes or not in tree (mode %c)", p_s_tbS0, p_s_tbS0, n_op_mode);
+  }
+
+  /* Check parameters. */
+  switch (n_op_mode) {
+#ifdef REISERFS_FSCK
+    case M_INTERNAL:
+      break;
+    case M_INSERT:
+      if ( n_item_num < 0 || n_item_num > B_NR_ITEMS(p_s_tbS0) )
+	reiserfs_panic(p_s_tb->tb_sb,"PAP-8325: fix_nodes: Incorrect item number %d (in S0 - %d) in case of insert",
+		       n_item_num, B_NR_ITEMS(p_s_tbS0));
+#else
+    case M_INSERT:
+      if ( n_item_num <= 0 || n_item_num > B_NR_ITEMS(p_s_tbS0) )
+	reiserfs_panic(p_s_tb->tb_sb,"PAP-8330: fix_nodes: Incorrect item number %d (in S0 - %d) in case of insert",
+		       n_item_num, B_NR_ITEMS(p_s_tbS0));
+#endif
+      break;
+    case M_PASTE:
+    case M_DELETE:
+    case M_CUT:
+      if ( n_item_num < 0 || n_item_num >= B_NR_ITEMS(p_s_tbS0) ) {
+	print_block (p_s_tbS0, 0, -1, -1);
+	printk("mode = %c insert_size = %d\n", n_op_mode, p_s_tb->insert_size[0]);
+	reiserfs_panic(p_s_tb->tb_sb,"PAP-8335: fix_nodes: Incorrect item number(%d)", n_item_num);
+      }
+      break;
+    default:
+      reiserfs_panic(p_s_tb->tb_sb,"PAP-8340: fix_nodes: Incorrect mode of operation");
+  }
+#endif
+
+
+  if (get_mem_for_virtual_node (p_s_tb) == SCHEDULE_OCCURRED) {
+    return SCHEDULE_OCCURRED;
+  }
+
+#if 0
+  /* get two empty nodes those will be used for preserving of shifted items */
+  if  ( (n_ret_value = ready_preserve_list (p_s_tb, p_s_tbS0)) != CARRY_ON ) {
+    return n_ret_value; /* No disk space, or schedule occurred */
+  }
+#endif
+
+  /* Starting from the leaf level; for all levels n_h of the tree. */
+  for ( n_h = 0; n_h < MAX_HEIGHT && p_s_tb->insert_size[n_h]; n_h++ ) { 
+    if ( (n_ret_value = get_direct_parent(p_s_tb, n_h)) != CARRY_ON ) {
+      return n_ret_value;
+    }
+
+    if ( (n_ret_value = check_balance (th, n_op_mode, p_s_tb, n_h, n_item_num,
+				       n_pos_in_item, p_s_ins_ih)) != CARRY_ON ) {
+      if ( n_ret_value == NO_BALANCING_NEEDED ) {
+        /* No balancing for higher levels needed. */
+	if ( (n_ret_value = get_neighbors(p_s_tb, n_h)) != CARRY_ON ) {
+	  return n_ret_value;
+	}
+        if ( n_h != MAX_HEIGHT - 1 )  
+	   p_s_tb->insert_size[n_h + 1] = 0;
+	/* ok, analysis and resource gathering are complete */
+        break;
+      }
+
+      return n_ret_value;
+    }
+
+    if ( (n_ret_value = get_neighbors(p_s_tb, n_h)) != CARRY_ON ) {
+      return n_ret_value;
+    }
+
+    if ( (n_ret_value = get_empty_nodes(th, p_s_tb, n_h)) != CARRY_ON ) {
+      return n_ret_value; /* No disk space, or schedule occurred and
+			     analysis may be invalid and needs to be redone. */
+    }
+    
+    if ( ! PATH_H_PBUFFER(p_s_tb->tb_path, n_h) ) {
+      /* We have a positive insert size but no nodes exist on this
+	 level, this means that we are creating a new root. */
+
+#ifdef CONFIG_REISERFS_CHECK
+      if ( p_s_tb->blknum[n_h] != 1 )
+        reiserfs_panic(p_s_tb->tb_sb,"PAP-8350: fix_nodes: creating new empty root");
+#endif /* CONFIG_REISERFS_CHECK */
+
+      if ( n_h < MAX_HEIGHT - 1 )
+	      p_s_tb->insert_size[n_h + 1] = 0;
+    }
+    else
+      if ( ! PATH_H_PBUFFER(p_s_tb->tb_path, n_h + 1) ) {
+        if ( p_s_tb->blknum[n_h] > 1 ) {
+	  /* The tree needs to be grown, so this node S[n_h]
+	     which is the root node is split into two nodes, and
+	     a new node (S[n_h+1]) will be created to become the root node.  */
+	  
+#ifdef CONFIG_REISERFS_CHECK
+	  if ( n_h == MAX_HEIGHT - 1 )
+            reiserfs_panic(p_s_tb->tb_sb, "PAP-8355: fix_nodes: attempt to create too high of a tree");
+#endif /* CONFIG_REISERFS_CHECK */
+
+          p_s_tb->insert_size[n_h + 1] = (DC_SIZE + KEY_SIZE) * (p_s_tb->blknum[n_h] - 1) + DC_SIZE;
+        }
+        else
+	  if ( n_h < MAX_HEIGHT - 1 )
+	    p_s_tb->insert_size[n_h + 1] = 0;
+      }
+      else
+        p_s_tb->insert_size[n_h + 1] = (DC_SIZE + KEY_SIZE) * (p_s_tb->blknum[n_h] - 1);
+  }
+
+  return CARRY_ON; /* schedule did not occur */
+}
+
+
+void unfix_nodes(
+		 struct reiserfs_transaction_handle *th,
+		 struct tree_balance * p_s_tb
+		 ) {
+  struct path * p_s_path = p_s_tb->tb_path;
+  int		n_counter;
+  int i, j;
+  struct buffer_head * bh;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( ! p_s_tb->vn_buf )
+    reiserfs_panic (p_s_tb->tb_sb,
+		    "PAP-16050: unfix_nodes: pointer to the virtual node is NULL");
+#endif
+
+
+  /* Release path buffers. */
+  pathrelse(p_s_path);
+
+
+  for ( n_counter = 0; n_counter < MAX_HEIGHT; n_counter++ ) {
+    /* Release fathers and neighbors. */
+    brelse(p_s_tb->L[n_counter]);
+    brelse(p_s_tb->R[n_counter]);
+    brelse(p_s_tb->FL[n_counter]);
+    brelse(p_s_tb->FR[n_counter]);
+    brelse(p_s_tb->CFL[n_counter]);
+    brelse(p_s_tb->CFR[n_counter]);
+  }
+
+  /* Could be optimized. Will be done by PAP someday */
+  for ( n_counter = 0; n_counter < MAX_FEB_SIZE; n_counter++ ) {
+    if ( p_s_tb->FEB[n_counter] ) {
+      /* release what was not used */
+      reiserfs_free_block(th, p_s_tb->tb_sb, p_s_tb->FEB[n_counter]->b_blocknr);
+      COMPLETE_BITMAP_DIRTING_AFTER_FREEING(p_s_tb->tb_sb, p_s_tb->FEB[n_counter]->b_blocknr / (p_s_tb->tb_sb->s_blocksize * 8));
+
+      bforget(p_s_tb->FEB[n_counter]);
+      /* tree balance bitmap of bitmaps has bit set already */
+    }
+    /* release used as new nodes including a new root */
+    brelse (p_s_tb->used[n_counter]);
+  }
+
+  /* free nodes which had to be used in preserving, but are left unused */
+  for (n_counter = 0; n_counter < MAX_PRESERVE_NODES; n_counter ++) {
+    if (p_s_tb->tb_nodes_for_preserving[n_counter]) {
+      reiserfs_free_block(th, p_s_tb->tb_sb, p_s_tb->tb_nodes_for_preserving[n_counter]->b_blocknr);
+      COMPLETE_BITMAP_DIRTING_AFTER_FREEING(p_s_tb->tb_sb, 
+					    p_s_tb->tb_nodes_for_preserving[n_counter]->b_blocknr / (p_s_tb->tb_sb->s_blocksize * 8));
+      bforget(p_s_tb->tb_nodes_for_preserving[n_counter]);
+      /* tree balance bitmap of bitmaps has bit set already */
+    }
+    /* release buffers containing preserved nodes */
+    brelse (p_s_tb->preserved[n_counter]);
+  }
+
+  /* if any of bitmaps was dirtied atomically, then DIRTY_BITMAP_MAP
+     has corresponding bit set. If so, do b_count++ and brelse for the
+     buffer. It will refile_buffer to the correct list */
+  for (i = 0, j = 0; i < SB_BMAP_NR (p_s_tb->tb_sb); i ++) {
+    if (test_bit (i, DIRTY_BITMAP_MAP (p_s_tb))) {
+      bh = SB_AP_BITMAP (p_s_tb->tb_sb)[i];
+      bh->b_count ++;
+      brelse (bh);
+      j ++;
+    }
+  }
+
+  reiserfs_kfree (DIRTY_BITMAP_MAP (p_s_tb), p_s_tb->vn_buf_size, p_s_tb->tb_sb);
+
+  if (j) {
+    /* super block was dirtied atomically in reiserfs_free_block, or
+       in reiserfs_new_blocknrs. Make sure it is refiled */
+    SB_BUFFER_WITH_SB (p_s_tb->tb_sb)->b_count ++;
+    brelse (SB_BUFFER_WITH_SB (p_s_tb->tb_sb));
+  }
+} 
+
+
+
+
+
+
diff -urN linux/fs/reiserfs/hashes.c /tmp/linux/fs/reiserfs/hashes.c
--- linux/fs/reiserfs/hashes.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/hashes.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,235 @@
+/*
+ * Keyed 32-bit hash function using TEA in a Davis-Meyer function
+ *   H0 = Key
+ *   Hi = E Mi(Hi-1) + Hi-1
+ *
+ * (see Applied Cryptography, 2nd edition, p448).
+ *
+ * Jeremy Fitzhardinge <jeremy@zip.com.au> 1998
+ * 
+ * Jeremy has agreed to the contents of reiserfs/README. -Hans
+ * Yura's function is added (04/07/2000)
+ */
+
+//
+// keyed_hash
+// yura_hash
+// r5_hash
+//
+
+#include <asm/types.h>
+
+#define DELTA 0x9E3779B9
+#define FULLROUNDS 10		/* 32 is overkill, 16 is strong crypto */
+#define PARTROUNDS 6		/* 6 gets complete mixing */
+
+#ifndef __KERNEL__
+typedef __u32 u32;
+#endif
+
+/* a, b, c, d - data; h0, h1 - accumulated hash */
+#define TEACORE(rounds)							\
+	do {								\
+		u32 sum = 0;						\
+		int n = rounds;						\
+		u32 b0, b1;						\
+									\
+		b0 = h0;						\
+		b1 = h1;						\
+									\
+		do							\
+		{							\
+			sum += DELTA;					\
+			b0 += ((b1 << 4)+a) ^ (b1+sum) ^ ((b1 >> 5)+b);	\
+			b1 += ((b0 << 4)+c) ^ (b0+sum) ^ ((b0 >> 5)+d);	\
+		} while(--n);						\
+									\
+		h0 += b0;						\
+		h1 += b1;						\
+	} while(0)
+
+
+u32 keyed_hash(const char *msg, int len)
+{
+	u32 k[] = { 0x9464a485, 0x542e1a94, 0x3e846bff, 0xb75bcfc3}; 
+
+	u32 h0 = k[0], h1 = k[1];
+	u32 a, b, c, d;
+	u32 pad;
+	int i;
+ 
+
+	//	assert(len >= 0 && len < 256);
+
+	pad = (u32)len | ((u32)len << 8);
+	pad |= pad << 16;
+
+	while(len >= 16)
+	{
+		a = (u32)msg[ 0]      |
+		    (u32)msg[ 1] << 8 |
+		    (u32)msg[ 2] << 16|
+		    (u32)msg[ 3] << 24;
+		b = (u32)msg[ 4]      |
+		    (u32)msg[ 5] << 8 |
+		    (u32)msg[ 6] << 16|
+		    (u32)msg[ 7] << 24;
+		c = (u32)msg[ 8]      |
+		    (u32)msg[ 9] << 8 |
+		    (u32)msg[10] << 16|
+		    (u32)msg[11] << 24;
+		d = (u32)msg[12]      |
+		    (u32)msg[13] << 8 |
+		    (u32)msg[14] << 16|
+		    (u32)msg[15] << 24;
+		
+		TEACORE(PARTROUNDS);
+
+		len -= 16;
+		msg += 16;
+	}
+
+	if (len >= 12)
+	{
+	    	//assert(len < 16);
+		if (len >= 16)
+		    *(int *)0 = 0;
+
+		a = (u32)msg[ 0]      |
+		    (u32)msg[ 1] << 8 |
+		    (u32)msg[ 2] << 16|
+		    (u32)msg[ 3] << 24;
+		b = (u32)msg[ 4]      |
+		    (u32)msg[ 5] << 8 |
+		    (u32)msg[ 6] << 16|
+		    (u32)msg[ 7] << 24;
+		c = (u32)msg[ 8]      |
+		    (u32)msg[ 9] << 8 |
+		    (u32)msg[10] << 16|
+		    (u32)msg[11] << 24;
+
+		d = pad;
+		for(i = 12; i < len; i++)
+		{
+			d <<= 8;
+			d |= msg[i];
+		}
+	}
+	else if (len >= 8)
+	{
+	    	//assert(len < 12);
+		if (len >= 12)
+		    *(int *)0 = 0;
+		a = (u32)msg[ 0]      |
+		    (u32)msg[ 1] << 8 |
+		    (u32)msg[ 2] << 16|
+		    (u32)msg[ 3] << 24;
+		b = (u32)msg[ 4]      |
+		    (u32)msg[ 5] << 8 |
+		    (u32)msg[ 6] << 16|
+		    (u32)msg[ 7] << 24;
+
+		c = d = pad;
+		for(i = 8; i < len; i++)
+		{
+			c <<= 8;
+			c |= msg[i];
+		}
+	}
+	else if (len >= 4)
+	{
+	    	//assert(len < 8);
+		if (len >= 8)
+		    *(int *)0 = 0;
+		a = (u32)msg[ 0]      |
+		    (u32)msg[ 1] << 8 |
+		    (u32)msg[ 2] << 16|
+		    (u32)msg[ 3] << 24;
+
+		b = c = d = pad;
+		for(i = 4; i < len; i++)
+		{
+			b <<= 8;
+			b |= msg[i];
+		}
+	}
+	else
+	{
+	    	//assert(len < 4);
+		if (len >= 4)
+		    *(int *)0 = 0;
+		a = b = c = d = pad;
+		for(i = 0; i < len; i++)
+		{
+			a <<= 8;
+			a |= msg[i];
+		}
+	}
+
+	TEACORE(FULLROUNDS);
+
+/*	return 0;*/
+	return h0^h1;
+}
+
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+u32 yura_hash (const char *msg, int len)
+{
+    int j, pow;
+    u32 a, c;
+    int i;
+    
+    for (pow=1,i=1; i < len; i++) pow = pow * 10; 
+    
+    if (len == 1) 
+	a = msg[0]-48;
+    else
+	a = (msg[0] - 48) * pow;
+    
+    for (i=1; i < len; i++) {
+	c = msg[i] - 48; 
+	for (pow=1,j=i; j < len-1; j++) pow = pow * 10; 
+	a = a + c * pow;
+    }
+    
+    for (; i < 40; i++) {
+	c = '0' - 48; 
+	for (pow=1,j=i; j < len-1; j++) pow = pow * 10; 
+	a = a + c * pow;
+    }
+    
+    for (; i < 256; i++) {
+	c = i; 
+	for (pow=1,j=i; j < len-1; j++) pow = pow * 10; 
+	a = a + c * pow;
+    }
+    
+    a = a << 7;
+    return a;
+}
+
+u32 r5_hash (const char *msg, int len)
+{
+  u32 a=0;
+  int i;
+
+  for (i = 0; i < len; i ++) {
+    a += msg[i] << 4;
+    a += msg[i] >> 4;
+    a *= 11;
+  }
+  return a;
+}
+
+
+
+
+
+
+
+
+
+
diff -urN linux/fs/reiserfs/ibalance.c /tmp/linux/fs/reiserfs/ibalance.c
--- linux/fs/reiserfs/ibalance.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/ibalance.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,1159 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+#ifdef __KERNEL__
+
+#include <asm/uaccess.h>
+#include <linux/string.h>
+#include <linux/sched.h>
+#include <linux/reiserfs_fs.h>
+
+#else
+
+#include "nokernel.h"
+
+#endif
+
+
+/* this is one and only function that is used outside (do_balance.c) */
+int	balance_internal (
+			  struct reiserfs_transaction_handle *th,
+			  struct tree_balance * ,
+			  int,
+			  int,
+			  struct item_head * ,
+			  struct buffer_head ** 
+			  );
+#ifdef CONFIG_REISERFS_CHECK
+extern struct buffer_head *buffers[MAX_HEIGHT];
+#endif
+
+/* modes of internal_shift_left, internal_shift_right and internal_insert_childs */
+#define INTERNAL_SHIFT_FROM_S_TO_L 0
+#define INTERNAL_SHIFT_FROM_R_TO_S 1
+#define INTERNAL_SHIFT_FROM_L_TO_S 2
+#define INTERNAL_SHIFT_FROM_S_TO_R 3
+#define INTERNAL_INSERT_TO_S 4
+#define INTERNAL_INSERT_TO_L 5
+#define INTERNAL_INSERT_TO_R 6
+
+static void	internal_define_dest_src_infos (
+						int shift_mode,
+						struct tree_balance * tb,
+						int h,
+						struct buffer_info * dest_bi,
+						struct buffer_info * src_bi,
+						int * d_key,
+						struct buffer_head ** cf
+						)
+{
+#ifdef CONFIG_REISERFS_CHECK
+  memset (dest_bi, 0, sizeof (struct buffer_info));
+  memset (src_bi, 0, sizeof (struct buffer_info));
+#endif
+  /* define dest, src, dest parent, dest position */
+  switch (shift_mode) {
+  case INTERNAL_SHIFT_FROM_S_TO_L:	/* used in internal_shift_left */
+    src_bi->bi_bh = PATH_H_PBUFFER (tb->tb_path, h);
+    src_bi->bi_parent = PATH_H_PPARENT (tb->tb_path, h);
+    src_bi->bi_position = PATH_H_POSITION (tb->tb_path, h + 1);
+    dest_bi->bi_bh = tb->L[h];
+    dest_bi->bi_parent = tb->FL[h];
+    dest_bi->bi_position = get_left_neighbor_position (tb, h);
+    *d_key = tb->lkey[h];
+    *cf = tb->CFL[h];
+    break;
+  case INTERNAL_SHIFT_FROM_L_TO_S:
+    src_bi->bi_bh = tb->L[h];
+    src_bi->bi_parent = tb->FL[h];
+    src_bi->bi_position = get_left_neighbor_position (tb, h);
+    dest_bi->bi_bh = PATH_H_PBUFFER (tb->tb_path, h);
+    dest_bi->bi_parent = PATH_H_PPARENT (tb->tb_path, h);
+    dest_bi->bi_position = PATH_H_POSITION (tb->tb_path, h + 1); /* dest position is analog of dest->b_item_order */
+    *d_key = tb->lkey[h];
+    *cf = tb->CFL[h];
+    break;
+
+  case INTERNAL_SHIFT_FROM_R_TO_S:	/* used in internal_shift_left */
+    src_bi->bi_bh = tb->R[h];
+    src_bi->bi_parent = tb->FR[h];
+    src_bi->bi_position = get_right_neighbor_position (tb, h);
+    dest_bi->bi_bh = PATH_H_PBUFFER (tb->tb_path, h);
+    dest_bi->bi_parent = PATH_H_PPARENT (tb->tb_path, h);
+    dest_bi->bi_position = PATH_H_POSITION (tb->tb_path, h + 1);
+    *d_key = tb->rkey[h];
+    *cf = tb->CFR[h];
+    break;
+  case INTERNAL_SHIFT_FROM_S_TO_R:
+    src_bi->bi_bh = PATH_H_PBUFFER (tb->tb_path, h);
+    src_bi->bi_parent = PATH_H_PPARENT (tb->tb_path, h);
+    src_bi->bi_position = PATH_H_POSITION (tb->tb_path, h + 1);
+    dest_bi->bi_bh = tb->R[h];
+    dest_bi->bi_parent = tb->FR[h];
+    dest_bi->bi_position = get_right_neighbor_position (tb, h);
+    *d_key = tb->rkey[h];
+    *cf = tb->CFR[h];
+    break;
+
+  case INTERNAL_INSERT_TO_L:
+    dest_bi->bi_bh = tb->L[h];
+    dest_bi->bi_parent = tb->FL[h];
+    dest_bi->bi_position = get_left_neighbor_position (tb, h);
+    break;
+
+  case INTERNAL_INSERT_TO_S:
+    dest_bi->bi_bh = PATH_H_PBUFFER (tb->tb_path, h);
+    dest_bi->bi_parent = PATH_H_PPARENT (tb->tb_path, h);
+    dest_bi->bi_position = PATH_H_POSITION (tb->tb_path, h + 1);
+    break;
+
+  case INTERNAL_INSERT_TO_R:
+    dest_bi->bi_bh = tb->R[h];
+    dest_bi->bi_parent = tb->FR[h];
+    dest_bi->bi_position = get_right_neighbor_position (tb, h);
+    break;
+
+  default:
+    reiserfs_panic (tb->tb_sb, "internal_define_dest_src_infos", "shift type is unknown (%d)", shift_mode);
+  }
+}
+
+
+
+/* Insert count node pointers into buffer cur before position to + 1.
+ * Insert count items into buffer cur before position to.
+ * Items and node pointers are specified by inserted and bh respectively.
+ */ 
+static void	internal_insert_childs (
+					struct reiserfs_transaction_handle *th, 
+					struct buffer_info * cur_bi,
+					/*	struct buffer_head * prev_cur,*/
+					int to, int count,
+					struct item_head * inserted,
+					struct buffer_head ** bh
+					)
+{
+  struct buffer_head * cur = cur_bi->bi_bh;
+  struct block_head * blkh;
+  int nr;
+  struct key * ih;
+  struct disk_child new_dc[2];
+  struct disk_child * dc;
+  struct super_block *s ;
+  int i;
+
+  if (count <= 0)
+    return;
+
+  nr = (blkh = B_BLK_HEAD(cur))->blk_nr_item;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (count > 2)
+    reiserfs_panic (0, "internal_insert_childs", "too many children (%d) are to be inserted", count);
+  if (blkh->blk_free_space < count * (KEY_SIZE + DC_SIZE))
+    reiserfs_panic (0, "internal_insert_childs", "no enough free space (%d), needed %d bytes", 
+		    blkh->blk_free_space, count * (KEY_SIZE + DC_SIZE));
+#endif /* CONFIG_REISERFS_CHECK */
+
+  /* prepare space for count disk_child */
+  dc = B_N_CHILD(cur,to+1);
+
+  memmove (dc + count, dc, (nr+1-(to+1)) * DC_SIZE);
+
+  /* copy to_be_insert disk children */
+  for (i = 0; i < count; i ++) {
+    new_dc[i].dc_size =
+      MAX_CHILD_SIZE(bh[i]) - B_BLK_HEAD(bh[i])->blk_free_space;
+    new_dc[i].dc_block_number = bh[i]->b_blocknr;
+  }
+  memcpy (dc, new_dc, DC_SIZE * count);
+
+  
+  /* prepare space for count items  */
+  ih = B_N_PDELIM_KEY (cur, ((to == -1) ? 0 : to));
+
+  memmove (ih + count, ih, (nr - to) * KEY_SIZE + (nr + 1 + count) * DC_SIZE);
+
+  /* copy item headers (keys) */
+  memcpy (ih, inserted, KEY_SIZE);
+  if ( count > 1 )
+    memcpy (ih + 1, inserted + 1, KEY_SIZE);
+
+  /* sizes, item number */
+  blkh->blk_nr_item += count;
+  blkh->blk_free_space -= count * (DC_SIZE + KEY_SIZE);
+
+  /* reiserfs_mark_buffer_dirty(cur,0);	journal victim */ /* not preserved, internal */
+  s = th->t_super ;
+  journal_mark_dirty(th, s, cur);/* not preserved, internal */
+
+
+  if (cur_bi->bi_parent) {
+    B_N_CHILD (cur_bi->bi_parent,cur_bi->bi_position)->dc_size += count * (DC_SIZE + KEY_SIZE);
+    /* reiserfs_mark_buffer_dirty(cur_bi->bi_parent,0); journal victim */	/* not preserved, internal */
+    journal_mark_dirty(th, s, cur_bi->bi_parent);	/* not preserved, internal */
+  }
+
+}
+
+
+/* Delete del_num items and node pointers from buffer cur starting from *
+ * the first_i'th item and first_p'th pointers respectively.		*/
+static void	internal_delete_pointers_items (
+						struct reiserfs_transaction_handle *th,
+						struct buffer_info * cur_bi,
+						int first_p, 
+						int first_i, 
+						int del_num
+						)
+{
+  struct buffer_head * cur = cur_bi->bi_bh;
+  int nr;
+  struct block_head * blkh;
+  struct key * key;
+  struct disk_child * dc;
+  struct super_block *s ;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (cur == NULL)
+    reiserfs_panic (0, "internal_delete_pointers_items1: buffer is 0");
+	
+  if (del_num < 0)
+    reiserfs_panic (0, "internal_delete_pointers_items2",
+		    "negative number of items (%d) can not be deleted", del_num);
+
+  if (first_p < 0 || first_p + del_num > B_NR_ITEMS (cur) + 1 || first_i < 0)
+    reiserfs_panic (0, "internal_delete_pointers_items3",
+		    "first pointer order (%d) < 0 or "
+		    "no so many pointers (%d), only (%d) or "
+		    "first key order %d < 0", first_p, 
+		    first_p + del_num, B_NR_ITEMS (cur) + 1, first_i);
+#endif /* CONFIG_REISERFS_CHECK */
+  if ( del_num == 0 )
+    return;
+
+  nr = (blkh = B_BLK_HEAD(cur))->blk_nr_item;
+
+  if ( first_p == 0 && del_num == nr + 1 ) {
+#ifdef CONFIG_REISERFS_CHECK
+    if ( first_i != 0 )
+      reiserfs_panic (0, "internal_delete_pointers_items5",
+		      "first deleted key must have order 0, not %d", first_i);
+#endif /* CONFIG_REISERFS_CHECK */
+    make_empty_node (cur_bi);
+    return;
+  }
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (first_i + del_num > B_NR_ITEMS (cur)) {
+    printk("first_i = %d del_num = %d\n",first_i,del_num);
+    reiserfs_panic (0, "internal_delete_pointers_items4: :"
+		    "no so many keys (%d) in the node (%b)(%z)", first_i + del_num, cur, cur);
+  }
+#endif /* CONFIG_REISERFS_CHECK */
+
+
+  /* deleting */
+  dc = B_N_CHILD (cur, first_p);
+
+  memmove (dc, dc + del_num, (nr + 1 - first_p - del_num) * DC_SIZE);
+  key = B_N_PDELIM_KEY (cur, first_i);
+  memmove (key, key + del_num, (nr - first_i - del_num) * KEY_SIZE + (nr + 1 - del_num) * DC_SIZE);
+
+
+  /* sizes, item number */
+  blkh->blk_nr_item -= del_num;
+  blkh->blk_free_space += del_num * (KEY_SIZE +  DC_SIZE);
+
+  s = th->t_super ;
+  /* reiserfs_mark_buffer_dirty (cur, 0); journal victim */
+  journal_mark_dirty (th, s, cur);
+
+ 
+  if (cur_bi->bi_parent) {
+    B_N_CHILD (cur_bi->bi_parent, cur_bi->bi_position)->dc_size -= del_num * (KEY_SIZE +  DC_SIZE);
+    /* reiserfs_mark_buffer_dirty(cur_bi->bi_parent,0); journal victim */	/* not preserved, internal */
+    journal_mark_dirty(th, s, cur_bi->bi_parent);	/* not preserved, internal */
+  }
+}
+
+
+/* delete n node pointers and items starting from given position */
+static void	internal_delete_childs (
+					struct reiserfs_transaction_handle *th,
+					struct buffer_info * cur_bi, 
+					int from, 
+					int n
+					)
+{
+  int i_from;
+
+  i_from = (from == 0) ? from : from - 1;
+
+  /* delete n pointers starting from `from' position in CUR;
+     delete n keys starting from 'i_from' position in CUR;
+     */
+  internal_delete_pointers_items (th, cur_bi, from, i_from, n);
+}
+
+
+/* copy cpy_num node pointers and cpy_num - 1 items from buffer src to buffer dest
+* last_first == FIRST_TO_LAST means, that we copy first items from src to tail of dest
+ * last_first == LAST_TO_FIRST means, that we copy last items from src to head of dest 
+ */
+static void internal_copy_pointers_items (
+					  struct reiserfs_transaction_handle *th,
+					  struct buffer_info * dest_bi,
+					  struct buffer_head * src,
+					  int last_first, int cpy_num
+					  )
+{
+  /* ATTENTION! Number of node pointers in DEST is equal to number of items in DEST *
+   * as delimiting key have already inserted to buffer dest.*/
+  struct buffer_head * dest = dest_bi->bi_bh;
+  int nr_dest, nr_src;
+  int dest_order, src_order;
+  struct block_head * blkh;
+  struct key * key;
+  struct disk_child * dc;
+  struct super_block *s ;
+
+  nr_src = B_NR_ITEMS (src);
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( dest == NULL || src == NULL )
+    reiserfs_panic (0, "internal_copy_pointers_items", "src (%p) or dest (%p) buffer is 0", src, dest);
+
+  if (last_first != FIRST_TO_LAST && last_first != LAST_TO_FIRST)
+    reiserfs_panic (0, "internal_copy_pointers_items",
+		    "invalid last_first parameter (%d)", last_first);
+
+  if ( nr_src < cpy_num - 1 )
+    reiserfs_panic (0, "internal_copy_pointers_items", "no so many items (%d) in src (%d)", cpy_num, nr_src);
+
+  if ( cpy_num < 0 )
+    reiserfs_panic (0, "internal_copy_pointers_items", "cpy_num less than 0 (%d)", cpy_num);
+
+  if (cpy_num - 1 + B_NR_ITEMS(dest) > (int)MAX_NR_KEY(dest))
+    reiserfs_panic (0, "internal_copy_pointers_items",
+		    "cpy_num (%d) + item number in dest (%d) can not be more than MAX_NR_KEY(%d)",
+		    cpy_num, B_NR_ITEMS(dest), MAX_NR_KEY(dest));
+#endif
+
+  if ( cpy_num == 0 )
+    return;
+
+	/* coping */
+  nr_dest = (blkh = B_BLK_HEAD(dest))->blk_nr_item;
+
+  /*dest_order = (last_first == LAST_TO_FIRST) ? 0 : nr_dest;*/
+  /*src_order = (last_first == LAST_TO_FIRST) ? (nr_src - cpy_num + 1) : 0;*/
+  (last_first == LAST_TO_FIRST) ?	(dest_order = 0, src_order = nr_src - cpy_num + 1) :
+    (dest_order = nr_dest, src_order = 0);
+
+  /* prepare space for cpy_num pointers */
+  dc = B_N_CHILD (dest, dest_order);
+
+  memmove (dc + cpy_num, dc, (nr_dest - dest_order) * DC_SIZE);
+
+	/* insert pointers */
+  memcpy (dc, B_N_CHILD (src, src_order), DC_SIZE * cpy_num);
+
+
+  /* prepare space for cpy_num - 1 item headers */
+  key = B_N_PDELIM_KEY(dest, dest_order);
+  memmove (key + cpy_num - 1, key,
+	   KEY_SIZE * (nr_dest - dest_order) + DC_SIZE * (nr_dest + cpy_num));
+
+
+  /* insert headers */
+  memcpy (key, B_N_PDELIM_KEY (src, src_order), KEY_SIZE * (cpy_num - 1));
+
+  /* sizes, item number */
+  blkh->blk_nr_item += cpy_num - 1;
+  blkh->blk_free_space -= KEY_SIZE * (cpy_num - 1) + DC_SIZE * cpy_num;
+
+  s = th->t_super ;
+  /* reiserfs_mark_buffer_dirty(dest,0); journal victim */
+  journal_mark_dirty(th, s, dest);
+  if (dest_bi->bi_parent) {
+    B_N_CHILD(dest_bi->bi_parent,dest_bi->bi_position)->dc_size +=
+      KEY_SIZE * (cpy_num - 1) + DC_SIZE * cpy_num;
+
+    /* not preserved, preserves are in balance_leaf() and  balance_leaf_when_delete() */
+    /* reiserfs_mark_buffer_dirty (dest_bi->bi_parent,0); journal victim */	
+    journal_mark_dirty(th, s, dest_bi->bi_parent);	
+  }
+
+}
+
+
+/* Copy cpy_num node pointers and cpy_num - 1 items from buffer src to buffer dest.
+ * Delete cpy_num - del_par items and node pointers from buffer src.
+ * last_first == FIRST_TO_LAST means, that we copy/delete first items from src.
+ * last_first == LAST_TO_FIRST means, that we copy/delete last items from src.
+ */
+static void	internal_move_pointers_items (
+					      struct reiserfs_transaction_handle *th,
+					      struct buffer_info * dest_bi, 
+					      struct buffer_info * src_bi, 
+					      int last_first, int cpy_num, int del_par
+					      )
+{
+  int first_pointer;
+  int first_item;
+
+  internal_copy_pointers_items (th, dest_bi, src_bi->bi_bh, last_first, cpy_num);
+
+
+  if (last_first == FIRST_TO_LAST) {	/* shift_left occurs */
+    first_pointer = 0;
+    first_item = 0;
+    /* delete cpy_num - del_par pointers and keys starting for pointers with first_pointer, 
+       for key - with first_item */
+    internal_delete_pointers_items (th, src_bi, first_pointer, first_item, cpy_num - del_par);
+  } else {			/* shift_right occurs */
+    int i, j;
+
+    i = ( cpy_num - del_par == ( j = B_NR_ITEMS(src_bi->bi_bh)) + 1 ) ? 0 : j - cpy_num + del_par;
+
+    internal_delete_pointers_items (th, src_bi, j + 1 - cpy_num + del_par, i, cpy_num - del_par);
+  }
+}
+
+/* Insert n_src'th key of buffer src before n_dest'th key of buffer dest. */
+static void internal_insert_key (
+				 struct reiserfs_transaction_handle *th,
+				 struct buffer_info * dest_bi, 
+				 int dest_position_before,                 /* insert key before key with n_dest number */
+				 struct buffer_head * src, 
+				 int src_position
+				 )
+{
+  struct buffer_head * dest = dest_bi->bi_bh;
+  int nr;
+  struct block_head * blkh;
+  struct key * key;
+  struct super_block *s ;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (dest == NULL || src == NULL)
+    reiserfs_panic (0, "internal_insert_key", "sourse(%p) or dest(%p) buffer is 0", src, dest);
+
+  if (dest_position_before < 0 || src_position < 0)
+    reiserfs_panic (0, "internal_insert_key", "source(%d) or dest(%d) key number less than 0", 
+		    src_position, dest_position_before);
+
+  if (dest_position_before > B_NR_ITEMS (dest) || src_position >= B_NR_ITEMS(src))
+    reiserfs_panic (0, "internal_insert_key", 
+		    "invalid position in dest (%d (key number %d)) or in src (%d (key number %d))",
+		    dest_position_before, B_NR_ITEMS (dest), src_position, B_NR_ITEMS(src));
+
+  if (B_BLK_HEAD(dest)->blk_free_space < KEY_SIZE)
+    reiserfs_panic (0, "internal_insert_key", 
+		    "no enough free space (%d) in dest buffer", B_BLK_HEAD(dest)->blk_free_space);
+#endif
+
+  nr = (blkh=B_BLK_HEAD(dest))->blk_nr_item;
+
+  /* prepare space for inserting key */
+  key = B_N_PDELIM_KEY (dest, dest_position_before);
+  memmove (key + 1, key, (nr - dest_position_before) * KEY_SIZE + (nr + 1) * DC_SIZE);
+
+  /* insert key */
+  memcpy (key, B_N_PDELIM_KEY(src, src_position), KEY_SIZE);
+
+  /* Change dirt, free space, item number fields. */
+  blkh->blk_nr_item ++;
+  blkh->blk_free_space -= KEY_SIZE;
+
+  s = th->t_super ;
+  journal_mark_dirty(th, s, dest);
+
+  if (dest_bi->bi_parent) {
+    B_N_CHILD(dest_bi->bi_parent,dest_bi->bi_position)->dc_size += KEY_SIZE;
+    journal_mark_dirty(th, s, dest_bi->bi_parent); /* not preserved, preserves are in balance_leaf() and 
+    							balance_leaf_when_delete() */
+  }
+}
+
+
+
+/* Insert d_key'th (delimiting) key from buffer cfl to tail of dest. 
+ * Copy pointer_amount node pointers and pointer_amount - 1 items from buffer src to buffer dest.
+ * Replace  d_key'th key in buffer cfl.
+ * Delete pointer_amount items and node pointers from buffer src.
+ */
+/* this can be invoked both to shift from S to L and from R to S */
+static void	internal_shift_left (
+				     struct reiserfs_transaction_handle *th,
+				     int mode,	/* INTERNAL_FROM_S_TO_L | INTERNAL_FROM_R_TO_S */
+				     struct tree_balance * tb,
+				     int h,
+				     int pointer_amount
+				     )
+{
+  struct buffer_info dest_bi, src_bi;
+  struct buffer_head * cf;
+  int d_key_position;
+
+  internal_define_dest_src_infos (mode, tb, h, &dest_bi, &src_bi, &d_key_position, &cf);
+
+  /*printk("pointer_amount = %d\n",pointer_amount);*/
+
+  if (pointer_amount) {
+    /* insert delimiting key from common father of dest and src to node dest into position B_NR_ITEM(dest) */
+    internal_insert_key (th, &dest_bi, B_NR_ITEMS(dest_bi.bi_bh), cf, d_key_position);
+
+    if (B_NR_ITEMS(src_bi.bi_bh) == pointer_amount - 1) {
+      if (src_bi.bi_position/*src->b_item_order*/ == 0)
+	replace_key (th, cf, d_key_position, src_bi.bi_parent/*src->b_parent*/, 0);
+    } else
+      replace_key (th, cf, d_key_position, src_bi.bi_bh, pointer_amount - 1);
+  }
+  /* last parameter is del_parameter */
+  internal_move_pointers_items (th, &dest_bi, &src_bi, FIRST_TO_LAST, pointer_amount, 0);
+
+}
+
+/* Insert delimiting key to L[h].
+ * Copy n node pointers and n - 1 items from buffer S[h] to L[h].
+ * Delete n - 1 items and node pointers from buffer S[h].
+ */
+/* it always shifts from S[h] to L[h] */
+static void	internal_shift1_left (
+				      struct reiserfs_transaction_handle *th,
+				      struct tree_balance * tb, 
+				      int h, 
+				      int pointer_amount
+				      )
+{
+  struct buffer_info dest_bi, src_bi;
+  struct buffer_head * cf;
+  int d_key_position;
+
+  internal_define_dest_src_infos (INTERNAL_SHIFT_FROM_S_TO_L, tb, h, &dest_bi, &src_bi, &d_key_position, &cf);
+
+  if ( pointer_amount > 0 ) /* insert lkey[h]-th key  from CFL[h] to left neighbor L[h] */
+    internal_insert_key (th, &dest_bi, B_NR_ITEMS(dest_bi.bi_bh), cf, d_key_position);
+  /*		internal_insert_key (tb->L[h], B_NR_ITEM(tb->L[h]), tb->CFL[h], tb->lkey[h]);*/
+
+  /* last parameter is del_parameter */
+  internal_move_pointers_items (th, &dest_bi, &src_bi, FIRST_TO_LAST, pointer_amount, 1);
+  /*	internal_move_pointers_items (tb->L[h], tb->S[h], FIRST_TO_LAST, pointer_amount, 1);*/
+}
+
+
+/* Insert d_key'th (delimiting) key from buffer cfr to head of dest. 
+ * Copy n node pointers and n - 1 items from buffer src to buffer dest.
+ * Replace  d_key'th key in buffer cfr.
+ * Delete n items and node pointers from buffer src.
+ */
+static void internal_shift_right (
+				  struct reiserfs_transaction_handle *th,
+				  int mode,	/* INTERNAL_FROM_S_TO_R | INTERNAL_FROM_L_TO_S */
+				  struct tree_balance * tb,
+				  int h,
+				  int pointer_amount
+				  )
+{
+  struct buffer_info dest_bi, src_bi;
+  struct buffer_head * cf;
+  int d_key_position;
+  int nr;
+
+
+  internal_define_dest_src_infos (mode, tb, h, &dest_bi, &src_bi, &d_key_position, &cf);
+
+  nr = B_NR_ITEMS (src_bi.bi_bh);
+
+  if (pointer_amount > 0) {
+    /* insert delimiting key from common father of dest and src to dest node into position 0 */
+    internal_insert_key (th, &dest_bi, 0, cf, d_key_position);
+    if (nr == pointer_amount - 1) {
+#ifdef CONFIG_REISERFS_CHECK
+      if ( src_bi.bi_bh != PATH_H_PBUFFER (tb->tb_path, h)/*tb->S[h]*/ || dest_bi.bi_bh != tb->R[h])
+	reiserfs_panic (tb->tb_sb, "internal_shift_right", "src (%p) must be == tb->S[h](%p) when it disappears",
+			src_bi.bi_bh, PATH_H_PBUFFER (tb->tb_path, h));
+#endif
+      /* when S[h] disappers replace left delemiting key as well */
+      if (tb->CFL[h])
+	replace_key(th, cf, d_key_position, tb->CFL[h], tb->lkey[h]);
+    } else
+      replace_key(th, cf, d_key_position, src_bi.bi_bh, nr - pointer_amount);
+  }      
+
+  /* last parameter is del_parameter */
+  internal_move_pointers_items (th, &dest_bi, &src_bi, LAST_TO_FIRST, pointer_amount, 0);
+}
+
+/* Insert delimiting key to R[h].
+ * Copy n node pointers and n - 1 items from buffer S[h] to R[h].
+ * Delete n - 1 items and node pointers from buffer S[h].
+ */
+/* it always shift from S[h] to R[h] */
+static void	internal_shift1_right (
+				       struct reiserfs_transaction_handle *th,
+				       struct tree_balance * tb, 
+				       int h, 
+				       int pointer_amount
+				       )
+{
+  struct buffer_info dest_bi, src_bi;
+  struct buffer_head * cf;
+  int d_key_position;
+
+  internal_define_dest_src_infos (INTERNAL_SHIFT_FROM_S_TO_R, tb, h, &dest_bi, &src_bi, &d_key_position, &cf);
+
+  if (pointer_amount > 0) /* insert rkey from CFR[h] to right neighbor R[h] */
+    internal_insert_key (th, &dest_bi, 0, cf, d_key_position);
+  /*		internal_insert_key (tb->R[h], 0, tb->CFR[h], tb->rkey[h]);*/
+	
+  /* last parameter is del_parameter */
+  internal_move_pointers_items (th, &dest_bi, &src_bi, LAST_TO_FIRST, pointer_amount, 1);
+  /*	internal_move_pointers_items (tb->R[h], tb->S[h], LAST_TO_FIRST, pointer_amount, 1);*/
+}
+
+
+/* Delete insert_num node pointers together with their left items
+ * and balance current node.*/
+static void	balance_internal_when_delete (
+					      struct reiserfs_transaction_handle *th,
+					      struct tree_balance * tb, 
+					      int h, 
+					      int child_pos
+					      )
+{
+  int insert_num;
+  int n;
+  struct buffer_head * tbSh = PATH_H_PBUFFER (tb->tb_path, h);
+  struct buffer_info bi;
+
+  insert_num = tb->insert_size[h] / ((int)(DC_SIZE + KEY_SIZE));
+  
+  /* delete child-node-pointer(s) together with their left item(s) */
+  bi.bi_bh = tbSh;
+
+  bi.bi_parent = PATH_H_PPARENT (tb->tb_path, h);
+
+  bi.bi_position = PATH_H_POSITION (tb->tb_path, h + 1);
+
+
+  internal_delete_childs (th, &bi, child_pos, -insert_num);
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( tb->blknum[h] > 1 )
+    reiserfs_panic (tb->tb_sb, "balance_internal_when_delete", "tb->blknum[%d]=%d when insert_size < 0",
+		    h, tb->blknum[h]);
+#endif /* CONFIG_REISERFS_CHECK */
+
+  n = B_NR_ITEMS(tbSh);
+
+  if ( tb->lnum[h] == 0 && tb->rnum[h] == 0 ) {
+    if ( tb->blknum[h] == 0 ) {
+      /* node S[h] (root of the tree) is empty now */
+      struct buffer_head *new_root;
+
+#ifdef CONFIG_REISERFS_CHECK
+      if (n || B_BLK_HEAD (tbSh)->blk_free_space != MAX_CHILD_SIZE(tbSh) - DC_SIZE)
+	reiserfs_panic (tb->tb_sb, "balance_internal_when_delete", "buffer must have only 0 keys (%d)",
+			n);
+
+      if (bi.bi_parent)
+	reiserfs_panic (tb->tb_sb, "balance_internal_when_delete", "root has parent (%p)", bi.bi_parent);
+#endif /* CONFIG_REISERFS_CHECK */
+		
+      /* choose a new root */
+      if ( ! tb->L[h-1] || ! B_NR_ITEMS(tb->L[h-1]) )
+	new_root = tb->R[h-1];
+      else
+	new_root = tb->L[h-1];
+      /* switch super block's tree root block number to the new value */
+      tb->tb_sb->u.reiserfs_sb.s_rs->s_root_block = new_root->b_blocknr;
+      tb->tb_sb->u.reiserfs_sb.s_rs->s_tree_height --;
+
+      /* reiserfs_mark_buffer_dirty(tb->tb_sb->u.reiserfs_sb.s_sbh,1); journal victim */	/* not preserved, super_block */
+      journal_mark_dirty(th, tb->tb_sb, tb->tb_sb->u.reiserfs_sb.s_sbh);	/* not preserved, super_block */
+      tb->tb_sb->s_dirt = 1;
+
+      /* mark buffer S[h] not uptodate and put it in free list */
+      reiserfs_invalidate_buffer(th, tb, tbSh, 1); /* preserve not needed, internal */
+      return;
+    }
+    return;
+  }
+
+  if ( tb->L[h] && tb->lnum[h] == -B_NR_ITEMS(tb->L[h]) - 1 ) { /* join S[h] with L[h] */
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( tb->rnum[h] != 0 )
+      reiserfs_panic (tb->tb_sb, "balance_internal_when_delete", "invalid tb->rnum[%d]==%d when joining S[h] with L[h]",
+		      h, tb->rnum[h]);
+#endif /* CONFIG_REISERFS_CHECK */
+
+    internal_shift_left (th, INTERNAL_SHIFT_FROM_S_TO_L, tb, h, n + 1);/*tb->L[h], tb->CFL[h], tb->lkey[h], tb->S[h], n+1);*/
+    reiserfs_invalidate_buffer(th, tb, tbSh, 1); /* preserve not needed, internal, 1 mean free block */
+
+    return;
+  }
+
+  if ( tb->R[h] &&  tb->rnum[h] == -B_NR_ITEMS(tb->R[h]) - 1 ) { /* join S[h] with R[h] */
+#ifdef CONFIG_REISERFS_CHECK
+    if ( tb->lnum[h] != 0 )
+      reiserfs_panic (tb->tb_sb, "balance_internal_when_delete", "invalid tb->lnum[%d]==%d when joining S[h] with R[h]",
+		      h, tb->lnum[h]);
+#endif /* CONFIG_REISERFS_CHECK */
+    /*internal_shift_right (tb, h, tb->S[h], tb->CFR[h], tb->rkey[h], tb->R[h], n+1);*/
+    internal_shift_right (th, INTERNAL_SHIFT_FROM_S_TO_R, tb, h, n + 1);
+    reiserfs_invalidate_buffer(th, tb,tbSh, 1);/* preserve not needed, internal */
+    return;
+  }
+
+  if ( tb->lnum[h] < 0 ) { /* borrow from left neighbor L[h] */
+#ifdef CONFIG_REISERFS_CHECK
+    if ( tb->rnum[h] != 0 )
+      reiserfs_panic (tb->tb_sb, "balance_internal_when_delete", "invalid tb->rnum[%d]==%d when borrow from L[h]",
+		      h, tb->rnum[h]);
+#endif /* CONFIG_REISERFS_CHECK */
+    /*internal_shift_right (tb, h, tb->L[h], tb->CFL[h], tb->lkey[h], tb->S[h], -tb->lnum[h]);*/
+    internal_shift_right (th, INTERNAL_SHIFT_FROM_L_TO_S, tb, h, -tb->lnum[h]);
+    return;
+  }
+
+  if ( tb->rnum[h] < 0 ) { /* borrow from right neighbor R[h] */
+#ifdef CONFIG_REISERFS_CHECK
+    if ( tb->lnum[h] != 0 )
+      reiserfs_panic (tb->tb_sb, "balance_internal_when_delete", "invalid tb->lnum[%d]==%d when borrow from R[h]",
+		      h, tb->lnum[h]);
+#endif /* CONFIG_REISERFS_CHECK */
+    internal_shift_left (th, INTERNAL_SHIFT_FROM_R_TO_S, tb, h, -tb->rnum[h]);/*tb->S[h], tb->CFR[h], tb->rkey[h], tb->R[h], -tb->rnum[h]);*/
+    return;
+  }
+
+  if ( tb->lnum[h] > 0 ) { /* split S[h] into two parts and put them into neighbors */
+#ifdef CONFIG_REISERFS_CHECK
+    if ( tb->rnum[h] == 0 || tb->lnum[h] + tb->rnum[h] != n + 1 )
+      reiserfs_panic (tb->tb_sb, "balance_internal_when_delete", 
+		      "invalid tb->lnum[%d]==%d or tb->rnum[%d]==%d when S[h](item number == %d) is split between them",
+		      h, tb->lnum[h], h, tb->rnum[h], n);
+#endif /* CONFIG_REISERFS_CHECK */
+
+    internal_shift_left (th, INTERNAL_SHIFT_FROM_S_TO_L, tb, h, tb->lnum[h]);/*tb->L[h], tb->CFL[h], tb->lkey[h], tb->S[h], tb->lnum[h]);*/
+    /*internal_shift_right (tb, h, tb->S[h], tb->CFR[h], tb->rkey[h], tb->R[h], tb->rnum[h]);*/
+    internal_shift_right (th, INTERNAL_SHIFT_FROM_S_TO_R, tb, h, tb->rnum[h]);
+    reiserfs_invalidate_buffer (th, tb, tbSh, 1);/* preserve not needed, internal */
+
+    return;
+  }
+  reiserfs_panic (tb->tb_sb, "balance_internal_when_delete", "unexpected tb->lnum[%d]==%d or tb->rnum[%d]==%d",
+		  h, tb->lnum[h], h, tb->rnum[h]);
+}
+
+/* Replace delimiting key of buffers L[h] and S[h] by the given key.*/
+void	replace_lkey (
+		      struct reiserfs_transaction_handle *th,
+		      struct tree_balance * tb,
+		      int h,
+		      struct item_head * key
+		      )
+{
+#ifdef CONFIG_REISERFS_CHECK
+  if (tb->L[h] == NULL || tb->CFL[h] == NULL)
+    reiserfs_panic (tb->tb_sb, "replace_lkey: 12255: "
+		    "L[h](%p) and CFL[h](%p) must exist in replace_lkey", tb->L[h], tb->CFL[h]);
+#endif
+
+  if (B_NR_ITEMS(PATH_H_PBUFFER(tb->tb_path, h)) == 0)
+    return;
+
+  memcpy (B_N_PDELIM_KEY(tb->CFL[h],tb->lkey[h]), key, KEY_SIZE);
+
+  journal_mark_dirty(th,  tb->tb_sb, tb->CFL[h]);	/* not preserved, preserves are in balance_leaf() and  balance_leaf_when_delete() */
+}
+
+
+/* Replace delimiting key of buffers S[h] and R[h] by the given key.*/
+void	replace_rkey (
+		      struct reiserfs_transaction_handle *th,
+		      struct tree_balance * tb,
+		      int h,
+		      struct item_head * key
+		      )
+{
+#ifdef CONFIG_REISERFS_CHECK
+  if (tb->R[h] == NULL || tb->CFR[h] == NULL)
+    reiserfs_panic (tb->tb_sb, "replace_rkey: 12260: "
+		    "R[h](%p) and CFR[h](%p) must exist in replace_rkey", tb->R[h], tb->CFR[h]);
+
+  if (B_NR_ITEMS(tb->R[h]) == 0)
+    reiserfs_panic (tb->tb_sb, "replace_rkey: 12265: "
+		    "R[h] can not be empty if it exists (item number=%d)", B_NR_ITEMS(tb->R[h]));
+#endif
+
+  memcpy (B_N_PDELIM_KEY(tb->CFR[h],tb->rkey[h]), key, KEY_SIZE);
+
+  journal_mark_dirty(th, tb->tb_sb, tb->CFR[h]);	/* not preserved, preserves are in balance_leaf() and  
+  							   balance_leaf_when_delete() */
+}
+
+
+
+int	balance_internal (
+			  struct reiserfs_transaction_handle *th,
+			  struct tree_balance * tb,			/* tree_balance structure 		*/
+			  int h,					/* level of the tree 			*/
+			  int child_pos,
+			  struct item_head * insert_key,		/* key for insertion on higher level   	*/
+			  struct buffer_head ** insert_ptr	/* node for insertion on higher level*/
+			  )
+  /* if inserting/pasting
+   {
+   child_pos is the position of the node-pointer in S[h] that	 *
+   pointed to S[h-1] before balancing of the h-1 level;		 *
+   this means that new pointers and items must be inserted AFTER *
+   child_pos
+   }
+   else 
+   {
+   it is the position of the leftmost pointer that must be deleted (together with
+   its corresponding key to the left of the pointer)
+   as a result of the previous level's balancing.
+   }
+*/
+{
+  struct buffer_head * tbSh = PATH_H_PBUFFER (tb->tb_path, h);
+  struct buffer_info bi;
+  int order;		/* we return this: it is 0 if there is no S[h], else it is tb->S[h]->b_item_order */
+  int insert_num, n, k;
+  struct buffer_head * S_new;
+  struct item_head new_insert_key;
+  struct buffer_head * new_insert_ptr = NULL;
+  struct item_head * new_insert_key_addr = insert_key;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( h < 1 )      
+    reiserfs_panic (tb->tb_sb, "balance_internal", "h (%d) can not be < 1 on internal level", h);
+#endif /* CONFIG_REISERFS_CHECK */
+
+  order = ( tbSh ) ? PATH_H_POSITION (tb->tb_path, h + 1)/*tb->S[h]->b_item_order*/ : 0;
+
+  /* Using insert_size[h] calculate the number insert_num of items
+     that must be inserted to or deleted from S[h]. */
+  insert_num = tb->insert_size[h]/((int)(KEY_SIZE + DC_SIZE));
+
+  /* Check whether insert_num is proper **/
+#ifdef CONFIG_REISERFS_CHECK
+  if ( insert_num < -2  ||  insert_num > 2 )
+    reiserfs_panic (tb->tb_sb, "balance_internal",
+		    "incorrect number of items inserted to the internal node (%d)", insert_num);
+
+  if ( h > 1  && (insert_num > 1 || insert_num < -1) )
+    reiserfs_panic (tb->tb_sb, "balance_internal",
+		    "incorrect number of items (%d) inserted to the internal node on a level (h=%d) higher than last internal level", 
+		    insert_num, h);
+#endif /* CONFIG_REISERFS_CHECK */
+
+  /* Make balance in case insert_num < 0 */
+  if ( insert_num < 0 ) {
+    balance_internal_when_delete (th, tb, h, child_pos);
+    return order;
+  }
+ 
+  k = 0;
+  if ( tb->lnum[h] > 0 ) {
+    /* shift lnum[h] items from S[h] to the left neighbor L[h].
+       check how many of new items fall into L[h] or CFL[h] after shifting */
+    n = B_BLK_HEAD(tb->L[h])->blk_nr_item; /* number of items in L[h] */
+    if ( tb->lnum[h] <= child_pos ) {
+      /* new items don't fall into L[h] or CFL[h] */
+      internal_shift_left (th, INTERNAL_SHIFT_FROM_S_TO_L, tb, h, tb->lnum[h]);
+      /*internal_shift_left (tb->L[h],tb->CFL[h],tb->lkey[h],tbSh,tb->lnum[h]);*/
+      child_pos -= tb->lnum[h];
+    } else if ( tb->lnum[h] > child_pos + insert_num ) {
+      /* all new items fall into L[h] */
+      internal_shift_left (th, INTERNAL_SHIFT_FROM_S_TO_L, tb, h, tb->lnum[h] - insert_num);
+      /*			internal_shift_left(th, tb->L[h],tb->CFL[h],tb->lkey[h],tbSh,
+				tb->lnum[h]-insert_num);
+				*/
+      /* insert insert_num keys and node-pointers into L[h] */
+      bi.bi_bh = tb->L[h];
+      bi.bi_parent = tb->FL[h];
+      bi.bi_position = get_left_neighbor_position (tb, h);
+      internal_insert_childs (th, &bi,/*tb->L[h], tb->S[h-1]->b_next*/ n + child_pos + 1,
+			      insert_num,insert_key,insert_ptr);
+
+      insert_num = 0; 
+    } else {
+      struct disk_child * dc;
+
+      /* some items fall into L[h] or CFL[h], but some don't fall */
+      internal_shift1_left(th, tb,h,child_pos+1);
+      /* calculate number of new items that fall into L[h] */
+      k = tb->lnum[h] - child_pos - 1;
+
+      bi.bi_bh = tb->L[h];
+      bi.bi_parent = tb->FL[h];
+      bi.bi_position = get_left_neighbor_position (tb, h);
+      internal_insert_childs (th, &bi,/*tb->L[h], tb->S[h-1]->b_next,*/ n + child_pos + 1,k,
+			      insert_key,insert_ptr);
+
+      replace_lkey(th, tb,h,insert_key + k);
+
+      /* replace the first node-ptr in S[h] by node-ptr to insert_ptr[k] */
+      (dc = B_N_CHILD(tbSh, 0))->dc_size =
+	MAX_CHILD_SIZE(insert_ptr[k]) -
+	B_BLK_HEAD(insert_ptr[k])->blk_free_space;
+      dc->dc_block_number = insert_ptr[k]->b_blocknr; 
+
+      /* reiserfs_mark_buffer_dirty(tbSh, 0); journal_victim */ /* no need to preserve, internal */
+      journal_mark_dirty(th, tb->tb_sb, tbSh); /* no need to preserve, internal */
+
+      k++;
+      insert_key += k;
+      insert_ptr += k;
+      insert_num -= k;
+      child_pos = 0;
+    }
+  }	/* tb->lnum[h] > 0 */
+
+  if ( tb->rnum[h] > 0 ) {
+    /*shift rnum[h] items from S[h] to the right neighbor R[h]*/
+    /* check how many of new items fall into R or CFR after shifting */
+    n = B_BLK_HEAD (tbSh)->blk_nr_item; /* number of items in S[h] */
+    if ( n - tb->rnum[h] >= child_pos )
+      /* new items fall into S[h] */
+      /*internal_shift_right(tb,h,tbSh,tb->CFR[h],tb->rkey[h],tb->R[h],tb->rnum[h]);*/
+      internal_shift_right (th, INTERNAL_SHIFT_FROM_S_TO_R, tb, h, tb->rnum[h]);
+    else
+      if ( n + insert_num - tb->rnum[h] < child_pos )
+	{
+	  /* all new items fall into R[h] */
+	  /*internal_shift_right(tb,h,tbSh,tb->CFR[h],tb->rkey[h],tb->R[h],
+	    tb->rnum[h] - insert_num);*/
+	  internal_shift_right (th, INTERNAL_SHIFT_FROM_S_TO_R, tb, h, tb->rnum[h] - insert_num);
+
+	  /* insert insert_num keys and node-pointers into R[h] */
+	  bi.bi_bh = tb->R[h];
+	  bi.bi_parent = tb->FR[h];
+	  bi.bi_position = get_right_neighbor_position (tb, h);
+	  internal_insert_childs (th, &bi, /*tb->R[h],tb->S[h-1]->b_next*/ child_pos - n - insert_num + tb->rnum[h] - 1,
+				  insert_num,insert_key,insert_ptr);
+	  insert_num = 0;
+	}
+      else
+	{
+	  struct disk_child * dc;
+
+	  /* one of the items falls into CFR[h] */
+	  internal_shift1_right(th, tb,h,n - child_pos + 1);
+	  /* calculate number of new items that fall into R[h] */
+	  k = tb->rnum[h] - n + child_pos - 1;
+
+	  bi.bi_bh = tb->R[h];
+	  bi.bi_parent = tb->FR[h];
+	  bi.bi_position = get_right_neighbor_position (tb, h);
+	  internal_insert_childs (th, &bi, /*tb->R[h], tb->R[h]->b_child,*/ 0, k, insert_key + 1, insert_ptr + 1);
+
+	  replace_rkey(th, tb,h,insert_key + insert_num - k - 1);
+
+	  /* replace the first node-ptr in R[h] by node-ptr insert_ptr[insert_num-k-1]*/
+	  (dc = B_N_CHILD(tb->R[h], 0))->dc_size =
+	    MAX_CHILD_SIZE(insert_ptr[insert_num-k-1]) -
+	    B_BLK_HEAD(insert_ptr[insert_num-k-1])->blk_free_space;
+	  dc->dc_block_number = insert_ptr[insert_num-k-1]->b_blocknr;
+
+	  /* reiserfs_mark_buffer_dirty(tb->R[h],0); journal victim *//* no need to preserve, internal */
+	  journal_mark_dirty(th, tb->tb_sb, tb->R[h]);/* no need to preserve, internal */
+
+	  /*
+	    insert_ptr[insert_num-k-1]->b_item_order = 0;
+	    insert_ptr[insert_num-k-1]->b_parent = tb->R[h];
+	    reiserfs_insert_into_child_list(insert_ptr[insert_num-k-1],tb->R[h]->b_child);
+	    */
+			
+	  insert_num -= (k + 1);
+	}
+  }
+
+  /** Fill new node that appears instead of S[h] **/
+#ifdef CONFIG_REISERFS_CHECK
+  if ( tb->blknum[h] > 2 )
+    reiserfs_panic(0, "balance_internal", "blknum can not be > 2 for internal level");
+  if ( tb->blknum[h] < 0 )
+    reiserfs_panic(0, "balance_internal", "blknum can not be < 0");
+#endif /* CONFIG_REISERFS_CHECK */
+
+  if ( ! tb->blknum[h] )
+    { /* node S[h] is empty now */
+#ifdef CONFIG_REISERFS_CHECK
+      if ( ! tbSh )
+	reiserfs_panic(0,"balance_internal", "S[h] is equal NULL");
+#endif /* CONFIG_REISERFS_CHECK */
+
+      /* Mark buffer as invalid and put it to head of free list. */
+      reiserfs_invalidate_buffer(th, tb,tbSh, 1);/* do not preserve, internal node*/
+      return order;
+    }
+
+  if ( ! tbSh ) {
+    /* create new root */
+    struct disk_child  * dc;
+    struct buffer_head * tbSh_1 = PATH_H_PBUFFER (tb->tb_path, h - 1);
+
+
+    if ( tb->blknum[h] != 1 )
+      reiserfs_panic(0, "balance_internal", "One new node required for creating the new root");
+    /* S[h] = empty buffer from the list FEB. */
+    tbSh = get_FEB (tb);
+    B_BLK_HEAD(tbSh)->blk_level = h + 1;
+
+    /* Put the unique node-pointer to S[h] that points to S[h-1]. */
+
+    (dc = B_N_CHILD(tbSh, 0))->dc_block_number = tbSh_1->b_blocknr;
+    dc->dc_size = MAX_CHILD_SIZE (tbSh_1) - B_BLK_HEAD(tbSh_1)->blk_free_space;
+
+    tb->insert_size[h] -= DC_SIZE;
+    B_BLK_HEAD(tbSh)->blk_free_space -= DC_SIZE;
+
+    /* reiserfs_mark_buffer_dirty(tbSh,0); journal victim *//* no need to preserve, internal */
+    journal_mark_dirty(th, tb->tb_sb, tbSh);/* no need to preserve, internal */
+    
+    /* put new root into path structure */
+    PATH_OFFSET_PBUFFER(tb->tb_path, ILLEGAL_PATH_ELEMENT_OFFSET) = tbSh;
+
+    /* Change root in structure super block. */
+    tb->tb_sb->u.reiserfs_sb.s_rs->s_root_block = tbSh->b_blocknr;
+    tb->tb_sb->u.reiserfs_sb.s_rs->s_tree_height ++;
+    /* reiserfs_mark_buffer_dirty(tb->tb_sb->u.reiserfs_sb.s_sbh, 1); journal victim *//* no need to preserve, super */	
+    journal_mark_dirty(th, tb->tb_sb, tb->tb_sb->u.reiserfs_sb.s_sbh);/* no need to preserve, super */	
+    tb->tb_sb->s_dirt = 1;
+  }
+	
+  if ( tb->blknum[h] == 2 ) {
+    int snum;
+    struct buffer_info dest_bi, src_bi;
+
+
+    /* S_new = free buffer from list FEB */
+    S_new = get_FEB(tb);
+
+#ifdef CONFIG_REISERFS_CHECK
+    buffers[h] = S_new;
+#endif
+
+    B_BLK_HEAD(S_new)->blk_level = h + 1;
+    
+
+    dest_bi.bi_bh = S_new;
+    dest_bi.bi_parent = 0;
+    dest_bi.bi_position = 0;
+    src_bi.bi_bh = tbSh;
+    src_bi.bi_parent = PATH_H_PPARENT (tb->tb_path, h);
+    src_bi.bi_position = PATH_H_POSITION (tb->tb_path, h + 1);
+		
+    n = B_BLK_HEAD(tbSh)->blk_nr_item; /* number of items in S[h] */
+    snum = (insert_num + n + 1)/2;
+    if ( n - snum >= child_pos ) {
+      /* new items don't fall into S_new */
+      /*	store the delimiting key for the next level */
+      /* new_insert_key = (n - snum)'th key in S[h] */
+      memcpy (&new_insert_key,B_N_PDELIM_KEY(tbSh,n - snum),
+	      KEY_SIZE);
+      /* last parameter is del_par */
+      internal_move_pointers_items (th, &dest_bi, &src_bi, LAST_TO_FIRST, snum, 0);
+      /*            internal_move_pointers_items(S_new, tbSh, LAST_TO_FIRST, snum, 0);*/
+    } else if ( n + insert_num - snum < child_pos ) {
+      /* all new items fall into S_new */
+      /*	store the delimiting key for the next level */
+      /* new_insert_key = (n + insert_item - snum)'th key in S[h] */
+      memcpy(&new_insert_key,B_N_PDELIM_KEY(tbSh,n + insert_num - snum),
+	     KEY_SIZE);
+      /* last parameter is del_par */
+      internal_move_pointers_items (th, &dest_bi, &src_bi, LAST_TO_FIRST, snum - insert_num, 0);
+      /*			internal_move_pointers_items(S_new,tbSh,1,snum - insert_num,0);*/
+
+      /* insert insert_num keys and node-pointers into S_new */
+      internal_insert_childs (th, &dest_bi, /*S_new,tb->S[h-1]->b_next,*/child_pos - n - insert_num + snum - 1,
+			      insert_num,insert_key,insert_ptr);
+
+      insert_num = 0;
+    } else {
+      struct disk_child * dc;
+
+      /* some items fall into S_new, but some don't fall */
+      /* last parameter is del_par */
+      internal_move_pointers_items (th, &dest_bi, &src_bi, LAST_TO_FIRST, n - child_pos + 1, 1);
+      /*			internal_move_pointers_items(S_new,tbSh,1,n - child_pos + 1,1);*/
+      /* calculate number of new items that fall into S_new */
+      k = snum - n + child_pos - 1;
+
+      internal_insert_childs (th, &dest_bi, /*S_new,*/ 0, k, insert_key + 1, insert_ptr+1);
+
+      /* new_insert_key = insert_key[insert_num - k - 1] */
+      memcpy(&new_insert_key,insert_key + insert_num - k - 1,
+	     KEY_SIZE);
+      /* replace first node-ptr in S_new by node-ptr to insert_ptr[insert_num-k-1] */
+
+      (dc = B_N_CHILD(S_new,0))->dc_size =
+	MAX_CHILD_SIZE(insert_ptr[insert_num-k-1]) -
+	B_BLK_HEAD(insert_ptr[insert_num-k-1])->blk_free_space;
+      dc->dc_block_number =	insert_ptr[insert_num-k-1]->b_blocknr; 
+
+      /* reiserfs_mark_buffer_dirty(S_new,0); journal victim *//* no need to preserve, new */
+      journal_mark_dirty(th, tb->tb_sb, S_new);/* no need to preserve, new */
+			
+      insert_num -= (k + 1);
+    }
+    /* new_insert_ptr = node_pointer to S_new */
+    new_insert_ptr = S_new;
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( buffer_locked(S_new) )
+      reiserfs_panic (tb->tb_sb, "balance_internal", "locked buffer S_new[]");
+    if (S_new->b_count != 1)
+      if (!(buffer_journaled(S_new) && S_new->b_count == 2)) {
+	printk ("REISERFS: balance_internal: S_new->b_count != 1 (%u)\n", S_new->b_count);
+      }
+#endif /* CONFIG_REISERFS_CHECK */
+
+    /*
+      S_new->b_count --;
+      */
+    /*brelse(S_new);*/
+  }
+
+  n = B_BLK_HEAD(tbSh)->blk_nr_item; /*number of items in S[h] */
+
+#ifdef REISERFS_FSCK
+  if ( -1 <= child_pos && child_pos <= n && insert_num > 0 ) {
+#else
+  if ( 0 <= child_pos && child_pos <= n && insert_num > 0 ) {
+#endif
+    bi.bi_bh = tbSh;
+    bi.bi_parent = PATH_H_PPARENT (tb->tb_path, h);
+    bi.bi_position = PATH_H_POSITION (tb->tb_path, h + 1);
+#ifdef REISERFS_FSCK
+    if (child_pos == -1) {
+      /* this is a little different from original do_balance: 
+	 here we insert the minimal keys in the tree, that has never happened when file system works */
+      if (tb->CFL[h-1] || insert_num != 1 || h != 1)
+	die ("balance_internal: invalid child_pos");
+/*      insert_child (tb->S[h], tb->S[h-1], child_pos, insert_num, B_N_ITEM_HEAD(tb->S[0],0), insert_ptr);*/
+      internal_insert_childs (th, &bi, child_pos, insert_num, B_N_PITEM_HEAD (PATH_PLAST_BUFFER (tb->tb_path), 0), insert_ptr);
+    } else
+#endif
+    internal_insert_childs (th, 
+			    &bi,/*tbSh,*/
+			    /*		( tb->S[h-1]->b_parent == tb->S[h] ) ? tb->S[h-1]->b_next :  tb->S[h]->b_child->b_next,*/
+			    child_pos,insert_num,insert_key,insert_ptr
+			    );
+  }
+
+
+  memcpy (new_insert_key_addr,&new_insert_key,KEY_SIZE);
+  insert_ptr[0] = new_insert_ptr;
+
+  return order;
+}
+
diff -urN linux/fs/reiserfs/inode.c /tmp/linux/fs/reiserfs/inode.c
--- linux/fs/reiserfs/inode.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/inode.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,927 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+#ifdef __KERNEL__
+
+#include <linux/sched.h>
+#include <linux/reiserfs_fs.h>
+#include <linux/locks.h>
+#include <asm/uaccess.h>
+
+#else
+
+#include "nokernel.h"
+int reiserfs_notify_change(struct dentry * dentry, struct iattr * attr){return 0;}
+
+#endif
+
+
+void reiserfs_delete_inode (struct inode * inode)
+{
+    int writers_to_wait_for ;
+    int jbegin_count = JOURNAL_PER_BALANCE_CNT * 2; 
+    int windex ;
+    struct reiserfs_transaction_handle th ;
+
+    writers_to_wait_for = 1; 
+    journal_begin(&th, inode->i_sb, jbegin_count) ;
+    windex = push_journal_writer("delete_inode") ;
+    reiserfs_update_inode_transaction(inode) ;
+
+    /* The = 0 happens when we abort creating a new inode for some reason like lack of space.. */
+    if (INODE_PKEY(inode)->k_objectid != 0) {
+	reiserfs_delete_object (&th, inode);
+	reiserfs_release_objectid (&th, inode->i_ino, inode->i_sb);
+    } else {
+	/* no object items are in the tree */
+	;
+    }
+    pop_journal_writer(windex) ;
+    journal_end(&th, inode->i_sb, jbegin_count) ;
+    clear_inode (inode); /* note this must go after the journal_end to prevent deadlock */
+}
+
+#if 0
+static void copy_data_blocks_to_inode (struct inode * inode, struct item_head * ih, __u32 * ind_item)
+{
+    int first_log_block = (ih->ih_key.k_offset - 1) / inode->i_sb->s_blocksize; /* first log block addressed by indirect item */
+    int i, j;
+  
+    for (i = first_log_block, j = 0; i < REISERFS_N_BLOCKS && j < I_UNFM_NUM (ih); i ++, j ++) {
+#ifdef CONFIG_REISERFS_CHECK
+	if (inode->u.reiserfs_i.i_data [i] && inode->u.reiserfs_i.i_data [i] != ind_item [j])
+	    reiserfs_panic (inode->i_sb, "vs-13000: reiserfs_bmap: log block %d, data block %d is seet and doe not match to unfmptr %d",
+			    i, inode->u.reiserfs_i.i_data [i], ind_item [j]);
+#endif
+	inode->u.reiserfs_i.i_data [i] = ind_item [j];
+    }
+}
+#endif/*0*/
+
+/* convert logical file block to appropriate unformatted node. */
+int reiserfs_bmap (struct inode * inode, int block)
+{
+    struct key offset_key;
+    struct path path_to_blocknr;
+    int pos_in_item;
+    int repeat;
+    struct buffer_head * bh;
+    struct item_head * ih;
+    int blocknr;
+
+    increment_i_read_sync_counter(inode) ;
+    offset_key.k_offset = block * inode->i_sb->s_blocksize + 1;
+    if (INODE_OFFSET_IN_DIRECT (inode, offset_key.k_offset)) {
+	decrement_i_read_sync_counter(inode) ;
+	return 0;
+    }
+
+    /*
+      if (block < REISERFS_N_BLOCKS && inode->u.reiserfs_i.i_data [block]) {
+      inode->i_sb->u.reiserfs_sb.s_bmaps_without_search ++;
+      return inode->u.reiserfs_i.i_data [block];
+      }
+    */
+
+
+    copy_short_key (&offset_key, INODE_PKEY (inode));
+    offset_key.k_uniqueness = TYPE_INDIRECT;
+
+    init_path (&path_to_blocknr);
+    if (search_for_position_by_key (inode->i_sb, &offset_key, &path_to_blocknr, &pos_in_item, &repeat) == POSITION_NOT_FOUND) {
+	/*reiserfs_warning ("vs-13020: reiserfs_bmap: there is no required byte (%k) in the file of size %ld. Found item \n%h\n", 
+	  &offset_key, inode->i_size, PATH_PITEM_HEAD (&path_to_blocknr));*/
+	pathrelse (&path_to_blocknr);
+	decrement_i_read_sync_counter(inode) ;
+	return 0;
+    }
+
+    bh = PATH_PLAST_BUFFER (&path_to_blocknr);
+    ih = B_N_PITEM_HEAD (bh, PATH_LAST_POSITION (&path_to_blocknr));
+
+    if (I_IS_INDIRECT_ITEM (ih)) {
+	__u32 * ind_item = (__u32 *)B_I_PITEM (bh, ih);
+
+	/*
+	  copy_data_blocks_to_inode (inode, ih, ind_item);
+	*/
+	blocknr = ind_item [pos_in_item];
+	pathrelse (&path_to_blocknr);
+	inode->i_sb->u.reiserfs_sb.s_bmaps ++;
+	decrement_i_read_sync_counter(inode) ;
+	return blocknr;
+    }
+
+    reiserfs_warning ("vs-13030: reiserfs_bmap: found item \n%h\n is not indirect one\n", ih);
+    pathrelse (&path_to_blocknr);
+    decrement_i_read_sync_counter(inode) ;
+    return 0;
+}
+
+
+#define has_tail(inode) ((inode)->u.reiserfs_i.i_first_direct_byte != NO_BYTES_IN_DIRECT_ITEM)
+#define tail_offset(inode) ((inode)->u.reiserfs_i.i_first_direct_byte - 1)
+
+/* "we are reading into the page cache, not into any process's virtual
+   memory". Stephen C. Tweedie, therefore no need for local buffer */
+static int read_file_tail (struct inode * inode, loff_t offset, char * buf, int size)
+{
+    struct key key;
+    int repeat, pos_in_item;
+    struct path path;
+    struct item_head * ih;
+    char * p;
+    int chars, left, read;
+    struct buffer_head * bh;
+    int block;
+    struct buffer_head * pbh[PAGE_SIZE / 512];
+    int i, j;
+
+    p = buf;
+    left = size;
+    read = 0;
+
+    /* read unformatted nodes first */
+    i = 0;
+    while (offset < tail_offset (inode)) {
+	block = offset >> inode->i_sb->s_blocksize_bits;
+	block = inode->i_op->bmap(inode, block);
+	pbh[i] = getblk (inode->i_dev, block, inode->i_sb->s_blocksize);
+	offset += inode->i_sb->s_blocksize;
+	i ++;
+    }
+  
+    if (i)
+	ll_rw_block (READ, i, pbh);
+
+    p = buf;
+    for (j = 0; j < i; j ++) {
+	wait_on_buffer (pbh[j]);
+	memcpy (p, pbh[j]->b_data, pbh[j]->b_size);
+	read += pbh[j]->b_size;
+	left -= pbh[j]->b_size;
+	p += pbh[j]->b_size;
+	brelse (pbh[j]);
+    }
+
+
+    /* read direct item(s) */
+
+    if (offset != tail_offset (inode)) {
+#ifdef CONFIG_REISERFS_CHECK
+	reiserfs_warning ("vs-18010: read_file_tail: given offset (%d) is not stored in direct item. \
+Inode's first direct byte %d\n", offset, inode->u.reiserfs_i.i_first_direct_byte);
+#endif
+	return 0;
+    }
+
+    init_path (&path);
+
+    copy_short_key (&key, INODE_PKEY(inode));
+    key.k_offset = offset + 1;
+    key.k_uniqueness = TYPE_DIRECT;
+
+    while (left) {
+	if (search_for_position_by_key (inode->i_sb, &key, &path, &pos_in_item, &repeat) == POSITION_NOT_FOUND) {
+	    break;
+	}
+
+	bh = PATH_PLAST_BUFFER (&path);
+	ih = B_N_PITEM_HEAD (bh, PATH_LAST_POSITION (&path));
+
+	chars = ih->ih_item_len - pos_in_item;
+	if (chars > left)
+	    chars = left;
+
+	memcpy (p, B_I_PITEM (bh, ih) + pos_in_item, chars);
+	key.k_offset += chars;
+	read += chars;
+	left -= chars;
+	p += chars;
+	if (PATH_LAST_POSITION (&path) != B_NR_ITEMS (bh) - 1)
+	    /* that was last direct item of the tail */
+	    break;
+    }
+
+    pathrelse (&path);
+    return read;
+}
+
+
+int reiserfs_readpage (struct file * file, struct page * page) 
+{
+    struct inode * inode;
+    int ret ;
+
+				/* If you get this from the page it is one less indirection -Hans */
+    inode = file->f_dentry->d_inode;
+
+    increment_i_read_sync_counter(inode) ;
+    if (has_tail (inode) && tail_offset (inode) < page->offset + PAGE_SIZE) {
+	/* there is a tail and it is in this page */
+	memset ((char *)page_address (page), 0, PAGE_SIZE);
+	read_file_tail (inode, page->offset, (char *)page_address (page), PAGE_SIZE);
+	set_bit (PG_uptodate, &page->flags);
+	ret = 0 ;
+    }  else {
+	ret = generic_readpage (file, page);
+    }
+    decrement_i_read_sync_counter(inode) ;
+    return ret ;
+}
+
+/* Iget accepts only super block and inode number as it hashes inodes
+   using device identifier and inode number. If iget could not find
+   required inode in its hash queues, then it calls
+   reiserfs_read_inode passing to it only inode
+   number. Reiserfs_read_inode must know the key. That is why we keep
+   key in global array before iget.
+   */
+
+#define KEY_ARRAY_SIZE 100
+
+static struct {
+    unsigned long objectid;
+    unsigned long dirid;
+    kdev_t dev ;
+} g_key_array [KEY_ARRAY_SIZE] = {{0,},};
+
+
+static unsigned long look_for_key  (struct super_block *s, unsigned long objectid)
+{
+    int i;
+
+    for (i = 0; i < sizeof (g_key_array) / sizeof (g_key_array[0]); i ++) {
+	if (g_key_array[i].objectid == objectid && 
+	    g_key_array[i].dev == s->s_dev)
+	    return g_key_array[i].dirid;
+    }
+    return (unsigned long)-1;
+}
+
+#define ROUND_UP(x,n) (((x)+(n)-1u) & ~((n)-1u))
+
+/* looks for stat data in the tree, and fills up the fields of in-core
+   inode stat data fields */
+void reiserfs_read_inode (struct inode * inode)
+{
+    struct path path_to_sd;
+    struct stat_data * sd;
+    struct item_head *ih ;
+    int repeat;
+
+    init_path (&path_to_sd);
+
+    inode->i_op = NULL;
+    inode->i_mode = 0;
+
+    /* form key of the stat data */
+    inode->u.reiserfs_i.i_key[0] = look_for_key (inode->i_sb, inode->i_ino);
+    inode->u.reiserfs_i.i_key[1] = inode->i_ino;
+    inode->u.reiserfs_i.i_key[2] = SD_OFFSET;
+    inode->u.reiserfs_i.i_key[3] = TYPE_STAT_DATA;
+    if (inode->u.reiserfs_i.i_key[0] == (unsigned long)-1) {
+#ifdef CONFIG_REISERFS_CHECK
+	reiserfs_warning ("vs-13040: reiserfs_read_inode: could not find k_dir_id (objectid = %lu)\n",
+			  inode->i_ino);
+#endif
+	/* VERY slow search through all possible packing localities. */
+	if (search_by_objectid(inode->i_sb, INODE_PKEY (inode), &path_to_sd, &repeat, DISK_LEAF_NODE_LEVEL, READ_BLOCKS) != ITEM_FOUND) {
+	    pathrelse (&path_to_sd);
+	    make_bad_inode(inode) ;
+	    return ;
+	}
+
+	/* ok, the search found our item.  Pull the packing locality from 
+	** the path it returned
+	*/
+	ih = PATH_PITEM_HEAD(&path_to_sd) ;
+	inode->u.reiserfs_i.i_key[0] = ih->ih_key.k_dir_id ;
+    }
+
+    /* look for the object stat data */
+    if (search_by_key (inode->i_sb, INODE_PKEY (inode), &path_to_sd, &repeat, DISK_LEAF_NODE_LEVEL, READ_BLOCKS) == ITEM_NOT_FOUND) {
+	pathrelse (&path_to_sd);
+	make_bad_inode(inode) ;
+	return;
+    }
+
+    sd = B_N_STAT_DATA (PATH_PLAST_BUFFER (&path_to_sd), PATH_LAST_POSITION (&path_to_sd));
+    inode->i_mode = le16_to_cpu (sd->sd_mode);
+    inode->i_uid = le16_to_cpu (sd->sd_uid);
+    inode->i_gid = le16_to_cpu (sd->sd_gid);
+    inode->i_nlink = le16_to_cpu (sd->sd_nlink);
+    inode->i_size = le32_to_cpu (sd->sd_size);
+    inode->i_mtime = le32_to_cpu (sd->sd_mtime);
+    inode->i_atime = le32_to_cpu (sd->sd_atime);
+    inode->i_ctime = le32_to_cpu (sd->sd_ctime);
+    inode->i_blksize = inode->i_sb->s_blocksize;
+
+    // thanks to Dirk Mueller for fixing "du bug"
+    if (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode)) {
+	inode->i_rdev = le32_to_cpu (sd->u.sd_rdev);
+	inode->i_blocks = 0;
+    } else {
+	/* for regular files we calculate any tail as a whole block */
+	unsigned long blocks;
+
+	inode->i_rdev = 0;
+	inode->i_blocks = le32_to_cpu (sd->u.sd_blocks);
+	blocks = (inode->i_size + 511) >> 9;
+	blocks = ROUND_UP (blocks, inode->i_blksize >> 9);
+	if (inode->i_blocks > blocks) {
+	    // there was a bug in <=3.5.23 when i_blocks could take negative
+	    // values. Starting from 3.5.17 this value could even be stored in
+	    // stat data. For such files we set i_blocks based on file
+	    // size. Just 2 notes: this can be wrong for sparce files. On-disk value will be
+	    // only updated if file's inode will ever change
+	    inode->i_blocks = blocks;
+	}
+    }
+    inode->u.reiserfs_i.i_first_direct_byte = le32_to_cpu(sd->sd_first_direct_byte);
+    pathrelse (&path_to_sd);
+    
+    if (S_ISREG (inode->i_mode))
+	inode->i_op = &reiserfs_file_inode_operations;
+    else if (S_ISDIR (inode->i_mode)) {
+	inode->i_op = &reiserfs_dir_inode_operations;
+	inode->i_blocks = inode->i_size / 512 + ((inode->i_size % 512) ? 1 : 0);
+    } else if (S_ISLNK (inode->i_mode))
+	inode->i_op = &reiserfs_symlink_inode_operations;
+    else if (S_ISCHR (inode->i_mode))
+	inode->i_op = &chrdev_inode_operations;
+    else if (S_ISBLK (inode->i_mode))
+	inode->i_op = &blkdev_inode_operations;
+    else if (S_ISFIFO (inode->i_mode))
+	init_fifo (inode);
+    inode->u.reiserfs_i.i_pack_on_close = 0 ;
+
+    /* nopack = 0, by default */
+    inode->u.reiserfs_i.nopack = 0;
+}
+
+
+void store_key (struct super_block *s, struct key * key)
+{
+    int i;
+
+    for (i = 0; i < sizeof (g_key_array) / sizeof (g_key_array[0]); i ++) {
+	if (g_key_array[i].objectid == 0) {
+	    g_key_array[i].dirid = key->k_dir_id;
+	    g_key_array[i].objectid = key->k_objectid;
+	    g_key_array[i].dev = s->s_dev ;
+	    return;
+	}
+    }
+    reiserfs_warning ("vs-13042: store_key: table of keys is full\n");
+}
+
+void forget_key (struct super_block *s, struct key * key)
+{
+    int i;
+
+    for (i = 0; i < sizeof (g_key_array) / sizeof (g_key_array[0]); i ++) {
+	if (g_key_array[i].objectid == key->k_objectid &&
+	    g_key_array[i].dev == s->s_dev) {
+	    g_key_array[i].objectid = 0;
+	    return;
+	}
+    }
+    reiserfs_warning ("vs-13045: forget_key: could not find key in the table [%k]\n", key);
+}
+
+
+struct inode * reiserfs_iget (struct super_block * s, struct key * key)
+{
+    struct inode * inode;
+
+    store_key (s, key);
+    inode = iget (s, key->k_objectid);
+    if (!inode) {
+	forget_key (s, key);
+        return NULL ;
+    }
+    if (comp_short_keys (INODE_PKEY (inode), key)) {
+	reiserfs_warning ("vs-13048: reiserfs_iget: key in inode %k and key in entry %k do not match\n",
+			  INODE_PKEY (inode), key);
+	iput (inode);
+	inode = 0;
+    }
+    forget_key (s, key);
+    return inode;
+}
+
+
+// this actually updates stat data
+static struct buffer_head * reiserfs_update_inode (struct reiserfs_transaction_handle *th, struct inode * inode, 
+                  			           struct path * path_to_sd, int read_blocks)
+{
+    struct stat_data * sd;
+    int repeat;
+    struct buffer_head * bh;
+
+
+    init_path (path_to_sd);
+
+    /* look for the object */
+    if (search_by_key (inode->i_sb, INODE_PKEY (inode), path_to_sd, &repeat, DISK_LEAF_NODE_LEVEL, read_blocks) == ITEM_NOT_FOUND) {
+	if (read_blocks == DONT_READ_BLOCKS) {
+	    /* this is called from if_in_ram_update_sd */
+	    /*printk ("reiserfs: stat data not found in memory\n");*/
+	    return 0;
+	}
+	if (inode->i_nlink == 0) {
+#ifdef CONFIG_REISERFS_CHECK 
+	    printk ("vs-13050: reiserfs_update_inode: i_nlink == 0, stat data not found, (this condition can occur without being an error).\n");
+#endif
+	    return 0;
+	}
+	print_block (PATH_PLAST_BUFFER (path_to_sd), PRINT_LEAF_ITEMS, -1, -1);
+	reiserfs_panic(inode->i_sb, "vs-13060: reiserfs_update_inode: stat data of object %k (nlink == %d) not found (pos %d)\n", 
+		       INODE_PKEY (inode), inode->i_nlink, PATH_LAST_POSITION (path_to_sd));
+    }
+    bh = PATH_PLAST_BUFFER (path_to_sd);
+    sd = B_N_STAT_DATA (bh, PATH_LAST_POSITION (path_to_sd));
+    sd->sd_mode = cpu_to_le16 (inode->i_mode);
+    sd->sd_uid = cpu_to_le16 (inode->i_uid);
+    sd->sd_gid = cpu_to_le16 (inode->i_gid);
+    sd->sd_nlink = cpu_to_le16 (inode->i_nlink);
+    sd->sd_size = cpu_to_le32 (inode->i_size);
+    sd->sd_atime = cpu_to_le32 (inode->i_atime);
+    sd->sd_ctime = cpu_to_le32 (inode->i_ctime);
+    sd->sd_mtime = cpu_to_le32 (inode->i_mtime);
+    if (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode))
+	sd->u.sd_rdev = cpu_to_le32 (inode->i_rdev);
+    else sd->u.sd_blocks = cpu_to_le32 (inode->i_blocks);
+    sd->sd_first_direct_byte = cpu_to_le32 (inode->u.reiserfs_i.i_first_direct_byte);
+    /* reiserfs_mark_buffer_dirty (bh, 1); journal victim */
+    reiserfs_update_inode_transaction(inode) ;
+    journal_mark_dirty(th, inode->i_sb, bh);
+    return bh;
+}
+
+/* looks for stat data, then copies fields to it, marks the buffer
+   containing stat data as dirty */
+void reiserfs_write_inode (struct inode * inode)
+{
+    struct path path_to_sd;
+    int windex ;
+    struct reiserfs_transaction_handle th ;
+    int jbegin_count = JOURNAL_PER_BALANCE_CNT * 3; 
+
+    journal_begin(&th, inode->i_sb, jbegin_count) ;
+    windex = push_journal_writer("write_inode") ;
+    reiserfs_update_inode_transaction(inode) ;
+    reiserfs_update_inode (&th, inode, &path_to_sd, READ_BLOCKS);
+    pathrelse (&path_to_sd);
+    pop_journal_writer(windex) ;
+    journal_end(&th, inode->i_sb, jbegin_count) ;
+}
+
+
+int reiserfs_sync_inode (struct reiserfs_transaction_handle *th, struct inode * inode)
+{
+    int err = 0;
+    struct path path_to_sd;
+    struct buffer_head * bh;
+
+    bh = reiserfs_update_inode (th, inode, &path_to_sd, READ_BLOCKS);
+#if 0
+    if (bh && buffer_dirty (bh)) {
+	ll_rw_block(WRITE, 1, &bh);
+	wait_on_buffer(bh);
+	if (buffer_req(bh) && !buffer_uptodate(bh)) {
+	    printk ("reiserfs_sync_inode: IO error syncing reiserfs stat data ["
+		    "device:\"%s\", object:[%u %u]], blocknr:%lu\n",
+		    kdevname(inode->i_dev), inode->u.reiserfs_i.i_key[0], inode->u.reiserfs_i.i_key[1],
+		    bh->b_blocknr);
+	    err = -1;
+	}
+    } else {
+	if (bh == 0)
+	    err = -1;
+    }
+#endif
+    pathrelse (&path_to_sd);
+    return err;
+}
+
+
+/* stat data of object has been inserted, this inserts the item
+   containing "." and ".." entries */
+static int reiserfs_new_directory (struct reiserfs_transaction_handle *th, 
+			           struct super_block * sb, struct item_head * ih, struct path * path, const struct inode * dir)
+{
+    char empty_dir [EMPTY_DIR_SIZE];
+    struct reiserfs_de_head * deh;
+    char * body;
+    int repeat;
+	
+    /* item head of empty directory item */
+    ih->ih_key.k_offset = DOT_OFFSET;
+    ih->ih_key.k_uniqueness = DIRENTRY_UNIQUENESS;
+    ih->u.ih_entry_count = 2;
+    ih->ih_item_len = EMPTY_DIR_SIZE;
+
+    body = empty_dir;
+    deh = (struct reiserfs_de_head *)body;
+
+    deh[0].deh_location = ih->ih_item_len - strlen (".");
+    deh[0].deh_offset = DOT_OFFSET;
+    deh[0].deh_dir_id = ih->ih_key.k_dir_id;
+    deh[0].deh_objectid = ih->ih_key.k_objectid;
+    mark_de_without_sd (&(deh[0]));
+    mark_de_visible (&(deh[0]));
+    body[deh[0].deh_location] = '.';
+
+    deh[1].deh_location = deh[0].deh_location - strlen ("..");
+    deh[1].deh_offset = DOT_DOT_OFFSET;
+
+    /* objectid of ".." directory */
+    deh[1].deh_dir_id = INODE_PKEY (dir)->k_dir_id;
+    deh[1].deh_objectid = INODE_PKEY (dir)->k_objectid;
+    mark_de_without_sd (&(deh[1]));
+    mark_de_visible (&(deh[1]));
+    body[deh[1].deh_location] = '.';
+    body[deh[1].deh_location + 1] = '.';
+
+    /* look for place in the tree for new item */
+    if (search_by_key (sb, &ih->ih_key,  path, &repeat, DISK_LEAF_NODE_LEVEL, READ_BLOCKS) == ITEM_FOUND)
+	reiserfs_panic (sb, "vs-13070: reiserfs_new_directory: object with this key exists [%lu %lu %lu %lu]",
+			ih->ih_key.k_dir_id, ih->ih_key.k_objectid, ih->ih_key.k_offset, ih->ih_key.k_uniqueness);
+
+    /* insert item, that is empty directory item */
+    return reiserfs_insert_item (th, sb, path, ih, body, REISERFS_KERNEL_MEM, 0, NOTHING_SPECIAL);
+}
+
+
+/* stat data of object has been inserted, this inserts the item
+   containing the body of symlink */
+static int reiserfs_new_symlink (struct reiserfs_transaction_handle *th, 
+                                 struct super_block * sb, struct item_head * ih, struct path * path, const char * symname)
+{
+    int repeat;
+
+    /* item head of the body of symlink */
+    ih->ih_key.k_offset = 1;
+    ih->ih_key.k_uniqueness = TYPE_DIRECT;
+    ih->ih_item_len = strlen (symname);
+
+    /* look for place in the tree for new item */
+    if (search_by_key (sb, &ih->ih_key, path, &repeat, DISK_LEAF_NODE_LEVEL, READ_BLOCKS) == ITEM_FOUND)
+	reiserfs_panic (sb, "vs-13080: reiserfs_new_symlink: object with this key exists %k",
+			&(ih->ih_key));
+
+    /* insert item, that is body of symlink */
+    return reiserfs_insert_item (th, sb, path, ih, symname, REISERFS_KERNEL_MEM, 0, NOTHING_SPECIAL);
+}
+
+
+// used to copy inode's fields to stat data
+static void inode2sd_v1 (struct stat_data * sd, struct inode * inode)
+{
+    sd->sd_mode = cpu_to_le16 (inode->i_mode);
+    sd->sd_uid = cpu_to_le16 (inode->i_uid);
+    sd->sd_gid = cpu_to_le16 (inode->i_gid);
+    sd->sd_nlink = cpu_to_le16 (inode->i_nlink);
+    sd->sd_size = cpu_to_le32 (inode->i_size);
+    sd->sd_atime = cpu_to_le32 (inode->i_atime);
+    sd->sd_ctime = cpu_to_le32 (inode->i_ctime);
+    sd->sd_mtime = cpu_to_le32 (inode->i_mtime);
+    if (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode))
+	sd->u.sd_rdev = cpu_to_le32 (inode->i_rdev);
+    else
+	sd->u.sd_blocks = cpu_to_le32 (inode->i_blocks);
+
+    sd->sd_first_direct_byte = cpu_to_le32 (inode->u.reiserfs_i.i_first_direct_byte);
+}
+
+
+/* inserts the stat data into the tree, and then calls reiserfs_new_directory
+   (to insert ".", ".." item if new object is directory) or
+   reiserfs_new_symlink (to insert symlink body if new object is
+   symlink) or nothing (if new object is regular file) */
+struct inode * reiserfs_new_inode (struct reiserfs_transaction_handle *th,
+				   const struct inode * dir, int mode, const char * symname, 
+				   struct dentry *dentry, struct inode *inode, int * err)
+{
+    struct super_block * sb;
+    /* struct inode * inode; */
+    struct path path_to_key;
+    struct item_head ih;
+    struct stat_data sd;
+    int retvalue;
+    int repeat;
+
+    init_path (&path_to_key);
+
+    if (!dir || !dir->i_nlink) { 
+	iput(inode) ;
+	*err = -EPERM;
+	return NULL;
+    } 
+    sb = dir->i_sb;
+    inode->i_sb = sb;
+    inode->i_flags = inode->i_sb->s_flags;
+
+    ih.ih_key.k_dir_id = INODE_PKEY (dir)->k_objectid;
+    ih.ih_key.k_objectid = reiserfs_get_unused_objectid (th, dir->i_sb);
+    ih.ih_key.k_offset = SD_OFFSET;
+    ih.ih_key.k_uniqueness = TYPE_STAT_DATA;
+    ih.u.ih_free_space = MAX_US_INT;
+    ih.ih_item_len = SD_SIZE;
+
+    /* find proper place for inserting of stat data */
+    if (search_by_key (sb, &ih.ih_key, &path_to_key, &repeat, DISK_LEAF_NODE_LEVEL, READ_BLOCKS) == ITEM_FOUND) {
+	pathrelse (&path_to_key);
+	iput (inode);
+	*err = -EEXIST;
+	return 0;
+    }
+
+    // set inode's fields which are common for both stat data and
+    // in-code inode
+    inode->i_mode = mode;
+    inode->i_nlink = (S_ISDIR (mode) ? 2 : 1);
+    inode->i_uid = current->fsuid;
+    if (dir->i_mode & S_ISGID) {
+	inode->i_gid = dir->i_gid;
+	if (S_ISDIR(mode))
+	    inode->i_mode |= S_ISGID;
+    } else
+	inode->i_gid = current->fsgid;
+
+    inode->i_mtime = inode->i_atime = inode->i_ctime = CURRENT_TIME;
+    inode->i_size = (S_ISDIR (mode) ? EMPTY_DIR_SIZE : 0);
+    inode->u.reiserfs_i.i_first_direct_byte = S_ISLNK(mode) ? 1 : 
+      NO_BYTES_IN_DIRECT_ITEM;
+
+    // set up stat data
+    inode2sd_v1 (&sd, inode);
+    
+    /* insert the stat data into the tree */
+    retvalue = reiserfs_insert_item (th, sb, &path_to_key, &ih, (char *)(&sd), REISERFS_KERNEL_MEM, 0, NOTHING_SPECIAL);
+    if (retvalue == NO_DISK_SPACE) {
+	iput (inode);
+	*err = -ENOSPC;
+	return 0;
+    }
+
+    if (S_ISDIR(mode)) {
+	/* insert item with "." and ".." */
+	retvalue = reiserfs_new_directory (th, sb, &ih, &path_to_key, dir);
+    }
+
+    if (S_ISLNK(mode)) {
+	/* insert body of symlink */
+	retvalue = reiserfs_new_symlink (th, sb, &ih, &path_to_key, symname);
+    }
+    if (retvalue == NO_DISK_SPACE) {
+	/* we must delete stat data here */
+	memcpy (INODE_PKEY (inode), &(ih.ih_key), SHORT_KEY_SIZE);
+	iput (inode);
+	*err = -ENOSPC;
+	return 0;
+    }
+
+    inode->i_dev = sb->s_dev;
+    inode->i_ino = ih.ih_key.k_objectid;
+    inode->i_op = NULL;
+    // symlinks have i_blocks == 0
+    inode->i_blocks = (inode->i_size + 511) >> 9;
+    inode->i_blksize = sb->s_blocksize;
+
+    memcpy (INODE_PKEY (inode), &(ih.ih_key), SHORT_KEY_SIZE);
+    insert_inode_hash (inode);
+    /*  mark_inode_dirty (inode);*/
+    return inode;
+}
+
+
+extern spinlock_t inode_lock;
+
+                                /* This is the problem we are trying to solve: with reiserfs it is a bad thing to defer
+                                an update of stat data if the buffer holding the stat data is in RAM. If we defer it,
+                                then when we later do the update, the buffer might not still be in RAM.  Consider that
+                                inodes have a longer lifetime in RAM, and you can see that this is especially important.
+
+                                Unfortunately, sometimes we need to pass an inode, sometimes an iattr, sometimes just
+                                one field, to the function that updates the stat data.  Umpteen different functions that
+                                do essentially the same thing.  For now I implement just a straight replacement for
+                                mark_inode_dirty -Hans */
+                                /* This function is inefficient if only one field of the inode was changed. 
+                                 This function can cause schedule.  */
+void if_in_ram_update_sd (struct reiserfs_transaction_handle *th, struct inode * inode)
+{
+    struct path path_to_sd;
+
+    reiserfs_update_inode (th, inode, &path_to_sd, READ_BLOCKS);
+    pathrelse (&path_to_sd);
+    return ;
+
+#ifdef DIRTY_LATER
+    mark_inode_dirty(inode);
+    return;
+#else /* DIRTY_LATER */
+
+    struct path path_to_sd;
+    extern struct list_head inode_in_use;
+
+
+    init_path (&path_to_sd);
+  
+    if (!inode->i_nlink || (S_ISFIFO (inode->i_mode))) { 
+	mark_inode_dirty(inode);    /* let us deferr the update since in this case we always follow this with an iput which
+				       will do the update (that is to say, the removal) of the stat data */
+	return;
+    }
+
+#ifdef CONFIG_REISERFS_CHECK
+    if (inode->i_sb) {
+#endif /* CONFIG_REISERFS_CHECK */
+
+	/* wait until inode is not locked and then lock it */
+	while ((inode->i_state & I_LOCK)) {
+	    __wait_on_inode(inode);
+	}
+	inode->i_state |=  I_LOCK;
+
+	if (reiserfs_update_inode (th, inode, &path_to_sd, DONT_READ_BLOCKS)) {
+	    spin_lock(&inode_lock);   /* locks inode lists (not the inode) */
+	    if ((inode->i_state & I_DIRTY)) {
+		struct list_head *insert = &inode_in_use;
+
+		/* mark inode clean and take it off dirty list */
+		inode->i_state &= ~I_DIRTY;
+#ifdef CONFIG_REISERFS_CHECK
+		/* Only add valid (ie hashed) inodes to the in_use list */
+		if (!list_empty(&inode->i_hash)) {
+#endif /* CONFIG_REISERFS_CHECK */
+		    list_del(&inode->i_list);
+		    list_add(&inode->i_list, insert);
+#ifdef CONFIG_REISERFS_CHECK
+		} 
+		else 
+		    printk("reiser-1805: if_in_ram_update_sd: a dirty inode was not on any inode list\n");
+#endif /* CONFIG_REISERFS_CHECK */
+	    }
+	    spin_unlock(&inode_lock);
+	    /*mark_buffer_dirty (bh, 1);*/
+	} else {   /* stat data item for this inode no longer in RAM */
+	    /* if stat data was not found then it must be because it is no longer in RAM, so just mark it
+	       dirty for now, and let somebody else write it */
+	    mark_inode_dirty(inode); 
+	}  
+	inode->i_state &= ~I_LOCK;
+	wake_up(&inode->i_wait);
+	pathrelse(&path_to_sd);
+#ifdef CONFIG_REISERFS_CHECK
+    }
+    else
+	printk("reiser-1804: if_in_ram_update_sd: !sb, should not happen\n");
+#endif /* CONFIG_REISERFS_CHECK */
+
+#endif /* ! DIRTY_LATER */
+}
+
+
+#ifdef __KERNEL__
+/* this generates little if any speedup compared to if_in_ram_update_sd(), might not be worthwhile
+   code, probably search_by_key dominates cpu consumption */
+void if_in_ram_update_some_sd (struct reiserfs_transaction_handle *th, struct inode * inode,  struct iattr * attr)
+{
+    struct stat_data * sd;
+    struct path path_to_sd;
+    int repeat;
+    struct buffer_head * bh;
+    extern struct list_head inode_in_use;
+    unsigned int ia_valid = attr->ia_valid;
+
+    init_path (&path_to_sd);
+
+    if (!inode->i_nlink || (S_ISFIFO (inode->i_mode))) { /* Vladimir, what do you think, is the S_FIFO needed here?-Hans */
+	mark_inode_dirty(inode);    /* let us deferr the update since in this case we always follow this with an iput which
+				       will do the update (that is to say, the removal) of the stat data */
+	return;
+    }
+
+#ifdef CONFIG_REISERFS_CHECK
+    if (inode->i_sb) {
+#endif /* CONFIG_REISERFS_CHECK */
+
+                                /* wait until inode is not locked and then lock it */
+	while ((inode->i_state & I_LOCK)) {
+	    __wait_on_inode(inode);
+	}
+	inode->i_state |=  I_LOCK;  /* depends on interrupts never locking inodes, I suppose it is correct.... */
+
+	/* look for the stat data item, if we find it in RAM, sync to it now, else put the inode on the list of dirty inodes
+	   to be synced someday using mark_inode_dirty and perhaps the sync will happen when the buffer is back in RAM or
+	   after a bunch of other changes to it have been successfully cached by deferring the I/O */
+	if (search_by_key (inode->i_sb, INODE_PKEY (inode), &path_to_sd, &repeat, DISK_LEAF_NODE_LEVEL, DONT_READ_BLOCKS) == ITEM_FOUND) {
+                                /* all of these branches are very inefficient, but the search_by_key is probably the
+                                   worst of it */
+	    bh = PATH_PLAST_BUFFER (&path_to_sd);
+	    sd = B_N_STAT_DATA (bh, PATH_LAST_POSITION (&path_to_sd));
+	    if (ia_valid & ATTR_MODE)
+		sd->sd_mode = cpu_to_le16 (inode->i_mode);
+	    if (ia_valid & ATTR_UID)
+		sd->sd_uid = cpu_to_le16 (inode->i_uid);
+	    if (ia_valid & ATTR_GID)
+		sd->sd_gid = cpu_to_le16 (inode->i_gid);
+	    if (ia_valid & ATTR_SIZE)
+		sd->sd_size = cpu_to_le32 (inode->i_size);
+	    if (ia_valid & ATTR_MTIME)
+		sd->sd_mtime = cpu_to_le32 (inode->i_mtime);
+	    if (ia_valid & ATTR_CTIME)
+		sd->sd_ctime = cpu_to_le32 (inode->i_ctime);
+	    spin_lock(&inode_lock);   /* locks inode lists (not the inode) */
+	    if ((inode->i_state & I_DIRTY)) {
+		struct list_head *insert = &inode_in_use;
+
+		/*      mark inode clean and take it off dirty list */
+		inode->i_state &= ~I_DIRTY;
+#ifdef CONFIG_REISERFS_CHECK
+		/* Only add valid (ie hashed) inodes to the in_use list */
+		if (!list_empty(&inode->i_hash)) {
+#endif /* CONFIG_REISERFS_CHECK */
+		    list_del(&inode->i_list);
+		    list_add(&inode->i_list, insert);
+#ifdef CONFIG_REISERFS_CHECK
+		}
+		else 
+		    printk("reiser-1806: if_in_ram_update_some_sd: a dirty inode was not on any inode list, maybe a pipe?");
+#endif /* CONFIG_REISERFS_CHECK */
+	    }
+	    spin_unlock(&inode_lock);
+	    /* mark_buffer_dirty (bh, 1); journal victim */
+	    journal_mark_dirty (th, inode->i_sb, bh);
+	} else {   /* stat data item for this inode no longer in RAM */
+	    inode->i_state &= ~I_LOCK; /* probably should combine two lines that unlock inode by postponing the unlock.... */
+                                /* if inode was not found then it must be because it is no longer in RAM, so just
+                                   mark it dirty for now, and let somebody else write it */
+	    mark_inode_dirty(inode); 
+	}  
+	inode->i_state &= ~I_LOCK;
+	wake_up(&inode->i_wait);
+	pathrelse(&path_to_sd);
+#ifdef CONFIG_REISERFS_CHECK
+    }
+    else
+	printk("reiser-1807: if_in_ram_update_some_sd: !sb, should not happen");
+#endif /* CONFIG_REISERFS_CHECK */
+}
+
+
+void inline reiserfs_inode_setattr(struct reiserfs_transaction_handle *th, struct inode * inode, struct iattr * attr)
+{
+    unsigned int ia_valid = attr->ia_valid;
+    /* struct path path_to_sd; */
+
+    if (ia_valid & ATTR_UID)
+	inode->i_uid = attr->ia_uid;
+    if (ia_valid & ATTR_GID)
+	inode->i_gid = attr->ia_gid;
+    if (ia_valid & ATTR_SIZE)
+	inode->i_size = attr->ia_size;
+    if (ia_valid & ATTR_ATIME)
+	inode->i_atime = attr->ia_atime;
+    if (ia_valid & ATTR_MTIME)
+	inode->i_mtime = attr->ia_mtime;
+    if (ia_valid & ATTR_CTIME)
+	inode->i_ctime = attr->ia_ctime;
+    if (ia_valid & ATTR_MODE) {
+	inode->i_mode = attr->ia_mode;
+	if (!in_group_p(inode->i_gid) && !capable(CAP_FSETID))
+	    inode->i_mode &= ~S_ISGID;
+    }
+#ifdef DIRTY_LATER
+    mark_inode_dirty(inode) ;  
+    /* reiserfs_update_inode (inode, &path_to_sd, READ_BLOCKS); 
+       pathrelse (&path_to_sd); */
+#else /* note, this code is broken and MUST not be used */
+    if_in_ram_update_some_sd (th, inode, attr);
+#endif
+}
+
+
+int reiserfs_notify_change(struct dentry * dentry, struct iattr * attr)
+{
+    struct inode *inode = dentry->d_inode;
+    struct reiserfs_transaction_handle th ;
+    int error;
+
+    /* I'm cheating here.  reiserfs_inode_setattr does not make journal calls with
+    ** dirty later turned on.  Dirty later must be on for now, so I'm not doing
+    ** a journal_begin here.
+    */
+    th.t_trans_id = 0 ; 
+    error = inode_change_ok(inode, attr);
+    if (!error)
+	reiserfs_inode_setattr(&th, inode, attr);
+    return error;
+}
+
+/* I believe that further optimizing this code is best done by optimizing search_by_key.. */
+
+#endif /* __KERNEL__ */
diff -urN linux/fs/reiserfs/ioctl.c /tmp/linux/fs/reiserfs/ioctl.c
--- linux/fs/reiserfs/ioctl.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/ioctl.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,129 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+#ifdef __KERNEL__
+
+#include <linux/fs.h>
+#include <linux/reiserfs_fs.h>
+#include <linux/sched.h>
+#include <asm/uaccess.h>
+#include <linux/smp_lock.h>
+
+#else
+
+#include "nokernel.h"
+
+#endif
+
+
+/*
+** reiserfs_ioctl - handler for ioctl for inode
+** supported commands:
+**  1) REISERFS_IOC_UNPACK - try to unpack tail from direct item into indirect
+**                           and prevent packing file (argument arg has to be non-zero)
+**  2) That's all for a while ...
+*/
+int reiserfs_ioctl (struct inode * inode, struct file * filp, unsigned int cmd,
+		unsigned long arg)
+{
+	switch (cmd) {
+	    case REISERFS_IOC_UNPACK:
+		if (arg)
+		    return reiserfs_unpack (inode, filp);
+			
+	    default:
+		return -ENOTTY;
+	}
+}
+
+
+/*
+** reiserfs_unpack
+** Function try to convert tail from direct item into indirect.
+** It set up nopack attribute in the inode.u.reiserfs_i.nopack
+*/
+int reiserfs_unpack (struct inode * inode, struct file * filp)
+{
+    int retval, windex, repeat, exitcode;
+    struct path path;
+    struct key key;
+    struct buffer_head * bh, * unbh = 0;
+    struct item_head * ih;
+    int jbegin_count = JOURNAL_PER_BALANCE_CNT * 3 ;
+    struct reiserfs_transaction_handle th ;
+
+    lock_kernel();
+    init_path (&path);
+    unbh = NULL;
+    exitcode = 0;
+    
+    down(&inode->i_sem) ;
+    journal_begin(&th, inode->i_sb, jbegin_count) ;
+    windex = push_journal_writer("reiserfs_ioctl_unpack") ;
+    
+search_again:
+    /* find file's tail */    
+    copy_key(&key, INODE_PKEY (inode));    
+    key.k_offset =  (inode->i_size & (~(inode->i_sb->s_blocksize - 1))) + 1;
+    key.k_uniqueness = TYPE_DIRECT;
+    retval = search_by_key (inode->i_sb, &key, &path, &repeat,
+			    DISK_LEAF_NODE_LEVEL, READ_BLOCKS);
+		    
+    if (retval == ITEM_NOT_FOUND) {
+	if (unbh) {
+	    /* something happened with file while allocating unformatted node */
+	    reiserfs_free_block (&th, inode->i_sb, unbh->b_blocknr);
+	    bforget (unbh);
+	}
+	/* don't pack tail until next open */
+	inode->u.reiserfs_i.nopack = 1;
+    } else {
+	/* tail found */
+	ih = B_N_PITEM_HEAD(bh = PATH_PLAST_BUFFER(&path),
+			PATH_LAST_POSITION(&path));
+	/* allocate new unformatted node to place tail */
+	if (unbh == NULL) {
+	    retval = get_new_buffer(&th, inode->i_sb, bh, &unbh, &path, inode, 
+	                          (inode->i_size & (~(inode->i_sb->s_blocksize - 1))) + 1);
+	    if (!unbh) {
+		/* can't allocate block */
+		pathrelse (&path);
+		exitcode = -ENOSPC;
+		goto finish;
+	    }
+	    if (retval & SCHEDULE_OCCURRED) {
+		/* get_new_buffer was blocked and path had ability to change */
+		pathrelse (&path);
+		goto search_again;
+	    }
+	}
+	mark_buffer_uptodate (unbh, 1);
+	/* try to convert direct item into indirect */
+	retval = direct_to_indirect (&th, inode->i_sb, inode, &path, 
+				     0 /* n_item_zeros_to_add */,
+				     NULL /* p_c_buf */, 
+				     inode->i_size & (inode->i_sb->s_blocksize - 1) /* tail's size */,
+				     unbh);
+	if (retval < 0) {
+	    /* direct_to_indirect() did not convert item */
+	    reiserfs_free_block (&th, inode->i_sb, unbh->b_blocknr);
+	    bforget (unbh);
+	    inode->u.reiserfs_i.nopack = 0;
+	    exitcode = -ENOSPC; /* FIXME: what error has to be returned here? -az */
+	    goto finish;
+	}
+	/* direct_to_indirect brelses the unformatted node for us */
+	pathrelse (&path);
+	/* don't pack tail until next open */
+	inode->u.reiserfs_i.nopack = 1;
+    }
+    
+finish:
+    pop_journal_writer(windex);
+    pathrelse(&path) ;
+    journal_end(&th, inode->i_sb, jbegin_count);
+    up(&inode->i_sem) ;
+    unlock_kernel();    
+    return exitcode;
+}
diff -urN linux/fs/reiserfs/journal.c /tmp/linux/fs/reiserfs/journal.c
--- linux/fs/reiserfs/journal.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/journal.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,3006 @@
+/*
+** The background commits make this code very interelated, and overly complex.  I need to 
+** rethink things a bit....The major players:
+**
+** journal_begin -- call with the number of blocks you expect to log.  If the current transaction is too
+** 		    old, it will block until the current transaction is finished, and then start a new one.
+**		    Usually, your transaction will get joined in with previous ones for speed.
+**
+** journal_join  -- same as journal_begin, but won't block on the current transaction regardless of age.  Don't ever call
+**                  this.  Ever.  There are only two places it should be called from, and they are both inside this file.
+**
+** journal_mark_dirty_nolog -- if the block is in the current transaction or might get overwritten by a log replay, 
+**                             calls journal_mark_dirty.  Otherwise, just mark it dirty and move on.
+**
+** journal_mark_dirty -- adds blocks into this transaction.  clears any flags that might make them get sent to disk
+**                       and then marks them BH_JDirty.  Puts the buffer head into the current transaction hash.  If
+**		         it was already in the hash, the old one is removed first to keep the replay order correct.
+**
+** journal_end -- if the current transaction is batchable, it does nothing
+**                   otherwise, it could do an async/synchronous commit, or
+**                   a full flush of all log and real blocks in the 
+**                   transaction.
+**
+** flush_old_commits -- if the current transaction is too old, it is ended and commit blocks are sent to disk.  
+**			Forces commit blocks to disk for all backgrounded commits that have been around too long.
+**		     -- Note, if you call this as an immediate flush from 
+**		        from within kupdate, it will ignore the immediate flag
+**
+** The commit thread -- a writer process for async commits.  It allows a 
+**                      a process to request a log flush on a task queue.
+**                      the commit will happen once the commit thread wakes up.
+**                      The benefit here is the writer (with whatever
+**                      related locks it has) doesn't have to wait for the
+**                      log blocks to hit disk if it doesn't want to.
+*/
+
+#ifdef __KERNEL__
+
+#include <asm/uaccess.h>
+#include <asm/system.h>
+
+#include <linux/sched.h>
+
+#include <linux/vmalloc.h>
+#include <linux/reiserfs_fs.h>
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/fcntl.h>
+#include <linux/locks.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+#include <linux/smp_lock.h>
+
+#else
+
+#include "nokernel.h"
+
+#endif
+
+#define JOURNAL_MBUF_COUNT 128 	/* number of buffer heads for logs block to keep in ram */
+#define JOURNAL_TRANS_HALF 1018   /* must be correct to keep the desc and commit structs at 4k */
+
+/* cnode stat bits.  Move these into reiserfs_fs.h */
+
+#define BLOCK_FREED 2		/* this block was freed, and can't be written.  */
+#define BLOCK_FREED_HOLDER 3    /* this block was freed during this transaction, and can't be written */
+
+#define BLOCK_NEEDS_FLUSH 4	/* used in flush_journal list */
+
+/* flags for do_journal_end */
+#define FLUSH_ALL   1		/* flush commit and real blocks */
+#define COMMIT_NOW  2		/* end and commit this transaction */
+#define WAIT        4		/* wait for the log blocks to hit the disk*/
+
+static int do_journal_end(struct reiserfs_transaction_handle *,struct super_block *,unsigned long nblocks,int flags) ;
+static int flush_journal_list(struct super_block *s, struct reiserfs_journal_list *jl, int old_only,int flushall) ;
+static int flush_commit_list(struct super_block *s, struct reiserfs_journal_list *jl, int flushall)  ;
+static int can_dirty(struct reiserfs_journal_cnode *cn) ;
+
+#if 0 /* see this in reiserfs_fs.h */
+
+#define JOURNAL_TRANS_HALF 1018   /* must be correct to keep the desc and commit structs at 4k */
+
+/* first block written in a commit.  BUG, not 64bit safe */
+struct reiserfs_journal_desc {
+  unsigned long j_trans_id ;			/* id of commit */
+  unsigned long j_len ;			/* length of commit. len +1 is the commit block */
+  unsigned long j_mount_id ;				/* mount id of this trans*/
+  unsigned long j_realblock[JOURNAL_TRANS_HALF] ; /* real locations for each block */
+  char j_magic[12] ;
+} ;
+
+/* last block written in a commit BUG, not 64bit safe */
+struct reiserfs_journal_commit {
+  unsigned long j_trans_id ;			/* must match j_trans_id from the desc block */
+  unsigned long j_len ;			/* ditto */
+  unsigned long j_realblock[JOURNAL_TRANS_HALF] ; /* real locations for each block */
+  char j_digest[16] ;			/* md5 sum of all the blocks involved, including desc and commit. not used, kill it */
+} ;
+
+/* this header block gets written whenever a transaction is considered fully flushed, and is more recent than the
+** last fully flushed transaction.  fully flushed means all the log blocks and all the real blocks are on disk,
+** and this transaction does not need to be replayed.
+*/
+struct reiserfs_journal_header {
+  unsigned long j_last_flush_trans_id ;		/* id of last fully flushed transaction */
+  unsigned long j_first_unflushed_offset ;      /* offset in the log of where to start replay after a crash */
+  unsigned long j_mount_id ;
+} ;
+#endif /* see this in reiserfs_fs.h */
+
+static void init_journal_hash(struct super_block *p_s_sb) {
+  memset(SB_JOURNAL(p_s_sb)->j_hash_table, 0, JOURNAL_HASH_SIZE * sizeof(struct reiserfs_journal_cnode *)) ;
+}
+
+/*
+** clears BH_Dirty and sticks the buffer on the clean list.  Called because I can't allow refile_buffer to
+** make schedule happen after I've freed a block.  Look at remove_from_transaction and journal_mark_freed for
+** more details.
+*/
+static int reiserfs_clean_and_file_buffer(struct buffer_head *bh) {
+  if (bh) {
+    clear_bit(BH_Dirty, &bh->b_state) ;
+    if (bh->b_list != BUF_CLEAN) {
+      reiserfs_file_buffer(bh, BUF_CLEAN) ;
+    }
+  }
+  return 0 ;
+}
+
+static struct reiserfs_bitmap_node *
+allocate_bitmap_node(struct super_block *p_s_sb) {
+  struct reiserfs_bitmap_node *bn ;
+  static int id = 0 ;
+
+  bn = kmalloc(sizeof(struct reiserfs_bitmap_node), GFP_BUFFER) ;
+  if (!bn) {
+    return NULL ;
+  }
+  bn->data = kmalloc(p_s_sb->s_blocksize, GFP_BUFFER) ;
+  if (!bn->data) {
+    kfree(bn) ;
+    return NULL ;
+  }
+  bn->id = id++ ;
+  memset(bn->data, 0, p_s_sb->s_blocksize) ;
+  INIT_LIST_HEAD(&bn->list) ;
+  return bn ;
+}
+
+static struct reiserfs_bitmap_node *
+get_bitmap_node(struct super_block *p_s_sb) {
+  struct reiserfs_bitmap_node *bn = NULL;
+  struct list_head *entry = SB_JOURNAL(p_s_sb)->j_bitmap_nodes.next ;
+
+  SB_JOURNAL(p_s_sb)->j_used_bitmap_nodes++ ;
+repeat:
+
+  if(entry != &SB_JOURNAL(p_s_sb)->j_bitmap_nodes) {
+    bn = list_entry(entry, struct reiserfs_bitmap_node, list) ;
+    list_del(entry) ;
+    memset(bn->data, 0, p_s_sb->s_blocksize) ;
+    SB_JOURNAL(p_s_sb)->j_free_bitmap_nodes-- ;
+    return bn ;
+  }
+  bn = allocate_bitmap_node(p_s_sb) ;
+  if (!bn) {
+    current->policy = SCHED_YIELD ;
+    schedule() ;
+    goto repeat ;
+  }
+  return bn ;
+}
+static inline void free_bitmap_node(struct super_block *p_s_sb,
+                                    struct reiserfs_bitmap_node *bn) {
+  SB_JOURNAL(p_s_sb)->j_used_bitmap_nodes-- ;
+  if (SB_JOURNAL(p_s_sb)->j_free_bitmap_nodes > REISERFS_MAX_BITMAP_NODES) {
+    kfree(bn->data) ;
+    kfree(bn) ;
+  } else {
+    list_add(&bn->list, &SB_JOURNAL(p_s_sb)->j_bitmap_nodes) ;
+    SB_JOURNAL(p_s_sb)->j_free_bitmap_nodes++ ;
+  }
+}
+
+static void allocate_bitmap_nodes(struct super_block *p_s_sb) {
+  int i ;
+  struct reiserfs_bitmap_node *bn = NULL ;
+  for (i = 0 ; i < REISERFS_MIN_BITMAP_NODES ; i++) {
+    bn = allocate_bitmap_node(p_s_sb) ;
+    if (bn) {
+      list_add(&bn->list, &SB_JOURNAL(p_s_sb)->j_bitmap_nodes) ;
+      SB_JOURNAL(p_s_sb)->j_free_bitmap_nodes++ ;
+    } else {
+      break ; // this is ok, we'll try again when more are needed 
+    }
+  }
+}
+
+static int set_bit_in_list_bitmap(struct super_block *p_s_sb, int block,
+                                  struct reiserfs_list_bitmap *jb) {
+  int bmap_nr = block / (p_s_sb->s_blocksize << 3) ;
+  int bit_nr = block % (p_s_sb->s_blocksize << 3) ;
+
+  if (!jb->bitmaps[bmap_nr]) {
+    jb->bitmaps[bmap_nr] = get_bitmap_node(p_s_sb) ;
+  }
+  set_bit(bit_nr, jb->bitmaps[bmap_nr]->data) ;
+  return 0 ;
+}
+static void cleanup_bitmap_list(struct super_block *p_s_sb,
+                                struct reiserfs_list_bitmap *jb) {
+  int i;
+  for (i = 0 ; i < SB_BMAP_NR(p_s_sb) ; i++) {
+    if (jb->bitmaps[i]) {
+      free_bitmap_node(p_s_sb, jb->bitmaps[i]) ;
+      jb->bitmaps[i] = NULL ;
+    }
+  }
+}
+
+/*
+** only call this on FS unmount.
+*/
+static int free_list_bitmaps(struct super_block *p_s_sb,
+                             struct reiserfs_list_bitmap *jb_array) {
+  int i ;
+  struct reiserfs_list_bitmap *jb ;
+  for (i = 0 ; i < JOURNAL_NUM_BITMAPS ; i++) {
+    jb = jb_array + i ;
+    jb->journal_list = NULL ;
+    cleanup_bitmap_list(p_s_sb, jb) ;
+    vfree(jb->bitmaps) ;
+    jb->bitmaps = NULL ;
+  }
+  return 0;
+}
+
+static int free_bitmap_nodes(struct super_block *p_s_sb) {
+  struct list_head *next = SB_JOURNAL(p_s_sb)->j_bitmap_nodes.next ;
+  struct reiserfs_bitmap_node *bn ;
+
+  while(next != &SB_JOURNAL(p_s_sb)->j_bitmap_nodes) {
+    bn = list_entry(next, struct reiserfs_bitmap_node, list) ;
+    list_del(next) ;
+    kfree(bn->data) ;
+    kfree(bn) ;
+    next = SB_JOURNAL(p_s_sb)->j_bitmap_nodes.next ;
+    SB_JOURNAL(p_s_sb)->j_free_bitmap_nodes-- ;
+  }
+
+  return 0 ;
+}
+
+/*
+** get memory for JOURNAL_NUM_BITMAPS worth of bitmaps. 
+** jb_array is the array to be filled in.
+*/
+int reiserfs_allocate_list_bitmaps(struct super_block *p_s_sb,
+                                   struct reiserfs_list_bitmap *jb_array,
+				   int bmap_nr) {
+  int i ;
+  int failed = 0 ;
+  struct reiserfs_list_bitmap *jb ;
+  int mem = bmap_nr * sizeof(struct reiserfs_bitmap_node *) ;
+
+  for (i = 0 ; i < JOURNAL_NUM_BITMAPS ; i++) {
+    jb = jb_array + i ;
+    jb->journal_list = NULL ;
+    jb->bitmaps = vmalloc( mem ) ;
+    if (!jb->bitmaps) {
+      reiserfs_warning("clm-2000, unable to allocate bitmaps for journal lists\n") ;
+      failed = 1;   
+      break ;
+    }
+    memset(jb->bitmaps, 0, mem) ;
+  }
+  if (failed) {
+    free_list_bitmaps(p_s_sb, jb_array) ;
+    return -1 ;
+  }
+  return 0 ;
+}
+
+/*
+** find an available list bitmap.  If you can't find one, flush a commit list and try again
+*/
+static struct reiserfs_list_bitmap *get_list_bitmap(struct super_block *p_s_sb, struct reiserfs_journal_list *jl) {
+  int i,j ; 
+  struct reiserfs_list_bitmap *jb = NULL ;
+
+  for (j = 0 ; j < (JOURNAL_NUM_BITMAPS * 3) ; j++) {
+    i = SB_JOURNAL(p_s_sb)->j_list_bitmap_index ;
+    SB_JOURNAL(p_s_sb)->j_list_bitmap_index = (i + 1) % JOURNAL_NUM_BITMAPS ;
+    jb = SB_JOURNAL(p_s_sb)->j_list_bitmap + i ;
+    if (SB_JOURNAL(p_s_sb)->j_list_bitmap[i].journal_list) {
+      flush_commit_list(p_s_sb, SB_JOURNAL(p_s_sb)->j_list_bitmap[i].journal_list, 1) ;
+      if (!SB_JOURNAL(p_s_sb)->j_list_bitmap[i].journal_list) {
+	break ;
+      }
+    } else {
+      break ;
+    }
+  }
+  if (jb->journal_list) { /* double check to make sure if flushed correctly */
+    return NULL ;
+  }
+  jb->journal_list = jl ;
+  return jb ;
+}
+
+/* 
+** allocates a new chunk of X nodes, and links them all together as a list.
+** Uses the cnode->next and cnode->prev pointers
+** returns NULL on failure
+*/
+static struct reiserfs_journal_cnode *allocate_cnodes(int num_cnodes) {
+  struct reiserfs_journal_cnode *head ;
+  int i ;
+  if (num_cnodes <= 0) {
+    return NULL ;
+  }
+  head = vmalloc(num_cnodes * sizeof(struct reiserfs_journal_cnode)) ;
+  if (!head) {
+    return NULL ;
+  }
+  memset(head, 0, num_cnodes * sizeof(struct reiserfs_journal_cnode)) ;
+  head[0].prev = NULL ;
+  head[0].next = head + 1 ;
+  for (i = 1 ; i < num_cnodes; i++) {
+    head[i].prev = head + (i - 1) ;
+    head[i].next = head + (i + 1) ; /* if last one, overwrite it after the if */
+  }
+  head[num_cnodes -1].next = NULL ;
+  return head ;
+}
+
+/*
+** pulls a cnode off the free list, or returns NULL on failure 
+*/
+static struct reiserfs_journal_cnode *get_cnode(struct super_block *p_s_sb) {
+  struct reiserfs_journal_cnode *cn ;
+  if (SB_JOURNAL(p_s_sb)->j_cnode_free <= 0) {
+    return NULL ;
+  }
+  SB_JOURNAL(p_s_sb)->j_cnode_used++ ;
+  SB_JOURNAL(p_s_sb)->j_cnode_free-- ;
+  cn = SB_JOURNAL(p_s_sb)->j_cnode_free_list ;
+  if (!cn) {
+    return cn ;
+  }
+  if (cn->next) {
+    cn->next->prev = NULL ;
+  }
+  SB_JOURNAL(p_s_sb)->j_cnode_free_list = cn->next ;
+  memset(cn, 0, sizeof(struct reiserfs_journal_cnode)) ;
+  return cn ;
+}
+
+/*
+** returns a cnode to the free list 
+*/
+static void free_cnode(struct super_block *p_s_sb, struct reiserfs_journal_cnode *cn) {
+  SB_JOURNAL(p_s_sb)->j_cnode_used-- ;
+  SB_JOURNAL(p_s_sb)->j_cnode_free++ ;
+  /* memset(cn, 0, sizeof(struct reiserfs_journal_cnode)) ; */
+  cn->next = SB_JOURNAL(p_s_sb)->j_cnode_free_list ;
+  if (SB_JOURNAL(p_s_sb)->j_cnode_free_list) {
+    SB_JOURNAL(p_s_sb)->j_cnode_free_list->prev = cn ;
+  }
+  cn->prev = NULL ; /* not needed with the memset, but I might kill the memset, and forget to do this */
+  SB_JOURNAL(p_s_sb)->j_cnode_free_list = cn ;
+}
+
+
+/* buffer is in current transaction */
+inline int buffer_journaled(struct buffer_head *bh) {
+  if (bh)
+    return test_bit(BH_JDirty, &bh->b_state) ;
+  else
+    return 0 ;
+}
+
+/* disk block was taken off free list before being in a finished transation, or written to disk
+** journal_new blocks can be reused immediately, for any purpose
+*/ 
+inline int buffer_journal_new(struct buffer_head *bh) {
+  if (bh) 
+    return test_bit(BH_JNew, &bh->b_state) ;
+  else
+    return 0 ;
+}
+
+inline int mark_buffer_journal_new(struct buffer_head *bh) {
+  if (bh) {
+    set_bit(BH_JNew, &bh->b_state) ;
+  }
+  return 0 ;
+}
+
+inline int mark_buffer_not_journaled(struct buffer_head *bh) {
+  if (bh) 
+    clear_bit(BH_JDirty, &bh->b_state) ;
+  return 0 ;
+}
+
+/* return a cnode with same dev, block number and size in table, or null if not found */
+static inline struct reiserfs_journal_cnode *get_journal_hash_dev(struct reiserfs_journal_cnode **table,
+                                   				  kdev_t dev,long bl,int size) {
+  struct reiserfs_journal_cnode *cn ;
+  cn = journal_hash(table, dev, bl) ;
+  while(cn) {
+    if ((cn->blocknr == bl) && (cn->dev == dev))
+      return cn ;
+    cn = cn->hnext ;
+  }
+  return (struct reiserfs_journal_cnode *)0 ;
+}
+
+/* returns a cnode with same size, block number and dev as bh in the current transaction hash.  NULL if not found */
+static inline struct reiserfs_journal_cnode *get_journal_hash(struct super_block *p_s_sb, struct buffer_head *bh) {
+  struct reiserfs_journal_cnode *cn ;
+  if (bh) {
+    cn =  get_journal_hash_dev(SB_JOURNAL(p_s_sb)->j_hash_table, bh->b_dev, bh->b_blocknr, bh->b_size) ;
+  }
+  else {
+    return (struct reiserfs_journal_cnode *)0 ;
+  }
+  return cn ;
+}
+
+/*
+** once upon a time, the journal would deadlock.  a lot.  Now, which CONFIG_REISERFS_CHECK, anytime someone enters a
+** transaction, it pushes itself into this ugly static list, and pops itself off before calling journal_end.
+** I made a SysRq key to dump the list, and tell me what the writers are when I'm deadlocked.
+*/
+static char *journal_writers[512] ;
+int push_journal_writer(char *s) {
+#ifdef CONFIG_REISERFS_CHECK
+  int i ;
+  for (i = 0 ; i < 512 ; i++) {
+    if (!journal_writers[i]) {
+      journal_writers[i] = s ;
+      return i ;
+    }
+  }
+  return -1 ;
+#else
+  return 0 ;
+#endif
+}
+int pop_journal_writer(int index) {
+#ifdef CONFIG_REISERFS_CHECK
+  if (index >= 0) {
+    journal_writers[index] = NULL ;
+  }
+#endif
+  return 0 ;
+}
+int dump_journal_writers(void) {
+  int i ;
+  for (i = 0 ; i < 512 ; i++) {
+    if (journal_writers[i]) {
+      printk("%d: %s\n", i, journal_writers[i]) ;
+    }
+  }
+  return 0 ;
+}
+
+/*
+** this actually means 'can this block be reallocated yet?'.  If you set search_all, a block can only be allocated
+** if it is not in the current transaction, was not freed by the current transaction, and has no chance of ever
+** being overwritten by a replay after crashing.
+**
+** If you don't set search_all, a block can only be allocated if it is not in the current transaction.  Since deleting
+** a block removes it from the current transaction, this case should never happen.  If you don't set search_all, make
+** sure you never write the block without logging it.
+**
+** next_zero_bit is a suggestion about the next block to try for find_forward.
+** when bl is rejected because it is set in a journal list bitmap, we search
+** for the next zero bit in the bitmap that rejected bl.  Then, we return that
+** through next_zero_bit for find_forward to try.
+**
+** Just because we return something in next_zero_bit does not mean we won't
+** reject it on the next call to reiserfs_in_journal
+**
+*/
+int reiserfs_in_journal(struct super_block *p_s_sb, kdev_t dev, unsigned long bl, int size, int search_all, unsigned long *next_zero_bit) {
+  struct reiserfs_journal_cnode *cn ;
+  struct reiserfs_list_bitmap *jb ;
+  int i ;
+  int bmap_nr = bl / (p_s_sb->s_blocksize << 3) ;
+  int bit_nr = bl % (p_s_sb->s_blocksize << 3) ;
+  int tmp_bit ;
+
+  *next_zero_bit = 0 ; /* always start this at zero. */
+
+  /* we we aren't logging all blocks are safe for reuse */
+  if (reiserfs_dont_log(p_s_sb)) {
+    return 0 ;
+  }
+
+  /* If we aren't doing a search_all, this is a metablock, and it will be logged before use.
+  ** if we crash before the transaction that freed it commits,  this transaction won't
+  ** have committed either, and the block will never be written
+  */
+  if (search_all) { 
+    for (i = 0 ; i < JOURNAL_NUM_BITMAPS ; i++) {
+      jb = SB_JOURNAL(p_s_sb)->j_list_bitmap + i ;
+      if (jb->journal_list && jb->bitmaps[bmap_nr] &&
+          test_bit(bit_nr, jb->bitmaps[bmap_nr]->data)) {
+	tmp_bit = find_next_zero_bit((unsigned long *)
+	                             (jb->bitmaps[bmap_nr]->data),
+	                             p_s_sb->s_blocksize, bit_nr+1) ; 
+	*next_zero_bit = bmap_nr * (p_s_sb->s_blocksize << 3) + tmp_bit ;
+	return 1 ;
+      }
+    }
+  }
+
+  /* is it in any old transactions? */
+  if (search_all && (cn = get_journal_hash_dev(SB_JOURNAL(p_s_sb)->j_list_hash_table, dev,bl,size))) {
+    return 1; 
+  }
+
+  /* is it in the current transaction.  This should never happen */
+  if ((cn = get_journal_hash_dev(SB_JOURNAL(p_s_sb)->j_hash_table, dev,bl,size))) {
+    return 1; 
+  }
+
+  /* safe for reuse */
+  return 0 ;
+}
+
+/* insert cn into table
+*/
+inline void insert_journal_hash(struct reiserfs_journal_cnode **table, struct reiserfs_journal_cnode *cn) {
+  struct reiserfs_journal_cnode *cn_orig ;
+
+  cn_orig = journal_hash(table, cn->dev, cn->blocknr) ;
+  cn->hnext = cn_orig ;
+  cn->hprev = NULL ;
+  if (cn_orig) {
+    cn_orig->hprev = cn ;
+  }
+  journal_hash(table, cn->dev, cn->blocknr) =  cn ;
+}
+
+/* lock the current transaction */
+inline static void lock_journal(struct super_block *p_s_sb) {
+  while(atomic_read(&(SB_JOURNAL(p_s_sb)->j_wlock)) > 0) {
+    sleep_on(&(SB_JOURNAL(p_s_sb)->j_wait)) ;
+  }
+  atomic_set(&(SB_JOURNAL(p_s_sb)->j_wlock), 1) ;
+}
+
+/* unlock the current transaction */
+inline static void unlock_journal(struct super_block *p_s_sb) {
+  atomic_dec(&(SB_JOURNAL(p_s_sb)->j_wlock)) ;
+  wake_up(&(SB_JOURNAL(p_s_sb)->j_wait)) ;
+}
+
+/*
+** this used to be much more involved, and I'm keeping it just in case things get ugly again.
+** it gets called by flush_commit_list, and cleans up any data stored about blocks freed during a
+** transaction.
+*/
+static void cleanup_freed_for_journal_list(struct super_block *p_s_sb, struct reiserfs_journal_list *jl) {
+
+  struct reiserfs_list_bitmap *jb = jl->j_list_bitmap ;
+  if (jb) {
+    cleanup_bitmap_list(p_s_sb, jb) ;
+  }
+  jl->j_list_bitmap->journal_list = NULL ;
+  jl->j_list_bitmap = NULL ;
+}
+
+/*
+** if this journal list still has commit blocks unflushed, send them to disk.
+**
+** log areas must be flushed in order (transaction 2 can't commit before transaction 1)
+** Before the commit block can by written, every other log block must be safely on disk
+**
+*/
+static int flush_commit_list(struct super_block *s, struct reiserfs_journal_list *jl, int flushall) {
+  int i, count ;
+  int index = 0 ;
+  int bn ;
+  int retry_count = 0 ;
+  struct buffer_head *tbh ;
+  struct reiserfs_journal_list *other_jl ;
+
+  if (atomic_read(&jl->j_older_commits_done)) {
+    return 0 ;
+  }
+
+  /* before we can put our commit blocks on disk, we have to make sure 
+  ** everyone older than us is on disk too 
+  */
+  if (jl->j_len <= 0) {
+    return 0 ;
+  }
+  if (flushall) {
+    /* we _must_ make sure the transactions are committed in order.  Start 
+    ** with the index after this one, wrap all the way around 
+    */
+    index = (jl - SB_JOURNAL_LIST(s)) + 1 ;
+    for (i = 0 ; i < JOURNAL_LIST_COUNT ; i++) {
+      other_jl = SB_JOURNAL_LIST(s) + ( (index + i) % JOURNAL_LIST_COUNT) ;
+      if (other_jl && other_jl != jl && 
+          other_jl->j_len > 0 && 
+          other_jl->j_trans_id > 0 && 
+          other_jl->j_trans_id <= jl->j_trans_id && 
+	  (atomic_read(&(jl->j_older_commits_done)) == 0)) {
+        flush_commit_list(s, other_jl, 0) ;
+      }
+    }
+  }
+
+  /* don't flush the commit list for the current transactoin */
+  if (jl == ((SB_JOURNAL_LIST(s) + SB_JOURNAL_LIST_INDEX(s)))) {
+    return 0 ;
+  }
+
+  /* make sure nobody is trying to flush this one at the same time */
+  if (atomic_read(&(jl->j_commit_flushing))) {
+    sleep_on(&(jl->j_commit_wait)) ;
+    if (flushall) {
+      atomic_set(&(jl->j_older_commits_done), 1) ;
+    }
+    return 0 ;
+  }
+  
+  /* this commit is done, exit */
+  if (atomic_read(&(jl->j_commit_left)) <= 0) {
+    if (flushall) {
+      atomic_set(&(jl->j_older_commits_done), 1) ;
+    }
+    return 0 ;
+  }
+  /* locks this journal list for us */
+  atomic_set(&(jl->j_commit_flushing), 1) ; 
+
+  if (jl->j_len > JOURNAL_TRANS_MAX) {
+    reiserfs_panic(s, "clm-2001: flush_commit_list: length is %lu, list number %d\n", jl->j_len, jl - SB_JOURNAL_LIST(s)) ;
+    return 0 ;
+  }
+
+  /* start by checking all the commit blocks in this transaction.  
+  ** Add anyone not on disk into tbh.  Stop checking once 
+  ** commit_left <= 1, because that means we
+  ** only have the commit block left 
+  */
+retry:
+  count = 0 ;
+  for (i = 0 ; atomic_read(&(jl->j_commit_left)) > 1 && 
+               i < (jl->j_len + 1) ; i++) {  /* everything but commit_bh */
+
+    bn = SB_JOURNAL_BLOCK(s) + (jl->j_start + i) % JOURNAL_BLOCK_COUNT  ;
+
+    /* FIXME, I use get hash table under the assumption that if it is not 
+    ** in the hash, it has gotten to disk and been relsed.  This OK?
+    */
+    tbh = get_hash_table(s->s_dev, bn, s->s_blocksize) ;
+
+/* kill this sanity check */
+if (count > (jl->j_len + 2)) {
+reiserfs_panic(s, "clm-2002: flush_commit_list: BAD count(%d) > orig_commit_left(%d)!\n", count, jl->j_len) ;
+}
+    if (tbh) {
+      if (buffer_locked(tbh)) { /* wait on it, redo it just to make sure */
+	wait_on_buffer(tbh) ;
+	if (!buffer_uptodate(tbh)) {
+	  reiserfs_panic(s, "clm-2003, buffer write failed\n") ;
+	}
+      } 
+      if (buffer_dirty(tbh)) {
+	printk("clm-2004: flush_commit_list, block already dirty!\n") ;
+      } else {				
+	mark_buffer_dirty(tbh, 0) ; 
+      }
+      tbh->b_end_io = reiserfs_journal_end_io ; /* not needed */
+      ll_rw_block(WRITE, 1, &tbh) ;
+      count++ ;
+      tbh->b_count-- ; /* once for our get_hash */
+    } 
+  }
+
+  /* wait on everyone in tbh before writing commit block*/
+  if (count > 0) {
+    for (i = 0 ; atomic_read(&(jl->j_commit_left)) > 1 && 
+		 i < (jl->j_len + 1) ; i++) {  /* everything but commit_bh */
+      bn = SB_JOURNAL_BLOCK(s) + (jl->j_start + i) % JOURNAL_BLOCK_COUNT  ;
+
+      tbh = get_hash_table(s->s_dev, bn, s->s_blocksize) ;
+      if (tbh) {
+	wait_on_buffer(tbh) ;
+	if (!buffer_uptodate(tbh)) {
+	  reiserfs_panic(s, "clm-2005, buffer write failed\n") ;
+	}
+	tbh->b_count-- ; /* once for our get_hash */
+	bforget(tbh) ;    /* once due to original getblk in do_journal_end */
+      }
+      /* if tbh is NULL, we are assuming the block is safely on disk */
+      atomic_dec(&(jl->j_commit_left)) ;
+    }
+  }
+
+  /* this should never happen, but if it does, we're in big trouble */
+  if (atomic_read(&(jl->j_commit_left)) != 1) { 
+    if (retry_count < 2) {
+      printk("clm-2006: flush_commit_list, not all log blocks on disk yet, trying again\n") ;
+      retry_count++ ;
+      goto retry;
+    }
+    reiserfs_panic(s, "clm-2007: flush_commit_list: BAD, j_commit_left is %lu, should be 1\n", 
+                       atomic_read(&(jl->j_commit_left)));
+  }
+
+  mark_buffer_dirty(jl->j_commit_bh,0) ; 
+  ll_rw_block(WRITE, 1, &(jl->j_commit_bh)) ;
+  wait_on_buffer(jl->j_commit_bh) ;
+  if (!buffer_uptodate(jl->j_commit_bh)) {
+    reiserfs_panic(s, "clm-2008: buffer write failed\n") ;
+  }
+  atomic_dec(&(jl->j_commit_left)) ;
+  bforget(jl->j_commit_bh) ;
+
+  /* now, every commit block is on the disk.  It is safe to allow blocks 
+  ** freed during this transaction to be reallocated 
+  */
+  cleanup_freed_for_journal_list(s, jl) ;
+
+  if (flushall) {
+    atomic_set(&(jl->j_older_commits_done), 1) ;
+  }
+  atomic_set(&(jl->j_commit_flushing), 0) ;
+  wake_up(&(jl->j_commit_wait)) ;
+  return 0 ;
+}
+
+/*
+** flush_journal_list frequently needs to find a newer transaction for a given block.  This does that, or 
+** returns NULL if it can't find anything 
+*/
+static struct reiserfs_journal_list *find_newer_jl_for_cn(struct reiserfs_journal_cnode *cn) {
+  kdev_t dev = cn->dev;
+  unsigned long blocknr = cn->blocknr ;
+
+  cn = cn->hprev ;
+  while(cn) {
+    if (cn->dev == dev && cn->blocknr == blocknr && cn->jlist) {
+      return cn->jlist ;
+    }
+    cn = cn->hprev ;
+  }
+  return NULL ;
+}
+
+
+/*
+** once all the real blocks have been flushed, it is safe to remove them from the
+** journal list for this transaction.  Aside from freeing the cnode, this also allows the
+** block to be reallocated for data blocks if it had been deleted.
+*/
+static void remove_all_from_journal_list(struct super_block *p_s_sb, struct reiserfs_journal_list *jl, int debug) {
+  struct buffer_head fake_bh ;
+  struct reiserfs_journal_cnode *cn, *last ;
+  cn = jl->j_realblock ;
+
+  /* which is better, to lock once around the whole loop, or
+  ** to lock for each call to remove_from_journal_list?
+  */
+  while(cn) {
+    if (cn->blocknr != 0) {
+      if (debug) {
+        printk("block %lu, bh is %d, state %d\n", cn->blocknr, cn->bh ? 1: 0, 
+	        cn->state) ;
+      }
+      fake_bh.b_blocknr = cn->blocknr ;
+      fake_bh.b_dev = cn->dev ;
+      cn->state = 0 ;
+      remove_from_journal_list(p_s_sb, jl, &fake_bh, 1) ;
+    }
+    last = cn ;
+    cn = cn->next ;
+    free_cnode(p_s_sb, last) ;
+  }
+  jl->j_realblock = NULL ;
+}
+
+/*
+** if this timestamp is greater than the timestamp we wrote last to the header block, write it to the header block.
+** once this is done, I can safely say the log area for this transaction won't ever be replayed, and I can start
+** releasing blocks in this transaction for reuse as data blocks.
+** called by flush_journal_list, before it calls remove_all_from_journal_list
+**
+*/
+static int do_update_journal_header_block(struct super_block *p_s_sb, unsigned long offset, unsigned long trans_id) {
+  struct reiserfs_journal_header *jh ;
+  if (trans_id >= SB_JOURNAL(p_s_sb)->j_last_flush_trans_id) {
+    if (buffer_locked((SB_JOURNAL(p_s_sb)->j_header_bh)))  {
+      wait_on_buffer((SB_JOURNAL(p_s_sb)->j_header_bh)) ;
+      if (!buffer_uptodate(SB_JOURNAL(p_s_sb)->j_header_bh)) {
+        return -1  ;
+      }
+    }
+    SB_JOURNAL(p_s_sb)->j_last_flush_trans_id = trans_id ;
+    SB_JOURNAL(p_s_sb)->j_first_unflushed_offset = offset ;
+    jh = (struct reiserfs_journal_header *)(SB_JOURNAL(p_s_sb)->j_header_bh->b_data) ;
+    jh->j_last_flush_trans_id = trans_id ;
+    jh->j_first_unflushed_offset = offset ;
+    jh->j_mount_id = SB_JOURNAL(p_s_sb)->j_mount_id ;
+    set_bit(BH_Dirty, &(SB_JOURNAL(p_s_sb)->j_header_bh->b_state)) ;
+    ll_rw_block(WRITE, 1, &(SB_JOURNAL(p_s_sb)->j_header_bh)) ;
+    wait_on_buffer((SB_JOURNAL(p_s_sb)->j_header_bh)) ; 
+    if (!buffer_uptodate(SB_JOURNAL(p_s_sb)->j_header_bh)) {
+      return -1 ;
+    }
+  }
+  return 0 ;
+}
+static int update_journal_header_block_nopanic(struct super_block *p_s_sb, 
+                                               unsigned long offset, 
+					       unsigned long trans_id) {
+  if (do_update_journal_header_block(p_s_sb, offset, trans_id) < 0) {
+    printk("clm-2071: Failed to update journal header block\n") ;
+    return -1 ;
+  }
+  return 0 ;
+}
+
+static int update_journal_header_block(struct super_block *p_s_sb, 
+                                       unsigned long offset, 
+				       unsigned long trans_id) {
+  if (do_update_journal_header_block(p_s_sb, offset, trans_id) < 0) {
+    reiserfs_panic(p_s_sb, "clm-2072: Failed to update journal header block\n");
+    return -1 ;
+  }
+  return 0 ;
+}
+
+/*
+** returns 1 if all older journal lists have been flushed
+*/
+static int older_journal_lists_are_flushed(struct super_block *p_s_sb, unsigned long trans_id) {
+  int i ;
+  struct reiserfs_journal_list *jl ;
+  for (i = 0 ; i < JOURNAL_LIST_COUNT ; i++) {
+    jl = SB_JOURNAL_LIST(p_s_sb) + i ;
+    if (jl && jl->j_len > 0 && jl->j_trans_id < trans_id && atomic_read(&(jl->j_nonzerolen)) > 0) {
+      return 0 ;
+    }
+  }
+  return 1 ;
+}
+
+/* 
+** flush any and all journal lists older than you are 
+** can only be called from flush_journal_list
+*/
+static int flush_older_journal_lists(struct super_block *p_s_sb, struct reiserfs_journal_list *jl, unsigned long trans_id) {
+  int i, index ;
+  struct reiserfs_journal_list *other_jl ;
+
+  index = jl - SB_JOURNAL_LIST(p_s_sb) ;
+  for (i = 0 ; i < JOURNAL_LIST_COUNT ; i++) {
+    other_jl = SB_JOURNAL_LIST(p_s_sb) + ((index + i) % JOURNAL_LIST_COUNT) ;
+    if (other_jl && other_jl->j_len > 0 && 
+        other_jl->j_trans_id > 0 && 
+	other_jl->j_trans_id < trans_id && 
+	other_jl != jl) {
+
+      /* not old only, not flush all */
+      flush_journal_list(p_s_sb, other_jl, 0, 0) ; 
+
+    }
+  }
+  return 0 ;
+}
+
+/* flush a journal list, both commit and real blocks
+** set old_only to one if you only want to touch journal_lists that are fully flushed and done with.  This allows
+** you to free the memory they are using
+**
+** always set flushall to 1, unless you are flushing all of them, or you are calling from inside
+** flush_journal_list
+**
+** IMPORTANT.  This can only be called while there are no journal writers, and the journal is locked.  That means
+** it can only be called from do_journal_end.  If you set old_only, you can call from other places.  journal_release
+** can call this because there aren't any writers then.
+*/
+static int flush_journal_list(struct super_block *s, 
+                              struct reiserfs_journal_list *jl, 
+			      int old_only, 
+			      int flushall) {
+  struct reiserfs_journal_list *pjl ;
+  struct reiserfs_journal_cnode *cn ;
+  int count ;
+  int was_jwait = 0 ;
+  int was_dirty = 0 ;
+  struct buffer_head *saved_bh ; /* inc'd to keep the buffer head from going away while we flush it */
+  unsigned long j_len_saved = jl->j_len ;
+
+  if (j_len_saved <= 0) {
+    return 0 ;
+  }
+  /* pretest to avoid the locking */
+  if (old_only && (atomic_read(&(jl->j_nonzerolen)) > 0 || 
+                   atomic_read(&(jl->j_flushing)))) {
+    return 0 ;
+  }
+  while (atomic_read(&(jl->j_commit_flushing)) && !old_only) { /* if someone is getting the commit list, we must wait for them */
+    sleep_on(&(jl->j_commit_wait)) ;
+  }
+  /* if someone is flushing this list, we must wait for them */
+  while (atomic_read(&(jl->j_flushing))) {
+    sleep_on(&(jl->j_flush_wait)) ;
+  }
+
+  /* this list is now ours, we can change anything we want */
+  atomic_set(&(jl->j_flushing), 1) ;
+
+  count = 0 ;
+  if (j_len_saved > JOURNAL_TRANS_MAX) {
+    reiserfs_panic(s, "clm-2011: flush_journal_list, length is %lu, list number %d\n", j_len_saved, jl - SB_JOURNAL_LIST(s)) ;
+    atomic_dec(&(jl->j_flushing)) ;
+    return 0 ;
+  }
+
+  /* if all the work is already done, get out of here */
+  if (atomic_read(&(jl->j_nonzerolen)) <= 0 && 
+      atomic_read(&(jl->j_commit_left)) <= 0) {
+    if (flushall) {
+      flush_older_journal_lists(s, jl, jl->j_trans_id) ;
+    } else if (old_only && !older_journal_lists_are_flushed(s,jl->j_trans_id)) {
+      /* only flush if we were not called old_only */
+      atomic_dec(&(jl->j_flushing)) ;
+      wake_up(&(jl->j_flush_wait)) ;
+      return 0 ;
+    } 
+    update_journal_header_block(s, (jl->j_start + jl->j_len + 2) % 
+                                    JOURNAL_BLOCK_COUNT, jl->j_trans_id) ;
+    remove_all_from_journal_list(s, jl, 0) ;
+    jl->j_len = 0 ;
+    jl->j_start = 0 ;
+    jl->j_commit_bh = NULL ;
+    jl->j_trans_id = 0 ;
+    atomic_dec(&(jl->j_flushing)) ;
+    wake_up(&(jl->j_flush_wait)) ;
+    return 0 ;
+  } 
+
+  /* if we were called old_only, we're done. */
+  if (old_only) { 
+    atomic_dec(&(jl->j_flushing)) ;
+    wake_up(&(jl->j_flush_wait)) ;
+    return 0 ;
+  }
+
+  /* not old only, start by putting the commit list on disk.  This will also 
+  ** flush the commit lists of any olders transactions, which is important
+  */
+  flush_commit_list(s, jl, 1) ;
+
+  /* are we done now? */
+  if (atomic_read(&(jl->j_nonzerolen)) <= 0 && 
+      atomic_read(&(jl->j_commit_left)) <= 0) {
+    if (flushall) {
+      flush_older_journal_lists(s, jl, jl->j_trans_id) ;
+    } 
+    update_journal_header_block(s, (jl->j_start + jl->j_len + 2) % 
+                                    JOURNAL_BLOCK_COUNT, jl->j_trans_id) ;
+    remove_all_from_journal_list(s, jl, 0) ;
+    jl->j_len = 0 ;
+    jl->j_start = 0 ;
+    jl->j_commit_bh = NULL ;
+    jl->j_trans_id = 0 ;
+    atomic_dec(&(jl->j_flushing)) ;
+    wake_up(&(jl->j_flush_wait)) ;
+    return 0 ;
+  }
+
+  /* loop through each cnode, see if we need to write it, or wait on a 
+  ** more recent transaction, or just ignore it 
+  */
+  if (atomic_read(&(SB_JOURNAL(s)->j_wcount)) != 0) {
+    reiserfs_panic(s, "clm-2012: panic journal list is flushing, wcount is not 0\n") ;
+  }
+  cn = jl->j_realblock ;
+  while(cn) {
+    was_jwait = 0 ;
+    was_dirty = 0 ;
+    saved_bh = NULL ;
+    /* blocknr of 0 is no longer in the hash, ignore it */
+    if (cn->blocknr == 0) {
+      goto free_cnode ;
+    }
+    pjl = find_newer_jl_for_cn(cn) ;
+    /* the order is important here.  We check pjl to make sure we
+    ** don't clear BH_JDirty_wait if we aren't the one writing this
+    ** block to disk
+    */
+    if (!pjl && cn->bh) {
+      saved_bh = cn->bh ;
+      saved_bh->b_count++ ;  /* we do this to make sure nobody releases 
+                                the buffer while we are working with it */
+      if (buffer_journal_dirty(saved_bh)) {
+        was_jwait = 1 ;
+	mark_buffer_notjournal_dirty(saved_bh) ;
+	saved_bh->b_count-- ; /* brelse the inc from journal_mark_dirty */
+      }
+      if (can_dirty(cn)) {
+        was_dirty = 1 ;
+      }
+    }
+
+    /* if someone has this block in a newer transaction, just make
+    ** sure they are commited, and don't try writing it to disk
+    */
+    if (pjl) {
+      flush_commit_list(s, pjl, 1) ;
+      goto free_cnode ;
+    }
+
+    /* bh == NULL when the block got to disk on its own, OR, 
+    ** the block got freed in a future transaction 
+    */
+    if (saved_bh == NULL) {
+      goto free_cnode ;
+    }
+
+    /* this should no longer be possible at all.  kupdate_one_transaction
+    ** has this list locked while it flushes, so we should never see
+    ** buffers here that are not marked jdirty_wait.
+    */
+    if ((!was_jwait) && !buffer_locked(saved_bh)) {
+printk("clm-2013: BAD! buffer %lu %cdirty %cjwait, not in a newer tranasction\n", saved_bh->b_blocknr,
+        was_dirty ? ' ' : '!', was_jwait ? ' ' : '!') ;
+    }
+    /* this should not be possible either, kupdate_one_transaction waits
+    ** on the buffer before releasing the lock for this list.  We should
+    ** never see locked buffers here
+    */
+    if (buffer_locked(saved_bh)) {
+      printk("clm-2083: locked buffer %lu in flush_journal_list\n", 
+              saved_bh->b_blocknr) ;
+      wait_on_buffer(saved_bh) ;
+      if (!buffer_uptodate(saved_bh)) {
+        reiserfs_panic(s, "clm-2014: buffer write failed\n") ;
+      }
+    } 
+    if (was_dirty) { 
+      /* we have to flush this buffer.  Inc the count so it won't go away */
+      saved_bh->b_count++ ;  
+      set_bit(BLOCK_NEEDS_FLUSH, &cn->state) ;
+      set_bit(BH_Dirty, &saved_bh->b_state) ;
+      ll_rw_block(WRITE, 1, &saved_bh) ;
+      count++ ;
+    } else {
+      printk("clm-2082: unable to flush buffer %lu in flush_journal_list\n",
+              saved_bh->b_blocknr) ;
+    }
+free_cnode:
+    cn = cn->next ;
+    if (saved_bh) {
+      /* we incremented to keep it from going away during this loop.
+      ** it might still be incremented if the buffer needs to be flushed
+      */
+      saved_bh->b_count-- ; 
+      if (saved_bh->b_count < 0) {
+        printk("clm-2015: saved_bh->b_count < 0") ;
+      }
+    }
+  }
+
+  if (count > 0) {
+    cn = jl->j_realblock ;
+    while(cn) {
+      if (test_bit(BLOCK_NEEDS_FLUSH, &cn->state)) {
+	if (!cn->bh) {
+	  reiserfs_panic(s, "clm-2060: cn->bh is NULL\n") ;
+	}
+	wait_on_buffer(cn->bh) ;
+	if (!cn->bh) {
+	  reiserfs_panic(s, "clm-2061: cn->bh is NULL\n") ;
+	}
+	if (!buffer_uptodate(cn->bh)) {
+	  reiserfs_panic(s, "clm-2062: buffer write failed\n") ;
+	}
+	/* undo the inc from when this func called ll_rw_block */
+        brelse(cn->bh) ; 
+      }
+      cn = cn->next ;
+    }
+  }
+  /* before we can update the journal header block, we 
+  ** _must_ flush all real blocks from all older transactions to disk 
+  */
+  if (flushall) {
+    flush_older_journal_lists(s, jl, jl->j_trans_id) ;
+  } 
+  
+  /* before we can remove everything from the hash tables for this 
+  ** transaction, we must make sure it can never be replayed
+  */
+  update_journal_header_block(s, (jl->j_start + jl->j_len + 2) % 
+                                  JOURNAL_BLOCK_COUNT, jl->j_trans_id) ;
+  remove_all_from_journal_list(s, jl, 0) ;
+  jl->j_len = 0 ;
+  atomic_set(&(jl->j_nonzerolen), 0) ;
+  jl->j_start = 0 ;
+  jl->j_realblock = NULL ;
+  jl->j_commit_bh = NULL ;
+  jl->j_trans_id = 0 ;
+  atomic_dec(&(jl->j_flushing)) ;
+  wake_up(&(jl->j_flush_wait)) ;
+  return 0 ;
+} 
+
+static int kupdate_one_transaction(struct super_block *s,
+                                    struct reiserfs_journal_list *jl) 
+{
+    struct reiserfs_journal_list *pjl ; /* previous list for this cn */
+    struct reiserfs_journal_cnode *cn, *walk_cn ;
+    unsigned long blocknr ;
+    int run = 0 ;
+    int orig_trans_id = jl->j_trans_id ;
+    struct buffer_head *saved_bh ; 
+    int ret = 0 ;
+
+    /* if someone is getting the commit list, we must wait for them */
+    while (atomic_read(&(jl->j_commit_flushing))) {
+        sleep_on(&(jl->j_commit_wait)) ;
+    }
+    /* if someone is flushing this list, we must wait for them */
+    while (atomic_read(&(jl->j_flushing))) {
+        sleep_on(&(jl->j_flush_wait)) ;
+    }
+    /* was it flushed while we slept? */
+    if (jl->j_len <= 0 || jl->j_trans_id != orig_trans_id) {
+        return 0 ;
+    }
+
+    /* this list is now ours, we can change anything we want */
+    atomic_set(&(jl->j_flushing), 1) ;
+
+loop_start:
+    cn = jl->j_realblock ;
+    while(cn) {
+        saved_bh = NULL ;
+        /* if the blocknr == 0, this has been cleared from the hash,
+        ** skip it
+        */
+        if (cn->blocknr == 0) {
+            goto next ;
+        }
+        /* if there is a more recent transaction with this block in
+        ** it, don't flush it here, and don't update the transaction
+        */
+        pjl = find_newer_jl_for_cn(cn) ;
+        if (run == 0 && !pjl && cn->bh && buffer_journal_dirty(cn->bh) &&
+            can_dirty(cn)) 
+        {
+            if (cn->bh->b_count <= 1) {
+                set_bit(BLOCK_NEEDS_FLUSH, &cn->state) ;
+                set_bit(BH_Dirty, &cn->bh->b_state) ;
+                ll_rw_block(WRITE, 1, &cn->bh) ;
+            } else {
+                /* someone else is using this buffer.  We can't 
+                ** send it to disk right now because they might
+                ** be changing/logging it.
+                */
+                ret = 1 ;
+            }
+        } else if (test_bit(BLOCK_NEEDS_FLUSH, &cn->state)) {
+            clear_bit(BLOCK_NEEDS_FLUSH, &cn->state) ;
+            if (!pjl && cn->bh) {
+                wait_on_buffer(cn->bh) ;
+            }
+            /* check again, someone could have logged while we scheduled */
+            pjl = find_newer_jl_for_cn(cn) ;
+
+            /* before the JDirty_wait bit is set, the 
+            ** buffer is added to the hash list.  So, if we are
+            ** run in the middle of a do_journal_end, we will notice
+            ** if this buffer was logged and added from the latest
+            ** transaction.  In this case, we don't want to decrement
+            ** b_count
+            */
+            if (!pjl && cn->bh && buffer_journal_dirty(cn->bh)) {
+                blocknr = cn->blocknr ;
+                walk_cn = cn ;
+                saved_bh= cn->bh ;
+                /* update all older transactions to show this block
+                ** was flushed
+                */
+                mark_buffer_notjournal_dirty(cn->bh) ;
+                while(walk_cn) {
+                    if (walk_cn->bh && walk_cn->blocknr == blocknr && 
+                         walk_cn->dev == cn->dev) {
+                        if (walk_cn->jlist) {
+                            atomic_dec(&(walk_cn->jlist->j_nonzerolen)) ;
+                        }
+                        walk_cn->bh = NULL ;
+                    }
+                    walk_cn = walk_cn->hnext ;
+                }
+                if (saved_bh->b_count < 1) {
+                    reiserfs_warning("clm-2081: bad count on %lu\n", 
+                                      saved_bh->b_blocknr) ;
+                }
+                saved_bh->b_count-- ;
+            }
+        }
+next:
+        cn = cn->next ;
+    }
+    /* the first run through the loop sends all the dirty buffers to
+    ** ll_rw_block.
+    ** the second run through the loop does all the accounting
+    */
+    if (run++ == 0) {
+        goto loop_start ;
+    }
+
+    atomic_set(&(jl->j_flushing), 0) ;
+    wake_up(&(jl->j_flush_wait)) ;
+    return ret ;
+}
+/* since we never give dirty buffers to bdflush/kupdate, we have to
+** flush them ourselves.  This runs through the journal lists, finds
+** old metadata in need of flushing and sends it to disk.
+** this does not end transactions, commit anything, or free
+** cnodes.
+**
+** returns the highest transaction id that was flushed last time
+*/
+static unsigned long reiserfs_journal_kupdate(struct super_block *s, 
+                                              unsigned long last) {
+    struct reiserfs_journal_list *jl ;
+    int i ;
+    int start ;
+    time_t age ;
+    int ret = 0 ;
+    unsigned long max_trans_id ; /* never flush anything higher than this */
+    unsigned long max_flushed = 0; /* highest we flush in this loop */
+
+    start = SB_JOURNAL_LIST_INDEX(s) ;
+    max_trans_id = SB_JOURNAL(s)->j_trans_id ;
+
+    /* safety check to prevent flush attempts during a mount */
+    if (start < 0) {
+        return 0 ;
+    }
+    i = (start + 1) % JOURNAL_LIST_COUNT ;
+    while(i != start) {
+        jl = SB_JOURNAL_LIST(s) + i  ;
+        age = CURRENT_TIME - jl->j_timestamp ;
+        if (jl->j_trans_id > last &&
+            jl->j_len > 0 && 
+            // age >= (JOURNAL_MAX_COMMIT_AGE * 2) && 
+            jl->j_trans_id < max_trans_id && 
+            atomic_read(&(jl->j_nonzerolen)) > 0 &&
+            atomic_read(&(jl->j_commit_left)) == 0) {
+
+            /* if ret was already 1, we want to preserve that */
+            ret |= kupdate_one_transaction(s, jl) ;
+            if (ret == 0 && max_flushed < jl->j_trans_id)
+                max_flushed = jl->j_trans_id ;
+        }
+        i = (i + 1) % JOURNAL_LIST_COUNT ;
+    }
+    return max_flushed ;
+}
+
+/*
+** removes any nodes in table with name block and dev as bh.
+** only touchs the hnext and hprev pointers.
+*/
+void remove_journal_hash(struct reiserfs_journal_cnode **table, struct reiserfs_journal_list *jl,struct buffer_head *bh,
+                         int remove_freed){
+  struct reiserfs_journal_cnode *cur ;
+  struct reiserfs_journal_cnode **head ;
+
+  if (!bh)
+    return ;
+
+  head= &(journal_hash(table, bh->b_dev, bh->b_blocknr)) ;
+  if (!head) {
+    return ;
+  }
+  cur = *head ;
+  while(cur) {
+    if (cur->blocknr == bh->b_blocknr && cur->dev == bh->b_dev && (jl == NULL || jl == cur->jlist) && 
+        (!test_bit(BLOCK_FREED, &cur->state) || remove_freed)) {
+      if (cur->hnext) {
+        cur->hnext->hprev = cur->hprev ;
+      }
+      if (cur->hprev) {
+	cur->hprev->hnext = cur->hnext ;
+      } else {
+	*head = cur->hnext ;
+      }
+      cur->blocknr = 0 ;
+      cur->dev = 0 ;
+      cur->state = 0 ;
+      if (cur->bh && cur->jlist) /* anybody who clears the cur->bh will also dec the nonzerolen */
+	atomic_dec(&(cur->jlist->j_nonzerolen)) ;
+      cur->bh = NULL ;
+      cur->jlist = NULL ;
+    } 
+    cur = cur->hnext ;
+  }
+}
+
+static void free_journal_ram(struct super_block *p_s_sb) {
+  vfree(SB_JOURNAL(p_s_sb)->j_cnode_free_orig) ;
+  free_list_bitmaps(p_s_sb, SB_JOURNAL(p_s_sb)->j_list_bitmap) ;
+  free_bitmap_nodes(p_s_sb) ; /* must be after free_list_bitmaps */
+  if (SB_JOURNAL(p_s_sb)->j_header_bh) {
+    brelse(SB_JOURNAL(p_s_sb)->j_header_bh) ;
+  }
+  vfree(SB_JOURNAL(p_s_sb)) ;
+}
+
+/*
+** call on unmount.  Only set error to 1 if you haven't made your way out
+** of read_super() yet.  Any other caller must keep error at 0.
+*/
+static int do_journal_release(struct reiserfs_transaction_handle *th, struct super_block *p_s_sb, int error) {
+  struct reiserfs_transaction_handle myth ;
+
+  /* we only want to flush out transactions if we were called with error == 0
+  */
+  if (!error && !(p_s_sb->s_flags & MS_RDONLY)) {
+    /* end the current trans */
+    do_journal_end(th, p_s_sb,10, FLUSH_ALL) ;
+
+    /* make sure something gets logged to force our way into the flush code */
+    journal_join(&myth, p_s_sb, 1) ;
+    journal_mark_dirty(&myth, p_s_sb, SB_BUFFER_WITH_SB(p_s_sb)) ;
+    do_journal_end(&myth, p_s_sb,1, FLUSH_ALL) ;
+  }
+
+  SB_JOURNAL(p_s_sb)->j_state |= JOURNAL_UNMOUNTING ;
+  wake_up(&(SB_JOURNAL(p_s_sb)->j_commit_thread_wait)) ;
+  sleep_on(&(SB_JOURNAL(p_s_sb)->j_commit_thread_done)) ;
+  free_journal_ram(p_s_sb) ;
+  return 0 ;
+}
+
+/*
+** call on unmount.  flush all journal trans, release all alloc'd ram
+*/
+int journal_release(struct reiserfs_transaction_handle *th, struct super_block *p_s_sb) {
+  return do_journal_release(th, p_s_sb, 0) ;
+}
+/*
+** only call from an error condition inside reiserfs_read_super!
+*/
+int journal_release_error(struct reiserfs_transaction_handle *th, struct super_block *p_s_sb) {
+  return do_journal_release(th, p_s_sb, 1) ;
+}
+
+/* compares description block with commit block.  returns 1 if they differ, 0 if they are the same */
+static int journal_compare_desc_commit(struct super_block *p_s_sb, struct reiserfs_journal_desc *desc, 
+			               struct reiserfs_journal_commit *commit) {
+  if (commit->j_trans_id != desc->j_trans_id || commit->j_len != desc->j_len || commit->j_len > JOURNAL_TRANS_MAX || 
+      commit->j_len <= 0 
+  ) {
+    return 1 ;
+  }
+  return 0 ;
+}
+/* returns 0 if it did not find a description block  
+** returns -1 if it found a corrupt commit block
+** returns 1 if both desc and commit were valid 
+*/
+static int journal_transaction_is_valid(struct super_block *p_s_sb, struct buffer_head *d_bh, unsigned long *oldest_invalid_trans_id, unsigned long *newest_mount_id) {
+  struct reiserfs_journal_desc *desc ;
+  struct reiserfs_journal_commit *commit ;
+  struct reiserfs_super_block *rs = SB_DISK_SUPER_BLOCK(p_s_sb) ;
+  struct buffer_head *c_bh ;
+  unsigned long offset ;
+
+  if (!d_bh) 
+    return 0 ;
+
+  desc = (struct reiserfs_journal_desc *)d_bh->b_data ;
+  if (desc->j_len > 0 && !memcmp(desc->j_magic, JOURNAL_DESC_MAGIC, 8)) {
+    if (oldest_invalid_trans_id && *oldest_invalid_trans_id && 
+        desc->j_trans_id > *oldest_invalid_trans_id) {
+#ifdef CONFIG_REISERFS_CHECK
+	printk("clm-2017: transaction is valid returning because trans_id %lu is greater than oldest_invalid %lu\n", desc->j_trans_id, *oldest_invalid_trans_id) ;
+#endif
+      return 0 ;
+    }
+    if (newest_mount_id && *newest_mount_id > desc->j_mount_id) {
+#ifdef CONFIG_REISERFS_CHECK 
+      printk("clm-2018: transaction is valid returning because mount_id %lu is less than newest_mount_id %lu\n", desc->j_mount_id, *newest_mount_id) ;
+#endif
+      return -1 ;
+    }
+    offset = d_bh->b_blocknr - rs->s_journal_block ;
+
+    /* ok, we have a journal description block, lets see if the 
+    ** transaction was valid 
+    */
+    c_bh = bread(p_s_sb->s_dev, rs->s_journal_block + 
+                 ((offset + desc->j_len + 1) % JOURNAL_BLOCK_COUNT), 
+    		  p_s_sb->s_blocksize) ;
+    if (!c_bh)
+      return 0 ;
+
+    commit = (struct reiserfs_journal_commit *)c_bh->b_data ;
+    if (journal_compare_desc_commit(p_s_sb, desc, commit)) {
+#ifdef CONFIG_REISERFS_CHECK
+      printk("clm-2059: journal_transaction_is_valid, commit offset %ld had bad time %ld or length %ld\n", c_bh->b_blocknr - rs->s_journal_block,commit->j_trans_id, commit->j_len);
+#endif
+      brelse(c_bh) ;
+      if (oldest_invalid_trans_id)
+        *oldest_invalid_trans_id = desc->j_trans_id ;
+#ifdef CONFIG_REISERFS_CHECK 
+	printk("clm-2019: transaction_is_valid setting oldest invalid trans_id to %lu\n", desc->j_trans_id) ;
+#endif
+      return -1; 
+    }
+    brelse(c_bh) ;
+#ifdef CONFIG_REISERFS_CHECK 
+    printk("clm-2020: found valid transaction start offset %lu, len %lu id %lu\n", d_bh->b_blocknr - rs->s_journal_block , desc->j_len, desc->j_trans_id) ;
+#endif
+    return 1 ;
+  } else {
+    return 0 ;
+  }
+}
+
+static void brelse_array(struct buffer_head **heads, int num) {
+  int i ;
+  for (i = 0 ; i < num ; i++) {
+    brelse(heads[i]) ;
+  }
+}
+
+/*
+** given the start, and values for the oldest acceptable transactions,
+** this either reads in a replays a transaction, or returns because the transaction
+** is invalid, or too old.
+*/
+static int journal_read_transaction(struct super_block *p_s_sb, unsigned long cur_dblock, unsigned long oldest_start, 
+				    unsigned long oldest_trans_id, unsigned long newest_mount_id) {
+  struct reiserfs_journal_desc *desc ;
+  struct reiserfs_journal_commit *commit ;
+  struct reiserfs_super_block *rs = SB_DISK_SUPER_BLOCK(p_s_sb) ;
+  unsigned long trans_id = 0 ;
+  struct buffer_head *c_bh ;
+  struct buffer_head *d_bh ;
+  struct buffer_head **log_blocks = NULL ;
+  struct buffer_head **real_blocks = NULL ;
+  unsigned long trans_offset ;
+  int i;
+
+  d_bh = bread(p_s_sb->s_dev, cur_dblock, p_s_sb->s_blocksize) ;
+  if (!d_bh)
+    return 1 ;
+  desc = (struct reiserfs_journal_desc *)d_bh->b_data ;
+  trans_offset = d_bh->b_blocknr - rs->s_journal_block ;
+
+#ifdef CONFIG_REISERFS_CHECK 
+  printk("clm-2021: journal_read_transaction, offset %lu, len %lu mount_id %lu\n", d_bh->b_blocknr - rs->s_journal_block, desc->j_len, desc->j_mount_id) ;
+#endif
+
+  if (desc->j_trans_id < oldest_trans_id) {
+
+#ifdef CONFIG_REISERFS_CHECK 
+    printk("clm-2022: journal_read_trans skipping because %lu is too old\n", cur_dblock - rs->s_journal_block) ;
+#endif
+
+    brelse(d_bh) ;
+    return 1 ;
+  }
+  if (desc->j_mount_id != newest_mount_id) {
+#ifdef CONFIG_REISERFS_CHECK
+    printk("clm-2023: journal_read_trans skipping because %lu is != newest_mount_id %lu\n", desc->j_mount_id, newest_mount_id) ;
+#endif
+    brelse(d_bh) ;
+    return 1 ;
+  }
+  c_bh = bread(p_s_sb->s_dev, rs->s_journal_block + ((trans_offset + desc->j_len + 1) % JOURNAL_BLOCK_COUNT), 
+    		p_s_sb->s_blocksize) ;
+  if (!c_bh) {
+    brelse(d_bh) ;
+    return 1 ;
+  }
+  commit = (struct reiserfs_journal_commit *)c_bh->b_data ;
+  if (journal_compare_desc_commit(p_s_sb, desc, commit)) {
+#ifdef CONFIG_REISERFS_CHECK
+    printk("clm-2024: journal_read_transaction, commit offset %ld had bad time %ld or length %ld\n", c_bh->b_blocknr - rs->s_journal_block,commit->j_trans_id, commit->j_len);
+#endif
+    brelse(c_bh) ;
+    brelse(d_bh) ;
+    return 1; 
+  }
+  trans_id = desc->j_trans_id ;
+  /* now we know we've got a good transaction, and it was inside the valid time ranges */
+  log_blocks = kmalloc(desc->j_len * sizeof(struct buffer_head *), GFP_KERNEL) ;
+  real_blocks = kmalloc(desc->j_len * sizeof(struct buffer_head *), GFP_KERNEL) ;
+  if (!log_blocks  || !real_blocks) {
+    brelse(c_bh) ;
+    brelse(d_bh) ;
+    kfree(log_blocks) ;
+    kfree(real_blocks) ;
+    reiserfs_warning("clm-2025: kmalloc failed, unable to mount FS\n") ;
+    return -1 ;
+  }
+  /* get all the buffer heads */
+  for(i = 0 ; i < desc->j_len ; i++) {
+    log_blocks[i] = getblk(p_s_sb->s_dev, rs->s_journal_block + (trans_offset + 1 + i) % JOURNAL_BLOCK_COUNT, p_s_sb->s_blocksize);
+    if (i < JOURNAL_TRANS_HALF) {
+      real_blocks[i] = getblk(p_s_sb->s_dev, desc->j_realblock[i], p_s_sb->s_blocksize) ;
+    } else {
+      real_blocks[i] = getblk(p_s_sb->s_dev, commit->j_realblock[i - JOURNAL_TRANS_HALF], p_s_sb->s_blocksize) ;
+    }
+    if (real_blocks[i]->b_blocknr >= rs->s_journal_block &&
+        real_blocks[i]->b_blocknr < (rs->s_journal_block+JOURNAL_BLOCK_COUNT)) {
+      reiserfs_warning("clm-2026: REPLAY FAILURE fsck required! Trying to replay onto a log block\n") ;
+      brelse_array(log_blocks, i) ;
+      brelse_array(real_blocks, i) ;
+      brelse(c_bh) ;
+      brelse(d_bh) ;
+      kfree(log_blocks) ;
+      kfree(real_blocks) ;
+      return -1 ;
+    }
+  }
+  /* read in the log blocks, memcpy to the corresponding real block */
+  ll_rw_block(READ, desc->j_len, log_blocks) ;
+  for (i = 0 ; i < desc->j_len ; i++) {
+    wait_on_buffer(log_blocks[i]) ;
+    if (!buffer_uptodate(log_blocks[i])) {
+      reiserfs_warning("clm-2027: REPLAY FAILURE fsck required! buffer write failed\n") ;
+      brelse_array(log_blocks + i, desc->j_len - i) ;
+      brelse_array(real_blocks, desc->j_len) ;
+      brelse(c_bh) ;
+      brelse(d_bh) ;
+      kfree(log_blocks) ;
+      kfree(real_blocks) ;
+      return -1 ;
+    }
+    memcpy(real_blocks[i]->b_data, log_blocks[i]->b_data, real_blocks[i]->b_size) ;
+    mark_buffer_uptodate(real_blocks[i], 1) ;
+    brelse(log_blocks[i]) ;
+  }
+  /* flush out the real blocks */
+  for (i = 0 ; i < desc->j_len ; i++) {
+    set_bit(BH_Dirty, &(real_blocks[i]->b_state)) ;
+    ll_rw_block(WRITE, 1, real_blocks + i) ;
+  }
+  for (i = 0 ; i < desc->j_len ; i++) {
+    wait_on_buffer(real_blocks[i]) ; 
+    if (!buffer_uptodate(real_blocks[i])) {
+      reiserfs_warning("clm-2028: REPLAY FAILURE, fsck required! buffer write failed\n") ;
+      brelse_array(real_blocks + i, desc->j_len - i) ;
+      brelse(c_bh) ;
+      brelse(d_bh) ;
+      kfree(log_blocks) ;
+      kfree(real_blocks) ;
+      return -1 ;
+    }
+    brelse(real_blocks[i]) ;
+  }
+  cur_dblock = rs->s_journal_block + ((trans_offset + desc->j_len + 2) % JOURNAL_BLOCK_COUNT) ;
+
+#ifdef CONFIG_REISERFS_CHECK
+    printk("clm-2029: setting journal start to offset %ld\n", cur_dblock - rs->s_journal_block) ;
+#endif
+
+  /* init starting values for the first transaction, in case this is the last transaction to be replayed. */
+  SB_JOURNAL(p_s_sb)->j_start = cur_dblock - rs->s_journal_block ;
+  SB_JOURNAL(p_s_sb)->j_last_flush_trans_id = trans_id ;
+  SB_JOURNAL(p_s_sb)->j_trans_id = trans_id + 1;
+  brelse(c_bh) ;
+  brelse(d_bh) ;
+  kfree(log_blocks) ;
+  kfree(real_blocks) ;
+  return 0 ;
+}
+
+/*
+** read and replay the log
+** on a clean unmount, the journal header's next unflushed pointer will be to an invalid
+** transaction.  This tests that before finding all the transactions in the log, whic makes normal mount times fast.
+**
+** After a crash, this starts with the next unflushed transaction, and replays until it finds one too old, or invalid.
+**
+** On exit, it sets things up so the first transaction will work correctly.
+*/
+static int journal_read(struct super_block *p_s_sb) {
+  struct reiserfs_super_block *rs = SB_DISK_SUPER_BLOCK(p_s_sb) ;
+  struct reiserfs_journal_desc *desc ;
+  unsigned long last_flush_trans_id = 0 ;
+  unsigned long oldest_trans_id = 0;
+  unsigned long oldest_invalid_trans_id = 0 ;
+  time_t start ;
+  unsigned long last_flush_start = 0;
+  unsigned long oldest_start = 0;
+  unsigned long cur_dblock = 0 ;
+  unsigned long newest_mount_id = 9 ;
+  struct buffer_head *d_bh ;
+  struct reiserfs_journal_header *jh ;
+  int valid_journal_header = 0 ;
+  int replay_count = 0 ;
+  int continue_replay = 1 ;
+  int ret ;
+
+  cur_dblock = rs->s_journal_block ;
+#ifdef CONFIG_REISERFS_CHECK
+  printk("clm-2030: reading through journal entries\n") ;
+#else
+  printk("Checking ReiserFS transaction log (device %s) ...\n", 
+          kdevname(p_s_sb->s_dev)) ;
+#endif
+  start = CURRENT_TIME ;
+
+  /* step 1, read in the journal header block.  Check the transaction is says is the first unflushed,
+  ** and if that transaction is not valid, replay is done
+  */
+  SB_JOURNAL(p_s_sb)->j_header_bh = bread(p_s_sb->s_dev, rs->s_journal_block + JOURNAL_BLOCK_COUNT, p_s_sb->s_blocksize) ;
+  if (!SB_JOURNAL(p_s_sb)->j_header_bh) {
+    return -1 ;
+  }
+  jh = (struct reiserfs_journal_header *)(SB_JOURNAL(p_s_sb)->j_header_bh->b_data) ;
+  if (jh->j_first_unflushed_offset >= 0 && jh->j_first_unflushed_offset < JOURNAL_BLOCK_COUNT &&
+      jh->j_last_flush_trans_id > 0) {
+    last_flush_start = rs->s_journal_block + jh->j_first_unflushed_offset ;
+    last_flush_trans_id = jh->j_last_flush_trans_id ;
+#ifdef CONFIG_REISERFS_CHECK
+    printk("clm-2031: found in header: first_unflushed_offset %lu, last_flushed_trans_id %lu\n", jh->j_first_unflushed_offset, last_flush_trans_id) ;
+#endif
+    valid_journal_header = 1 ;
+
+    /* now, we try to read the first unflushed offset.  If it is not valid, there is nothing more we can do,
+    ** and it makes no sense to read through the whole log.
+    */
+    d_bh = bread(p_s_sb->s_dev, rs->s_journal_block + jh->j_first_unflushed_offset, p_s_sb->s_blocksize) ;
+    ret = journal_transaction_is_valid(p_s_sb, d_bh, NULL, NULL) ;
+    if (!ret) {
+      continue_replay = 0 ;
+    }
+    if (d_bh) 
+      brelse(d_bh) ;
+  }
+  if (continue_replay && is_read_only(p_s_sb->s_dev)) {
+      printk("clm-2076: device is readonly, unable to replay log\n") ;
+      brelse(SB_JOURNAL(p_s_sb)->j_header_bh);
+      SB_JOURNAL(p_s_sb)->j_header_bh = NULL ;
+      return -1;
+  }
+  if (continue_replay && (p_s_sb->s_flags & MS_RDONLY)) {
+    printk("Warning, log recovery starting on readonly filesystem\n") ;
+  }
+
+  /* ok, there are transactions that need to be replayed.  
+  ** start with the first log block, find all the valid transactions, 
+  ** and pick out the oldest.
+  */
+  while(continue_replay && cur_dblock < 
+        (rs->s_journal_block + JOURNAL_BLOCK_COUNT)) {
+    d_bh = bread(p_s_sb->s_dev, cur_dblock, p_s_sb->s_blocksize) ;
+    if ((ret = journal_transaction_is_valid(p_s_sb, d_bh, 
+                                         &oldest_invalid_trans_id, 
+					 &newest_mount_id)) == 1) {
+      desc = (struct reiserfs_journal_desc *)d_bh->b_data ;
+      if (oldest_start == 0) { /* init all oldest_ values */
+        oldest_trans_id = desc->j_trans_id ;
+	oldest_start = d_bh->b_blocknr ;
+	newest_mount_id = desc->j_mount_id ;
+#ifdef CONFIG_REISERFS_CHECK
+	printk("clm-2032: Setting oldest_start to offset %lu, trans_id %lu\n", 
+	        oldest_start - rs->s_journal_block, oldest_trans_id) ;
+#endif
+      } else if (oldest_trans_id > desc->j_trans_id) { /* one just read was older */
+        oldest_trans_id = desc->j_trans_id ;
+	oldest_start = d_bh->b_blocknr ;
+#ifdef CONFIG_REISERFS_CHECK
+	printk("clm-2033: Resetting oldest_start to offset %lu, trans_id %lu\n",
+		oldest_start - rs->s_journal_block, oldest_trans_id) ;
+#endif
+      }
+      if (newest_mount_id < desc->j_mount_id) {
+        newest_mount_id = desc->j_mount_id ;
+#ifdef CONFIG_REISERFS_CHECK
+	printk("clm-2034: Setting newest_mount_id to %lu\n", desc->j_mount_id) ;
+#endif
+      }
+      cur_dblock += desc->j_len + 2 ;
+    } 
+    else {
+      cur_dblock++ ;
+    }
+    if (d_bh) {
+      brelse(d_bh) ;
+    }
+  }
+  /* step three, starting at the oldest transaction, replay */
+  if (last_flush_start > 0) {
+    oldest_start = last_flush_start ;
+    oldest_trans_id = last_flush_trans_id ;
+  } 
+  cur_dblock = oldest_start ;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (oldest_trans_id)  {
+    printk("clm-2035: Starting replay from offset %lu, trans_id %lu\n", cur_dblock - rs->s_journal_block, oldest_trans_id) ;
+  }
+#endif
+
+  replay_count = 0 ;
+  while(continue_replay && oldest_trans_id > 0) {
+    ret = journal_read_transaction(p_s_sb, cur_dblock, oldest_start, oldest_trans_id, newest_mount_id) ;
+    if (ret < 0) {
+      brelse(SB_JOURNAL(p_s_sb)->j_header_bh);
+      SB_JOURNAL(p_s_sb)->j_header_bh = NULL ;
+      return ret ;
+    } else if (ret != 0) {
+      break ;
+    }
+    cur_dblock = rs->s_journal_block + SB_JOURNAL(p_s_sb)->j_start ;
+    replay_count++ ;
+  }
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (oldest_trans_id == 0) {
+    printk("clm-2036: No valid transactions found\n") ;
+  }
+#endif
+
+  /* j_start does not get set correctly if we don't replay any transactions.
+  ** if we had a valid journal_header, set j_start to the first unflushed transaction value,
+  ** copy the trans_id from the header
+  */
+  if (valid_journal_header && replay_count == 0) { 
+    SB_JOURNAL(p_s_sb)->j_start = jh->j_first_unflushed_offset ;
+    SB_JOURNAL(p_s_sb)->j_trans_id = jh->j_last_flush_trans_id + 1 ;
+    SB_JOURNAL(p_s_sb)->j_last_flush_trans_id = jh->j_last_flush_trans_id ;
+    SB_JOURNAL(p_s_sb)->j_mount_id = jh->j_mount_id + 1;
+  } else {
+    SB_JOURNAL(p_s_sb)->j_mount_id = newest_mount_id + 1 ;
+  }
+  SB_JOURNAL(p_s_sb)->j_first_unflushed_offset = SB_JOURNAL(p_s_sb)->j_start ; 
+
+#ifdef CONFIG_REISERFS_CHECK
+  printk("clm-2038: Replayed %d transactions in %lu seconds, mount_id now %lu\n", replay_count, CURRENT_TIME - start, SB_JOURNAL(p_s_sb)->j_mount_id) ;
+#else
+  if (replay_count > 0) {
+    printk("Replayed %d transactions in %lu seconds\n", 
+	    replay_count, CURRENT_TIME - start) ;
+  }
+#endif
+
+  if (!is_read_only(p_s_sb->s_dev) && 
+      update_journal_header_block_nopanic(p_s_sb, SB_JOURNAL(p_s_sb)->j_start, 
+			      SB_JOURNAL(p_s_sb)->j_last_flush_trans_id) < 0) {
+    brelse(SB_JOURNAL(p_s_sb)->j_header_bh);
+    SB_JOURNAL(p_s_sb)->j_header_bh = NULL ;
+    return -1 ;
+  }
+  return 0 ;
+}
+
+
+struct reiserfs_journal_commit_task {
+  struct super_block *p_s_sb ;
+  int jindex ;
+  int wake_on_finish ; /* if this is one, we wake the task_done queue, if it
+                       ** is zero, we free the whole struct on finish
+		       */
+  struct reiserfs_journal_commit_task *self ;
+  struct wait_queue *task_done ;
+  struct tq_struct task ;
+} ;
+
+static void reiserfs_journal_commit_task_func(struct reiserfs_journal_commit_task *ct) {
+
+  struct reiserfs_journal_list *jl = SB_JOURNAL_LIST(ct->p_s_sb) + ct->jindex ;
+  struct super_block *s = ct->p_s_sb; 
+  flush_commit_list(s, jl, 1) ; 
+  if (jl->j_len > 0 && atomic_read(&(jl->j_nonzerolen)) > 0 &&
+      atomic_read(&(jl->j_commit_left)) == 0) {
+    kupdate_one_transaction(s, jl) ;
+  }
+  kfree(ct->self) ;
+}
+
+static void setup_commit_task_arg(struct reiserfs_journal_commit_task *ct,
+                                  struct super_block *p_s_sb, 
+				  int jindex) {
+  if (!ct) {
+    reiserfs_panic(NULL, "clm-2039: setup_commit_task_arg called with NULL struct\n") ;
+  }
+  ct->p_s_sb = p_s_sb ;
+  ct->jindex = jindex ;
+  ct->task_done = NULL ;
+  ct->task.next = NULL ;
+  ct->task.sync = 0 ;
+  ct->task.routine = (void *)(void *)reiserfs_journal_commit_task_func ; 
+  ct->self = ct ;
+  ct->task.data = (void *)ct ;
+}
+
+static void commit_flush_async(struct super_block *p_s_sb, int jindex) {
+  struct reiserfs_journal_commit_task *ct ;
+  ct = kmalloc(sizeof(struct reiserfs_journal_commit_task), GFP_KERNEL) ;
+  if (ct) {
+    setup_commit_task_arg(ct, p_s_sb, jindex) ;
+    queue_task(&(ct->task), &(SB_JOURNAL(p_s_sb)->j_commit_thread_tq));
+    wake_up(&(SB_JOURNAL(p_s_sb)->j_commit_thread_wait)) ;
+  } else {
+    flush_commit_list(p_s_sb, SB_JOURNAL_LIST(p_s_sb) + jindex, 1) ;
+  }
+}
+
+/*
+** this is the commit thread.  It is started with kernel_thread on
+** FS mount, and journal_release() waits for it to exit.
+**
+** It could do a periodic commit, but there is a lot code for that
+** elsewhere right now, and I only wanted to implement this little
+** piece for starters.
+**
+** All we do here is sleep on the j_commit_thread_wait wait queue, and
+** then run the per filesystem commit task queue when we wakeup.
+*/
+static int reiserfs_journal_commit_thread(struct super_block *p_s_sb) {
+  unsigned long kupdate_last_id = 0 ;
+  time_t last_kupdate = 0;
+  time_t now ;
+
+  lock_kernel() ;
+  exit_files(current);
+  exit_mm(current);
+
+  spin_lock_irq(&current->sigmask_lock);
+  sigfillset(&current->blocked);
+  recalc_sigpending(current);
+  spin_unlock_irq(&current->sigmask_lock);
+
+  current->session = 1;
+  current->pgrp = 1;
+  sprintf(current->comm, "kreiserfsd") ;
+  while(1) {
+/* printk("commit thread awake\n") ;  */
+    run_task_queue(&(SB_JOURNAL(p_s_sb)->j_commit_thread_tq)) ;
+    if (SB_JOURNAL(p_s_sb)->j_state & JOURNAL_UNMOUNTING) {
+      /* just in case */
+      run_task_queue(&(SB_JOURNAL(p_s_sb)->j_commit_thread_tq)) ;
+      break ;
+    }
+    now = CURRENT_TIME ;
+    if (now - last_kupdate >= JOURNAL_MAX_COMMIT_AGE) {
+        kupdate_last_id = reiserfs_journal_kupdate(p_s_sb, kupdate_last_id) ;
+        last_kupdate = now ;
+    }
+    interruptible_sleep_on_timeout(&(SB_JOURNAL(p_s_sb)->j_commit_thread_wait),
+                                   JOURNAL_MAX_COMMIT_AGE) ;
+  }
+  wake_up(&(SB_JOURNAL(p_s_sb)->j_commit_thread_done)) ;
+  unlock_kernel() ;
+/* printk("commit thread exiting\n") ;  */
+  return 0 ;
+}
+
+/*
+** must be called once on fs mount.  calls journal_read for you
+*/
+int journal_init(struct super_block *p_s_sb) {
+  int num_cnodes = JOURNAL_BLOCK_COUNT * 2 ;
+
+  if (sizeof(struct reiserfs_journal_commit) != 4096 ||
+      sizeof(struct reiserfs_journal_desc) != 4096
+     ) {
+    printk("clm-2040: commit or desc struct not 4096 %d %d\n", sizeof(struct reiserfs_journal_commit), 
+        sizeof(struct reiserfs_journal_desc)) ;
+    return 1 ;
+  }
+  /* sanity check to make sure they don't overflow the journal */
+  if (JOURNAL_BLOCK_COUNT > SB_DISK_SUPER_BLOCK(p_s_sb)->s_orig_journal_size) {
+    printk("clm-2041: current JOURNAL_BLOCK_COUNT (%d) is too big.  This FS was created with a journal size of %d blocks\n",
+            JOURNAL_BLOCK_COUNT, SB_DISK_SUPER_BLOCK(p_s_sb)->s_orig_journal_size) ;
+    return 1 ;
+  }
+  SB_JOURNAL(p_s_sb) = vmalloc(sizeof (struct reiserfs_journal)) ;
+
+  if (!SB_JOURNAL(p_s_sb)) {
+    printk("clm-2042: unable to get memory for journal structure\n") ;
+    return 1 ;
+  }
+  memset(SB_JOURNAL(p_s_sb), 0, sizeof(struct reiserfs_journal)) ;
+  SB_JOURNAL(p_s_sb)->j_list_bitmap_index = 0 ;
+  SB_JOURNAL_LIST_INDEX(p_s_sb) = -10000 ; /* make sure flush_old_commits does not try to flush a list while replay is on */
+  memset(SB_JOURNAL_LIST(p_s_sb), 0, sizeof(struct reiserfs_journal_list) * JOURNAL_LIST_COUNT) ; 
+  memset(SB_JOURNAL(p_s_sb)->j_list_hash_table, 0, JOURNAL_LIST_HASH_SIZE * sizeof(struct reiserfs_journal_cnode *)) ;
+  memset(journal_writers, 0, sizeof(char *) * 512) ; /* debug code */
+
+  INIT_LIST_HEAD(&SB_JOURNAL(p_s_sb)->j_bitmap_nodes) ;
+
+  reiserfs_allocate_list_bitmaps(p_s_sb, SB_JOURNAL(p_s_sb)->j_list_bitmap, 
+                                 SB_BMAP_NR(p_s_sb)) ;
+  allocate_bitmap_nodes(p_s_sb) ;
+  SB_JOURNAL(p_s_sb)->j_start = 0 ;
+  SB_JOURNAL(p_s_sb)->j_len = 0 ;
+  SB_JOURNAL(p_s_sb)->j_len_alloc = 0 ;
+  atomic_set(&(SB_JOURNAL(p_s_sb)->j_wcount), 0) ;
+  SB_JOURNAL(p_s_sb)->j_bcount = 0 ;	  
+  SB_JOURNAL(p_s_sb)->j_trans_start_time = 0 ;	  
+  SB_JOURNAL(p_s_sb)->j_last = NULL ;	  
+  SB_JOURNAL(p_s_sb)->j_first = NULL ;     
+  SB_JOURNAL(p_s_sb)->j_join_wait = NULL ; 
+  SB_JOURNAL(p_s_sb)->j_commit_thread_wait = NULL ; 
+  SB_JOURNAL(p_s_sb)->j_commit_thread_done = NULL ; 
+  SB_JOURNAL(p_s_sb)->j_commit_thread_tq = NULL ; 
+  SB_JOURNAL(p_s_sb)->j_wait = NULL ; 
+  SB_JOURNAL(p_s_sb)->j_trans_id = 10 ; 
+  SB_JOURNAL(p_s_sb)->j_mount_id = 10 ; 
+  SB_JOURNAL(p_s_sb)->j_state = 0 ;
+  SB_JOURNAL(p_s_sb)->j_dobalance_wait = NULL ; 
+  atomic_set(&(SB_JOURNAL(p_s_sb)->j_jlock), 0) ;
+  SB_JOURNAL(p_s_sb)->j_dobalance_lock = 0 ; 
+  atomic_set(&(SB_JOURNAL(p_s_sb)->j_wlock), 0) ;
+  SB_JOURNAL(p_s_sb)->j_cnode_free_list = allocate_cnodes(num_cnodes) ;
+  SB_JOURNAL(p_s_sb)->j_cnode_free_orig = SB_JOURNAL(p_s_sb)->j_cnode_free_list ;
+  SB_JOURNAL(p_s_sb)->j_cnode_free = SB_JOURNAL(p_s_sb)->j_cnode_free_list ? num_cnodes : 0 ;
+  SB_JOURNAL(p_s_sb)->j_cnode_used = 0 ;
+  SB_JOURNAL(p_s_sb)->j_must_wait = 0 ;
+  init_journal_hash(p_s_sb) ;
+  SB_JOURNAL_LIST(p_s_sb)[0].j_list_bitmap = get_list_bitmap(p_s_sb, SB_JOURNAL_LIST(p_s_sb)) ;
+  if (!(SB_JOURNAL_LIST(p_s_sb)[0].j_list_bitmap)) {
+    reiserfs_warning("clm-2043, get_list_bitmap failed for journal list 0\n") ;
+    return 1 ;
+  }
+  if (journal_read(p_s_sb) < 0) {
+    reiserfs_warning("Replay Failure, unable to mount\n") ;
+    free_journal_ram(p_s_sb) ;
+    return 1 ;
+  }
+  SB_JOURNAL_LIST_INDEX(p_s_sb) = 0 ; /* once the read is done, we can set this where it belongs */
+  kernel_thread((void *)(void *)reiserfs_journal_commit_thread, (void *)p_s_sb, CLONE_FS | CLONE_FILES | CLONE_VM) ;
+  return 0 ;
+}
+
+
+/* lock and unlock for dobalance.  I'm not convinced these work.  They are locked after fix_node returns
+** CARRY_ON, and unlocked after do_balance returns.
+*/
+int journal_lock_dobalance(struct super_block *p_s_sb) {
+  return 0 ;
+#if 0
+  while(SB_JOURNAL(p_s_sb)->j_dobalance_lock) {
+    sleep_on(&(SB_JOURNAL(p_s_sb)->j_dobalance_wait)) ;
+  }
+  SB_JOURNAL(p_s_sb)->j_dobalance_lock = 1 ;
+  return 0 ;
+#endif
+}
+
+int journal_unlock_dobalance(struct super_block *p_s_sb) {
+  return 0 ;
+#if 0
+  SB_JOURNAL(p_s_sb)->j_dobalance_lock = 0 ;
+  wake_up(&(SB_JOURNAL(p_s_sb)->j_dobalance_wait)) ;
+  return 0 ;
+#endif
+}
+
+/*
+** test for a polite end of the current transaction.  Used by file_write, and should
+** be used by delete to make sure they don't write more than can fit inside a single
+** transaction
+*/
+int journal_transaction_should_end(struct reiserfs_transaction_handle *th, int new_alloc) {
+  time_t now = CURRENT_TIME ;
+  if ( SB_JOURNAL(th->t_super)->j_must_wait > 0 ||
+       (SB_JOURNAL(th->t_super)->j_len_alloc + new_alloc) >= JOURNAL_MAX_BATCH || 
+       atomic_read(&(SB_JOURNAL(th->t_super)->j_jlock)) ||
+      (now - SB_JOURNAL(th->t_super)->j_trans_start_time) > JOURNAL_MAX_TRANS_AGE ||
+       SB_JOURNAL(th->t_super)->j_cnode_free < (JOURNAL_TRANS_MAX * 3)) { 
+    return 1 ;
+  }
+  return 0 ;
+}
+
+/* join == true if you must join an existing transaction.
+** join == false if you can deal with waiting for others to finish
+**
+** this will block until the transaction is joinable.  send the number of blocks you
+** expect to use in nblocks.
+*/
+static int do_journal_begin_r(struct reiserfs_transaction_handle *th, struct super_block * p_s_sb,unsigned long nblocks,int join) {
+  time_t now = CURRENT_TIME ;
+  int windex  ;
+  int old_trans_id  ;
+
+#ifdef CONFIG_REISERFS_CHECK 
+  if (p_s_sb->s_flags & MS_RDONLY) {
+    printk("clm-2078: calling journal_begin on readonly FS\n") ;
+    *((char *)0) = 1 ;
+  }
+#endif
+  lock_journal(p_s_sb) ;
+
+  /* if there is no room in the journal OR
+  ** if this transaction is too old, and we weren't called joinable, wait for it to finish before beginning 
+  ** we don't sleep if there aren't other writers
+  */
+
+  if (  (!join && SB_JOURNAL(p_s_sb)->j_must_wait > 0) ||
+     ( !join && (SB_JOURNAL(p_s_sb)->j_len_alloc + nblocks + 2) >= JOURNAL_MAX_BATCH) || 
+     (!join && atomic_read(&(SB_JOURNAL(p_s_sb)->j_wcount)) > 0 && SB_JOURNAL(p_s_sb)->j_trans_start_time > 0 && 
+      (now - SB_JOURNAL(p_s_sb)->j_trans_start_time) > JOURNAL_MAX_TRANS_AGE) ||
+     (!join && atomic_read(&(SB_JOURNAL(p_s_sb)->j_jlock)) ) ||
+     (!join && SB_JOURNAL(p_s_sb)->j_cnode_free < (JOURNAL_TRANS_MAX * 3))) {
+
+    unlock_journal(p_s_sb) ; /* allow others to finish this transaction */
+
+    /* if writer count is 0, we can just force this transaction to end, and start
+    ** a new one afterwards.
+    */
+    if (atomic_read(&(SB_JOURNAL(p_s_sb)->j_wcount)) <= 0) {
+      struct reiserfs_transaction_handle myth ;
+      journal_join(&myth, p_s_sb, 1) ;
+      windex = push_journal_writer("journal_begin") ;
+      journal_mark_dirty(&myth, p_s_sb, SB_BUFFER_WITH_SB(p_s_sb)) ;
+      pop_journal_writer(windex) ;
+      do_journal_end(&myth, p_s_sb,1,COMMIT_NOW) ;
+    } else {
+      /* but if the writer count isn't zero, we have to wait for the current writers to finish.
+      ** They won't batch on transaction end once we set j_jlock
+      */
+      atomic_set(&(SB_JOURNAL(p_s_sb)->j_jlock), 1) ;
+      old_trans_id = SB_JOURNAL(p_s_sb)->j_trans_id ;
+      while(atomic_read(&(SB_JOURNAL(p_s_sb)->j_jlock)) &&
+            SB_JOURNAL(p_s_sb)->j_trans_id == old_trans_id) {
+	sleep_on(&(SB_JOURNAL(p_s_sb)->j_join_wait)) ;
+      }
+    }
+    lock_journal(p_s_sb) ; /* relock to continue */
+  }
+
+  if (SB_JOURNAL(p_s_sb)->j_trans_start_time == 0) { /* we are the first writer, set trans_id */
+    SB_JOURNAL(p_s_sb)->j_trans_start_time = now ;
+  }
+  atomic_inc(&(SB_JOURNAL(p_s_sb)->j_wcount)) ;
+  SB_JOURNAL(p_s_sb)->j_len_alloc += nblocks ;
+  th->t_blocks_logged = 0 ;
+  th->t_blocks_allocated = nblocks ;
+  th->t_super = p_s_sb ;
+  th->t_trans_id = SB_JOURNAL(p_s_sb)->j_trans_id ;
+  th->t_caller = "Unknown" ;
+  unlock_journal(p_s_sb) ;
+  p_s_sb->s_dirt = 1; 
+  return 0 ;
+}
+
+
+int journal_join(struct reiserfs_transaction_handle *th, struct super_block *p_s_sb, unsigned long nblocks) {
+  return do_journal_begin_r(th, p_s_sb, nblocks, 1) ;
+}
+
+int journal_begin(struct reiserfs_transaction_handle *th, struct super_block  * p_s_sb, unsigned long nblocks) {
+  return do_journal_begin_r(th, p_s_sb, nblocks, 0) ;
+}
+
+/* not used at all */
+int journal_prepare(struct super_block  * p_s_sb, struct buffer_head *bh) {
+  return 0 ;
+}
+
+/*
+** puts bh into the current transaction.  If it was already there, reorders removes the
+** old pointers from the hash, and puts new ones in (to make sure replay happen in the right order).
+**
+** if it was dirty, cleans and files onto the clean list.  I can't let it be dirty again until the
+** transaction is committed.
+** 
+** if j_len, is bigger than j_len_alloc, it pushes j_len_alloc to 10 + j_len.
+*/
+int journal_mark_dirty(struct reiserfs_transaction_handle *th, struct super_block *p_s_sb, struct buffer_head *bh) {
+  struct reiserfs_journal_cnode *cn = NULL;
+  int count_already_incd = 0 ;
+
+  if (th->t_trans_id != SB_JOURNAL(p_s_sb)->j_trans_id) {
+    reiserfs_panic(th->t_super, "clm-2044: handle trans id %d != current trans id %d\n", 
+                   th->t_trans_id, SB_JOURNAL(p_s_sb)->j_trans_id);
+  }
+
+  p_s_sb->s_dirt = 1 ;
+  /* already in this transaction, we are done */
+  if (buffer_journaled(bh)) {
+    return 0 ;
+  }
+  if (atomic_read(&(SB_JOURNAL(p_s_sb)->j_wcount)) <= 0) {
+    printk("clm-2045: journal_mark_dirty returning because j_wcount was %d\n", atomic_read(&(SB_JOURNAL(p_s_sb)->j_wcount))) ;
+    return 1 ;
+  }
+  /* this error means I've screwed up, and we've overflowed the transaction.  Nothing can be done here, except make the
+  ** FS readonly or panic.
+  */ 
+  if (SB_JOURNAL(p_s_sb)->j_len >= JOURNAL_TRANS_MAX) { 
+    reiserfs_panic(th->t_super, "clm-2046: journal_mark_dirty: j_len (%lu) is too big\n", SB_JOURNAL(p_s_sb)->j_len) ;
+  }
+
+  if (buffer_journal_dirty(bh)) {
+    count_already_incd = 1 ;
+    mark_buffer_notjournal_dirty(bh) ;
+  }
+
+  if (buffer_dirty(bh)) {
+    clear_bit(BH_Dirty, &bh->b_state) ;
+  }
+
+  if (buffer_journaled(bh)) { /* must double check after getting lock */
+    goto done ;
+  }
+  if (buffer_locked(bh)) {
+    printk("clm-2080: buffer is locked in journal_mark_dirty!\n") ;
+  }
+
+  if (SB_JOURNAL(p_s_sb)->j_len > SB_JOURNAL(p_s_sb)->j_len_alloc) {
+    SB_JOURNAL(p_s_sb)->j_len_alloc = SB_JOURNAL(p_s_sb)->j_len + JOURNAL_PER_BALANCE_CNT ;
+  }
+
+  set_bit(BH_JDirty, &bh->b_state) ;
+
+  /* now put this guy on the end */
+  if (!cn) {
+    cn = get_cnode(p_s_sb) ;
+    if (!cn) {
+      reiserfs_panic(p_s_sb, "get_cnode failed!\n"); 
+    }
+
+    if (th->t_blocks_logged == th->t_blocks_allocated) {
+      th->t_blocks_allocated += JOURNAL_PER_BALANCE_CNT ;
+      SB_JOURNAL(p_s_sb)->j_len_alloc += JOURNAL_PER_BALANCE_CNT ;
+    }
+    th->t_blocks_logged++ ;
+    SB_JOURNAL(p_s_sb)->j_len++ ;
+
+    cn->bh = bh ;
+    cn->blocknr = bh->b_blocknr ;
+    cn->dev = bh->b_dev ;
+    cn->jlist = NULL ;
+    insert_journal_hash(SB_JOURNAL(p_s_sb)->j_hash_table, cn) ;
+    if (!count_already_incd) {
+      bh->b_count++ ;
+    }
+  }
+  cn->next = NULL ;
+  cn->prev = SB_JOURNAL(p_s_sb)->j_last ;
+  cn->bh = bh ;
+  if (SB_JOURNAL(p_s_sb)->j_last) {
+    SB_JOURNAL(p_s_sb)->j_last->next = cn ;
+    SB_JOURNAL(p_s_sb)->j_last = cn ;
+  } else {
+    SB_JOURNAL(p_s_sb)->j_first = cn ;
+    SB_JOURNAL(p_s_sb)->j_last = cn ;
+  }
+done:
+  return 0 ;
+}
+
+/*
+** if buffer already in current transaction, do a journal_mark_dirty
+** otherwise, just mark it dirty and move on.  Used for writes to meta blocks
+** that don't need journaling
+*/
+int journal_mark_dirty_nolog(struct reiserfs_transaction_handle *th, struct super_block *p_s_sb, struct buffer_head *bh) {
+  if (buffer_journaled(bh) || buffer_journal_dirty(bh)) {
+    clear_bit(BH_Dirty, &bh->b_state) ;
+    wait_on_buffer(bh) ;
+    return journal_mark_dirty(th, p_s_sb, bh) ;
+  }
+  if (get_journal_hash_dev(SB_JOURNAL(p_s_sb)->j_list_hash_table, bh->b_dev,bh->b_blocknr,bh->b_size)) {
+    clear_bit(BH_Dirty, &bh->b_state) ;
+    wait_on_buffer(bh) ;
+    return journal_mark_dirty(th, p_s_sb, bh) ;
+  }
+  mark_buffer_dirty(bh , 0) ;
+  return 0 ;
+}
+
+int journal_end(struct reiserfs_transaction_handle *th, struct super_block *p_s_sb, unsigned long nblocks) {
+  return do_journal_end(th, p_s_sb, nblocks, 0) ;
+}
+
+/* removes from the current transaction, relsing and descrementing any counters.  
+** also files the removed buffer directly onto the clean list
+**
+** called by journal_mark_freed when a block has been deleted
+**
+** returns 1 if it cleaned and relsed the buffer. 0 otherwise
+*/
+int remove_from_transaction(struct super_block *p_s_sb, unsigned long blocknr, int already_cleaned) {
+  struct buffer_head *bh ;
+  struct reiserfs_journal_cnode *cn ;
+  int ret = 0;
+
+  cn = get_journal_hash_dev(SB_JOURNAL(p_s_sb)->j_hash_table, p_s_sb->s_dev, blocknr, p_s_sb->s_blocksize) ;
+  if (!cn || !cn->bh) {
+    return ret ;
+  }
+  bh = cn->bh ;
+  if (cn->prev) {
+    cn->prev->next = cn->next ;
+  }
+  if (cn->next) {
+    cn->next->prev = cn->prev ;
+  }
+  if (cn == SB_JOURNAL(p_s_sb)->j_first) {
+    SB_JOURNAL(p_s_sb)->j_first = cn->next ;  
+  }
+  if (cn == SB_JOURNAL(p_s_sb)->j_last) {
+    SB_JOURNAL(p_s_sb)->j_last = cn->prev ;
+  }
+  remove_journal_hash(SB_JOURNAL(p_s_sb)->j_hash_table, NULL, bh, 0) ; 
+  mark_buffer_not_journaled(bh) ; /* don't log this one */
+
+  if (!already_cleaned) {
+    mark_buffer_notjournal_dirty(bh) ; 
+    bh->b_count-- ;
+    if (bh->b_count < 0) {
+      printk("clm-2047: remove from trans, b_count < 0\n") ;
+    }
+    if (!buffer_locked) reiserfs_clean_and_file_buffer(bh) ; 
+    ret = 1 ;
+  }
+  SB_JOURNAL(p_s_sb)->j_len-- ;
+  SB_JOURNAL(p_s_sb)->j_len_alloc-- ;
+  free_cnode(p_s_sb, cn) ;
+  return ret ;
+}
+
+/* removes from a specific journal list hash */
+int remove_from_journal_list(struct super_block *s, struct reiserfs_journal_list *jl, struct buffer_head *bh, int remove_freed) {
+  remove_journal_hash(SB_JOURNAL(s)->j_list_hash_table, jl, bh, remove_freed) ;
+  return 0 ;
+}
+
+/*
+** for any cnode in a journal list, it can only be dirtied of all the
+** transactions that include it are commited to disk.
+** this checks through each transaction, and returns 1 if you are allowed to dirty,
+** and 0 if you aren't
+**
+** called by kupdate_one_transaction and flush_journal_list, to make sure
+** it is safe to send a buffer to disk
+*/
+static int can_dirty(struct reiserfs_journal_cnode *cn) {
+  kdev_t dev = cn->dev ;
+  unsigned long blocknr = cn->blocknr  ;
+  struct reiserfs_journal_cnode *cur = cn->hprev ;
+  int can_dirty = 1 ;
+  
+  /* first test hprev, these are all newer than cn, so any
+  ** node here with the same blocknr and dev means we can't write this
+  ** node to disk right now
+  */
+  while(cur && can_dirty) {
+    if (cur->jlist && cur->bh && cur->blocknr && cur->dev == dev && 
+        cur->blocknr == blocknr) {
+      can_dirty = 0 ;
+    }
+    cur = cur->hprev ;
+  }
+  /* then test hnext, these are all older than cn.  As long as they
+  ** are all committed to the log, it is safe to write this block
+  */
+  cur = cn->hnext ;
+  while(cur && can_dirty) {
+    if (cur->jlist && cur->jlist->j_len > 0 && 
+        atomic_read(&(cur->jlist->j_commit_left)) > 0 && cur->bh && 
+        cur->blocknr && cur->dev == dev && cur->blocknr == blocknr ) {
+      can_dirty = 0 ;
+    }
+    cur = cur->hnext ;
+  }
+  return can_dirty ;
+}
+
+#if 0 /* not used anymore, kept for reference */
+/*
+** Whereever possible, this dirties and releases the real blocks associated with a transaction
+** 
+** called by flush_commit_list, after all the log blocks for a transaction are on
+** disk.  
+*/
+static void dirty_journal_list(struct super_block *p_s_sb, struct reiserfs_journal_list *jl) {
+  struct buffer_head *tbh ;
+  struct reiserfs_journal_cnode *cn ;
+  int dirty_it  ;
+
+  cn = jl->j_realblock ;
+  while(cn) {
+    /* remove_from_journal_list invalidates the bh in j_realblock, must copy it first */
+    tbh = cn->bh ;
+
+    /* default to dirty the block */
+    dirty_it = 1 ;
+    if (cn->blocknr && tbh) {
+      /* we only want to dirty the block if all the log blocks in all his transactions are on disk */
+      dirty_it = can_dirty(cn) ;
+      if (dirty_it) {
+	if (buffer_journal_dirty(tbh)) {
+	  // mark_buffer_dirty(tbh, 0) ;
+          set_bit(BH_Dirty, &tbh->b_state) ;
+	} 
+      }
+    }
+    cn = cn->next ;
+  }
+}
+#endif
+
+/* syncs the commit blocks, but does not force the real buffers to disk
+** will wait until the current transaction is done/commited before returning 
+*/
+int journal_end_sync(struct reiserfs_transaction_handle *th, struct super_block *p_s_sb, unsigned long nblocks) {
+  if (SB_JOURNAL(p_s_sb)->j_len <= 0) {
+    journal_mark_dirty(th, th->t_super, SB_BUFFER_WITH_SB(p_s_sb)) ;
+  }
+  return do_journal_end(th, p_s_sb, nblocks, COMMIT_NOW | WAIT) ;
+}
+
+#if 0
+#ifdef __KERNEL__
+int show_reiserfs_locks(void) {
+  struct super_block *p_s_sb = NULL ;
+
+  p_s_sb = sb_entry(super_blocks.next);
+  while (p_s_sb != sb_entry(&super_blocks)) {
+    if (reiserfs_is_super(p_s_sb)) {
+printk("journal lock is %d, join lock is %d, dobalance lock is %d writers %d must wait is %d\n", 
+        atomic_read(&(SB_JOURNAL(p_s_sb)->j_wlock)),
+        atomic_read(&(SB_JOURNAL(p_s_sb)->j_jlock)),
+	SB_JOURNAL(p_s_sb)->j_dobalance_lock, atomic_read(&(SB_JOURNAL(p_s_sb)->j_wcount)),
+	SB_JOURNAL(p_s_sb)->j_must_wait) ;
+    }
+    p_s_sb = sb_entry(p_s_sb->s_list.next);
+  }
+  dump_journal_writers() ;
+  return 0 ;
+}
+#endif /* __KERNEL__ */
+#endif /* 0 */
+
+/*
+** used to get memory back from async commits that are floating around
+** and to reclaim any blocks deleted but unusable because their commits
+** haven't hit disk yet.  called from bitmap.c
+**
+** if it starts flushing things, it ors SCHEDULE_OCCURRED into repeat.
+** note, this is just if schedule has a chance of occuring.  I need to 
+** change flush_commit_lists to have a repeat parameter too.
+**
+*/
+void flush_async_commits(struct super_block *p_s_sb, int *repeat) {
+  int i ;
+  int windex ;
+
+  if (SB_JOURNAL(p_s_sb)->j_dobalance_lock) { /* don't flush while do_balance is running! */
+    return ;
+  }
+  (*repeat) |= SCHEDULE_OCCURRED ;
+  windex = push_journal_writer("flush_async_commits") ;
+  for (i = 0 ; i < JOURNAL_LIST_COUNT ; i++) {
+    if (i != SB_JOURNAL_LIST_INDEX(p_s_sb)) {
+      flush_commit_list(p_s_sb, SB_JOURNAL_LIST(p_s_sb) + i, 1) ; 
+    }
+  }
+  pop_journal_writer(windex) ;
+}
+
+/*
+** flushes any old transactions to disk
+** ends the current transaction if it is too old
+**
+** also calls flush_journal_list with old_only == 1, which allows me to reclaim
+** memory and such from the journal lists whose real blocks are all on disk.
+**
+** called by sync_dev_journal from buffer.c
+*/
+int flush_old_commits(struct super_block *p_s_sb, int immediate) {
+  int i ;
+  int count = 0;
+  int start ; 
+  time_t now ; 
+  int windex ;
+  int keep_dirty = 0 ;
+  struct reiserfs_transaction_handle th ; 
+
+  start =  SB_JOURNAL_LIST_INDEX(p_s_sb) ;
+  now = CURRENT_TIME ;
+
+  /* safety check so we don't flush while we are replaying the log during mount */
+  if (SB_JOURNAL_LIST_INDEX(p_s_sb) < 0) {
+    return 0  ;
+  }
+  if (!strcmp(current->comm, "kupdate")) {
+    immediate = 0 ;
+    keep_dirty = 1 ;
+  }
+  /* starting with oldest, loop until we get to the start */
+  i = (SB_JOURNAL_LIST_INDEX(p_s_sb) + 1) % JOURNAL_LIST_COUNT ;
+  while(i != start) {
+    if (SB_JOURNAL_LIST(p_s_sb)[i].j_len > 0 && ((now - SB_JOURNAL_LIST(p_s_sb)[i].j_timestamp) > JOURNAL_MAX_COMMIT_AGE ||
+       immediate)) {
+      /* we have to check again to be sure the current transaction did not change */
+      if (i != SB_JOURNAL_LIST_INDEX(p_s_sb))  {
+	flush_commit_list(p_s_sb, SB_JOURNAL_LIST(p_s_sb) + i, 1) ;
+      }
+    }
+    /* now we free ram used by the old journal lists */
+    if (SB_JOURNAL_LIST(p_s_sb)[i].j_len > 0 && i != SB_JOURNAL_LIST_INDEX(p_s_sb))  {
+      flush_journal_list(p_s_sb, SB_JOURNAL_LIST(p_s_sb) + i, 1, 0) ; /* old_only, and don't flush all, 
+      					                                 we only want to reclaim nodes if it will be fast */
+    }
+    i = (i + 1) % JOURNAL_LIST_COUNT ;
+    count++ ;
+  }
+  /* now, check the current transaction.  If there are no writers, and it is too old, finish it, and
+  ** force the commit blocks to disk
+  */
+  if (!immediate && atomic_read(&(SB_JOURNAL(p_s_sb)->j_wcount)) <= 0 &&  
+     SB_JOURNAL(p_s_sb)->j_trans_start_time > 0 && 
+     SB_JOURNAL(p_s_sb)->j_len > 0 && 
+     (now - SB_JOURNAL(p_s_sb)->j_trans_start_time) > JOURNAL_MAX_TRANS_AGE) {
+    journal_join(&th, p_s_sb, 1) ;
+    windex = push_journal_writer("flush_old_commits") ;
+    journal_mark_dirty(&th, p_s_sb, SB_BUFFER_WITH_SB(p_s_sb)) ;
+    pop_journal_writer(windex) ;
+    do_journal_end(&th, p_s_sb,1, COMMIT_NOW) ;
+    keep_dirty = 0 ;
+  } else if (immediate) { /* belongs above, but I wanted this to be very explicit as a special case.  If they say to 
+                             flush, we must be sure old transactions hit the disk too. */
+    journal_join(&th, p_s_sb, 1) ;
+    journal_mark_dirty(&th, p_s_sb, SB_BUFFER_WITH_SB(p_s_sb)) ;
+    do_journal_end(&th, p_s_sb,1, COMMIT_NOW | WAIT) ;
+  }
+  return keep_dirty ;
+}
+
+/*
+** returns 0 if do_journal_end should return right away, returns 1 if do_journal_end should finish the commit
+** 
+** if the current transaction is too old, but still has writers, this will wait on j_join_wait until all 
+** the writers are done.  By the time it wakes up, the transaction it was called has already ended, so it just
+** flushes the commit list and returns 0.
+**
+** Won't batch when flush or commit_now is set.  Also won't batch when others are waiting on j_join_wait.
+** 
+** Note, we can't allow the journal_end to proceed while there are still writers in the log.
+*/
+static int check_journal_end(struct reiserfs_transaction_handle *th, struct super_block  * p_s_sb, 
+                             unsigned long nblocks, int flags) {
+
+  time_t now ;
+  int flush = flags & FLUSH_ALL ;
+  int commit_now = flags & COMMIT_NOW ;
+  int wait_on_commit = flags & WAIT ;
+
+  if (th->t_trans_id != SB_JOURNAL(p_s_sb)->j_trans_id) {
+    reiserfs_panic(th->t_super, "clm-2049: handle trans id %d != current trans id %d\n", 
+                   th->t_trans_id, SB_JOURNAL(p_s_sb)->j_trans_id);
+  }
+
+  SB_JOURNAL(p_s_sb)->j_len_alloc -= (th->t_blocks_allocated - th->t_blocks_logged) ;
+  if (atomic_read(&(SB_JOURNAL(p_s_sb)->j_wcount)) > 0) { /* <= 0 is allowed.  unmounting might not call begin */
+    atomic_dec(&(SB_JOURNAL(p_s_sb)->j_wcount)) ;
+  }
+
+  /* BUG, deal with case where j_len is 0, but people previously freed blocks need to be released 
+  ** will be dealt with by next transaction that actually writes something, but should be taken
+  ** care of in this trans
+  */
+  if (SB_JOURNAL(p_s_sb)->j_len == 0) {
+    int wcount = atomic_read(&(SB_JOURNAL(p_s_sb)->j_wcount)) ;
+    unlock_journal(p_s_sb) ;
+    if (atomic_read(&(SB_JOURNAL(p_s_sb)->j_jlock))  > 0 && wcount <= 0) {
+      atomic_dec(&(SB_JOURNAL(p_s_sb)->j_jlock)) ;
+      wake_up(&(SB_JOURNAL(p_s_sb)->j_join_wait)) ;
+    }
+    return 0 ;
+  }
+  /* if wcount > 0, and we are called to with flush or commit_now,
+  ** we wait on j_join_wait.  We will wake up when the last writer has
+  ** finished the transaction, and started it on its way to the disk.
+  ** Then, we flush the commit or journal list, and just return 0 
+  ** because the rest of journal end was already done for this transaction.
+  */
+  if (atomic_read(&(SB_JOURNAL(p_s_sb)->j_wcount)) > 0) {
+    if (flush || commit_now) {
+      int orig_jindex = SB_JOURNAL_LIST_INDEX(p_s_sb) ;
+      atomic_set(&(SB_JOURNAL(p_s_sb)->j_jlock), 1) ;
+      if (flush) {
+        SB_JOURNAL(p_s_sb)->j_next_full_flush = 1 ;
+      }
+      unlock_journal(p_s_sb) ;
+      /* sleep while the current transaction is still j_jlocked */
+      while(atomic_read(&(SB_JOURNAL(p_s_sb)->j_jlock)) && 
+            SB_JOURNAL(p_s_sb)->j_trans_id == th->t_trans_id) {
+	sleep_on(&(SB_JOURNAL(p_s_sb)->j_join_wait)) ;
+      }
+      if (commit_now) {
+	if (wait_on_commit) {
+	  flush_commit_list(p_s_sb,  SB_JOURNAL_LIST(p_s_sb) + orig_jindex, 1) ;
+	} else {
+	  commit_flush_async(p_s_sb, orig_jindex) ; 
+	}
+      }
+      return 0 ;
+    } 
+    unlock_journal(p_s_sb) ;
+    return 0 ;
+  }
+
+  /* deal with old transactions where we are the last writers */
+  now = CURRENT_TIME ;
+  if ((now - SB_JOURNAL(p_s_sb)->j_trans_start_time) > JOURNAL_MAX_TRANS_AGE) {
+    commit_now = 1 ;
+    SB_JOURNAL(p_s_sb)->j_next_async_flush = 1 ;
+  }
+  /* don't batch when someone is waiting on j_join_wait */
+  /* don't batch when syncing the commit or flushing the whole trans */
+  if (!(SB_JOURNAL(p_s_sb)->j_must_wait > 0) && !(atomic_read(&(SB_JOURNAL(p_s_sb)->j_jlock))) && !flush && !commit_now && 
+      (SB_JOURNAL(p_s_sb)->j_len < JOURNAL_MAX_BATCH)  && 
+      SB_JOURNAL(p_s_sb)->j_len_alloc < JOURNAL_MAX_BATCH && SB_JOURNAL(p_s_sb)->j_cnode_free > (JOURNAL_TRANS_MAX * 3)) {
+    SB_JOURNAL(p_s_sb)->j_bcount++ ;
+    unlock_journal(p_s_sb) ;
+    return 0 ;
+  }
+
+  if (SB_JOURNAL(p_s_sb)->j_start > JOURNAL_BLOCK_COUNT) {
+    reiserfs_panic(p_s_sb, "clm-2050: journal_end: j_start (%d) is too high\n", SB_JOURNAL(p_s_sb)->j_start) ;
+  }
+  return 1 ;
+}
+
+/*
+** Does all the work that makes deleting blocks safe.
+** when deleting a block mark BH_JNew, just remove it from the current transaction, clean it's buffer_head and move on.
+** 
+** otherwise:
+** set a bit for the block in the journal bitmap.  That will prevent it from being allocated for unformatted nodes
+** before this transaction has finished.
+**
+** mark any cnodes for this block as BLOCK_FREED, and clear their bh pointers.  That will prevent any old transactions with
+** this block from trying to flush to the real location.  Since we aren't removing the cnode from the journal_list_hash,
+** the block can't be reallocated yet.
+**
+** Then remove it from the current transaction, decrementing any counters and filing it on the clean list.
+*/
+int journal_mark_freed(struct reiserfs_transaction_handle *th, struct super_block *p_s_sb, unsigned long blocknr) {
+  struct reiserfs_journal_cnode *cn = NULL ;
+  struct buffer_head *bh = NULL ;
+  struct reiserfs_list_bitmap *jb = NULL ;
+  int cleaned = 0 ;
+  
+  bh = get_hash_table(p_s_sb->s_dev, blocknr, p_s_sb->s_blocksize) ;
+  /* if it is journal new, we just remove it from this transaction */
+  if (bh && buffer_journal_new(bh)) {
+    cleaned = remove_from_transaction(p_s_sb, blocknr, cleaned) ;
+  } else {
+    /* set the bit for this block in the journal bitmap for this transaction */
+    jb = SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_list_bitmap ;
+    if (!jb) {
+      reiserfs_panic(p_s_sb, "clm-2051: journal_mark_freed, journal_list_bitmap is NULL\n") ;
+    }
+    set_bit_in_list_bitmap(p_s_sb, blocknr, jb) ;
+
+    /* we must keep the lock the whole time here.  We can't allow anyone to add/remove cnodes into
+    ** this hash bucket while we are processing it.  Note, the entire while loop is not allowed
+    ** the schedule.
+    */
+
+    cleaned = remove_from_transaction(p_s_sb, blocknr, cleaned) ;
+
+    /* find all older transactions with this block, make sure they don't try to write it out */
+    cn = get_journal_hash_dev(SB_JOURNAL(p_s_sb)->j_list_hash_table, p_s_sb->s_dev, blocknr, p_s_sb->s_blocksize) ;
+    while (cn) {
+      if (p_s_sb->s_dev == cn->dev && blocknr == cn->blocknr) {
+	set_bit(BLOCK_FREED, &cn->state) ;
+	if (cn->bh) {
+	  if (!cleaned) {
+	    /* remove_from_transaction will brelse the buffer if it was 
+	    ** in the current trans
+	    */
+	    mark_buffer_notjournal_dirty(cn->bh) ;
+	    if (!buffer_locked(cn->bh)) {
+	      reiserfs_clean_and_file_buffer(cn->bh) ;
+	    }
+	    cleaned = 1 ;
+	    cn->bh->b_count-- ;
+	    if (cn->bh->b_count < 0) {
+	      printk("clm-2052: cn->bh->b_count < 0\n") ;
+	    }
+	  }
+	  if (cn->jlist) { /* since we are clearing the bh, we MUST dec nonzerolen */
+	    atomic_dec(&(cn->jlist->j_nonzerolen)) ;
+	  }
+	  cn->bh = NULL ; 
+	} 
+      }
+      cn = cn->hnext ;
+    }
+  }
+
+  if (bh) {
+    bh->b_count-- ; /* get_hash incs this */
+    if (bh->b_count < 0) {
+      printk("clm-2053: bh->b_count < 0\n") ;
+    }
+  }
+  return 0 ;
+}
+
+void reiserfs_update_inode_transaction(struct inode *p_s_inode) {
+  
+  p_s_inode->u.reiserfs_i.i_transaction_index =
+            SB_JOURNAL_LIST_INDEX(p_s_inode->i_sb);
+
+  p_s_inode->u.reiserfs_i.i_transaction_id = 
+            SB_JOURNAL(p_s_inode->i_sb)->j_trans_id ;
+}
+
+int reiserfs_inode_in_this_transaction(struct inode *p_s_inode) {
+  if (p_s_inode->u.reiserfs_i.i_transaction_id ==
+      SB_JOURNAL(p_s_inode->i_sb)->j_trans_id || 
+      p_s_inode->u.reiserfs_i.i_transaction_id == 0) {
+    return 1; 
+  } 
+  return 0 ;
+}
+
+void reiserfs_commit_for_inode(struct inode *p_s_inode) {
+  struct reiserfs_journal_list *jl ;
+  struct reiserfs_transaction_handle th ;
+
+  jl = SB_JOURNAL_LIST(p_s_inode->i_sb) + 
+       p_s_inode->u.reiserfs_i.i_transaction_index ;
+
+  /* is it from the current transaction, or from an unknown transaction? */
+  if (reiserfs_inode_in_this_transaction(p_s_inode)) {
+    journal_join(&th, p_s_inode->i_sb, 1) ;
+    journal_end_sync(&th, p_s_inode->i_sb,1) ;
+  } else if (jl->j_trans_id == p_s_inode->u.reiserfs_i.i_transaction_id) {
+    flush_commit_list(p_s_inode->i_sb, jl, 1) ;
+  }
+  /* if the transaction id does not match, this list is long since flushed */
+}
+
+static int do_journal_end_nolog(struct reiserfs_transaction_handle *th) {
+  struct super_block *p_s_sb = th->t_super ;
+  struct reiserfs_journal_cnode *cn, *next ;
+  int old_start ;
+
+
+  /* mark all the buffers from this transaction dirty.  release them
+  ** right away clear any bits set while logging them.
+  */
+  cn = SB_JOURNAL(p_s_sb)->j_first ; 
+  while(cn) {
+    clear_bit(BH_JNew, &(cn->bh->b_state)) ;
+    if (test_and_clear_bit(BH_JDirty, &(cn->bh->b_state))) {
+      mark_buffer_dirty(cn->bh, 0) ;
+    }
+    brelse(cn->bh) ;
+    next = cn->next ;
+    free_cnode(p_s_sb, cn) ;
+    cn = next ;
+  }
+  /* unlock the journal list for committing and flushing 
+  ** FIXME: this only needs to be done once, as we don't update 
+  ** the journal list index
+  */
+  atomic_set(&(SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_commit_flushing), 0) ;
+  atomic_set(&(SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_flushing), 0) ;
+
+  /* reset journal values for the next transaction */
+  old_start = SB_JOURNAL(p_s_sb)->j_start ;
+  SB_JOURNAL(p_s_sb)->j_start = (SB_JOURNAL(p_s_sb)->j_start + SB_JOURNAL(p_s_sb)->j_len + 2) % JOURNAL_BLOCK_COUNT;
+  atomic_set(&(SB_JOURNAL(p_s_sb)->j_wcount), 0) ;
+  SB_JOURNAL(p_s_sb)->j_bcount = 0 ;
+  SB_JOURNAL(p_s_sb)->j_last = NULL ;
+  SB_JOURNAL(p_s_sb)->j_first = NULL ;
+  SB_JOURNAL(p_s_sb)->j_len = 0 ;
+  SB_JOURNAL(p_s_sb)->j_trans_start_time = 0 ;
+  SB_JOURNAL(p_s_sb)->j_trans_id++ ;
+  SB_JOURNAL(p_s_sb)->j_must_wait = 0 ;
+  SB_JOURNAL(p_s_sb)->j_len_alloc = 0 ;
+  SB_JOURNAL(p_s_sb)->j_next_full_flush = 0 ;
+  SB_JOURNAL(p_s_sb)->j_next_async_flush = 0 ;
+  init_journal_hash(p_s_sb) ; 
+  return 0 ;
+}
+
+/*
+** long and ugly.  If flush, will not return until all commit
+** blocks and all real buffers in the trans are on disk.
+** If no_async, won't return until all commit blocks are on disk.
+**
+** keep reading, there are comments as you go along
+*/
+static int do_journal_end(struct reiserfs_transaction_handle *th, struct super_block  * p_s_sb, unsigned long nblocks, 
+		          int flags) {
+  struct reiserfs_journal_cnode *cn, *next, *jl_cn; 
+  struct reiserfs_journal_cnode *last_cn = NULL;
+  struct reiserfs_journal_desc *desc ; 
+  struct reiserfs_journal_commit *commit ; 
+  struct buffer_head *c_bh ; /* commit bh */
+  struct buffer_head *d_bh ; /* desc bh */
+  int cur_write_start = 0 ; /* start index of current log write */
+  int cur_blocks_left = 0 ; /* number of journal blocks left to write */
+  int old_start ;
+  int i ;
+  int jindex ;
+  int orig_jindex ;
+  int flush = flags & FLUSH_ALL ;
+  int commit_now = flags & COMMIT_NOW ;
+  int wait_on_commit = flags & WAIT ;
+  struct reiserfs_super_block *rs ; 
+
+#ifdef CONFIG_REISERFS_CHECK 
+  if (p_s_sb->s_flags & MS_RDONLY) {
+    printk("clm-2079: calling journal_end on readonly FS\n") ;
+    *((char *)0) = 1 ;
+  }
+#endif
+  lock_journal(p_s_sb) ;
+  if (SB_JOURNAL(p_s_sb)->j_next_full_flush) {
+    flags |= FLUSH_ALL ;
+    flush = 1 ;
+  }
+  if (SB_JOURNAL(p_s_sb)->j_next_async_flush) {
+    flags |= COMMIT_NOW ;
+    commit_now = 1 ;
+  }
+
+  /* check_journal_end locks the journal, and unlocks if it does not return 1 
+  ** it tells us if we should continue with the journal_end, or just return
+  */
+  if (!check_journal_end(th, p_s_sb, nblocks, flags)) {
+    return 0 ;
+  }
+  if (reiserfs_dont_log(p_s_sb)) {
+    int ret = do_journal_end_nolog(th) ;
+    unlock_journal(p_s_sb) ;
+    atomic_set(&(SB_JOURNAL(p_s_sb)->j_jlock), 0) ;
+    /* wake up any body waiting to join. */
+    wake_up(&(SB_JOURNAL(p_s_sb)->j_join_wait)) ;
+    return ret ;
+  }
+  /* check_journal_end might set these, check again */
+  if (SB_JOURNAL(p_s_sb)->j_next_full_flush) {
+    flush = 1 ;
+  }
+  if (SB_JOURNAL(p_s_sb)->j_next_async_flush) {
+    commit_now = 1 ;
+  }
+  /*
+  ** j must wait means we have to flush the log blocks, and the real blocks for
+  ** this transaction
+  */
+  if (SB_JOURNAL(p_s_sb)->j_must_wait > 0) {
+    flush = 1 ;
+  }
+
+  rs = SB_DISK_SUPER_BLOCK(p_s_sb) ;
+  /* setup description block */
+  d_bh = getblk(p_s_sb->s_dev, rs->s_journal_block + SB_JOURNAL(p_s_sb)->j_start, p_s_sb->s_blocksize) ; 
+  mark_buffer_uptodate(d_bh, 1) ;
+  desc = (struct reiserfs_journal_desc *)(d_bh)->b_data ;
+  memset(desc, 0, sizeof(struct reiserfs_journal_desc)) ;
+  memcpy(desc->j_magic, JOURNAL_DESC_MAGIC, 8) ;
+  desc->j_trans_id = SB_JOURNAL(p_s_sb)->j_trans_id ;
+
+  /* setup commit block.  Don't write (keep it clean too) this one until after everyone else is written */
+  c_bh = getblk(p_s_sb->s_dev,  rs->s_journal_block + 
+  				        ((SB_JOURNAL(p_s_sb)->j_start + SB_JOURNAL(p_s_sb)->j_len + 1) % JOURNAL_BLOCK_COUNT), 
+					 p_s_sb->s_blocksize) ;
+  d_bh->b_end_io = reiserfs_journal_end_io ;
+  c_bh->b_end_io = reiserfs_journal_end_io ; 
+  commit = (struct reiserfs_journal_commit *)c_bh->b_data ;
+  memset(commit, 0, sizeof(struct reiserfs_journal_commit)) ;
+  commit->j_trans_id = SB_JOURNAL(p_s_sb)->j_trans_id ;
+  mark_buffer_uptodate(c_bh, 1) ;
+
+  /* init this journal list */
+  atomic_set(&(SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_older_commits_done), 0) ;
+  SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_trans_id = SB_JOURNAL(p_s_sb)->j_trans_id ;
+  SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_timestamp = SB_JOURNAL(p_s_sb)->j_trans_start_time ;
+  SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_commit_bh = c_bh ;
+  SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_start = SB_JOURNAL(p_s_sb)->j_start ;
+  SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_len = SB_JOURNAL(p_s_sb)->j_len ;  
+  atomic_set(&(SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_nonzerolen), SB_JOURNAL(p_s_sb)->j_len) ;
+  atomic_set(&(SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_commit_left), SB_JOURNAL(p_s_sb)->j_len + 2);
+  SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_realblock = NULL ;
+  atomic_set(&(SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_commit_flushing), 1) ;
+  atomic_set(&(SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_flushing), 1) ;
+
+  /* which is faster, locking/unlocking at the start and end of the for
+  ** or locking once per iteration around the insert_journal_hash?
+  ** eitherway, we are write locking insert_journal_hash.  The ENTIRE FOR
+  ** LOOP MUST not cause schedule to occur.
+  */
+
+  /* for each real block, add it to the journal list hash,
+  ** copy into real block index array in the commit or desc block
+  */
+  for (i = 0, cn = SB_JOURNAL(p_s_sb)->j_first ; cn ; cn = cn->next, i++) {
+    if (test_bit(BH_JDirty, &cn->bh->b_state) ) {
+      jl_cn = get_cnode(p_s_sb) ;
+      if (!jl_cn) {
+        reiserfs_panic(p_s_sb, "clm-2054, get_cnode returned NULL\n") ;
+      }
+      if (i == 0) {
+        SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_realblock = jl_cn ;
+      }
+      jl_cn->prev = last_cn ;
+      jl_cn->next = NULL ;
+      if (last_cn) {
+        last_cn->next = jl_cn ;
+      }
+      last_cn = jl_cn ;
+      if (cn->bh->b_blocknr >= rs->s_journal_block &&
+          cn->bh->b_blocknr < (rs->s_journal_block + JOURNAL_BLOCK_COUNT)) {
+        reiserfs_panic(p_s_sb, "clm-2055: Trying to log block %lu, which is a log block\n", cn->bh->b_blocknr) ;
+      }
+      jl_cn->blocknr = cn->bh->b_blocknr ; 
+      jl_cn->state = 0 ;
+      jl_cn->dev = cn->bh->b_dev ; 
+      jl_cn->bh = cn->bh ;
+      jl_cn->jlist = SB_JOURNAL_LIST(p_s_sb) + SB_JOURNAL_LIST_INDEX(p_s_sb) ;
+      insert_journal_hash(SB_JOURNAL(p_s_sb)->j_list_hash_table, jl_cn) ; 
+      if (i < JOURNAL_TRANS_HALF) {
+	desc->j_realblock[i] = cn->bh->b_blocknr ;
+      } else {
+	commit->j_realblock[i - JOURNAL_TRANS_HALF] = cn->bh->b_blocknr ;
+      }
+    } else {
+      i-- ;
+    }
+  }
+
+  desc->j_len = SB_JOURNAL(p_s_sb)->j_len  ;
+  desc->j_mount_id = SB_JOURNAL(p_s_sb)->j_mount_id ;
+  desc->j_trans_id = SB_JOURNAL(p_s_sb)->j_trans_id ;
+  commit->j_len = SB_JOURNAL(p_s_sb)->j_len  ;
+
+  /* special check in case all buffers in the journal were marked for not logging */
+  if (desc->j_len == 0) {
+    brelse(d_bh) ;
+    brelse(c_bh) ;
+    unlock_journal(p_s_sb) ;
+printk("clm-2056: do_journal_end: BAD desc->j_len is ZERO\n") ;
+    atomic_set(&(SB_JOURNAL(p_s_sb)->j_jlock), 0) ;
+    wake_up(&(SB_JOURNAL(p_s_sb)->j_join_wait)) ;
+    return 0 ;
+  }
+
+  /* first data block is j_start + 1, so add one to cur_write_start wherever you use it */
+  cur_write_start = SB_JOURNAL(p_s_sb)->j_start ;
+  cur_blocks_left = SB_JOURNAL(p_s_sb)->j_len  ;
+  cn = SB_JOURNAL(p_s_sb)->j_first ;
+  jindex = 1 ; /* start at one so we don't get the desc again */
+  while(cur_blocks_left > 0) {
+    /* copy all the real blocks into log area.  dirty log blocks */
+    if (test_bit(BH_JDirty, &cn->bh->b_state)) {
+      struct buffer_head *tmp_bh ;
+      tmp_bh = getblk(p_s_sb->s_dev, rs->s_journal_block + 
+		     ((cur_write_start + jindex) % JOURNAL_BLOCK_COUNT), 
+				       p_s_sb->s_blocksize) ;
+      tmp_bh->b_end_io = reiserfs_journal_end_io ;
+      mark_buffer_uptodate(tmp_bh, 1) ;
+      memcpy(tmp_bh->b_data, cn->bh->b_data, cn->bh->b_size) ;  
+      jindex++ ;
+    } else {
+      /* JDirty cleared sometime during transaction.  don't log this one */
+      printk("clm-2057: do_journal_end: BAD, buffer in journal hash, but not JDirty!\n") ;
+    }
+    cn = cn->next ;
+    cur_blocks_left-- ;
+  }
+
+  /* we are done  with both the c_bh and d_bh, but
+  ** c_bh must be written after all other commit blocks,
+  ** so we dirty/relse c_bh in journal_end_io, with commit_left <= 1.
+  */
+  set_writetime(c_bh, 1);
+  /*
+  mark_buffer_dirty(d_bh, 1) ;
+  brelse(d_bh) ;
+  */
+
+  /* now loop through and mark all buffers from this transaction as JDirty_wait
+  ** clear the JDirty bit, clear BH_JNew too.  
+  ** if they weren't JDirty, they weren't logged, just relse them and move on
+  */
+  cn = SB_JOURNAL(p_s_sb)->j_first ; 
+  while(cn) {
+    clear_bit(BH_JNew, &(cn->bh->b_state)) ;
+    if (test_bit(BH_JDirty, &(cn->bh->b_state))) {
+      set_bit(BH_JDirty_wait, &(cn->bh->b_state)) ; 
+      clear_bit(BH_JDirty, &(cn->bh->b_state)) ;
+    } else {
+      brelse(cn->bh) ;
+    }
+    next = cn->next ;
+    free_cnode(p_s_sb, cn) ;
+    cn = next ;
+  }
+
+  /* unlock the journal list for committing and flushing */
+  atomic_set(&(SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_commit_flushing), 0) ;
+  atomic_set(&(SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_flushing), 0) ;
+
+  orig_jindex = SB_JOURNAL_LIST_INDEX(p_s_sb) ;
+  jindex = (SB_JOURNAL_LIST_INDEX(p_s_sb) + 1) % JOURNAL_LIST_COUNT ; 
+  SB_JOURNAL_LIST_INDEX(p_s_sb) = jindex ;
+
+  /* honor the flush and async wishes from the caller */
+  if (flush) {
+    flush_commit_list(p_s_sb, SB_JOURNAL_LIST(p_s_sb) + orig_jindex, 1) ;
+    flush_journal_list(p_s_sb,  SB_JOURNAL_LIST(p_s_sb) + orig_jindex , 0, 1) ;  /* flush all */
+  } else if (commit_now) {
+    if (wait_on_commit) {
+      flush_commit_list(p_s_sb, SB_JOURNAL_LIST(p_s_sb) + orig_jindex, 1) ;
+    } else {
+      commit_flush_async(p_s_sb, orig_jindex) ; 
+    }
+  }
+  /* reset journal values for the next transaction */
+  old_start = SB_JOURNAL(p_s_sb)->j_start ;
+  SB_JOURNAL(p_s_sb)->j_start = (SB_JOURNAL(p_s_sb)->j_start + SB_JOURNAL(p_s_sb)->j_len + 2) % JOURNAL_BLOCK_COUNT;
+  atomic_set(&(SB_JOURNAL(p_s_sb)->j_wcount), 0) ;
+  SB_JOURNAL(p_s_sb)->j_bcount = 0 ;
+  SB_JOURNAL(p_s_sb)->j_last = NULL ;
+  SB_JOURNAL(p_s_sb)->j_first = NULL ;
+  SB_JOURNAL(p_s_sb)->j_len = 0 ;
+  SB_JOURNAL(p_s_sb)->j_trans_start_time = 0 ;
+  SB_JOURNAL(p_s_sb)->j_trans_id++ ;
+  SB_JOURNAL(p_s_sb)->j_must_wait = 0 ;
+  SB_JOURNAL(p_s_sb)->j_len_alloc = 0 ;
+  SB_JOURNAL(p_s_sb)->j_next_full_flush = 0 ;
+  SB_JOURNAL(p_s_sb)->j_next_async_flush = 0 ;
+  init_journal_hash(p_s_sb) ; 
+
+  /* if the next transaction has any chance of wrapping, flush 
+  ** transactions that might get overwritten.  If any journal lists are very 
+  ** old flush them as well.  Since data will get to disk every 30 seconds or
+  ** so, any list that has unflushed members after 2 minutes was a victim to
+  ** memory shortages during the end_io handler.  Clean things up for them
+  **
+  */
+  for (i =0 ; i < JOURNAL_LIST_COUNT ; i++) {
+    jindex = i ;
+    if (SB_JOURNAL_LIST(p_s_sb)[jindex].j_len > 0 && SB_JOURNAL(p_s_sb)->j_start <= SB_JOURNAL_LIST(p_s_sb)[jindex].j_start) {
+      if ((SB_JOURNAL(p_s_sb)->j_start + JOURNAL_TRANS_MAX + 1) >= SB_JOURNAL_LIST(p_s_sb)[jindex].j_start) {
+	flush_journal_list(p_s_sb, SB_JOURNAL_LIST(p_s_sb) + jindex, 0, 1) ; /* do flush all */
+      }
+    } else if (SB_JOURNAL_LIST(p_s_sb)[jindex].j_len > 0 && 
+              (SB_JOURNAL(p_s_sb)->j_start + JOURNAL_TRANS_MAX + 1) > JOURNAL_BLOCK_COUNT) {
+      if (((SB_JOURNAL(p_s_sb)->j_start + JOURNAL_TRANS_MAX + 1) % JOURNAL_BLOCK_COUNT) >= 
+            SB_JOURNAL_LIST(p_s_sb)[jindex].j_start) {
+	flush_journal_list(p_s_sb, SB_JOURNAL_LIST(p_s_sb) + jindex, 0,1 ) ; /* do flush all */
+      }
+    } 
+    /* this check should always be run, to send old lists to disk */
+    if (SB_JOURNAL_LIST(p_s_sb)[jindex].j_len > 0 && 
+              SB_JOURNAL_LIST(p_s_sb)[jindex].j_timestamp < 
+	      (CURRENT_TIME - (JOURNAL_MAX_TRANS_AGE * 4))) {
+	flush_journal_list(p_s_sb, SB_JOURNAL_LIST(p_s_sb) + jindex, 0,1 ) ; 
+    }
+  }
+
+  /* if the next journal_list is still in use, flush it */
+  if (SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_len != 0) {
+    flush_journal_list(p_s_sb, SB_JOURNAL_LIST(p_s_sb) + SB_JOURNAL_LIST_INDEX(p_s_sb), 0, 1) ; /* do flush all */
+  }
+
+  /* we don't want anyone flushing the new transaction's list */
+  atomic_set(&(SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_commit_flushing), 1) ;
+  atomic_set(&(SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_flushing), 1) ;
+  SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_list_bitmap = get_list_bitmap(p_s_sb, SB_JOURNAL_LIST(p_s_sb) + 
+											 SB_JOURNAL_LIST_INDEX(p_s_sb)) ;
+
+  if (!(SB_JOURNAL_LIST(p_s_sb)[SB_JOURNAL_LIST_INDEX(p_s_sb)].j_list_bitmap)) {
+    reiserfs_panic(p_s_sb, "clm-2058: do_journal_end, could not get a list bitmap\n") ;
+  }
+  unlock_journal(p_s_sb) ;
+  atomic_set(&(SB_JOURNAL(p_s_sb)->j_jlock), 0) ;
+  /* wake up any body waiting to join. */
+  wake_up(&(SB_JOURNAL(p_s_sb)->j_join_wait)) ;
+
+  return 0 ;
+}
diff -urN linux/fs/reiserfs/lbalance.c /tmp/linux/fs/reiserfs/lbalance.c
--- linux/fs/reiserfs/lbalance.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/lbalance.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,1379 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+#ifdef __KERNEL__
+
+#include <asm/uaccess.h>
+#include <linux/string.h>
+#include <linux/sched.h>
+#include <linux/reiserfs_fs.h>
+
+#else
+
+#include "nokernel.h"
+
+#endif
+
+/* these are used in do_balance.c */
+
+/* leaf_move_items
+   leaf_shift_left
+   leaf_shift_right
+   leaf_delete_items
+   leaf_insert_into_buf
+   leaf_paste_in_buffer
+   leaf_cut_from_buffer
+   leaf_paste_entries
+   */
+
+
+extern struct tree_balance init_tb;
+extern int init_item_pos;
+extern int init_pos_in_item;
+extern int init_mode;
+
+
+
+
+/* copy copy_count entries from source directory item to dest buffer (creating new item if needed) */
+static void leaf_copy_dir_entries (struct reiserfs_transaction_handle *th, 
+			           struct buffer_info * dest_bi, struct buffer_head * source, 
+				   int last_first, int item_num, int from, int copy_count)
+{
+  struct buffer_head * dest = dest_bi->bi_bh;
+  int item_num_in_dest;		/* either the number of target item,
+				   or if we must create a new item,
+				   the number of the item we will
+				   create it next to */
+  struct item_head * ih;
+  struct reiserfs_de_head * deh;
+  int copy_records_len;			/* length of all records in item to be copied */
+  char * records;
+
+  ih = B_N_PITEM_HEAD (source, item_num);
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (!I_IS_DIRECTORY_ITEM (ih))
+    reiserfs_panic(0, "vs-10000: leaf_copy_dir_entries: item must be directory item");
+#endif
+
+  /* length of all record to be copied and first byte of the last of them */
+  deh = B_I_DEH (source, ih);
+  if (copy_count) {
+    copy_records_len = (from ? deh[from - 1].deh_location : ih->ih_item_len) - 
+      deh[from + copy_count - 1].deh_location;
+    records = source->b_data + ih->ih_item_location + deh[from + copy_count - 1].deh_location;
+  } else {
+    copy_records_len = 0;
+    records = 0;
+  }
+
+  /* when copy last to first, dest buffer can contain 0 items */
+  item_num_in_dest = (last_first == LAST_TO_FIRST) ? (( B_NR_ITEMS(dest) ) ? 0 : -1) : (B_NR_ITEMS(dest) - 1);
+
+  /* if there are no items in dest or the first/last item in dest is not item of the same directory */
+  if ( (item_num_in_dest == - 1) ||
+#ifdef REISERFS_FSCK
+       (last_first == FIRST_TO_LAST && are_items_mergeable (B_N_PITEM_HEAD (dest, item_num_in_dest), ih, dest->b_size) == 0) ||
+       (last_first == LAST_TO_FIRST && are_items_mergeable (ih, B_N_PITEM_HEAD (dest, item_num_in_dest), dest->b_size) == 0)) {
+#else
+       (last_first == FIRST_TO_LAST && ih->ih_key.k_offset == DOT_OFFSET) ||
+       (last_first == LAST_TO_FIRST && COMP_SHORT_KEYS (&ih->ih_key, B_N_PKEY (dest, item_num_in_dest)))) {
+#endif
+    /* create new item in dest */
+    struct item_head new_ih;
+
+    /* form item header */
+    memcpy (&new_ih.ih_key, &ih->ih_key, KEY_SIZE);
+
+    /* calculate item len */
+    new_ih.ih_item_len = DEH_SIZE * copy_count + copy_records_len;
+    I_ENTRY_COUNT(&new_ih) = 0;
+    
+    if (last_first == LAST_TO_FIRST) {
+      /* form key by the following way */
+      if (from < I_ENTRY_COUNT(ih)) {
+	new_ih.ih_key.k_offset = deh[from].deh_offset;
+	new_ih.ih_key.k_uniqueness = DIRENTRY_UNIQUENESS;
+	/*memcpy (&new_ih.ih_key.k_offset, &deh[from].deh_offset, SHORT_KEY_SIZE);*/
+      } else {
+	/* no entries will be copied to this item in this function */
+	new_ih.ih_key.k_offset = MAX_KEY_OFFSET;
+	/* this item is not yet valid, but we want I_IS_DIRECTORY_ITEM to return 1 for it, so we -1 */
+	new_ih.ih_key.k_uniqueness = DIRENTRY_UNIQUENESS/*TYPE_DIRECTORY_MAX*/;
+      }
+    }
+    new_ih.ih_reserved = ih->ih_reserved;
+    
+    /* insert item into dest buffer */
+    leaf_insert_into_buf (th, dest_bi, (last_first == LAST_TO_FIRST) ? 0 : B_NR_ITEMS(dest), &new_ih, NULL, REISERFS_KERNEL_MEM, 0);
+  } else {
+    /* prepare space for entries */
+    leaf_paste_in_buffer (th, dest_bi, (last_first==FIRST_TO_LAST) ? (B_NR_ITEMS(dest) - 1) : 0, MAX_US_INT,
+			  DEH_SIZE * copy_count + copy_records_len, records, REISERFS_KERNEL_MEM, 0
+			  );
+  }
+  
+  item_num_in_dest = (last_first == FIRST_TO_LAST) ? (B_NR_ITEMS(dest)-1) : 0;
+  
+  leaf_paste_entries (dest_bi->bi_bh, item_num_in_dest,
+		      (last_first == FIRST_TO_LAST) ? I_ENTRY_COUNT(B_N_PITEM_HEAD (dest, item_num_in_dest)) : 0,
+		      copy_count, deh + from, records,
+		      DEH_SIZE * copy_count + copy_records_len
+		      );
+}
+
+
+/* Copy the first (if last_first == FIRST_TO_LAST) or last (last_first == LAST_TO_FIRST) item or 
+   part of it or nothing (see the return 0 below) from SOURCE to the end 
+   (if last_first) or beginning (!last_first) of the DEST */
+/* returns 1 if anything was copied, else 0 */
+static int leaf_copy_boundary_item (struct reiserfs_transaction_handle *th, 
+                                    struct buffer_info * dest_bi, struct buffer_head * src, int last_first,
+				    int bytes_or_entries)
+{
+  struct buffer_head * dest = dest_bi->bi_bh;
+  int dest_nr_item, src_nr_item; /* number of items in the source and destination buffers */
+  struct item_head * ih;
+  struct item_head * dih;
+  
+  dest_nr_item = B_NR_ITEMS(dest);
+  
+  if ( last_first == FIRST_TO_LAST ) {
+    /* if ( DEST is empty or first item of SOURCE and last item of DEST are the items of different objects
+       or of different types ) then there is no need to treat this item differently from the other items
+       that we copy, so we return */
+    ih = B_N_PITEM_HEAD (src, 0);
+    dih = B_N_PITEM_HEAD (dest, dest_nr_item - 1);
+#ifdef REISERFS_FSCK
+    if (!dest_nr_item || (are_items_mergeable (dih, ih, src->b_size) == 0))
+#else
+    if (!dest_nr_item || (!is_left_mergeable (ih, src->b_size)))
+#endif
+      /* there is nothing to merge */
+      return 0;
+      
+#ifdef CONFIG_REISERFS_CHECK
+    if ( ! ih->ih_item_len )
+      reiserfs_panic (0, "vs-10010: leaf_copy_boundary_item: item can not have empty dynamic length");
+#endif
+      
+    if ( I_IS_DIRECTORY_ITEM(ih) ) {
+      if ( bytes_or_entries == -1 )
+	/* copy all entries to dest */
+	bytes_or_entries = I_ENTRY_COUNT(ih);
+      leaf_copy_dir_entries (th, dest_bi, src, FIRST_TO_LAST, 0, 0, bytes_or_entries);
+      return 1;
+    }
+      
+    /* copy part of the body of the first item of SOURCE to the end of the body of the last item of the DEST
+       part defined by 'bytes_or_entries'; if bytes_or_entries == -1 copy whole body; don't create new item header
+       */
+    if ( bytes_or_entries == -1 )
+      bytes_or_entries = ih->ih_item_len;
+
+#ifdef CONFIG_REISERFS_CHECK
+    else {
+      if (bytes_or_entries == ih->ih_item_len && I_IS_INDIRECT_ITEM(ih))
+	if (ih->u.ih_free_space)
+	  reiserfs_panic (0, "vs-10020: leaf_copy_boundary_item: "
+			  "last unformatted node must be filled entirely (free_space=%d)",
+			  ih->u.ih_free_space);
+    }
+#endif
+      
+    /* merge first item (or its part) of src buffer with the last
+       item of dest buffer. Both are of the same file */
+    leaf_paste_in_buffer (th, dest_bi,
+			  dest_nr_item - 1, dih->ih_item_len, bytes_or_entries, B_I_PITEM(src,ih), REISERFS_KERNEL_MEM, 0
+			  );
+      
+    if (I_IS_INDIRECT_ITEM(dih)) {
+#ifdef CONFIG_REISERFS_CHECK
+      if (dih->u.ih_free_space)
+	reiserfs_panic (0, "vs-10030: leaf_copy_boundary_item: " 
+			"merge to left: last unformatted node of non-last indirect item must be filled entirely (free_space=%d)",
+			ih->u.ih_free_space);
+#endif
+      if (bytes_or_entries == ih->ih_item_len)
+	dih->u.ih_free_space = ih->u.ih_free_space;
+    }
+    
+    return 1;
+  }
+  
+
+  /* copy boundary item to right (last_first == LAST_TO_FIRST) */
+
+  /* ( DEST is empty or last item of SOURCE and first item of DEST
+     are the items of different object or of different types )
+     */
+  src_nr_item = B_NR_ITEMS (src);
+  ih = B_N_PITEM_HEAD (src, src_nr_item - 1);
+  dih = B_N_PITEM_HEAD (dest, 0);
+
+#ifdef REISERFS_FSCK
+  if (!dest_nr_item || are_items_mergeable (ih, dih, src->b_size) == 0)
+#else
+  if (!dest_nr_item || !is_left_mergeable (dih, src->b_size))
+#endif
+    return 0;
+  
+  if ( I_IS_DIRECTORY_ITEM(ih)) {
+    if ( bytes_or_entries == -1 )
+      /* bytes_or_entries = entries number in last item body of SOURCE */
+      bytes_or_entries = I_ENTRY_COUNT(ih);
+    
+    leaf_copy_dir_entries (th, dest_bi, src, LAST_TO_FIRST, src_nr_item - 1, I_ENTRY_COUNT(ih) - bytes_or_entries, 
+                           bytes_or_entries);
+    return 1;
+  }
+
+  /* copy part of the body of the last item of SOURCE to the begin of the body of the first item of the DEST;
+     part defined by 'bytes_or_entries'; if byte_or_entriess == -1 copy whole body; change first item key of the DEST;
+     don't create new item header
+     */
+  
+#ifdef CONFIG_REISERFS_CHECK  
+  if (I_IS_INDIRECT_ITEM(ih) && ih->u.ih_free_space)
+    reiserfs_panic (0, "vs-10040: leaf_copy_boundary_item: " 
+		    "merge to right: last unformatted node of non-last indirect item must be filled entirely (free_space=%d)",
+		    ih->u.ih_free_space);
+#endif
+
+  if ( bytes_or_entries == -1 ) {
+    /* bytes_or_entries = length of last item body of SOURCE */
+    bytes_or_entries = ih->ih_item_len;
+
+#ifdef CONFIG_REISERFS_CHECK
+    if (dih->ih_key.k_offset != ih->ih_key.k_offset + I_BYTES_NUMBER (ih, src->b_size))/*I_DNM_DATA_LEN(ih))*/
+      reiserfs_panic (0, "vs-10050: leaf_copy_boundary_item: right item offset (%lu) must not be (%lu),it must be %lu",
+		      dih->ih_key.k_offset, ih->ih_key.k_offset + I_BYTES_NUMBER (ih, src->b_size), dih->ih_key.k_offset);
+#endif
+
+    /* change first item key of the DEST */
+    dih->ih_key.k_offset = ih->ih_key.k_offset;
+
+    /* item becomes non-mergeable */
+    /* or mergeable if left item was */
+    dih->ih_key.k_uniqueness = ih->ih_key.k_uniqueness;
+  } else {
+    /* merge to right only part of item */
+#ifdef CONFIG_REISERFS_CHECK
+    if ( ih->ih_item_len <= bytes_or_entries )
+      reiserfs_panic (0, "vs-10060: leaf_copy_boundary_item: no so much bytes %lu (needed %lu)",
+		      ih->ih_item_len, bytes_or_entries);
+#endif
+    
+    /* change first item key of the DEST */
+    if ( I_IS_DIRECT_ITEM(dih) ) {
+#ifdef CONFIG_REISERFS_CHECK
+      if (dih->ih_key.k_offset <= (unsigned long)bytes_or_entries)
+	reiserfs_panic (0, "vs-10070: leaf_copy_boundary_item: dih->ih_key.k_offset(%d) <= bytes_or_entries(%d)", 
+			dih->ih_key.k_offset, bytes_or_entries);
+#endif
+      dih->ih_key.k_offset -= bytes_or_entries;
+    } else {
+#ifdef CONFIG_REISERFS_CHECK
+      if (dih->ih_key.k_offset <=(bytes_or_entries/UNFM_P_SIZE)*dest->b_size )
+	reiserfs_panic (0, "vs-10080: leaf_copy_boundary_item: dih->ih_key.k_offset(%d) <= bytes_or_entries(%d)",
+                    dih->ih_key.k_offset, (bytes_or_entries/UNFM_P_SIZE)*dest->b_size);
+#endif
+      dih->ih_key.k_offset -= ((bytes_or_entries/UNFM_P_SIZE)*dest->b_size);
+    }
+  }
+  
+  leaf_paste_in_buffer (th, dest_bi, 0, 0, bytes_or_entries, B_I_PITEM(src,ih) + ih->ih_item_len - bytes_or_entries, REISERFS_KERNEL_MEM, 0);
+  return 1;
+}
+
+
+/* copy cpy_mun items from buffer src to buffer dest
+ * last_first == FIRST_TO_LAST means, that we copy cpy_num  items beginning from first-th item in src to tail of dest
+ * last_first == LAST_TO_FIRST means, that we copy cpy_num  items beginning from first-th item in src to head of dest
+ */
+static void leaf_copy_items_entirely (struct reiserfs_transaction_handle *th, struct buffer_info * dest_bi, 
+                                      struct buffer_head * src, int last_first,
+				      int first, int cpy_num)
+{
+  struct buffer_head * dest;
+  int nr;
+  int dest_before;
+  int last_loc, last_inserted_loc, location;
+  int i, j;
+  struct block_head * blkh;
+  struct item_head * ih;
+  struct super_block *s ;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (last_first != LAST_TO_FIRST  && last_first != FIRST_TO_LAST) 
+    reiserfs_panic (0, "vs-10090: leaf_copy_items_entirely: bad last_first parameter %d", last_first);
+
+  if (B_NR_ITEMS (src) - first < cpy_num)
+    reiserfs_panic (0, "vs-10100: leaf_copy_items_entirely: too few items in source %d, required %d from %d",
+		    B_NR_ITEMS(src), cpy_num, first);
+
+  if (cpy_num < 0)
+    reiserfs_panic (0, "vs-10110: leaf_copy_items_entirely: can not copy negative amount of items");
+
+  if ( ! dest_bi )
+    reiserfs_panic (0, "vs-10120: leaf_copy_items_entirely: can not copy negative amount of items");
+#endif
+
+  dest = dest_bi->bi_bh;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( ! dest )
+    reiserfs_panic (0, "vs-10130: leaf_copy_items_entirely: can not copy negative amount of items");
+#endif
+
+  if (cpy_num == 0)
+    return;
+
+  nr = (blkh = B_BLK_HEAD(dest))->blk_nr_item;
+  
+  /* we will insert items before 0-th or nr-th item in dest buffer. It depends of last_first parameter */
+  dest_before = (last_first == LAST_TO_FIRST) ? 0 : nr;
+
+  /* location of head of first new item */
+  ih = B_N_PITEM_HEAD (dest, dest_before);
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (blkh->blk_free_space < cpy_num * IH_SIZE) {
+    reiserfs_panic (0, "vs-10140: leaf_copy_items_entirely: not enough free space for headers %d (needed %d)",
+		    blkh->blk_free_space, cpy_num * IH_SIZE);
+  }
+#endif
+
+  /* prepare space for headers */
+  memmove (ih + cpy_num, ih, (nr-dest_before) * IH_SIZE);
+
+  /* copy item headers */
+  memcpy (ih, B_N_PITEM_HEAD (src, first), cpy_num * IH_SIZE);
+
+  blkh->blk_free_space -= IH_SIZE * cpy_num;
+
+  /* location of unmovable item */
+  j = location = (dest_before == 0) ? dest->b_size : (ih-1)->ih_item_location;
+  for (i = dest_before; i < nr + cpy_num; i ++)
+    ih[i-dest_before].ih_item_location =
+      (location -= ih[i-dest_before].ih_item_len);
+
+  /* prepare space for items */
+  last_loc = ih[nr+cpy_num-1-dest_before].ih_item_location;
+  last_inserted_loc = ih[cpy_num-1].ih_item_location;
+
+  /* check free space */
+#ifdef CONFIG_REISERFS_CHECK
+  if (blkh->blk_free_space < j - last_inserted_loc) {
+    reiserfs_panic (0, "vs-10150: leaf_copy_items_entirely: not enough free space for items %d (needed %d)",
+		    blkh->blk_free_space, j - last_inserted_loc);
+  }
+#endif
+
+  memmove (dest->b_data + last_loc,
+	   dest->b_data + last_loc + j - last_inserted_loc,
+	   last_inserted_loc - last_loc);
+
+  /* copy items */
+  memcpy (dest->b_data + last_inserted_loc, B_N_PITEM(src,(first + cpy_num - 1)),
+	  j - last_inserted_loc);
+
+  /* sizes, item number */
+  blkh->blk_nr_item += cpy_num;  
+  blkh->blk_free_space -= j - last_inserted_loc;
+  
+  s = th->t_super ;
+  journal_mark_dirty(th, s, dest);/* no need to preserve, recipient, not sender */
+
+  if (dest_bi->bi_parent) {
+#ifdef CONFIG_REISERFS_CHECK
+    if (B_N_CHILD (dest_bi->bi_parent, dest_bi->bi_position)->dc_block_number != dest->b_blocknr) {
+      reiserfs_panic (0, "vs-10160: leaf_copy_items_entirely: "
+		      "block number in bh does not match to field in disk_child structure %lu and %lu",
+		      dest->b_blocknr, B_N_CHILD (dest_bi->bi_parent, dest_bi->bi_position)->dc_block_number);
+    }
+#endif
+    B_N_CHILD (dest_bi->bi_parent, dest_bi->bi_position)->dc_size +=
+      j - last_inserted_loc + IH_SIZE * cpy_num;
+    
+    /* reiserfs_mark_buffer_dirty (dest_bi->bi_parent, 0); journal victim */	/* no preserve, internal node */
+    journal_mark_dirty(th, s, dest_bi->bi_parent);	/* no preserve, internal node */
+  }
+}
+
+
+/* This function splits the (liquid) item into two items (useful when
+   shifting part of an item into another node.) */
+static void leaf_item_bottle (struct reiserfs_transaction_handle *th, 
+			      struct buffer_info * dest_bi, struct buffer_head * src, int last_first,
+			      int item_num, int cpy_bytes)
+{
+  struct buffer_head * dest = dest_bi->bi_bh;
+  struct item_head * ih;
+  
+#ifdef CONFIG_REISERFS_CHECK  
+  if ( cpy_bytes == -1 ) 
+    reiserfs_panic (0, "vs-10170: leaf_item_bottle: bytes == - 1 means: do not split item");
+#endif
+
+  if ( last_first == FIRST_TO_LAST ) {
+    /* if ( if item in position item_num in buffer SOURCE is directory item ) */
+    if (I_IS_DIRECTORY_ITEM(ih = B_N_PITEM_HEAD(src,item_num)))
+      leaf_copy_dir_entries (th, dest_bi, src, FIRST_TO_LAST, item_num, 0, cpy_bytes);
+    else {
+      struct item_head n_ih;
+      
+      /* copy part of the body of the item number 'item_num' of SOURCE to the end of the DEST 
+	 part defined by 'cpy_bytes'; create new item header; change old item_header (????);
+	 n_ih = new item_header;
+	 */
+      memcpy (&n_ih, ih, IH_SIZE);
+      n_ih.ih_item_len = cpy_bytes;
+      if (I_IS_INDIRECT_ITEM(ih)) {
+#ifdef CONFIG_REISERFS_CHECK
+	if (cpy_bytes == ih->ih_item_len && ih->u.ih_free_space)
+	  reiserfs_panic (0, "vs-10180: leaf_item_bottle: " 
+			  "when whole indirect item is bottle to left neighbor, it must have free_space==0 (not %lu)",
+			  ih->u.ih_free_space);
+#endif
+	n_ih.u.ih_free_space = 0;
+      }
+
+#ifdef CONFIG_REISERFS_CHECK
+      if (is_left_mergeable (ih, src->b_size))
+	reiserfs_panic (0, "vs-10190: leaf_item_bottle: bad mergeability k_offet=%lu, k_uniqueness=%lu",
+			ih->ih_key.k_offset, ih->ih_key.k_uniqueness);
+#endif
+      n_ih.ih_reserved = ih->ih_reserved;;
+      leaf_insert_into_buf (th, dest_bi, B_NR_ITEMS(dest), &n_ih, B_N_PITEM (src, item_num), REISERFS_KERNEL_MEM, 0);
+    }
+  } else {
+    /*  if ( if item in position item_num in buffer SOURCE is directory item ) */
+    if (I_IS_DIRECTORY_ITEM(ih = B_N_PITEM_HEAD (src, item_num)))
+      leaf_copy_dir_entries (th, dest_bi, src, LAST_TO_FIRST, item_num, I_ENTRY_COUNT(ih) - cpy_bytes, cpy_bytes);
+    else {
+      struct item_head n_ih;
+      
+      /* copy part of the body of the item number 'item_num' of SOURCE to the begin of the DEST 
+	 part defined by 'cpy_bytes'; create new item header;
+	 n_ih = new item_header;
+	 */
+      memcpy (&n_ih, ih, SHORT_KEY_SIZE);
+      
+      if (I_IS_DIRECT_ITEM(ih)) {
+	n_ih.ih_key.k_offset = ih->ih_key.k_offset + ih->ih_item_len - cpy_bytes;
+	n_ih.ih_key.k_uniqueness = TYPE_DIRECT;
+	n_ih.u.ih_free_space = MAX_US_INT;
+      } else {
+	/* indirect item */
+#ifdef CONFIG_REISERFS_CHECK
+	if (!cpy_bytes && ih->u.ih_free_space)
+	  reiserfs_panic (0, "vs-10200: leaf_item_bottle: ih->ih_free_space must be 0 when indirect item will be appended");
+#endif
+	n_ih.ih_key.k_offset = ih->ih_key.k_offset + (ih->ih_item_len - cpy_bytes) / UNFM_P_SIZE * dest->b_size;
+	n_ih.ih_key.k_uniqueness = TYPE_INDIRECT;
+	n_ih.u.ih_free_space = ih->u.ih_free_space;
+      }
+      
+      /* set item length */
+      n_ih.ih_item_len = cpy_bytes;
+      n_ih.ih_reserved = ih->ih_reserved;
+      leaf_insert_into_buf (th, dest_bi, 0, &n_ih, B_N_PITEM(src,item_num) + ih->ih_item_len - cpy_bytes,
+			    REISERFS_KERNEL_MEM, 0);
+    }
+  }
+}
+
+
+/* If cpy_bytes equals minus one than copy cpy_num whole items from SOURCE to DEST.
+   If cpy_bytes not equal to minus one than copy cpy_num-1 whole items from SOURCE to DEST.
+   From last item copy cpy_num bytes for regular item and cpy_num directory entries for
+   directory item. */
+static int leaf_copy_items (struct reiserfs_transaction_handle *th,
+                            struct buffer_info * dest_bi, struct buffer_head * src, int last_first, int cpy_num,
+			    int cpy_bytes)
+{
+  struct buffer_head * dest;
+  int pos, i, src_nr_item, bytes;
+
+  dest = dest_bi->bi_bh;
+#ifdef CONFIG_REISERFS_CHECK
+  if (!dest || !src)
+    reiserfs_panic (0, "vs-10210: leaf_copy_items: !dest || !src");
+  
+  if ( last_first != FIRST_TO_LAST && last_first != LAST_TO_FIRST )
+    reiserfs_panic (0, "vs-10220: leaf_copy_items: last_first != FIRST_TO_LAST && last_first != LAST_TO_FIRST");
+
+  if ( B_NR_ITEMS(src) < cpy_num )
+    reiserfs_panic (0, "vs-10230: leaf_copy_items: No enough items: %d, required %d", B_NR_ITEMS(src), cpy_num);
+
+ if ( cpy_num < 0 )
+    reiserfs_panic (0, "vs-10240: leaf_copy_items: cpy_num < 0 (%d)", cpy_num);
+#endif
+
+ if ( cpy_num == 0 )
+   return 0;
+ 
+ if ( last_first == FIRST_TO_LAST ) {
+   /* copy items to left */
+   pos = 0;
+   if ( cpy_num == 1 )
+     bytes = cpy_bytes;
+   else
+     bytes = -1;
+   
+   /* copy the first item or it part or nothing to the end of the DEST (i = leaf_copy_boundary_item(DEST,SOURCE,0,bytes)) */
+   i = leaf_copy_boundary_item (th, dest_bi, src, FIRST_TO_LAST, bytes);
+   cpy_num -= i;
+   if ( cpy_num == 0 )
+     return i;
+   pos += i;
+   if ( cpy_bytes == -1 )
+     /* copy first cpy_num items starting from position 'pos' of SOURCE to end of DEST */
+     leaf_copy_items_entirely(th, dest_bi, src, FIRST_TO_LAST, pos, cpy_num);
+   else {
+     /* copy first cpy_num-1 items starting from position 'pos-1' of the SOURCE to the end of the DEST */
+     leaf_copy_items_entirely(th, dest_bi, src, FIRST_TO_LAST, pos, cpy_num-1);
+	     
+     /* copy part of the item which number is cpy_num+pos-1 to the end of the DEST */
+     leaf_item_bottle (th, dest_bi, src, FIRST_TO_LAST, cpy_num+pos-1, cpy_bytes);
+   } 
+ } else {
+   /* copy items to right */
+   src_nr_item = B_NR_ITEMS (src);
+   if ( cpy_num == 1 )
+     bytes = cpy_bytes;
+   else
+     bytes = -1;
+   
+   /* copy the last item or it part or nothing to the begin of the DEST (i = leaf_copy_boundary_item(DEST,SOURCE,1,bytes)); */
+   i = leaf_copy_boundary_item (th, dest_bi, src, LAST_TO_FIRST, bytes);
+   
+   cpy_num -= i;
+   if ( cpy_num == 0 )
+     return i;
+   
+   pos = src_nr_item - cpy_num - i;
+   if ( cpy_bytes == -1 ) {
+     /* starting from position 'pos' copy last cpy_num items of SOURCE to begin of DEST */
+     leaf_copy_items_entirely(th, dest_bi, src, LAST_TO_FIRST, pos, cpy_num);
+   } else {
+     /* copy last cpy_num-1 items starting from position 'pos+1' of the SOURCE to the begin of the DEST; */
+     leaf_copy_items_entirely(th, dest_bi, src, LAST_TO_FIRST, pos+1, cpy_num-1);
+
+     /* copy part of the item which number is pos to the begin of the DEST */
+     leaf_item_bottle (th, dest_bi, src, LAST_TO_FIRST, pos, cpy_bytes);
+   }
+ }
+ return i;
+}
+
+
+/* there are types of coping: from S[0] to L[0], from S[0] to R[0],
+   from R[0] to L[0]. for each of these we have to define parent and
+   positions of destination and source buffers */
+static void leaf_define_dest_src_infos (int shift_mode, struct tree_balance * tb, struct buffer_info * dest_bi,
+					struct buffer_info * src_bi, int * first_last,
+					struct buffer_head * Snew)
+{
+#ifdef CONFIG_REISERFS_CHECK
+  memset (dest_bi, 0, sizeof (struct buffer_info));
+  memset (src_bi, 0, sizeof (struct buffer_info));
+#endif
+
+  /* define dest, src, dest parent, dest position */
+  switch (shift_mode) {
+  case LEAF_FROM_S_TO_L:    /* it is used in leaf_shift_left */
+    src_bi->bi_bh = PATH_PLAST_BUFFER (tb->tb_path);
+    src_bi->bi_parent = PATH_H_PPARENT (tb->tb_path, 0);
+    src_bi->bi_position = PATH_H_B_ITEM_ORDER (tb->tb_path, 0);	/* src->b_item_order */
+    dest_bi->bi_bh = tb->L[0];
+    dest_bi->bi_parent = tb->FL[0];
+    dest_bi->bi_position = get_left_neighbor_position (tb, 0);
+    *first_last = FIRST_TO_LAST;
+    break;
+
+  case LEAF_FROM_S_TO_R:  /* it is used in leaf_shift_right */
+    src_bi->bi_bh = PATH_PLAST_BUFFER (tb->tb_path);
+    src_bi->bi_parent = PATH_H_PPARENT (tb->tb_path, 0);
+    src_bi->bi_position = PATH_H_B_ITEM_ORDER (tb->tb_path, 0);
+    dest_bi->bi_bh = tb->R[0];
+    dest_bi->bi_parent = tb->FR[0];
+    dest_bi->bi_position = get_right_neighbor_position (tb, 0);
+    *first_last = LAST_TO_FIRST;
+    break;
+
+  case LEAF_FROM_R_TO_L:  /* it is used in balance_leaf_when_delete */
+    src_bi->bi_bh = tb->R[0];
+    src_bi->bi_parent = tb->FR[0];
+    src_bi->bi_position = get_right_neighbor_position (tb, 0);
+    dest_bi->bi_bh = tb->L[0];
+    dest_bi->bi_parent = tb->FL[0];
+    dest_bi->bi_position = get_left_neighbor_position (tb, 0);
+    *first_last = FIRST_TO_LAST;
+    break;
+    
+  case LEAF_FROM_L_TO_R:  /* it is used in balance_leaf_when_delete */
+    src_bi->bi_bh = tb->L[0];
+    src_bi->bi_parent = tb->FL[0];
+    src_bi->bi_position = get_left_neighbor_position (tb, 0);
+    dest_bi->bi_bh = tb->R[0];
+    dest_bi->bi_parent = tb->FR[0];
+    dest_bi->bi_position = get_right_neighbor_position (tb, 0);
+    *first_last = LAST_TO_FIRST;
+    break;
+
+  case LEAF_FROM_S_TO_SNEW:
+    src_bi->bi_bh = PATH_PLAST_BUFFER (tb->tb_path);
+    src_bi->bi_parent = PATH_H_PPARENT (tb->tb_path, 0);
+    src_bi->bi_position = PATH_H_B_ITEM_ORDER (tb->tb_path, 0);
+    dest_bi->bi_bh = Snew;
+    dest_bi->bi_parent = 0;
+    dest_bi->bi_position = 0;
+    *first_last = LAST_TO_FIRST;
+    break;
+    
+  default:
+    reiserfs_panic (0, "vs-10250: leaf_define_dest_src_infos: shift type is unknown (%d)", shift_mode);
+  }
+#ifdef CONFIG_REISERFS_CHECK
+  if (src_bi->bi_bh == 0 || dest_bi->bi_bh == 0) {
+    reiserfs_panic (0, "vs-10260: leaf_define_dest_src_etc: mode==%d, source (%p) or dest (%p) buffer is initialized incorrectly",
+		    shift_mode, src_bi->bi_bh, dest_bi->bi_bh);
+  }
+#endif
+}
+
+
+
+
+/* copy mov_num items and mov_bytes of the (mov_num-1)th item to
+   neighbor. Delete them from source */
+int leaf_move_items (struct reiserfs_transaction_handle *th,
+                     int shift_mode, struct tree_balance * tb, int mov_num, int mov_bytes, struct buffer_head * Snew)
+{
+  int ret_value;
+  struct buffer_info dest_bi, src_bi;
+  int first_last;
+
+  leaf_define_dest_src_infos (shift_mode, tb, &dest_bi, &src_bi, &first_last, Snew);
+
+  ret_value = leaf_copy_items (th, &dest_bi, src_bi.bi_bh, first_last, mov_num, mov_bytes);
+
+  leaf_delete_items (th, &src_bi, first_last, (first_last == FIRST_TO_LAST) ? 0 : 
+                    (B_NR_ITEMS(src_bi.bi_bh) - mov_num), mov_num, mov_bytes);
+
+  
+  return ret_value;
+}
+
+
+/* Shift shift_num items (and shift_bytes of last shifted item if shift_bytes != -1)
+   from S[0] to L[0] and replace the delimiting key */
+int leaf_shift_left (struct reiserfs_transaction_handle *th, struct tree_balance * tb, int shift_num, int shift_bytes)
+{
+  struct buffer_head * S0 = PATH_PLAST_BUFFER (tb->tb_path);
+  int i;
+
+  /* move shift_num (and shift_bytes bytes) items from S[0] to left neighbor L[0] */
+  i = leaf_move_items (th, LEAF_FROM_S_TO_L, tb, shift_num, shift_bytes, 0);
+
+  if ( shift_num ) {
+    if (B_NR_ITEMS (S0) == 0) { /* number of items in S[0] == 0 */
+
+#ifdef CONFIG_REISERFS_CHECK
+      if ( shift_bytes != -1 )
+	reiserfs_panic (tb->tb_sb, "vs-10270: leaf_shift_left: S0 is empty now, but shift_bytes != -1 (%d)", shift_bytes);
+
+      if (init_mode == M_PASTE || init_mode == M_INSERT) {
+	print_tb (init_mode, init_item_pos, init_pos_in_item, &init_tb, "vs-10275");
+	reiserfs_panic (tb->tb_sb, "vs-10275: leaf_shift_left: balance condition corrupted (%c)", init_mode);
+      }
+#endif
+
+      if (PATH_H_POSITION (tb->tb_path, 1) == 0)
+	replace_key(th, tb->CFL[0], tb->lkey[0], PATH_H_PPARENT (tb->tb_path, 0), 0);
+      
+      /* change right_delimiting_key field in L0's block header */
+      copy_key (B_PRIGHT_DELIM_KEY(tb->L[0]), B_PRIGHT_DELIM_KEY (S0));
+
+    } else {     
+      /* replace lkey in CFL[0] by 0-th key from S[0]; */
+      replace_key(th, tb->CFL[0], tb->lkey[0], S0, 0);
+      
+      /* change right_delimiting_key field in L0's block header */
+      copy_key (B_PRIGHT_DELIM_KEY(tb->L[0]), B_N_PKEY (S0, 0));
+#ifdef CONFIG_REISERFS_CHECK
+      if (shift_bytes != -1 && !(I_IS_DIRECTORY_ITEM (B_N_PITEM_HEAD (S0, 0))
+				 && !I_ENTRY_COUNT (B_N_PITEM_HEAD (S0, 0)))) {
+	if (!is_left_mergeable (B_N_PITEM_HEAD (S0, 0), S0->b_size)) {
+	  reiserfs_panic (tb->tb_sb, "vs-10280: leaf_shift_left: item must be mergeable");
+	}
+      }
+#endif
+    }
+  }
+  
+  return i;
+}
+
+
+
+
+
+/* CLEANING STOPPED HERE */
+
+
+
+
+/* Shift shift_num (shift_bytes) items from S[0] to the right neighbor, and replace the delimiting key */
+int	leaf_shift_right(
+		struct reiserfs_transaction_handle *th,
+		struct tree_balance * tb, 
+		int shift_num,
+		int shift_bytes
+	)
+{
+  struct buffer_head * S0 = PATH_PLAST_BUFFER (tb->tb_path);
+  int ret_value;
+
+  /* move shift_num (and shift_bytes) items from S[0] to right neighbor R[0] */
+  ret_value = leaf_move_items (th, LEAF_FROM_S_TO_R, tb, shift_num, shift_bytes, 0);
+
+  /* replace rkey in CFR[0] by the 0-th key from R[0] */
+  if (shift_num) {
+    replace_key(th, tb->CFR[0], tb->rkey[0], tb->R[0], 0);
+
+    /* change right_delimiting_key field in S0's block header */
+    copy_key (B_PRIGHT_DELIM_KEY(S0), B_N_PKEY (tb->R[0], 0));
+  }
+
+  return ret_value;
+}
+
+
+
+static void	leaf_delete_items_entirely (
+				struct reiserfs_transaction_handle *th,
+				struct buffer_info * bi,
+				int first,
+				int del_num
+			);
+/*  If del_bytes == -1, starting from position 'first' delete del_num items in whole in buffer CUR.
+    If not. 
+    If last_first == 0. Starting from position 'first' delete del_num-1 items in whole. Delete part of body of
+    the first item. Part defined by del_bytes. Don't delete first item header
+    If last_first == 1. Starting from position 'first+1' delete del_num-1 items in whole. Delete part of body of
+    the last item . Part defined by del_bytes. Don't delete last item header.
+*/
+void	leaf_delete_items (
+			struct reiserfs_transaction_handle *th,
+			struct buffer_info * cur_bi,
+			int last_first, 
+			int first, int del_num, int del_bytes
+		)
+{
+    struct buffer_head * bh;
+    struct super_block *s ;
+    int item_amount = B_NR_ITEMS (bh = cur_bi->bi_bh);
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( !bh )
+    reiserfs_panic (0, "leaf_delete_items: 10155: bh is not defined");
+
+  if ( del_num < 0 )
+    reiserfs_panic (0, "leaf_delete_items: 10160: del_num can not be < 0. del_num==%d", del_num);
+
+  if ( first < 0 || first + del_num > item_amount )
+    reiserfs_panic (0, "leaf_delete_items: 10165: invalid number of first item to be deleted (%d) or "
+            "no so much items (%d) to delete (only %d)", first, first + del_num, item_amount);
+#endif
+
+  if ( del_num == 0 )
+    return;
+
+  if ( first == 0 && del_num == item_amount && del_bytes == -1 ) {
+    make_empty_node (cur_bi);
+    s = th->t_super ;
+    journal_mark_dirty(th, s, bh);	/* not preserved, preserves are in balance_leaf() and  balance_leaf_when_delete() */
+    return;
+  }
+
+  if ( del_bytes == -1 )
+    /* delete del_num items beginning from item in position first */
+    leaf_delete_items_entirely (th, cur_bi, first, del_num);
+  else {
+      if ( last_first == FIRST_TO_LAST ) {
+	        /* delete del_num-1 items beginning from item in position first  */
+	      leaf_delete_items_entirely (th, cur_bi, first, del_num-1);
+
+	      /* delete the part of the first item of the bh
+	         do not delete item header
+	         */
+	      leaf_cut_from_buffer (th, cur_bi, 0, 0, del_bytes);
+      } else  {
+	      struct item_head * ih;
+	      int len;
+
+	      /* delete del_num-1 items beginning from item in position first+1  */
+	      leaf_delete_items_entirely (th, cur_bi, first+1, del_num-1);
+
+	      if (I_IS_DIRECTORY_ITEM(ih = B_N_PITEM_HEAD(bh, B_NR_ITEMS(bh)-1))) 	/* the last item is directory  */
+	        /* len = numbers of directory entries in this item */
+	        len = I_ENTRY_COUNT(ih);
+	      else
+	        /* len = body len of item */
+ 	        len = ih->ih_item_len;
+
+	      /* delete the part of the last item of the bh 
+	         do not delete item header
+	         */
+	      leaf_cut_from_buffer (th, cur_bi, B_NR_ITEMS(bh)-1, len - del_bytes, del_bytes);
+	  }
+  }
+}
+
+
+/* insert item into the leaf node in position before */
+void	leaf_insert_into_buf (
+			struct reiserfs_transaction_handle *th,
+			struct buffer_info * bi,
+			int before,
+			struct item_head * inserted_item_ih,
+			const char * inserted_item_body,
+			int mem_mode,
+			int zeros_number
+		)
+{
+	struct buffer_head * bh = bi->bi_bh;
+	int nr;
+	struct block_head * blkh;
+	struct item_head * ih;
+	int i;
+	int last_loc, unmoved_loc;
+	char * to;
+	struct super_block *s ;
+
+
+  nr = (blkh = B_BLK_HEAD (bh))->blk_nr_item;
+
+#ifdef CONFIG_REISERFS_CHECK
+  /* check free space */
+  if (blkh->blk_free_space < inserted_item_ih->ih_item_len + IH_SIZE)
+    reiserfs_panic (0, "leaf_insert_into_buf: 10170: not enough free space: needed %d, available %d",
+		    inserted_item_ih->ih_item_len + IH_SIZE, blkh->blk_free_space);
+  if (zeros_number > inserted_item_ih->ih_item_len)
+    reiserfs_panic (0, "vs-10172: leaf_insert_into_buf: zero number == %d, item length == %d", zeros_number, inserted_item_ih->ih_item_len);
+#endif /* CONFIG_REISERFS_CHECK */
+
+
+  /* get item new item must be inserted before */
+  ih = B_N_PITEM_HEAD (bh, before);
+
+  /* prepare space for the body of new item */
+  last_loc = nr ? ih[nr - before - 1].ih_item_location : bh->b_size;
+  unmoved_loc = before ? (ih-1)->ih_item_location : bh->b_size;
+
+  memmove (bh->b_data + last_loc - inserted_item_ih->ih_item_len, 
+	   bh->b_data + last_loc, unmoved_loc - last_loc);
+
+  to = bh->b_data + unmoved_loc - inserted_item_ih->ih_item_len;
+  memset (to, 0, zeros_number);
+  to += zeros_number;
+
+  /* copy body to prepared space */
+  if (inserted_item_body)
+    //if (mem_mode == REISERFS_USER_MEM)
+    //  copy_from_user (to, inserted_item_body, inserted_item_ih->ih_item_len - zeros_number);
+    //else {
+      memmove (to, inserted_item_body, inserted_item_ih->ih_item_len - zeros_number);
+  //}
+  else
+      memset(to, '\0', inserted_item_ih->ih_item_len - zeros_number);
+  
+  /* insert item header */
+  memmove (ih + 1, ih, IH_SIZE * (nr - before));
+  memmove (ih, inserted_item_ih, IH_SIZE);
+  
+  /* change locations */
+  for (i = before; i < nr + 1; i ++)
+    ih[i-before].ih_item_location =
+      (unmoved_loc -= ih[i-before].ih_item_len);
+  
+  /* sizes, free space, item number */
+  blkh->blk_nr_item ++;
+  blkh->blk_free_space -= (IH_SIZE + inserted_item_ih->ih_item_len);
+
+  s = th->t_super ;
+  journal_mark_dirty(th, s, bh) ;
+
+  if (bi->bi_parent) { 
+    B_N_CHILD (bi->bi_parent, bi->bi_position)->dc_size += (IH_SIZE + inserted_item_ih->ih_item_len);
+    journal_mark_dirty(th, s, bi->bi_parent) ;
+  }
+
+
+}
+
+
+/* paste paste_size bytes to affected_item_num-th item. 
+   When item is a directory, this only prepare space for new entries */
+void	leaf_paste_in_buffer (
+			struct reiserfs_transaction_handle *th,
+			struct buffer_info * bi,
+			int affected_item_num,
+			int pos_in_item,
+			int paste_size,
+			const char * body,
+			int mem_mode,
+			int zeros_number
+		)
+{
+	struct buffer_head * bh = bi->bi_bh;
+	int nr;
+	struct block_head * blkh;
+	struct item_head * ih;
+	int i;
+	int last_loc, unmoved_loc;
+	struct super_block *s ;
+
+
+  nr = (blkh = B_BLK_HEAD(bh))->blk_nr_item;
+
+#ifdef CONFIG_REISERFS_CHECK
+  /* check free space */
+  if (blkh->blk_free_space < paste_size)
+    reiserfs_panic (0, "leaf_paste_in_buffer: 10175: not enough free space: needed %d, available %d",
+		    paste_size, blkh->blk_free_space);
+  if (zeros_number > paste_size) {
+    print_tb (init_mode, init_item_pos, init_pos_in_item, &init_tb, "10177");
+    reiserfs_panic (0, "vs-10177: leaf_paste_in_buffer: zero number == %d, paste_size == %d", zeros_number, paste_size);
+  }
+#endif /* CONFIG_REISERFS_CHECK */
+
+
+  /* item to be appended */
+  ih = B_N_PITEM_HEAD(bh, affected_item_num);
+
+  last_loc = ih[nr - affected_item_num - 1].ih_item_location;
+  unmoved_loc = affected_item_num ? (ih-1)->ih_item_location : bh->b_size;  
+
+  /* prepare space */
+  memmove (bh->b_data + last_loc - paste_size, bh->b_data + last_loc,
+ 	   unmoved_loc - last_loc);
+
+
+  /* change locations */
+  for (i = affected_item_num; i < nr; i ++)
+    ih[i-affected_item_num].ih_item_location -= paste_size;
+
+  if ( body ) {
+    if (!I_IS_DIRECTORY_ITEM(ih)) {
+      //if (mem_mode == REISERFS_USER_MEM) {
+      //memset (bh->b_data + unmoved_loc - paste_size, 0, zeros_number);
+      //copy_from_user (bh->b_data + unmoved_loc - paste_size + zeros_number, body, paste_size - zeros_number);
+      //} else 
+      {
+	if (!pos_in_item) {
+	  /* shift data to right */
+	  memmove (bh->b_data + ih->ih_item_location + paste_size, 
+		   bh->b_data + ih->ih_item_location, ih->ih_item_len);
+	  /* paste data in the head of item */
+	  memset (bh->b_data + ih->ih_item_location, 0, zeros_number);
+	  memcpy (bh->b_data + ih->ih_item_location + zeros_number, body, paste_size - zeros_number);
+	} else {
+	  memset (bh->b_data + unmoved_loc - paste_size, 0, zeros_number);
+	  memcpy (bh->b_data + unmoved_loc - paste_size + zeros_number, body, paste_size - zeros_number);
+	}
+      }
+    }
+  }
+  else
+    memset(bh->b_data + unmoved_loc - paste_size,'\0',paste_size);
+
+  ih->ih_item_len += paste_size;
+
+  /* change free space */
+  blkh->blk_free_space -= paste_size;
+
+  s = th->t_super ;
+  journal_mark_dirty(th, s, bh) ;
+
+  if (bi->bi_parent) { 
+    B_N_CHILD (bi->bi_parent, bi->bi_position)->dc_size += paste_size;
+    journal_mark_dirty(th, s, bi->bi_parent); /* no need to preserve, internal node */
+    journal_mark_dirty(th, s, bi->bi_parent) ;
+  }
+}
+
+/* cuts DEL_COUNT entries beginning from FROM-th entry. Directory item
+   does not have free space, so it moves DEHs and remaining records as
+   necessary. Return value is size of removed part of directory item
+   in bytes. */
+static int	leaf_cut_entries (
+				struct buffer_head * bh,
+				struct item_head * ih, 
+				int from, 
+				int del_count
+			)
+{
+  char * item;
+  struct reiserfs_de_head * deh;
+  int prev_record_offset;	/* offset of record, that is (from-1)th */
+  char * prev_record;		/* */
+  int cut_records_len;		/* length of all removed records */
+  int i;
+
+
+#ifdef CONFIG_REISERFS_CHECK
+  /* make sure, that item is directory and there are enough entries to
+     remove */
+  if (!I_IS_DIRECTORY_ITEM (ih))
+    reiserfs_panic (0, "leaf_cut_entries: 10180: item is not directory item");
+
+  if (I_ENTRY_COUNT(ih) < from + del_count)
+    reiserfs_panic (0, "leaf_cut_entries: 10185: item contains not enough entries: entry_cout = %d, from = %d, to delete = %d",
+		    I_ENTRY_COUNT(ih), from, del_count);
+#endif
+
+  if (del_count == 0)
+    return 0;
+
+  /* first byte of item */
+  item = bh->b_data + ih->ih_item_location;
+
+  /* entry head array */
+  deh = B_I_DEH (bh, ih);
+
+  /* first byte of remaining entries, those are BEFORE cut entries
+     (prev_record) and length of all removed records (cut_records_len) */
+  prev_record_offset = (from ? deh[from - 1].deh_location : ih->ih_item_len);
+  cut_records_len = prev_record_offset/*from_record*/ - deh[from + del_count - 1].deh_location;
+  prev_record = item + prev_record_offset;
+
+
+  /* adjust locations of remaining entries */
+  for (i = I_ENTRY_COUNT(ih) - 1; i > from + del_count - 1; i --)
+    deh[i].deh_location -= (DEH_SIZE * del_count);
+
+  for (i = 0; i < from; i ++)
+    deh[i].deh_location -= DEH_SIZE * del_count + cut_records_len;
+
+  I_ENTRY_COUNT(ih) -= del_count;
+
+  /* shift entry head array and entries those are AFTER removed entries */
+  memmove ((char *)(deh + from),
+	   deh + from + del_count, 
+	   prev_record - cut_records_len - (char *)(deh + from + del_count));
+  
+  /* shift records, those are BEFORE removed entries */
+  memmove (prev_record - cut_records_len - DEH_SIZE * del_count,
+	   prev_record, item + ih->ih_item_len - prev_record);
+
+  return DEH_SIZE * del_count + cut_records_len;
+}
+
+
+/*  when cut item is part of regular file
+        pos_in_item - first byte that must be cut
+        cut_size - number of bytes to be cut beginning from pos_in_item
+ 
+   when cut item is part of directory
+        pos_in_item - number of first deleted entry
+        cut_size - count of deleted entries
+    */
+void	leaf_cut_from_buffer (
+			struct reiserfs_transaction_handle *th,
+			struct buffer_info * bi,
+			int cut_item_num,
+			int pos_in_item,
+			int cut_size
+		)
+{
+    int nr;
+    struct buffer_head * bh = bi->bi_bh;
+    struct block_head * blkh;
+    struct item_head * ih;
+    int last_loc, unmoved_loc;
+    int i;
+    struct super_block *s ;
+
+    nr = (blkh = B_BLK_HEAD (bh))->blk_nr_item;
+
+    /* item head of truncated item */
+    ih = B_N_PITEM_HEAD (bh, cut_item_num);
+
+    if (I_IS_DIRECTORY_ITEM (ih)) {
+        /* first cut entry ()*/
+        cut_size = leaf_cut_entries (bh, ih, pos_in_item, cut_size);
+        if (pos_in_item == 0) {
+	        /* change key */
+#ifdef CONFIG_REISERFS_CHECK
+            if (cut_item_num)
+                reiserfs_panic (0, "leaf_cut_from_buffer: 10190: " 
+                    "when 0-th enrty of item is cut, that item must be first in the node, not %d-th", cut_item_num);
+#endif
+            /* change item key by key of first entry in the item */
+	    ih->ih_key.k_offset = B_I_DEH (bh, ih)->deh_offset;
+            /*memcpy (&ih->ih_key.k_offset, &(B_I_DEH (bh, ih)->deh_offset), SHORT_KEY_SIZE);*/
+	    }
+    } else {
+        /* item is direct or indirect */
+#ifdef CONFIG_REISERFS_CHECK
+        if (I_IS_STAT_DATA_ITEM (ih))
+	        reiserfs_panic (0, "leaf_cut_from_buffer: 10195: item is stat data");
+
+        if (pos_in_item && pos_in_item + cut_size != ih->ih_item_len )
+            reiserfs_panic (0, "cut_from_buf: 10200: invalid offset (%lu) or trunc_size (%lu) or ih_item_len (%lu)",
+                pos_in_item, cut_size, ih->ih_item_len);
+#endif
+
+        /* shift item body to left if cut is from the head of item */
+        if (pos_in_item == 0) {
+            memmove (bh->b_data + ih->ih_item_location, bh->b_data + ih->ih_item_location + cut_size,
+                ih->ih_item_len - cut_size);
+
+            /* change key of item */
+            if (I_IS_DIRECT_ITEM(ih))
+                ih->ih_key.k_offset += cut_size;
+            else {
+                ih->ih_key.k_offset += (cut_size / UNFM_P_SIZE) * bh->b_size;
+#ifdef CONFIG_REISERFS_CHECK
+                if ( ih->ih_item_len == cut_size && ih->u.ih_free_space )
+                    reiserfs_panic (0, "leaf_cut_from_buf: 10205: invalid ih_free_space (%lu)", ih->u.ih_free_space);
+#endif
+	        }
+	    }
+    }
+  
+
+    /* location of the last item */
+    last_loc = ih[nr - cut_item_num - 1].ih_item_location;
+
+    /* location of the item, which is remaining at the same place */
+    unmoved_loc = cut_item_num ? (ih-1)->ih_item_location : bh->b_size;
+
+
+    /* shift */
+    memmove (bh->b_data + last_loc + cut_size, bh->b_data + last_loc,
+	       unmoved_loc - last_loc - cut_size);
+
+    /* change item length */
+    ih->ih_item_len -= cut_size;
+  
+    if (I_IS_INDIRECT_ITEM(ih)) {
+        if (pos_in_item)
+            ih->u.ih_free_space = 0;
+    }
+
+    /* change locations */
+    for (i = cut_item_num; i < nr; i ++)
+        ih[i-cut_item_num].ih_item_location += cut_size;
+
+    /* size, free space */
+    blkh->blk_free_space += cut_size;
+
+    s = th->t_super ;
+    /* reiserfs_mark_buffer_dirty (bh, 0); journal victim */	/* should preserve_dirt but not preserve_shifted */
+    journal_mark_dirty(th, s, bh);	/* should preserve_dirt but not preserve_shifted */
+    
+    if (bi->bi_parent) { 
+      B_N_CHILD (bi->bi_parent, bi->bi_position)->dc_size -= cut_size; 
+      /* reiserfs_mark_buffer_dirty (bi->bi_parent, 0); journal victim *//* no need to preserve, internal node */
+      journal_mark_dirty(th, s, bi->bi_parent);/* no need to preserve, internal node */
+    }
+}
+
+
+/* delete del_num items from buffer starting from the first'th item */
+static void	leaf_delete_items_entirely (
+				struct reiserfs_transaction_handle *th,
+				struct buffer_info * bi,
+				int first,
+				int del_num
+			)
+{
+	struct buffer_head * bh = bi->bi_bh;
+    int nr;
+    int i, j;
+    int last_loc, last_removed_loc;
+    struct block_head * blkh;
+    struct item_head * ih;
+    struct super_block *s ;
+
+    s = th->t_super ;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (bh == NULL)
+    reiserfs_panic (0, "leaf_delete_items_entirely: 10210: buffer is 0");
+
+  if (del_num < 0)
+    reiserfs_panic (0, "leaf_delete_items_entirely: 10215: del_num less than 0 (%d)", del_num);
+#endif /* CONFIG_REISERFS_CHECK */
+
+  if (del_num == 0)
+    return;
+
+  nr = (blkh = B_BLK_HEAD(bh))->blk_nr_item;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (first < 0 || first + del_num > nr)
+    reiserfs_panic (0, "leaf_delete_items_entirely: 10220: first=%d, number=%d, there is %d items", first, del_num, nr);
+#endif /* CONFIG_REISERFS_CHECK */
+
+  if (first == 0 && del_num == nr) {
+    /* this does not work */
+    make_empty_node (bi);
+    
+    journal_mark_dirty(th, s, bh);	/* not preserved, preserves are in balance_leaf() and  balance_leaf_when_delete() */
+    return;
+  }
+
+  ih = B_N_PITEM_HEAD (bh, first);
+  
+  /* location of unmovable item */
+  j = (first == 0) ? bh->b_size : (ih-1)->ih_item_location;
+      
+  /* delete items */
+  last_loc = ih[nr-1-first].ih_item_location;
+  last_removed_loc = ih[del_num-1].ih_item_location;
+
+  memmove (bh->b_data + last_loc + j - last_removed_loc,
+	   bh->b_data + last_loc, last_removed_loc - last_loc);
+  
+  /* delete item headers */
+  memmove (ih, ih + del_num, (nr - first - del_num) * IH_SIZE);
+  
+  /* change item location */
+  for (i = first; i < nr - del_num; i ++)
+    ih[i-first].ih_item_location += j - last_removed_loc;
+
+  /* sizes, item number */
+  blkh->blk_nr_item -= del_num;
+  blkh->blk_free_space += j - last_removed_loc + IH_SIZE * del_num;
+
+/* not preserved, preserves are in balance_leaf() and  balance_leaf_when_delete() */
+  journal_mark_dirty(th, s, bh);
+  
+  if (bi->bi_parent) {
+    B_N_CHILD (bi->bi_parent, bi->bi_position)->dc_size -= j - last_removed_loc + IH_SIZE * del_num;
+    journal_mark_dirty(th, s, bi->bi_parent); /* not preserved, internal node */
+  }
+}
+
+
+
+
+
+/* paste new_entry_count entries (new_dehs, records) into position before to item_num-th item */
+void    leaf_paste_entries (
+			struct buffer_head * bh,
+			int item_num,
+			int before,
+			int new_entry_count,
+			struct reiserfs_de_head * new_dehs,
+			const char * records,
+			int paste_size
+		)
+{
+    struct item_head * ih;
+    char * item;
+    struct reiserfs_de_head * deh;
+    char * insert_point;
+    int i, old_entry_num;
+
+    if (new_entry_count == 0)
+        return;
+
+    ih = B_N_PITEM_HEAD(bh, item_num);
+
+#ifdef CONFIG_REISERFS_CHECK
+  /* make sure, that item is directory, and there are enough records in it */
+  if (!I_IS_DIRECTORY_ITEM (ih))
+    reiserfs_panic (0, "leaf_paste_entries: 10225: item is not directory item");
+
+  if (I_ENTRY_COUNT (ih) < before)
+    reiserfs_panic (0, "leaf_paste_entries: 10230: there are no entry we paste entries before. entry_count = %d, before = %d",
+		    I_ENTRY_COUNT (ih), before);
+#endif
+
+
+  /* first byte of dest item */
+  item = bh->b_data + ih->ih_item_location;
+
+  /* entry head array */
+  deh = B_I_DEH (bh, ih);
+
+  /* new records will be pasted at this point */
+  insert_point = item + (before ? deh[before - 1].deh_location : (ih->ih_item_len - paste_size));
+
+  /* adjust locations of records that will be AFTER new records */
+  for (i = I_ENTRY_COUNT(ih) - 1; i >= before; i --)
+    deh[i].deh_location += DEH_SIZE * new_entry_count;
+
+  /* adjust locations of records that will be BEFORE new records */
+  for (i = 0; i < before; i ++)
+    deh[i].deh_location += paste_size;
+
+  old_entry_num = I_ENTRY_COUNT(ih);
+  I_ENTRY_COUNT(ih) += new_entry_count;
+
+  /* prepare space for pasted records */
+  memmove (insert_point + paste_size, insert_point, item + (ih->ih_item_len - paste_size) - insert_point);
+
+  /* copy new records */
+  memcpy (insert_point + DEH_SIZE * new_entry_count, records,
+		   paste_size - DEH_SIZE * new_entry_count);
+  
+  /* prepare space for new entry heads */
+  deh += before;
+  memmove ((char *)(deh + new_entry_count), deh, insert_point - (char *)deh);
+
+  /* copy new entry heads */
+  deh = (struct reiserfs_de_head *)((char *)deh);
+  memcpy (deh, new_dehs, DEH_SIZE * new_entry_count);
+
+  /* set locations of new records */
+  for (i = 0; i < new_entry_count; i ++)
+    deh[i].deh_location += 
+      (- new_dehs[new_entry_count - 1].deh_location + insert_point + DEH_SIZE * new_entry_count - item);
+
+
+  /* change item key if neccessary (when we paste before 0-th entry */
+  if (!before)
+    {
+#ifdef CONFIG_REISERFS_CHECK
+/*
+      if ( old_entry_num && COMP_SHORT_KEYS ((unsigned long *)&ih->ih_key.k_offset,
+					     &(new_dehs->deh_offset)) <= 0)
+	reiserfs_panic (0, "leaf_paste_entries: 10235: new key must be less, that old key");
+*/
+#endif
+      ih->ih_key.k_offset = new_dehs->deh_offset;
+/*      memcpy (&ih->ih_key.k_offset, 
+		       &new_dehs->deh_offset, SHORT_KEY_SIZE);*/
+    }
+
+#ifdef CONFIG_REISERFS_CHECK
+  {
+    int prev, next;
+    /* check record locations */
+    deh = B_I_DEH (bh, ih);
+    for (i = 0; i < I_ENTRY_COUNT(ih); i ++) {
+      next = (i < I_ENTRY_COUNT(ih) - 1) ? deh[i + 1].deh_location : 0;
+      prev = (i != 0) ? deh[i - 1].deh_location : 0;
+      
+      if (prev && prev <= deh[i].deh_location)
+	reiserfs_warning ("vs-10240: leaf_paste_entries: directory item corrupted (%d %d)\n", prev, deh[i].deh_location);
+      if (next && next >= deh[i].deh_location)
+	reiserfs_warning ("vs-10250: leaf_paste_entries: directory item corrupted (%d %d)\n", prev, deh[i].deh_location);
+    }
+  }
+#endif
+
+}
+
+
+
diff -urN linux/fs/reiserfs/namei.c /tmp/linux/fs/reiserfs/namei.c
--- linux/fs/reiserfs/namei.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/namei.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,1044 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+#ifdef __KERNEL__
+
+#include <linux/sched.h>
+#include <linux/reiserfs_fs.h>
+
+#else
+
+#include "nokernel.h"
+
+#endif
+
+
+
+#define MAX_GEN_NUMBER  127
+
+
+#define SET_GENERATION_NUMBER(offset,gen_number) (GET_HASH_VALUE(offset)|(gen_number))
+
+
+
+/* Keyed 32-bit hash function using TEA in a Davis-Meyer function */
+static unsigned long get_third_component (struct super_block * s, 
+					  const char * name, int len)
+{
+  unsigned long res;
+
+  if (!len || (len == 1 && name[0] == '.'))
+    return DOT_OFFSET;
+  if (len == 2 && name[0] == '.' && name[1] == '.')
+    return DOT_DOT_OFFSET;
+
+  res = s->u.reiserfs_sb.s_hash_function (name, len);
+
+  res = GET_HASH_VALUE(res);
+  if (res == 0)
+    res = 128;
+  return res + MAX_GEN_NUMBER;
+}
+
+
+/* fills the structure with various parameters of directory entry,
+   including key of the pointed object */
+static void get_entry_attributes (struct reiserfs_dir_entry * de, int entry_num)
+{
+#ifdef CONFIG_REISERFS_CHECK
+  if (I_ENTRY_COUNT (de->de_ih) < entry_num)
+    reiserfs_panic (0, "yr-7006: get_entry_attributes: no such entry (%d-th) in the item (%d)",
+		    entry_num, I_ENTRY_COUNT (de->de_ih));
+  if (de->de_deh != B_I_DEH (de->de_bh, de->de_ih) + entry_num)
+    reiserfs_panic (0, "yr-7008: get_entry_attributes: dir entry header not found");
+    
+#endif /* CONFIG_REISERFS_CHECK */
+
+  /* few fields are set already (de_bh, de_item_num, de_deh) */
+  de->de_entrylen = I_DEH_N_ENTRY_LENGTH (de->de_ih, de->de_deh, entry_num);
+  de->de_namelen = de->de_entrylen - (de_with_sd (de->de_deh) ? SD_SIZE : 0);
+
+  de->de_name = B_I_PITEM (de->de_bh, de->de_ih) + de->de_deh->deh_location;
+
+  /* key of object pointed by entry */
+  de->de_dir_id = de->de_deh->deh_dir_id;
+  de->de_objectid = de->de_deh->deh_objectid;
+
+  /* key of the entry */
+  memcpy (&(de->de_entry_key.k_dir_id), &(de->de_ih->ih_key), SHORT_KEY_SIZE);
+  de->de_entry_key.k_offset = de->de_deh->deh_offset;
+  de->de_entry_key.k_uniqueness = DIRENTRY_UNIQUENESS;
+
+}
+
+
+static int try_name (struct reiserfs_dir_entry * de, 
+		     const char * name,
+		     int          namelen)
+{
+  int retval = POSITION_NOT_FOUND;
+
+  if ((namelen == de->de_namelen) &&
+      !memcmp(de->de_name, name, de->de_namelen))
+    retval = de_visible (de->de_deh) ? POSITION_FOUND : POSITION_FOUND_INVISIBLE;
+
+  return retval;
+}
+
+
+/* after this function de_entry_num is set correctly only if name
+   found or there was no entries with given hash value */
+static int linear_search_in_dir_item (struct key * key, struct reiserfs_dir_entry * de, const char * name, int namelen)
+{
+  int retval;
+  int i;
+
+  i = de->de_entry_num;
+
+  if (i == I_ENTRY_COUNT (de->de_ih) ||
+      GET_HASH_VALUE (de->de_deh[i].deh_offset) != GET_HASH_VALUE (key->k_offset)) {
+    i --;
+  }
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (de->de_deh != B_I_DEH (de->de_bh, de->de_ih))
+    reiserfs_panic (0, "vs-7010: linear_search_in_dir_item: array of entry headers not found");
+#endif /* CONFIG_REISERFS_CHECK */
+
+  de->de_deh += i;
+
+  for (; i >= 0; i --, de->de_deh --) {
+    if (GET_HASH_VALUE (de->de_deh->deh_offset) != GET_HASH_VALUE (key->k_offset)) {
+      return POSITION_NOT_FOUND;
+    }
+   
+    /* mark, that this generation number is used */
+    if (de->de_gen_number_bit_string)
+      set_bit (GET_GENERATION_NUMBER (de->de_deh->deh_offset), de->de_gen_number_bit_string);
+    
+    /* de_bh, de_item_num, de_ih, de_deh are already set. Set others fields */
+    get_entry_attributes (de, i);
+    if ((retval = try_name (de, name, namelen)) != POSITION_NOT_FOUND) {
+      de->de_entry_num = i;
+      return retval;
+    }
+  }
+
+  if (GET_GENERATION_NUMBER (de->de_ih->ih_key.k_offset) == 0)
+    return POSITION_NOT_FOUND;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if (de->de_ih->ih_key.k_offset <= DOT_DOT_OFFSET || de->de_item_num != 0)
+    reiserfs_panic (0, "vs-7015: linear_search_in_dir_item: item must be 0-th item in block (%d)", de->de_item_num);
+#endif /* CONFIG_REISERFS_CHECK */
+
+  return GOTO_PREVIOUS_ITEM;
+}
+
+
+static int reiserfs_find_entry (struct inode * dir, const char * name, int namelen, struct path * path_to_entry, struct reiserfs_dir_entry * de)
+{
+  struct key key_to_search;
+  int repeat;
+  int retval;
+
+  if (!dir || !dir->i_sb)
+    return POSITION_NOT_FOUND;
+
+  if ((unsigned int)namelen > REISERFS_MAX_NAME_LEN (dir->i_sb->s_blocksize))
+    return POSITION_NOT_FOUND;
+
+  /* there are no entries having the same third component of key, so
+     fourth key component is not used */
+  copy_key (&key_to_search, INODE_PKEY (dir));
+  key_to_search.k_offset = get_third_component (dir->i_sb, name, namelen);
+  key_to_search.k_uniqueness = DIRENTRY_UNIQUENESS;
+
+  while (1) {
+    /* search for a directory item using the formed key */
+    if (search_by_key (dir->i_sb, &key_to_search, path_to_entry, &repeat, DISK_LEAF_NODE_LEVEL, READ_BLOCKS) == ITEM_NOT_FOUND) {
+      /* take previous item */
+#ifdef CONFIG_REISERFS_CHECK
+      if (!PATH_LAST_POSITION (path_to_entry))
+	reiserfs_panic (dir->i_sb, "vs-7010: reiserfs_find_entry: search_by_key returned bad position == 0");
+#endif /* CONFIG_REISERFS_CHECK */
+      PATH_LAST_POSITION (path_to_entry) --;
+    }
+    
+    de->de_bh = PATH_PLAST_BUFFER (path_to_entry);
+    de->de_item_num = PATH_LAST_POSITION (path_to_entry);
+    de->de_ih = B_N_PITEM_HEAD (de->de_bh, de->de_item_num);
+    de->de_deh = B_I_DEH (de->de_bh, de->de_ih);
+
+#ifdef CONFIG_REISERFS_CHECK
+    if (!I_IS_DIRECTORY_ITEM (de->de_ih) || COMP_SHORT_KEYS (&(de->de_ih->ih_key), INODE_PKEY (dir)))
+      reiserfs_panic (dir->i_sb, "vs-7020: reiserfs_find_entry: item must be an item of the same directory item as inode");
+#endif /* CONFIG_REISERFS_CHECK */
+
+    /* we do not check whether bin_search_in_dir_item found the given key, even if so, we still have
+       to compare names */
+    bin_search_in_dir_item (de->de_ih, de->de_deh, &key_to_search, &(de->de_entry_num));
+
+    /* compare names for all entries having given hash value */
+    retval = linear_search_in_dir_item (&key_to_search, de, name, namelen);
+    if (retval != GOTO_PREVIOUS_ITEM)
+      /* there is no need to scan directory anymore. Given entry found or does not exist */
+      return retval;
+
+    /* there is left neighboring item of this directory and given entry can be there */
+    key_to_search.k_offset = de->de_ih->ih_key.k_offset - 1;
+    pathrelse (path_to_entry);
+
+  } /* while (1) */
+}
+
+
+/* add entry to the directory (entry can be hidden). Does not mark dir
+   inode dirty, do it after successesfull call to it */
+static int reiserfs_add_entry (struct reiserfs_transaction_handle *th, struct inode * dir, 
+                               const char * name, int namelen, struct key * object_key, struct reiserfs_dir_entry * de,
+			       int visible
+			       )
+{
+  struct key entry_key;
+  char * buffer;
+  char small_buf[32 + DEH_SIZE] ;
+  int buflen;
+  struct reiserfs_de_head * deh;
+  struct path path;
+  char bit_string [MAX_GEN_NUMBER / 8 + 1];
+  int gen_number;
+  int repeat;
+
+  init_path (&path);
+
+  if (!dir || !dir->i_sb)
+    return -ENOENT;
+
+  if ((unsigned int)namelen > REISERFS_MAX_NAME_LEN (dir->i_sb->s_blocksize))
+    return -ENAMETOOLONG;
+
+  /* each entry has unique key. compose it */
+  copy_key (&entry_key, INODE_PKEY(dir));
+  entry_key.k_offset = get_third_component (dir->i_sb, name, namelen);
+  entry_key.k_uniqueness = DIRENTRY_UNIQUENESS;
+
+  /* get memory for composing the entry */
+  buflen = DEH_SIZE + namelen;
+  if (buflen > sizeof(small_buf)) {
+    buffer = reiserfs_kmalloc (buflen, GFP_BUFFER, dir->i_sb);
+    if (buffer == 0)
+      return -ENOMEM;
+  } else {
+    buffer = small_buf ;
+  }
+
+  /* fill buffer : directory entry head, name[, dir objectid | , stat data | ,stat data, dir objectid ] */
+  deh = (struct reiserfs_de_head *)buffer;
+  deh->deh_location = 0;
+  deh->deh_offset = entry_key.k_offset;
+  deh->deh_state = 0;
+  /* put key (ino analog) to de */
+  deh->deh_dir_id = object_key->k_dir_id;
+  deh->deh_objectid = object_key->k_objectid;
+
+  /* copy name */
+  memcpy ((char *)(deh + 1), name, namelen);
+
+  /* entry is ready to be pasted into tree, set 'visibility' and 'stat data in entry' attributes */
+  mark_de_without_sd (deh);
+  visible ? mark_de_visible (deh) : mark_de_hidden (deh);
+
+  /* find the proper place for the new entry */
+  memset (bit_string, 0, sizeof (bit_string));
+  de->de_gen_number_bit_string = bit_string;
+  if (reiserfs_find_entry (dir, name, namelen, &path, de) == POSITION_FOUND) {
+    reiserfs_panic (dir->i_sb, "vs-7030: reiserfs_add_entry: entry with this key %k already exists",
+		    &entry_key);
+  }
+
+  if (find_first_nonzero_bit (bit_string, MAX_GEN_NUMBER + 1) < MAX_GEN_NUMBER + 1) {
+    /* there are few names with given hash value */
+    gen_number = find_first_zero_bit (bit_string, MAX_GEN_NUMBER + 1);
+    if (gen_number > MAX_GEN_NUMBER) {
+      /* there is no free generation number */
+      if (buffer != small_buf) 
+        reiserfs_kfree (buffer, buflen, dir->i_sb);
+      pathrelse (&path);
+      return -EHASHCOLLISION;
+    }
+    /* adjust offset of directory enrty */
+    deh->deh_offset = SET_GENERATION_NUMBER (deh->deh_offset, gen_number);
+    entry_key.k_offset = deh->deh_offset;
+
+    /* find place for new entry */
+    if (search_by_entry_key (dir->i_sb, &entry_key, &path, &(de->de_entry_num), &repeat)) {
+      reiserfs_panic (dir->i_sb, "reiserfs_add_entry: 7032: entry with this key (%k) already exists", &entry_key);
+    }
+  } else {
+    deh->deh_offset = SET_GENERATION_NUMBER (deh->deh_offset, 0);
+    entry_key.k_offset = deh->deh_offset;    
+  }
+  
+  /* perform the insertion of the entry that we have prepared */
+  if (reiserfs_paste_into_item (th, dir->i_sb, &path, &(de->de_entry_num), &entry_key, buffer, 
+                                buflen, REISERFS_KERNEL_MEM, 0) == -1) {
+    if (buffer != small_buf)
+      reiserfs_kfree (buffer, buflen, dir->i_sb);
+    return -ENOSPC;
+  }
+
+  if (buffer != small_buf)
+    reiserfs_kfree (buffer, buflen, dir->i_sb);
+  dir->i_size += buflen;
+  dir->i_blocks = ((dir->i_size + 511) >> 9);
+  dir->i_mtime = dir->i_ctime = CURRENT_TIME;
+  return 0;
+}
+
+
+struct dentry * reiserfs_lookup (struct inode * dir, struct dentry * dentry)
+{
+  struct inode * inode = 0;
+  struct reiserfs_dir_entry de;
+  struct path path_to_entry;
+  int error;
+
+  init_path (&path_to_entry);
+
+  de.de_gen_number_bit_string = 0;
+  error = reiserfs_find_entry (dir, dentry->d_name.name, dentry->d_name.len, &path_to_entry, &de);
+  pathrelse (&path_to_entry);
+  if (error == POSITION_FOUND) {
+    inode = reiserfs_iget (dir->i_sb, (struct key *)&(de.de_dir_id));
+    if (!inode)
+      return ERR_PTR(-EACCES);
+  }
+
+  d_add(dentry, inode);
+  return NULL;
+}
+
+
+int reiserfs_create (struct inode * dir, struct dentry *dentry, int mode)
+{
+  int error;
+  struct inode * inode;
+  struct reiserfs_dir_entry de;
+  int windex ;
+  int jbegin_count = JOURNAL_PER_BALANCE_CNT * 2 ;
+  struct reiserfs_transaction_handle th ;
+  int err;
+	
+	
+  inode = get_empty_inode() ;
+  if (!inode) {
+    return -ENOSPC ;
+  }
+  journal_begin(&th, dir->i_sb, jbegin_count) ;
+  th.t_caller = "create" ;
+  windex = push_journal_writer("reiserfs_create") ;
+  inode = reiserfs_new_inode (&th, dir, mode, 0, dentry, inode, &err);
+  if (!inode) {
+    pop_journal_writer(windex) ;
+    journal_end(&th, dir->i_sb, jbegin_count) ;
+    return err;
+  }
+  reiserfs_update_inode_transaction(inode) ;
+  reiserfs_update_inode_transaction(dir) ;
+	
+  inode->i_op = &reiserfs_file_inode_operations;
+  inode->i_mode = mode;
+
+  error = reiserfs_add_entry (&th, dir, dentry->d_name.name, dentry->d_name.len, INODE_PKEY (inode), &de, 1);
+  if (error) {
+    inode->i_nlink--;
+    if_in_ram_update_sd (&th, inode);
+    pop_journal_writer(windex) ;
+    journal_end(&th, dir->i_sb, jbegin_count) ;
+    iput (inode);
+    return error;
+  }
+  if_in_ram_update_sd (&th, dir); 
+  d_instantiate(dentry, inode);
+  pop_journal_writer(windex) ;
+  journal_end(&th, dir->i_sb, jbegin_count) ;
+  return 0;
+}
+
+
+int reiserfs_mknod (struct inode * dir, struct dentry *dentry, int mode, int rdev)
+{
+  int error;
+  struct inode * inode;
+  struct reiserfs_dir_entry de;
+  int windex ;
+  struct reiserfs_transaction_handle th ;
+  int jbegin_count = JOURNAL_PER_BALANCE_CNT * 3; 
+  int err;
+
+
+  inode = get_empty_inode() ;
+  if (!inode) {
+    return -ENOSPC ;
+  }
+  journal_begin(&th, dir->i_sb, jbegin_count) ;
+  windex = push_journal_writer("reiserfs_mknod") ;
+
+
+  inode = reiserfs_new_inode (&th, dir, mode, 0, dentry, inode, &err);
+  if (!inode) {
+    pop_journal_writer(windex) ;
+    journal_end(&th, dir->i_sb, jbegin_count) ;
+    return err;
+  }
+  reiserfs_update_inode_transaction(inode) ;
+  reiserfs_update_inode_transaction(dir) ;
+
+  inode->i_uid = current->fsuid;
+  inode->i_mode = mode;
+  inode->i_op = NULL;
+
+  if (S_ISREG(inode->i_mode))
+    inode->i_op = &reiserfs_file_inode_operations;
+  else if (S_ISCHR(inode->i_mode))
+    inode->i_op = &chrdev_inode_operations;
+  else if (S_ISBLK(inode->i_mode))
+    inode->i_op = &blkdev_inode_operations;
+  else if (S_ISFIFO(inode->i_mode))
+    init_fifo(inode);
+  if (S_ISBLK(mode) || S_ISCHR(mode))
+    inode->i_rdev = to_kdev_t(rdev);
+
+  if_in_ram_update_sd (&th, inode);
+
+  error = reiserfs_add_entry (&th, dir, dentry->d_name.name, dentry->d_name.len, INODE_PKEY (inode), &de, 1);
+  if (error) {
+    inode->i_nlink--;
+    if_in_ram_update_sd (&th, inode);
+    pop_journal_writer(windex) ;
+    journal_end(&th, dir->i_sb, jbegin_count) ;
+    iput (inode);
+    return error;
+  }
+  if_in_ram_update_sd (&th, dir);
+  d_instantiate(dentry, inode);
+  pop_journal_writer(windex) ;
+  journal_end(&th, dir->i_sb, jbegin_count) ;
+  return 0;
+}
+
+
+int reiserfs_mkdir (struct inode * dir, struct dentry *dentry, int mode)
+{
+  int error;
+  struct inode * inode;
+  struct reiserfs_dir_entry de;
+  int windex ;
+  struct reiserfs_transaction_handle th ;
+  int jbegin_count = JOURNAL_PER_BALANCE_CNT * 3; 
+  int err;
+
+
+  inode = get_empty_inode() ;
+  if (!inode) {
+    return -ENOSPC ;
+  }
+
+  journal_begin(&th, dir->i_sb, jbegin_count) ;
+  windex = push_journal_writer("reiserfs_mkdir") ;
+  
+  if (dir->i_nlink >= REISERFS_LINK_MAX) {
+    pop_journal_writer(windex) ;
+    journal_end(&th, dir->i_sb, jbegin_count) ;
+    iput(inode) ;
+    return -EMLINK;
+  }
+  
+  mode = S_IFDIR | (mode & (S_IRWXUGO|S_ISVTX) & ~current->fs->umask);
+  if (dir->i_mode & S_ISGID)
+    mode |= S_ISGID;
+  inode = reiserfs_new_inode (&th, dir, mode, 0, dentry, inode, &err);
+  if (!inode) {
+    pop_journal_writer(windex) ;
+    journal_end(&th, dir->i_sb, jbegin_count) ;
+    return err;
+  }
+  reiserfs_update_inode_transaction(inode) ;
+  reiserfs_update_inode_transaction(dir) ;
+
+  inode->i_op = &reiserfs_dir_inode_operations;
+
+  /* new inode and stat data are uptodate. Inode is clean. */
+  error = reiserfs_add_entry (&th, dir, dentry->d_name.name, dentry->d_name.len, INODE_PKEY (inode), &de, 1);
+  if (error) {
+    inode->i_nlink = 0;
+    if_in_ram_update_sd (&th, inode);
+    pop_journal_writer(windex) ;
+    journal_end(&th, dir->i_sb, jbegin_count) ;
+    iput (inode);
+    return error;
+  }
+
+  /* update dir inode, reiserfs_add_entry does not do that */
+  dir->i_nlink ++;
+  if_in_ram_update_sd (&th, dir);
+  d_instantiate(dentry, inode);
+  pop_journal_writer(windex) ;
+  journal_end(&th, dir->i_sb, jbegin_count) ;
+  return 0;
+}
+
+
+static int reiserfs_empty_dir(struct inode * inode)
+{
+  return inode->i_size == EMPTY_DIR_SIZE;
+}
+
+
+static int rmdir_not_allowed (struct inode * dir, struct reiserfs_dir_entry * de, struct dentry * dentry)
+{
+  if (!reiserfs_empty_dir (dentry->d_inode))
+    return -ENOTEMPTY;
+
+  if (de->de_objectid != dentry->d_inode->i_ino)
+    return -ENOENT;
+
+  /* where did this come from?  ext2 doesn't check this, seems to be 
+  ** wrong
+  */
+#if 0
+  if (!list_empty(&dentry->d_hash))
+    return -EBUSY;
+#endif
+
+  return 0;
+}
+
+
+int reiserfs_rmdir (struct inode * dir, struct dentry *dentry)
+{
+  struct inode * inode;
+  int retval;
+  struct reiserfs_dir_entry de;
+  struct path path;
+  struct super_block *s ;
+  int windex ;
+  struct reiserfs_transaction_handle th ;
+  int jbegin_count = JOURNAL_PER_BALANCE_CNT * 3; 
+
+  init_path (&path);
+
+  retval = -ENOENT;
+  de.de_gen_number_bit_string = 0;
+  journal_begin(&th, dir->i_sb, jbegin_count) ;
+  windex = push_journal_writer("reiesrfs_rmdir") ;
+  if (reiserfs_find_entry (dir, dentry->d_name.name, dentry->d_name.len, &path, &de) == POSITION_NOT_FOUND)
+    goto end_rmdir;
+
+  inode = dentry->d_inode;
+
+  /* we don't need call this so early here, I'm just being cautious */
+  reiserfs_update_inode_transaction(inode) ;
+  reiserfs_update_inode_transaction(dir) ;
+
+  retval = rmdir_not_allowed (dir, &de, dentry);
+  if (retval)
+    goto end_rmdir;
+
+  /* cut entry from dir directory */
+  if (reiserfs_cut_from_item (&th, dir, dir->i_sb, &path, &(de.de_entry_num), &(de.de_entry_key), 0, NOTHING_SPECIAL) == 0) {
+    retval = -ENOENT;
+  }
+  if (retval)
+    goto end_rmdir;
+
+  if (inode->i_nlink != 2)
+    printk ("reiserfs_rmdir: empty directory has nlink != 2 (%d)\n", inode->i_nlink);
+  inode->i_nlink = 0;
+  inode->i_ctime = dir->i_ctime = dir->i_mtime = CURRENT_TIME;
+  if_in_ram_update_sd (&th, inode);
+  dir->i_nlink --;
+  dir->i_size -= (DEH_SIZE + de.de_entrylen);
+  if_in_ram_update_sd (&th, dir);
+
+  s = dir->i_sb ;
+  pop_journal_writer(windex) ;
+  journal_end(&th, s, jbegin_count) ;
+  d_delete(dentry); /* note, we've moved this after the journal end */
+  return 0;
+	
+end_rmdir:
+  /* we must release path, because we did not call reiserfs_cut_from_item, or reiserfs_cut_from_item
+     does not release path if operation was not complete */
+  pathrelse (&path);
+  pop_journal_writer(windex) ;
+  journal_end(&th, dir->i_sb, jbegin_count) ;
+  return retval;	
+}
+
+
+int reiserfs_unlink (struct inode * dir, struct dentry *dentry)
+{
+  int retval;
+  struct inode * inode;
+  struct reiserfs_dir_entry de;
+  struct path path;
+  int windex ;
+  int call_journal_end = 1 ;
+  struct reiserfs_transaction_handle th ;
+  int jbegin_count = JOURNAL_PER_BALANCE_CNT * 3; 
+
+  init_path (&path);
+
+  retval = -ENOENT;
+	
+  journal_begin(&th, dir->i_sb, jbegin_count) ;
+  windex = push_journal_writer("reiserfs_unlink") ;
+	
+  de.de_gen_number_bit_string = 0;
+  if (reiserfs_find_entry (dir, dentry->d_name.name, dentry->d_name.len, &path, &de) == POSITION_NOT_FOUND) {
+    goto end_unlink;
+  }
+
+  inode = dentry->d_inode;
+
+  reiserfs_update_inode_transaction(inode) ;
+  reiserfs_update_inode_transaction(dir) ;
+
+  if (comp_short_keys ((struct key *)&(de.de_dir_id), INODE_PKEY (inode))) {
+    goto end_unlink;
+  }
+  
+  if (!inode->i_nlink) {
+    printk("reiserfs_unlink: deleting nonexistent file (%s:%lu), %d\n",
+	   kdevname(inode->i_dev), inode->i_ino, inode->i_nlink);
+    inode->i_nlink = 1;
+  }
+  if (reiserfs_cut_from_item (&th, dir, dir->i_sb, &path, &(de.de_entry_num), &(de.de_entry_key), 0, NOTHING_SPECIAL) == 0) {
+    retval = -ENOENT;
+    goto end_unlink;
+  }
+
+  inode->i_nlink--;
+  inode->i_ctime = CURRENT_TIME;
+  if_in_ram_update_sd (&th, inode);
+
+  dir->i_size -= (de.de_entrylen + DEH_SIZE);
+  dir->i_blocks = ((dir->i_size + 511) >> 9);
+  dir->i_ctime = dir->i_mtime = CURRENT_TIME;
+  if_in_ram_update_sd (&th, dir) ;
+  pop_journal_writer(windex) ;
+  journal_end(&th, dir->i_sb, jbegin_count) ;
+  call_journal_end = 0 ;
+  d_delete(dentry); 
+  retval = 0;
+
+end_unlink:
+  pathrelse (&path);
+  pop_journal_writer(windex) ;
+  if (call_journal_end) 
+    journal_end(&th, dir->i_sb, jbegin_count) ;
+  return retval;
+}
+
+
+int reiserfs_symlink (struct inode * dir, struct dentry * dentry, const char * symname)
+{
+  struct inode * inode;
+  struct reiserfs_dir_entry de;
+  int error;
+  int windex ;
+  struct reiserfs_transaction_handle th ;
+  int jbegin_count = JOURNAL_PER_BALANCE_CNT * 3; 
+  int err;
+
+  if (strlen (symname) + 1 + SD_SIZE > MAX_ITEM_LEN (dir->i_sb->s_blocksize)) {
+    // FIXME: shouldn't we truncate it instead
+    return -ENAMETOOLONG;
+  }
+  inode = get_empty_inode() ;
+  if (!inode) {
+    return -ENOSPC ;
+  }
+ 
+  journal_begin(&th, dir->i_sb, jbegin_count) ;
+  windex = push_journal_writer("reiserfs_symlink") ;
+  inode = reiserfs_new_inode (&th, dir, S_IFLNK, symname, dentry, inode, &err);
+  if (inode == 0) {
+    pop_journal_writer(windex) ;
+    journal_end(&th, dir->i_sb, jbegin_count) ;
+    return err;
+  }
+  reiserfs_update_inode_transaction(inode) ;
+  reiserfs_update_inode_transaction(dir) ;
+
+  inode->i_op = &reiserfs_symlink_inode_operations;
+  inode->i_size = strlen (symname);
+  inode->i_mode = S_IFLNK | S_IRWXUGO;
+  if_in_ram_update_sd (&th, inode);
+
+  error = reiserfs_add_entry (&th, dir, dentry->d_name.name, dentry->d_name.len, INODE_PKEY (inode), &de, 1);
+  if (error) {
+    inode->i_nlink--;
+    if_in_ram_update_sd (&th, inode);
+    pop_journal_writer(windex) ;
+    journal_end(&th, dir->i_sb, jbegin_count) ;
+    iput (inode);
+    return error;
+  }
+  if_in_ram_update_sd (&th, dir);
+  d_instantiate(dentry, inode);
+  pop_journal_writer(windex) ;
+  journal_end(&th, dir->i_sb, jbegin_count) ;
+  return 0;
+}
+
+
+int reiserfs_link (struct dentry * old_dentry, struct inode * dir, struct dentry * dentry)
+{
+  struct inode *inode = old_dentry->d_inode;
+  struct path path_to_entry;
+  struct reiserfs_dir_entry de;
+  int error;
+  int windex ;
+  struct reiserfs_transaction_handle th ;
+  int jbegin_count = JOURNAL_PER_BALANCE_CNT * 3; 
+  
+  init_path (&path_to_entry);
+
+  /* object must not be directory */
+  if (S_ISDIR(inode->i_mode)) {
+    return -EPERM;
+  }
+  
+  /* file has too many links */
+  if (inode->i_nlink >= REISERFS_LINK_MAX) {
+    return -EMLINK;
+  }
+
+  journal_begin(&th, dir->i_sb, jbegin_count) ;
+  windex = push_journal_writer("reiserfs_link") ;
+
+  reiserfs_update_inode_transaction(inode) ;
+  reiserfs_update_inode_transaction(dir) ;
+
+  de.de_gen_number_bit_string = 0;
+  if (reiserfs_find_entry (dir, dentry->d_name.name, dentry->d_name.len, &path_to_entry, &de) == POSITION_FOUND) {
+    pathrelse (&path_to_entry);
+    pop_journal_writer(windex) ;
+    journal_end(&th, dir->i_sb, jbegin_count) ;
+    return -EEXIST;
+  }
+  
+  pathrelse (&path_to_entry);
+
+  /* free preserve list if we should */
+/*  maybe_free_preserve_list (dir->i_sb);*/
+
+  /* create new entry */
+  error = reiserfs_add_entry (&th, dir, dentry->d_name.name, dentry->d_name.len, INODE_PKEY (inode), &de, 1);
+  if (error) {
+    pop_journal_writer(windex) ;
+    journal_end(&th, dir->i_sb, jbegin_count) ;
+    return error;
+  }
+  inode->i_nlink++;
+  inode->i_ctime = CURRENT_TIME;
+  if_in_ram_update_sd (&th, inode);
+  if_in_ram_update_sd (&th, dir);
+  inode->i_count++;
+  d_instantiate(dentry, inode);
+  pop_journal_writer(windex) ;
+  journal_end(&th, dir->i_sb, jbegin_count) ;
+  return 0;
+}
+
+
+static int de_still_valid (const char * name, int len, struct reiserfs_dir_entry * de)
+{
+  struct item_head * ih;
+  struct reiserfs_de_head * deh;
+
+  if (!de || !de->de_bh)
+    return 0;
+
+  deh = B_I_DEH (de->de_bh, ih = B_N_PITEM_HEAD (de->de_bh, de->de_item_num));
+  /* compare dir entry headers, record loacation and names */
+  if (memcmp (&(deh[de->de_entry_num]), de->de_deh, DEH_SIZE) ||
+      B_I_E_NAME (de->de_entry_num, de->de_bh, ih) != de->de_name ||
+      memcmp (name, de->de_name, len))
+    return 0;
+  return 1;
+}
+
+
+static int entry_points_to_object (const char * name, int len, struct reiserfs_dir_entry * de, struct inode * inode)
+{
+  if (!de_still_valid (name, len, de))
+    return 0;
+
+  if (inode) {
+    if (!de_visible (de->de_deh))
+      reiserfs_panic (0, "vs-7042: entry_points_to_object: entry must be visible");
+    return (de->de_objectid == inode->i_ino) ? 1 : 0;
+  }
+
+  /* this must be added hidden entry */
+  if (de_visible (de->de_deh))
+    reiserfs_panic (0, "vs-7043: entry_points_to_object: entry must be visible");
+
+  return 1;
+}
+
+
+/* sets key of parent directory in ".." entry */
+static void set_ino_in_dir_entry (struct reiserfs_dir_entry * de, struct key * key)
+{
+  de->de_deh->deh_dir_id = key->k_dir_id;;
+  de->de_deh->deh_objectid = key->k_objectid;
+  /*
+  *((unsigned long *)(de->de_name + 2)) = key->k_dir_id;
+  mark_de_with_directory_id (de->de_deh);*/
+}
+
+/* 
+ * process, that is going to call fix_nodes/do_balance must hold only
+ * one path. If it holds 2 or more, it can get into endless waiting in
+ * get_empty_nodes or its clones 
+ */
+static int do_reiserfs_rename (struct reiserfs_transaction_handle *th, struct inode * old_dir, struct dentry *old_dentry,
+			       struct inode * new_dir, struct dentry *new_dentry)
+{
+  int retval;
+  struct path old_entry_path, new_entry_path, dot_dot_entry_path;
+  struct reiserfs_dir_entry old_de, new_de, dot_dot_de;
+  struct inode * old_inode, * new_inode;
+  int new_entry_added = 0;
+
+  init_path (&old_entry_path);
+  init_path (&new_entry_path);
+  init_path (&dot_dot_entry_path);
+  goto start_up;
+
+try_again:
+  current->policy |= SCHED_YIELD;
+  schedule();
+	
+start_up:
+  old_inode = new_inode = NULL;
+  dot_dot_de.de_bh = 0;
+  new_de.de_bh = 0;
+
+  /* 
+   * look for the old name in old directory 
+   */
+  retval = -ENOENT;
+  old_de.de_gen_number_bit_string = 0;
+  if (reiserfs_find_entry (old_dir, old_dentry->d_name.name, old_dentry->d_name.len, &old_entry_path, &old_de) == POSITION_NOT_FOUND)
+    goto end_rename;
+
+  pathrelse (&old_entry_path);
+
+  old_inode = old_dentry->d_inode;
+  retval = -EPERM;
+
+  if ((old_dir->i_mode & S_ISVTX) && 
+      current->fsuid != old_inode->i_uid &&
+      current->fsuid != old_dir->i_uid && !fsuser())
+    goto end_rename;
+
+  new_inode = new_dentry->d_inode;
+
+  /* look for the new entry in target directory */
+  new_de.de_gen_number_bit_string = 0;
+  if (reiserfs_find_entry (new_dir, new_dentry->d_name.name, new_dentry->d_name.len, &new_entry_path, &new_de) == POSITION_FOUND) {
+    if (!new_inode) {
+      printk ("do_reiserfs_rename: new entry found, inode == 0 though\n");
+    }
+    /* this entry already exists, we can just set key of object */
+    new_entry_added = 1;
+  } else {
+#ifdef CONFIG_REISERFS_CHECK
+    if (new_entry_added) {
+      if (new_de.de_namelen != new_dentry->d_name.len || memcmp (new_de.de_name, new_dentry->d_name.name, new_de.de_namelen) ||
+	  de_visible (new_de.de_deh))
+	reiserfs_panic (old_dir->i_sb, "vs-7045: reiserfs_rename: suspicious entry found");
+    }
+#endif /* CONFIG_REISERFS_CHECK */
+  }
+  pathrelse (&new_entry_path);
+
+
+  if (new_inode == old_inode) {
+    retval = 0;
+    goto end_rename;
+  }
+
+  if (new_inode && S_ISDIR(new_inode->i_mode)) {
+    /* new name exists and points to directory */
+    retval = -EISDIR;
+    if (!S_ISDIR(old_inode->i_mode))
+      goto end_rename;
+    retval = -EINVAL;
+    if (is_subdir (new_dentry, old_dentry))
+      goto end_rename;
+    retval = -ENOTEMPTY;
+    if (!reiserfs_empty_dir (new_inode))
+      goto end_rename;
+    retval = -EBUSY;
+    if (new_inode->i_count > 1)
+      goto end_rename;
+  }
+
+  retval = -EPERM;
+  if (new_inode && (new_dir->i_mode & S_ISVTX) && 
+      current->fsuid != new_inode->i_uid &&
+      current->fsuid != new_dir->i_uid && !fsuser())
+    goto end_rename;
+
+  if (S_ISDIR(old_inode->i_mode)) {
+    /* old name points to directory */
+    retval = -ENOTDIR;
+    if (new_inode && !S_ISDIR(new_inode->i_mode))
+      goto end_rename;
+
+    retval = -EINVAL;
+    if (is_subdir(new_dentry, old_dentry))
+      goto end_rename;
+
+    retval = -EIO;
+    /* directory is renamed, its parent directory will be changed, so find ".." entry */
+    dot_dot_de.de_gen_number_bit_string = 0;
+    if (reiserfs_find_entry (old_inode, "..", 2, &dot_dot_entry_path, &dot_dot_de) == POSITION_NOT_FOUND)
+      goto end_rename;
+    if (dot_dot_de.de_objectid != old_dir->i_ino)
+      goto end_rename;
+    pathrelse (&dot_dot_entry_path);
+
+    retval = -EMLINK;
+    if (!new_inode && new_dir->i_nlink >= REISERFS_LINK_MAX)
+      goto end_rename;
+  }
+  
+  if (new_entry_added == 0) {
+    /* add new entry if we did not do it, but do not mark it as visible */
+    retval = reiserfs_add_entry (th, new_dir, new_dentry->d_name.name, new_dentry->d_name.len, INODE_PKEY (old_inode), &new_de, 0);
+    if (retval)
+      goto end_rename;
+    if_in_ram_update_sd (th, new_dir);
+    new_entry_added = 1;
+    goto try_again;
+  }
+
+
+  /* 
+   * look for old name, new name and ".." when renaming directories again
+   */
+  if (reiserfs_find_entry (old_dir, old_dentry->d_name.name, old_dentry->d_name.len, &old_entry_path, &old_de) == POSITION_NOT_FOUND)
+    reiserfs_panic (old_dir->i_sb, "vs-7050: reiserfs_rename: old name not found");
+  if (reiserfs_find_entry (new_dir, new_dentry->d_name.name, new_dentry->d_name.len, &new_entry_path, &new_de) == POSITION_NOT_FOUND)
+    reiserfs_panic (old_dir->i_sb, "vs-7055: reiserfs_rename: new name not found");
+  if (S_ISDIR(old_inode->i_mode) && reiserfs_find_entry (old_inode, "..", 2, &dot_dot_entry_path, &dot_dot_de) == POSITION_NOT_FOUND)
+    reiserfs_panic (old_dir->i_sb, "vs-7060: reiserfs_rename: \"..\" name not found");
+ 
+
+  /* sanity checking before doing the rename - avoid races */
+  if (!entry_points_to_object (new_dentry->d_name.name, new_dentry->d_name.len, &new_de, new_inode))
+    goto try_again;
+  if (!entry_points_to_object (old_dentry->d_name.name, old_dentry->d_name.len, &old_de, old_inode))
+    /* go to re-looking for old entry */
+    goto try_again;
+
+  if (S_ISDIR(old_inode->i_mode) && !entry_points_to_object ("..", 2, &dot_dot_de, old_dir))
+    /* go to re-looking for ".." entry of renamed directory */
+    goto try_again;
+  
+  /* ok, all the changes can be done in one fell swoop when we have
+     claimed all the buffers needed.*/
+
+  /* make old name hidden */
+  mark_de_hidden (old_de.de_deh);
+  journal_mark_dirty(th, old_dir->i_sb, old_de.de_bh) ;
+
+  /* make new name visible and set key of old object (if entry
+     existed, it is already visible, if not, key is correct already) */
+  mark_de_visible (new_de.de_deh);
+  new_de.de_deh->deh_dir_id = INODE_PKEY (old_inode)->k_dir_id;
+  new_de.de_deh->deh_objectid = INODE_PKEY (old_inode)->k_objectid;
+  journal_mark_dirty(th, old_dir->i_sb, new_de.de_bh) ;
+
+  old_dir->i_ctime = old_dir->i_mtime = CURRENT_TIME;
+  if_in_ram_update_sd (th, old_dir);
+
+  new_dir->i_ctime = new_dir->i_mtime = CURRENT_TIME;
+  if_in_ram_update_sd (th, new_dir);
+
+  if (new_inode) {
+    new_inode->i_nlink--;
+    new_inode->i_ctime = CURRENT_TIME;
+    if_in_ram_update_sd (th, new_inode);
+  }
+  if (dot_dot_de.de_bh) {
+    set_ino_in_dir_entry (&dot_dot_de, INODE_PKEY (new_dir));
+    journal_mark_dirty(th, old_dir->i_sb, dot_dot_de.de_bh) ;
+    old_dir->i_nlink--;
+    if_in_ram_update_sd (th, old_dir);
+    if (new_inode) {
+      new_inode->i_nlink--;
+      if_in_ram_update_sd (th, new_inode);
+    } else {
+      new_dir->i_nlink++;
+      if_in_ram_update_sd (th, new_dir);
+    }
+  }
+
+  /* ok, renaming done */
+  decrement_counters_in_path (&new_entry_path);
+  decrement_counters_in_path (&dot_dot_entry_path);
+
+  /* remove old name (it is hidden now) */
+  if (reiserfs_cut_from_item (th, old_dir, old_dir->i_sb, &old_entry_path, &(old_de.de_entry_num),
+			      &(old_de.de_entry_key), 0, PRESERVE_RENAMING) == 0)
+    printk ("reiserfs_rename: could not remove old name\n");
+  else {
+    old_dir->i_size -= DEH_SIZE + old_de.de_entrylen;
+    old_dir->i_blocks = ((old_dir->i_size + 511) >> 9);
+    if_in_ram_update_sd (th, old_dir);
+  }
+
+  retval = 0;
+
+end_rename:
+  pathrelse (&old_entry_path);
+  return retval;
+}
+
+
+int	reiserfs_rename (
+			 struct inode * old_dir, struct dentry *old_dentry,
+			 struct inode * new_dir, struct dentry *new_dentry
+			 )
+{
+  static struct wait_queue * wait = NULL;
+  static int lock = 0;
+  int result;
+  int windex ;
+  struct reiserfs_transaction_handle th ;
+  int jbegin_count = JOURNAL_PER_BALANCE_CNT * 3; 
+  
+  while (lock)
+    sleep_on(&wait);
+  lock = 1;
+  journal_begin(&th, old_dir->i_sb, jbegin_count) ;
+  windex = push_journal_writer("reiesrfs_rename") ;
+  /* we are trusting if_in_ram_update_sd to update the transaction 
+  ** info in each inode as they get chagned
+  */
+  result = do_reiserfs_rename (&th, old_dir, old_dentry, new_dir, new_dentry);
+  pop_journal_writer(windex) ;
+  journal_end(&th, old_dir->i_sb, jbegin_count) ;
+  lock = 0;
+  wake_up(&wait);
+  return result;
+}
diff -urN linux/fs/reiserfs/objectid.c /tmp/linux/fs/reiserfs/objectid.c
--- linux/fs/reiserfs/objectid.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/objectid.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,151 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+#ifdef __KERNEL__
+
+#include <linux/string.h>
+#include <linux/locks.h>
+#include <linux/sched.h>
+#include <linux/reiserfs_fs.h>
+
+#else
+
+#include "nokernel.h"
+
+#endif
+
+
+/* When we allocate objectids we allocate the first unused objectid.
+   Each sequence of objectids in use (the odd sequences) is followed
+   by a sequence of objectids not in use (the even sequences).  We
+   only need to record the last objectid in each of these sequences
+   (both the odd and even sequences) in order to fully define the
+   boundaries of the sequences.  A consequence of allocating the first
+   objectid not in use is that under most conditions this scheme is
+   extremely compact.  The exception is immediately after a sequence
+   of operations which deletes a large number of objects of
+   non-sequential objectids, and even then it will become compact
+   again as soon as more objects are created.  Note that many
+   interesting optimizations of layout could result from complicating
+   objectid assignment, but we have deferred making them for now. */
+
+
+/* get unique object identifier */
+unsigned long	reiserfs_get_unused_objectid (struct reiserfs_transaction_handle *th, struct super_block * s)
+{
+  unsigned long unused_objectid;
+  struct reiserfs_super_block * disk_sb;
+  unsigned long * objectid_map;
+
+
+  disk_sb = SB_DISK_SUPER_BLOCK (s);
+  objectid_map = (unsigned long *)(disk_sb + 1); /* The objectid map follows the superblock. */
+  
+                                /* comment needed -Hans */
+  unused_objectid = objectid_map[1];
+  if (unused_objectid == TYPE_INDIRECT) {
+    printk ("REISERFS: get_objectid: no more object ids\n");
+    return 0;
+  }
+
+  /* This incrementation allocates the first unused objectid. That is to say, the first entry on the
+   objectid map is the first unused objectid, and by incrementing it we use it.  See below where we
+   check to see if we eliminated a sequence of unused objectids.... */
+  objectid_map[1] ++;
+
+  /* Now we check to see if we eliminated the last remaining member of
+     the first even sequence (and can eliminate the sequence by
+     eliminating its last objectid from oids), and can collapse the
+     first two odd sequences into one sequence.  If so, then the net
+     result is to eliminate a pair of objectids from oids.  We do this
+     by shifting the entire map to the left. */
+  if (disk_sb->s_oid_cursize > 2 && objectid_map[1] == objectid_map[2]) {
+    memmove (objectid_map + 1, objectid_map + 3, (disk_sb->s_oid_cursize - 3) * sizeof(unsigned long));
+    disk_sb->s_oid_cursize -= 2;
+  }
+
+  /* super block has been changed. Non-atomic mark_buffer_dirty is allowed here */
+  /* mark_buffer_dirty (SB_BUFFER_WITH_SB (s), 1); journal victim *//* no need to place buffer on preserve list */
+  journal_mark_dirty(th, s, SB_BUFFER_WITH_SB (s));/* no need to place buffer on preserve list */
+  s->s_dirt = 1;
+  return unused_objectid;
+}
+
+
+/* makes object identifier unused */
+void	reiserfs_release_objectid (struct reiserfs_transaction_handle *th, 
+				   unsigned long objectid_to_release, struct super_block * s)
+{
+  struct reiserfs_super_block * disk_sb;
+  unsigned long * objectid_map;
+  int i = 0;
+
+
+  /* return; */
+
+  /* mark_buffer_dirty (SB_BUFFER_WITH_SB (s), 1);  journal victim */
+  journal_mark_dirty(th, s, SB_BUFFER_WITH_SB (s)); 
+  s->s_dirt = 1;
+
+  /* let disk_sb serve as a convenient shorthand pointing to the Reiserfs
+     Specific portion of the super_block */
+  disk_sb = SB_DISK_SUPER_BLOCK (s);
+
+  /* This means/assumes that the objectid map immediately follows
+     the superblock in memory. */
+  objectid_map = (unsigned long *)(disk_sb + 1);
+
+  /* start at the beginning of the objectid map (i = 0) and go to the
+     end of it (i = disk_sb->s_oid_cursize).  Linear search is what we use,
+     though it is possible that binary search would be more efficient
+     after performing lots of deletions (which is when oids is large.)
+     We only check even i's. */
+  while (i < disk_sb->s_oid_cursize) {
+    if (objectid_to_release == objectid_map[i]) {
+      if (i == 0)
+	reiserfs_panic (s, "vs-15000: reiserfs_release_objectid: trying to free root object id (%lu)",
+			    objectid_to_release);
+      /* This incrementation unallocates the objectid. */
+      objectid_map[i]++;
+      /* Did we unallocate the last member of an odd sequence, and can shrink oids? */
+      if (objectid_map[i] == objectid_map[i+1]) {
+	/* shrink objectid map */
+	memmove (objectid_map + i, objectid_map + i + 2, 
+		 (disk_sb->s_oid_cursize - i - 2)*sizeof(unsigned long));
+	disk_sb->s_oid_cursize -= 2;
+#ifdef CONFIG_REISERFS_CHECK
+	if (disk_sb->s_oid_cursize < 2 || disk_sb->s_oid_cursize > disk_sb->s_oid_maxsize)
+	  reiserfs_panic (s, "vs-15005: reiserfs_release_objectid: "
+			  "objectid map corrupted cur_size == %d (max == %d)",
+			  disk_sb->s_oid_cursize, disk_sb->s_oid_maxsize);
+#endif
+      }
+      return;
+    }
+
+    if (objectid_to_release > objectid_map[i] && objectid_to_release < objectid_map[i+1]) {
+      /* size of objectid map is not changed */
+      if (objectid_to_release + 1 == objectid_map[i+1]) {
+	objectid_map[i+1]--;
+	return;
+      }
+
+      if (disk_sb->s_oid_cursize == disk_sb->s_oid_maxsize)
+	/* objectid map must be expanded, but there is no space */
+	return;
+
+      /* expand the objectid map*/
+      memmove (objectid_map+i+3, objectid_map+i+1, (disk_sb->s_oid_cursize-i-1) * sizeof(unsigned long));
+      objectid_map[i+1] = objectid_to_release;
+      objectid_map[i+2] = objectid_to_release + 1;
+      disk_sb->s_oid_cursize += 2;
+      return;
+    }
+    i += 2;
+  }
+
+  reiserfs_panic (0, "vs-15010: reiserfs_release_objectid: trying to free free object id (%lu)", objectid_to_release);
+}
+
+
+
diff -urN linux/fs/reiserfs/prints.c /tmp/linux/fs/reiserfs/prints.c
--- linux/fs/reiserfs/prints.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/prints.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,1110 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+#ifdef __KERNEL__
+
+#include <stdarg.h>
+#include <linux/sched.h>
+#include <linux/fs.h>
+#include <linux/reiserfs_fs.h>
+#include <linux/string.h>
+
+#else
+
+#include "nokernel.h"
+#include <stdarg.h>
+#include <limits.h>
+
+#endif
+
+
+static char error_buf[1024];
+static char fmt_buf[1024];
+static char off_buf[80];
+
+static char * offset (struct key * key)
+{
+  if (KEY_IS_DIRECTORY_KEY (key))
+    sprintf (off_buf, "%d(%d)", GET_HASH_VALUE (key->k_offset), GET_GENERATION_NUMBER (key->k_offset));
+  else
+    sprintf (off_buf, "%d", key->k_offset);
+  return off_buf;
+}
+
+
+static void sprintf_key (char * buf, struct key * key)
+{
+  if (key)
+    sprintf (buf, "[%d %d %s %d]", key->k_dir_id, key->k_objectid, offset (key), key->k_uniqueness);
+  else
+    sprintf (buf, "[NULL]");
+}
+
+
+static void sprintf_item_head (char * buf, struct item_head * ih)
+{
+  if (ih) {
+    sprintf_key (buf, &(ih->ih_key));
+    sprintf (buf + strlen (buf), ", item_len %d, item_location %d", ih->ih_item_len, ih->ih_item_location);
+  } else
+    sprintf (buf, "[NULL]");
+}
+
+
+static void sprintf_direntry (char * buf, struct reiserfs_dir_entry * de)
+{
+  char name[20];
+
+  memcpy (name, de->de_name, de->de_namelen > 19 ? 19 : de->de_namelen);
+  name [de->de_namelen > 19 ? 19 : de->de_namelen] = 0;
+  sprintf (buf, "\"%s\"==>[%d %d]", name, de->de_dir_id, de->de_objectid);
+}
+
+
+static void sprintf_block_head (char * buf, struct buffer_head * bh)
+{
+  sprintf (buf, "level=%d, nr_items=%d, free_space=%d rdkey ",
+	   B_LEVEL (bh), B_NR_ITEMS (bh), B_BLK_HEAD (bh)->blk_free_space);
+  if (B_LEVEL (bh) == DISK_LEAF_NODE_LEVEL)
+    sprintf_key (buf + strlen (buf), B_PRIGHT_DELIM_KEY (bh));
+}
+
+
+static void sprintf_buffer_head (char * buf, struct buffer_head * bh) 
+{
+  sprintf (buf, "dev x%x, size %ld, blocknr %ld, count %d, state(%s, %s, %s)",
+	   bh->b_dev, bh->b_size, bh->b_blocknr, bh->b_count,
+	   buffer_uptodate (bh) ? "UPTODATE" : "!UPTODATE",
+	   buffer_dirty (bh) ? "DIRTY" : "CLEAN",
+	   buffer_locked (bh) ? "LOCKED" : "UNLOCKED");
+}
+
+
+static void sprintf_disk_child (char * buf, struct disk_child * dc)
+{
+  sprintf (buf, "[dc_number=%lu, dc_size=%u]", dc->dc_block_number, dc->dc_size);
+}
+
+
+static char * is_there_reiserfs_struct (char * fmt, int * what, int * skip)
+{
+  char * k = fmt;
+
+  *skip = 0;
+  
+  while (1) {
+    k = strstr (k, "%");
+    if (!k)
+      break;
+    if (k && (k[1] == 'k' || k[1] == 'h' || k[1] == 't' ||
+	      k[1] == 'z' || k[1] == 'b' || k[1] == 'y')) {
+      *what = k[1];
+      break;
+    }
+    (*skip) ++;
+    k ++;
+  }
+  return k;
+}
+
+
+/* debugging reiserfs we used to print out a lot of different
+   variables, like keys, item headers, buffer heads etc. Values of
+   most fields matter. So it took a long time just to write
+   appropriative printk. With this reiserfs_warning you can use format
+   specification for complex structures like you used to do with
+   printfs for integers, doubles and pointers. For instance, to print
+   out key structure you have to write just: 
+   reiserfs_warning ("bad key %k", key); 
+   instead of 
+   printk ("bad key %lu %lu %lu %lu", key->k_dir_id, key->k_objectid, 
+           key->k_offset, key->k_uniqueness); 
+*/
+
+#define do_reiserfs_warning \
+{\
+  char * fmt1 = fmt_buf;\
+  va_list args;\
+  int i, j;\
+  char * k;\
+  char * p = error_buf;\
+  int what, skip;\
+\
+  strcpy (fmt1, fmt);\
+  va_start(args, fmt);\
+\
+  while (1) {\
+    k = is_there_reiserfs_struct (fmt1, &what, &skip);\
+    if (k != 0) {\
+      *k = 0;\
+      p += vsprintf (p, fmt1, args);\
+\
+      for (i = 0; i < skip; i ++)\
+	j = va_arg (args, int);\
+\
+      switch (what) {\
+      case 'k':\
+	sprintf_key (p, va_arg(args, struct key *));\
+	break;\
+      case 'h':\
+	sprintf_item_head (p, va_arg(args, struct item_head *));\
+	break;\
+      case 't':\
+	sprintf_direntry (p, va_arg(args, struct reiserfs_dir_entry *));\
+	break;\
+      case 'y':\
+	sprintf_disk_child (p, va_arg(args, struct disk_child *));\
+	break;\
+      case 'z':\
+	sprintf_block_head (p, va_arg(args, struct buffer_head *));\
+	break;\
+      case 'b':\
+	sprintf_buffer_head (p, va_arg(args, struct buffer_head *));\
+	break;\
+      }\
+      p += strlen (p);\
+      fmt1 = k + 2;\
+    } else {\
+      i = vsprintf (p, fmt1, args);\
+      break;\
+    }\
+  }\
+\
+  va_end(args);\
+}
+
+
+/* in addition to usual conversion specifiers this accepts reiserfs
+   specific conversion specifiers: 
+   %k to print key, 
+   %h to print item_head,
+   %t to print directory entry 
+   %z to print block head (arg must be struct buffer_head *
+   %b to print buffer_head
+*/
+void reiserfs_warning (const char * fmt, ...)
+{
+  do_reiserfs_warning;
+  printk ("%s", error_buf);
+}
+
+
+
+/* The format:
+
+           maintainer-errorid: [function-name:] message
+
+    where errorid is unique to the maintainer and function-name is
+    optional, is recommended, so that anyone can easily find the bug
+    with a simple grep for the short to type string
+    maintainer-errorid.  Don't bother with reusing errorids, there are
+    lots of numbers out there.
+
+    Example: 
+    
+    reiserfs_panic(
+	p_sb, "reiser-29: reiserfs_new_blocknrs: "
+	"one of search_start or rn(%d) is equal to MAX_B_NUM,"
+	"which means that we are optimizing location based on the bogus location of a temp buffer (%p).", 
+	rn, bh
+    );
+
+    Regular panic()s sometimes clear the screen before the message can
+    be read, thus the need for the while loop.  
+
+    Numbering scheme for panic used by Vladimir and Anatoly( Hans completely ignores this scheme, and considers it
+    pointless complexity):
+
+    panics in reiserfs_fs.h have numbers from 1000 to 1999
+    super.c				        2000 to 2999
+    preserve.c				    3000 to 3999
+    bitmap.c				    4000 to 4999
+    stree.c				        5000 to 5999
+    prints.c				    6000 to 6999
+    namei.c                     7000 to 7999
+    fix_nodes.c                 8000 to 8999
+    dir.c                       9000 to 9999
+	lbalance.c					10000 to 10999
+	ibalance.c		11000 to 11999 not ready
+	do_balan.c		12000 to 12999
+	inode.c			13000 to 13999
+	file.c			14000 to 14999
+    objectid.c                       15000 - 15999
+    buffer.c                         16000 - 16999
+    symlink.c                        17000 - 17999
+
+   .  */
+
+void flush_log_buf (void);
+extern int g_balances_number;
+
+#ifdef CONFIG_REISERFS_CHECK
+extern struct tree_balance * cur_tb;
+extern unsigned long log_size;
+#endif
+
+void reiserfs_panic (struct super_block * sb, const char * fmt, ...)
+{
+  do_reiserfs_warning;
+  printk ("%s", error_buf);
+
+#ifdef __KERNEL__
+
+  /* this is to prevent panic from syncing this filesystem */
+  if (sb && sb->s_lock)
+    sb->s_lock=0;
+  if (sb)
+    sb->s_flags |= MS_RDONLY;
+
+  panic ("REISERFS: panic (device %s): %s\n",
+	 sb ? kdevname(sb->s_dev) : "sb == 0", error_buf);
+#else
+  exit (0);
+#endif
+}
+
+static char * vi_type (struct virtual_item * vi)
+{
+  static char *types[]={"directory", "direct", "indirect", "stat data"};
+
+  if (vi->vi_type & VI_TYPE_STAT_DATA)
+    return types[3];
+  if (vi->vi_type & VI_TYPE_INDIRECT)
+    return types[2];
+  if (vi->vi_type & VI_TYPE_DIRECT)
+    return types[1];
+  if (vi->vi_type & VI_TYPE_DIRECTORY)
+    return types[0];
+
+  reiserfs_panic (0, "vi_type: 6000: unknown type (0x%x)", vi->vi_type);
+  return NULL;
+}
+
+void print_virtual_node (struct virtual_node * vn)
+{
+  int i, j;
+  
+
+  printk ("VIRTUAL NODE CONTAINS %d items, has size %d,%s,%s, ITEM_POS=%d POS_IN_ITEM=%d MODE=\'%c\'\n",
+	  vn->vn_nr_item, vn->vn_size,
+	  (vn->vn_vi[0].vi_type & VI_TYPE_LEFT_MERGEABLE )? "left mergeable" : "", 
+	  (vn->vn_vi[vn->vn_nr_item - 1].vi_type & VI_TYPE_RIGHT_MERGEABLE) ? "right mergeable" : "",
+	  vn->vn_affected_item_num, vn->vn_pos_in_item, vn->vn_mode);
+
+
+  for (i = 0; i < vn->vn_nr_item; i ++)
+    {
+      printk ("%s %d %d", vi_type (&vn->vn_vi[i]), i, vn->vn_vi[i].vi_item_len);
+      if (vn->vn_vi[i].vi_entry_sizes)
+	{
+	  printk ("It is directory with %d entries: ", vn->vn_vi[i].vi_entry_count);
+	  for (j = 0; j < vn->vn_vi[i].vi_entry_count; j ++)
+	    printk ("%d ", vn->vn_vi[i].vi_entry_sizes[j]);
+	}
+      printk ("\n");
+    }
+}
+
+
+void print_path (struct tree_balance * tb, struct path * path)
+{
+  int h = 0;
+  
+  if (tb) {
+    while (tb->insert_size[h]) {
+      printk ("block %lu (level=%d), position %d\n", PATH_H_PBUFFER (path, h) ? PATH_H_PBUFFER (path, h)->b_blocknr : 0,
+	      PATH_H_PBUFFER (path, h) ? B_BLK_HEAD (PATH_H_PBUFFER (path, h))->blk_level : 0,
+	      PATH_H_POSITION (path, h));
+      h ++;
+    }
+  } else {
+    int offset = path->path_length;
+    struct buffer_head * bh;
+    printk ("Offset    Bh     (b_blocknr, b_count) Position Nr_item\n");
+    while ( offset > ILLEGAL_PATH_ELEMENT_OFFSET ) {
+      bh = PATH_OFFSET_PBUFFER (path, offset);
+      printk ("%6d %10p (%9lu, %7d) %8d %7d\n", offset, 
+	      bh, bh ? bh->b_blocknr : 0, bh ? bh->b_count : 0,
+	      PATH_OFFSET_POSITION (path, offset), bh ? B_NR_ITEMS (bh) : -1);
+
+      offset --;
+    }
+  }
+
+#if 0
+  printk ("#####################\n");
+  printk ("Offset    Bh     (b_blocknr, b_count) Position Nr_item\n");
+  while ( offset > ILLEGAL_PATH_ELEMENT_OFFSET ) {
+    bh = PATH_OFFSET_PBUFFER (path, offset);
+    printk ("%6d %10p (%9lu, %7d) %8d %7d\n", offset, 
+	    bh, bh ? bh->b_blocknr : 0, bh ? bh->b_count : 0,
+	    PATH_OFFSET_POSITION (path, offset), bh ? B_NR_ITEMS (bh) : -1);
+
+    print_buffer_head(bh,"print_path");
+    offset --;
+  }
+  printk ("#####################\n");
+#endif
+}
+
+char * int2nameprefix (char * nameprefix, unsigned int num);
+char * k_offset_to_string (unsigned long offset, unsigned long uniqueness)
+{
+  static char buf[15];
+
+  if (offset == SD_OFFSET)		/* offset == 0 */
+    strcpy (buf, "SD");
+  else if (offset == DOT_OFFSET) {	/* offset == 1 */
+    /* directory and regular files can have k_offset == 1 */
+    if (uniqueness < TYPE_INDIRECT)
+      strcpy (buf, ".");
+    else
+      strcpy (buf, "1");
+  } else if (offset == DOT_DOT_OFFSET) {/* offset == 2 */
+    /* directory and regular files can have k_offset == 2 */
+    if (uniqueness < TYPE_INDIRECT)
+      strcpy (buf, "..");
+    else
+      strcpy (buf, "2");
+  } else {				/* offset > 2 */
+    if (uniqueness < TYPE_INDIRECT)
+       int2nameprefix (buf, offset);
+    else
+      sprintf (buf, "%lu", offset);
+  }
+  return buf;
+}
+
+char * k_uniqueness_to_string (unsigned long uniqueness)
+{
+  static char * item_type[] = {"SD", "IND", "TAIL", "DIR"};
+
+  if (uniqueness == TYPE_STAT_DATA)
+    return item_type[0];
+  if (uniqueness == TYPE_INDIRECT)
+    return item_type[1];
+  if (uniqueness == TYPE_DIRECT)
+    return item_type[2];
+  return item_type[3];
+}
+
+
+
+void print_de (struct reiserfs_dir_entry * de)
+{
+  printk ("entry key: [%d, %d, \"%s\" %s], object_key: [%u %u], b_blocknr=%lu, item_num=%d, pos_in_item=%d\n",
+	  de->de_entry_key.k_dir_id, de->de_entry_key.k_objectid, 
+	  k_offset_to_string (de->de_entry_key.k_offset, de->de_entry_key.k_uniqueness),
+	  k_uniqueness_to_string (de->de_entry_key.k_uniqueness),
+	  de->de_dir_id, de->de_objectid,
+	  de->de_bh->b_blocknr, de->de_item_num, de->de_entry_num);
+}
+
+void print_bi (struct buffer_info * bi, char * mes)
+{
+  printk ("%s: bh->b_blocknr=%lu, bh->b_item_order=%d, bh->b_parent->b_blocknr=%lu\n", 
+	  mes ? mes : "print_bi", bi->bi_bh->b_blocknr, bi->bi_position, bi->bi_parent ? bi->bi_parent->b_blocknr : 0);
+}
+
+
+static char * item_type (struct item_head * ih)
+{
+    static char * types[] = {
+        "stat data", "directory", "direct", "indirect", "unknown"
+    };
+
+    if (I_IS_STAT_DATA_ITEM(ih))
+        return types[0];
+    if (I_IS_DIRECTORY_ITEM(ih))
+        return types[1];
+    if (I_IS_DIRECT_ITEM(ih))
+        return types[2];
+    if (I_IS_INDIRECT_ITEM(ih))
+        return types[3];
+    return types[4];
+}
+
+char * int2nameprefix (char * nameprefix, unsigned int num)
+{
+  int j, k;
+  char * third;
+
+  if (!num)
+    {
+      nameprefix[0] = '0';
+      nameprefix[1] = 0;
+      return nameprefix;
+    }
+             
+  third = (char *)&num;
+  for (j = 3, k = 0; j >= 0 && third[j]; j --, k ++)
+    nameprefix[k] = third[j];
+  nameprefix[k] = 0;
+
+  return nameprefix;
+}
+
+static void check_directory_item (struct buffer_head * bh, struct item_head * ih)
+{
+  int i;
+  int namelen;
+  struct reiserfs_de_head * deh;
+
+  if (!I_IS_DIRECTORY_ITEM (ih))
+    return;
+
+  deh = B_I_DEH (bh, ih);
+  for (i = 0; i < I_ENTRY_COUNT (ih); i ++, deh ++) {
+    namelen = I_DEH_N_ENTRY_FILE_NAME_LENGTH (ih, deh, i);
+  }
+}
+
+void print_directory_item (struct buffer_head * bh, struct item_head * ih)
+{
+  int i;
+  int namelen;
+  struct reiserfs_de_head * deh;
+  char * name;
+  static char namebuf [80];
+
+  if (!I_IS_DIRECTORY_ITEM (ih))
+    return;
+
+  printk ("\n # %-15s%-30s%-15s%-15s%-15s\n", "Name", "Key of pointed object", "Hash", "Gen number", "Status");
+  deh = B_I_DEH (bh, ih);
+  for (i = 0; i < I_ENTRY_COUNT (ih); i ++, deh ++) {
+    namelen = I_DEH_N_ENTRY_FILE_NAME_LENGTH (ih, deh, i);
+    name = B_I_DEH_ENTRY_FILE_NAME (bh, ih, deh);
+    namebuf[0] = '"';
+    if (namelen > sizeof (namebuf) - 3) {
+      strncpy (namebuf + 1, name, sizeof (namebuf) - 3);
+      namebuf[sizeof (namebuf) - 2] = '"';
+      namebuf[sizeof (namebuf) - 1] = 0;
+    } else {
+      memcpy (namebuf + 1, name, namelen);
+      namebuf[namelen + 1] = '"';
+      namebuf[namelen + 2] = 0;
+    }
+
+
+    printk ("%d:  %-15s%-15d%-15d%-15d%-15d(%s)\n", 
+            i, namebuf,
+	    deh->deh_dir_id, deh->deh_objectid,
+/*
+            de_with_number(deh) ? (*(unsigned long *)(B_I_PITEM(bh,ih) + deh->deh_location + strlen (namebuf))) : 
+	    ((namelen == 1 && name[0] == '.') ? ih->ih_key.k_dir_id : ih->ih_key.k_objectid),
+            deh->deh_objectid,
+*/
+	    GET_HASH_VALUE (deh->deh_offset), GET_GENERATION_NUMBER (deh->deh_offset),
+	    (de_hidden (deh)) ? "HIDDEN" : "VISIBLE");
+  }
+}
+
+
+//
+// printing of indirect item
+//
+static void start_new_sequence (__u32 * start, int * len, __u32 new)
+{
+    *start = new;
+    *len = 1;
+}
+
+static int sequence_finished (__u32 start, int * len, __u32 new)
+{
+    if (start == INT_MAX)
+	return 1;
+
+    if (start == 0 && new == 0) {
+	(*len) ++;
+	return 0;
+    }
+    if (start != 0 && (start + *len) == new) {
+	(*len) ++;
+	return 0;
+    }
+    return 1;
+}
+
+static void print_sequence (__u32 start, int len)
+{
+    if (start == INT_MAX)
+	return;
+
+    if (len == 1)
+	printk (" %d", start);
+    else
+	printk (" %d(%d)", start, len);
+}
+
+void print_indirect_item (struct buffer_head * bh, int item_num)
+{
+    struct item_head * ih;
+    int j;
+    __u32 * unp, prev = INT_MAX;
+    int num;
+
+    ih = B_N_PITEM_HEAD (bh, item_num);
+    unp = (__u32 *)B_I_PITEM (bh, ih);
+
+    if (ih->ih_item_len % UNFM_P_SIZE)
+	printk ("print_indirect_item: invalid item len");  
+
+    printk ("%d pointers\n[ ", I_UNFM_NUM (ih));
+    for (j = 0; j < I_UNFM_NUM (ih); j ++) {
+	if (sequence_finished (prev, &num, unp[j])) {
+	    print_sequence (prev, num);
+	    start_new_sequence (&prev, &num, unp[j]);
+	}
+    }
+    print_sequence (prev, num);
+    printk ("]\n");
+}
+
+
+
+char timebuf[256];
+
+char * timestamp (time_t t)
+{
+#ifndef __KERNEL__
+  strftime (timebuf, 256, "%m/%d/%Y %T", localtime (&t));
+#else
+  sprintf (timebuf, "%d", (int) t);
+#endif
+  return timebuf;
+}
+
+
+/* this prints internal nodes (4 keys/items in line) (dc_number,
+   dc_size)[k_dirid, k_objectid, k_offset, k_uniqueness](dc_number,
+   dc_size)...*/
+static int print_internal (struct buffer_head * bh, int first, int last)
+{
+    struct key * key;
+    struct disk_child * dc;
+    int i;
+    int from, to;
+
+    if (!B_IS_KEYS_LEVEL (bh))
+	return 1;
+
+    if (first == -1) {
+	from = 0;
+	to = B_NR_ITEMS (bh);
+    } else {
+	from = first;
+	to = last < B_NR_ITEMS (bh) ? last : B_NR_ITEMS (bh);
+    }
+
+    reiserfs_warning ("INTERNAL NODE (%ld) contains %z\n",  bh->b_blocknr, bh);
+
+    dc = B_N_CHILD (bh, from);
+    reiserfs_warning ("PTR %d: %y ", from, dc);
+
+    for (i = from, key = B_N_PDELIM_KEY (bh, from), dc ++; i < to; i ++, key ++, dc ++) {
+	reiserfs_warning ("KEY %d: %k PTR %d: %y ", i, key, i + 1, dc);
+	if (i && i % 4 == 0)
+	    printk ("\n");
+    }
+    printk ("\n");
+    return 0;
+}
+
+
+static int is_symlink = 0;
+static int print_leaf (struct buffer_head * bh, int print_mode, int first, int last)
+{
+    struct block_head * blkh;
+    struct item_head * ih;
+    int i;
+    int from, to;
+
+    if (!B_IS_ITEMS_LEVEL (bh))
+	return 1;
+
+    blkh = B_BLK_HEAD (bh);
+    ih = B_N_PITEM_HEAD (bh,0);
+
+    printk ("\n===================================================================\n");
+    reiserfs_warning ("LEAF NODE (%ld) contains %z\n", bh->b_blocknr, bh);
+
+    if (!(print_mode & PRINT_LEAF_ITEMS)) {
+	reiserfs_warning ("FIRST ITEM_KEY: %k, LAST ITEM KEY: %k\n",
+			  &(ih->ih_key), &((ih + blkh->blk_nr_item - 1)->ih_key));
+	return 0;
+    }
+
+    if (first < 0 || first > blkh->blk_nr_item - 1) 
+	from = 0;
+    else 
+	from = first;
+
+    if (last < 0 || last > blkh->blk_nr_item)
+	to = blkh->blk_nr_item;
+    else
+	to = last;
+
+
+    printk ("------------------------------------------------------------------------------------------------------------------\n");
+    printk ("|##|   type    |           key           | ilen | free_space | reserved | loc  |   mode  |  size  | nl | direct byte | mtime | ctime | atime |\n");
+    for (i = from; i < to; i++) {
+	printk ("------------------------------------------------------------------------------------------------------------\n");
+	printk ("|%2d| %9s | %5d %5d %5d %5d | %4d | %10d | %8d | %4d |",
+		i, item_type(ih+i), ih[i].ih_key.k_dir_id, ih[i].ih_key.k_objectid, ih[i].ih_key.k_offset, ih[i].ih_key.k_uniqueness,
+		ih[i].ih_item_len, ih[i].u.ih_free_space, ih[i].ih_reserved, ih[i].ih_item_location);
+
+	if (I_IS_STAT_DATA_ITEM(ih+i)) {
+	    struct stat_data * sd = B_I_STAT_DATA (bh,ih+i);
+
+	    printk (" 0%-6o | %6u | %2u | %d | %s | %s | %s |\n", sd->sd_mode, sd->sd_size, sd->sd_nlink, sd->sd_first_direct_byte, 
+		    timestamp (sd->sd_mtime), timestamp (sd->sd_ctime), timestamp (sd->sd_atime));
+	    is_symlink = (S_ISLNK(sd->sd_mode)) ? 1 : 0;
+	    continue;
+	}
+	printk ("\n");
+	if (I_IS_DIRECTORY_ITEM(ih+i) && print_mode & PRINT_DIRECTORY_ITEMS) {
+	    print_directory_item (bh, ih+i);
+	    continue;
+	}
+
+	if (I_IS_INDIRECT_ITEM(ih+i)) {
+	    print_indirect_item (bh, i);
+	    continue;
+	}
+
+	if (I_IS_DIRECT_ITEM(ih+i)) {
+	    int j = 0;
+	    if (is_symlink || print_mode & PRINT_DIRECT_ITEMS) {
+		printk ("\"");
+		while (j < ih[i].ih_item_len)
+		    printk ("%c", B_I_PITEM(bh,ih+i)[j++]);
+		printk ("\"\n");
+	    }
+	    continue;
+	}
+    }
+    printk ("===================================================================\n");
+    return 0;
+}
+
+/*
+char buf[20];
+
+#include <linux/kdev_t.h>
+static char * devname (int dev)
+{
+  struct stat st;
+
+  if (fstat (dev, &st) != 0)
+    die ("stat failed");
+  sprintf (buf, "0x%x:0x%x", MAJOR((int)st.st_rdev), MINOR((int)st.st_rdev));
+  return buf;
+}
+*/
+
+static char * reiserfs_version (char * buf)
+{
+    __u16 * pversion;
+
+    pversion = (__u16 *)(buf + 30);
+    if (*pversion == 0)
+	return "0";
+    if (*pversion == 2)
+	return "2";
+    return "Unknown";
+}
+
+/* return 1 if this is not super block */
+static int print_super_block (struct buffer_head * bh)
+{
+    struct reiserfs_super_block * rs = (struct reiserfs_super_block *)(bh->b_data);
+    int skipped, data_blocks;
+    
+
+    if (strncmp (rs->s_magic,  REISERFS_SUPER_MAGIC_STRING, strlen ( REISERFS_SUPER_MAGIC_STRING)))
+	return 1;
+
+    printk ("%s\'s super block in block %ld\n======================\n", kdevname (bh->b_dev), bh->b_blocknr);
+    printk ("Reiserfs version %s\n", reiserfs_version (bh->b_data));
+
+#if 0
+    printk ("-------------------------------------------------------------------------------------------------------------------\n");
+    printk ("| block count | free block | used block | blocksize | tree height |  state  |   magic   | root block | bmap count |\n");
+    printk ("|             |   count    |  count     |           |             |         |           |            |            |\n");
+    printk ("|-----------------------------------------------------------------------------------------------------------------|\n");
+    printk ("| %11d | %10d | %10d | %9d | %11d | %7s | %9s | %10d | %10d |\n", 
+	    rs->s_block_count, rs->s_free_blocks, rs->s_block_count - rs->s_free_blocks, rs->s_blocksize, rs->s_tree_height, 
+	    (rs->s_state == REISERFS_VALID_FS) ? "VALID" : "ERROR", rs->s_magic, rs->s_root_block, rs->s_bmap_nr);
+    printk ("|-----------------------------------------------------------------------------------------------------------------|\n\n");
+#endif
+
+    printk ("Block count %u\n", rs->s_block_count);
+    printk ("Blocksize %d\n", rs->s_blocksize);
+    printk ("Free blocks %u\n", rs->s_free_blocks);
+    skipped = bh->b_blocknr; // FIXME: this would be confusing if
+    // someone stores reiserfs super block in reiserfs ;)
+    data_blocks = rs->s_block_count - skipped - 1 -
+	rs->s_bmap_nr - (rs->s_orig_journal_size + 1) - rs->s_free_blocks;
+    printk ("Busy blocks (skipped %d, bitmaps - %d, journal blocks - %d\n"
+	    "1 super blocks, %d data blocks\n", 
+	    skipped, rs->s_bmap_nr, 
+	    (rs->s_orig_journal_size + 1), data_blocks);
+    printk ("Root block %u\n", rs->s_root_block);
+    printk ("Journal block (first?) %d\n", rs->s_journal_block);
+    printk ("Journal dev %d\n", rs->s_journal_dev);    
+    printk ("Journal orig size %d\n", rs->s_orig_journal_size);
+    printk ("Filesystem state %s\n", (rs->s_state == REISERFS_VALID_FS) ? "VALID" : "ERROR");
+
+#if 0
+    __u32 s_journal_trans_max ;           /* max number of blocks in a transaction.  */
+    __u32 s_journal_block_count ;         /* total size of the journal. can change over time  */
+    __u32 s_journal_max_batch ;           /* max number of blocks to batch into a trans */
+    __u32 s_journal_max_commit_age ;      /* in seconds, how old can an async commit be */
+    __u32 s_journal_max_trans_age ;       /* in seconds, how old can a transaction be */
+#endif
+    printk ("Tree height %d\n", rs->s_tree_height);
+    return 0;
+}
+
+
+static int print_desc_block (struct buffer_head * bh)
+{
+    struct reiserfs_journal_desc * desc;
+
+    desc = (struct reiserfs_journal_desc *)(bh->b_data);
+    if (memcmp(desc->j_magic, JOURNAL_DESC_MAGIC, 8))
+	return 1;
+
+    printk ("Desc block %lu (j_trans_id %ld, j_mount_id %ld, j_len %ld)",
+	    bh->b_blocknr, desc->j_trans_id, desc->j_mount_id, desc->j_len);
+
+    return 0;
+}
+
+
+void print_block (struct buffer_head * bh, ...)//int print_mode, int first, int last)
+{
+    va_list args;
+    int mode, first, last;
+
+    va_start (args, bh);
+
+    if ( ! bh ) {
+	printk("print_block: buffer is NULL\n");
+	return;
+    }
+
+    mode = va_arg (args, int);
+    first = va_arg (args, int);
+    last = va_arg (args, int);
+    if (print_leaf (bh, mode, first, last))
+	if (print_internal (bh, first, last))
+	    if (print_super_block (bh))
+		if (print_desc_block (bh))
+		    printk ("Block %ld contains unformatted data\n", bh->b_blocknr);
+}
+
+
+
+void print_tb (int mode, int item_pos, int pos_in_item, struct tree_balance * tb, char * mes)
+{
+  int h = 0;
+  int i;
+  struct buffer_head * tbSh, * tbFh;
+
+
+  if (!tb)
+    return;
+
+  printk ("\n********************** PRINT_TB for %s *******************\n", mes);
+  printk ("MODE=%c, ITEM_POS=%d POS_IN_ITEM=%d\n", mode, item_pos, pos_in_item);
+  printk ("*********************************************************************\n");
+
+  printk ("* h *    S    *    L    *    R    *   F   *   FL  *   FR  *  CFL  *  CFR  *\n");
+/*
+01234567890123456789012345678901234567890123456789012345678901234567890123456789
+       1        2         3         4         5         6         7         8
+  printk ("*********************************************************************\n");
+*/
+  
+  
+  for (h = 0; h < sizeof(tb->insert_size) / sizeof (tb->insert_size[0]); h ++) {
+    if (PATH_H_PATH_OFFSET (tb->tb_path, h) <= tb->tb_path->path_length && 
+	PATH_H_PATH_OFFSET (tb->tb_path, h) > ILLEGAL_PATH_ELEMENT_OFFSET) {
+      tbSh = PATH_H_PBUFFER (tb->tb_path, h);
+      tbFh = PATH_H_PPARENT (tb->tb_path, h);
+    } else {
+      /*      printk ("print_tb: h=%d, PATH_H_PATH_OFFSET=%d, path_length=%d\n", 
+	      h, PATH_H_PATH_OFFSET (tb->tb_path, h), tb->tb_path->path_length);*/
+      tbSh = 0;
+      tbFh = 0;
+    }
+    printk ("* %d * %3ld(%2d) * %3ld(%2d) * %3ld(%2d) * %5ld * %5ld * %5ld * %5ld * %5ld *\n",
+	    h, 
+	    (tbSh) ? (tbSh->b_blocknr):(-1),
+	    (tbSh) ? tbSh->b_count : -1,
+	    (tb->L[h]) ? (tb->L[h]->b_blocknr):(-1),
+	    (tb->L[h]) ? tb->L[h]->b_count : -1,
+	    (tb->R[h]) ? (tb->R[h]->b_blocknr):(-1),
+	    (tb->R[h]) ? tb->R[h]->b_count : -1,
+	    (tbFh) ? (tbFh->b_blocknr):(-1),
+	    (tb->FL[h]) ? (tb->FL[h]->b_blocknr):(-1),
+	    (tb->FR[h]) ? (tb->FR[h]->b_blocknr):(-1),
+	    (tb->CFL[h]) ? (tb->CFL[h]->b_blocknr):(-1),
+	    (tb->CFR[h]) ? (tb->CFR[h]->b_blocknr):(-1));
+  }
+
+  printk ("*********************************************************************\n");
+
+
+  /* print balance parameters for leaf level */
+  h = 0;
+  printk ("* h * size * ln * lb * rn * rb * blkn * s0 * s1 * s1b * s2 * s2b * curb * lk * rk *\n");
+  printk ("* %d * %4d * %2d * %2d * %2d * %2d * %4d * %2d * %2d * %3d * %2d * %3d * %4d * %2d * %2d *\n",
+	  h, tb->insert_size[h], tb->lnum[h], tb->lbytes, tb->rnum[h],tb->rbytes, tb->blknum[h], 
+	  tb->s0num, tb->s1num,tb->s1bytes,  tb->s2num, tb->s2bytes, tb->cur_blknum, tb->lkey[h], tb->rkey[h]);
+
+
+/* this prints balance parameters for non-leaf levels */
+  do {
+    h++;
+    printk ("* %d * %4d * %2d *    * %2d *    * %2d *\n",
+    h, tb->insert_size[h], tb->lnum[h], tb->rnum[h], tb->blknum[h]);
+  } while (tb->insert_size[h]);
+
+  printk ("*********************************************************************\n");
+
+
+  /* print FEB list (list of buffers in form (bh (b_blocknr, b_count), that will be used for new nodes) */
+  h = 0;
+  for (i = 0; i < sizeof (tb->FEB) / sizeof (tb->FEB[0]); i ++)
+    printk ("%s%p (%lu %d)", i == 0 ? "FEB list: " : ", ", tb->FEB[i], tb->FEB[i] ? tb->FEB[i]->b_blocknr : 0,
+	    tb->FEB[i] ? tb->FEB[i]->b_count : 0);
+  printk ("\n");
+
+  printk ("********************** END OF PRINT_TB *******************\n\n");
+
+}
+
+
+void print_bmap_block (int i, struct buffer_head * bmap, int blocks, int silent)
+{
+    int j, k;
+    int bits = bmap->b_size * 8;
+    int zeros = 0, ones = 0;
+  
+    printk ("#%d: block %lu: ", i, bmap->b_blocknr);
+
+    if (test_bit (0, bmap->b_data)) {
+	/* first block addressed by this bitmap block is used */
+	ones ++;
+	if (!silent)
+	    printk ("Busy (%d-", i * bits);
+	for (j = 1; j < blocks; j ++) {
+	    while (test_bit (j, bmap->b_data)) {
+		ones ++;
+		if (j == blocks - 1) {
+		    if (!silent)
+			printk ("%d)\n", j + i * bits);
+		    goto end;
+		}
+		j++;
+	    }
+	    if (!silent)
+		printk ("%d) Free(%d-", j - 1 + i * bits, j + i * bits);
+
+	    while (!test_bit (j, bmap->b_data)) {
+		zeros ++;
+		if (j == blocks - 1) {
+		    if (!silent)
+			printk ("%d)\n", j + i * bits);
+		    goto end;
+		}
+		j++;
+	    }
+	    if (!silent)
+		printk ("%d) Busy(%d-", j - 1 + i * bits, j + i * bits);
+
+	    j --;
+	end:
+	}
+    } else {
+	/* first block addressed by this bitmap is free */
+	zeros ++;
+	if (!silent)
+	    printk ("Free (%d-", i * bits);
+	for (j = 1; j < blocks; j ++) {
+	    k = 0;
+	    while (!test_bit (j, bmap->b_data)) {
+		k ++;
+		if (j == blocks - 1) {
+		    if (!silent)
+			printk ("%d)\n", j + i * bits);
+		    zeros += k;
+		    goto end2;
+		}
+		j++;
+	    }
+	    zeros += k;
+	    if (!silent)
+		printk ("%d) Busy(%d-", j - 1 + i * bits, j + i * bits);
+	    
+	    k = 0;
+	    while (test_bit (j, bmap->b_data)) {
+		ones ++;
+		if (j == blocks - 1) {
+		    if (!silent)
+			printk ("%d)\n", j + i * bits);
+		    ones += k;
+		    goto end2;
+		}
+		j++;
+	    }
+	    ones += k;
+	    if (!silent)
+		printk ("%d) Free(%d-", j - 1 + i * bits, j + i * bits);
+	
+	    j --;
+	end2:
+	}
+    }
+
+    printk ("used %d, free %d\n", ones, zeros);
+}
+
+
+/* if silent == 1, do not print details */
+void print_bmap (struct super_block * s, int silent)
+{
+    int bmapnr = SB_BMAP_NR (s);
+    int i;
+    int blocks = s->s_blocksize * 8; /* adressed by bitmap */
+
+    printk ("Bitmap blocks are:\n");
+    for (i = 0; i < bmapnr; i ++) {
+
+	if (i == bmapnr - 1)
+	    if (SB_BLOCK_COUNT (s) % (s->s_blocksize * 8))
+		blocks = SB_BLOCK_COUNT (s) % (s->s_blocksize * 8);
+	print_bmap_block (i, SB_AP_BITMAP(s)[i], blocks, silent);
+    }
+
+    /* check unused part of last bitmap */
+    {
+	int bad_unused_bitmap = 0;
+	int ones;
+
+	ones = s->s_blocksize * 8 - SB_BLOCK_COUNT (s) % (s->s_blocksize * 8);
+	if (ones == s->s_blocksize * 8)
+	    ones = 0;
+      
+	for (i = s->s_blocksize * 8; --i >= blocks; )
+	    if (!test_bit (i, SB_AP_BITMAP (s)[bmapnr - 1]->b_data))
+		bad_unused_bitmap ++;
+
+	if (bad_unused_bitmap) {
+	    printk ("Unused part of bitmap is wrong: should be %d ones, found %d zeros\n",
+		    ones, bad_unused_bitmap);
+	}
+    }
+    
+}
+
+
+
+void print_objectid_map (struct super_block * s)
+{
+  int i;
+  struct reiserfs_super_block * rs;
+  unsigned long * omap;
+
+  rs = SB_DISK_SUPER_BLOCK (s);
+  omap = (unsigned long *)(rs + 1);
+  printk ("Map of objectids\n");
+      
+  for (i = 0; i < rs->s_oid_cursize; i ++) {
+    if (i % 2 == 0)
+      printk ("busy(%lu-%lu) ", omap[i], omap[i+1] - 1); 
+    else
+      printk ("free(%lu-%lu) ", 
+	      omap[i], ((i+1) == rs->s_oid_cursize) ? -1 : omap[i+1] - 1);
+    }
+  printk ("\n");
+  
+  printk ("Object id array has size %d (max %d):", rs->s_oid_cursize, 
+	  rs->s_oid_maxsize);
+  
+  for (i = 0; i < rs->s_oid_cursize; i ++)
+    printk ("%lu ", omap[i]); 
+  printk ("\n");
+
+}
+
+
+static void check_leaf_block_head (struct buffer_head * bh)
+{
+  struct block_head * blkh;
+
+  blkh = B_BLK_HEAD (bh);
+  if (le16_to_cpu (blkh->blk_nr_item) > (bh->b_size - BLKH_SIZE) / IH_SIZE)
+    reiserfs_panic (0, "vs-6010: check_leaf_block_head: invalid item number %z", bh);
+  if (le16_to_cpu (blkh->blk_free_space) > 
+      bh->b_size - BLKH_SIZE - IH_SIZE * le16_to_cpu (blkh->blk_nr_item))
+    reiserfs_panic (0, "vs-6020: check_leaf_block_head: invalid free space %z", bh);
+    
+}
+
+static void check_internal_block_head (struct buffer_head * bh)
+{
+  struct block_head * blkh;
+
+  return  ;
+  blkh = B_BLK_HEAD (bh);
+  if (!(le16_to_cpu (blkh->blk_level) > DISK_LEAF_NODE_LEVEL && le16_to_cpu (blkh->blk_level) <= MAX_HEIGHT))
+    reiserfs_panic (0, "vs-6025: check_internal_block_head: invalid level %z", bh);
+
+  if (le16_to_cpu (blkh->blk_nr_item) > (bh->b_size - BLKH_SIZE) / IH_SIZE)
+    reiserfs_panic (0, "vs-6030: check_internal_block_head: invalid item number %z", bh);
+
+  if (le16_to_cpu (blkh->blk_free_space) != 
+      bh->b_size - BLKH_SIZE - KEY_SIZE * le16_to_cpu (blkh->blk_nr_item) - DC_SIZE * (blkh->blk_nr_item + 1))
+    reiserfs_panic (0, "vs-6040: check_internal_block_head: invalid free space %z", bh);
+
+}
+
+
+void check_leaf (struct buffer_head * bh)
+{
+  int i;
+  struct item_head * ih;
+
+  if (!bh)
+    return;
+  check_leaf_block_head (bh);
+  for (i = 0, ih = B_N_PITEM_HEAD (bh, 0); i < B_NR_ITEMS (bh); i ++, ih ++) {
+    if (I_IS_DIRECTORY_ITEM (ih))
+      check_directory_item (bh, ih);
+  }
+}
+
+
+void check_internal (struct buffer_head * bh)
+{
+  if (!bh)
+    return;
+  check_internal_block_head (bh);
+}
+
+
+void print_statistics (struct super_block * s)
+{
+  /*
+  printk ("reiserfs_put_super: session statistics: balances %d, fix_nodes %d, preserve list freeings %d, \
+bmap with search %d, without %d, dir2ind %d, ind2dir %d\n",
+	  s->u.reiserfs_sb.s_do_balance, s->u.reiserfs_sb.s_fix_nodes, s->u.reiserfs_sb.s_preserve_list_freeings,
+	  s->u.reiserfs_sb.s_bmaps, s->u.reiserfs_sb.s_bmaps_without_search,
+	  s->u.reiserfs_sb.s_direct2indirect, s->u.reiserfs_sb.s_indirect2direct);
+  */
+
+}
diff -urN linux/fs/reiserfs/resize.c /tmp/linux/fs/reiserfs/resize.c
--- linux/fs/reiserfs/resize.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/resize.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,177 @@
+/* 
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+/*
+ * Written by Alexander Zarochentcev.
+ *
+ * The kernel part of the (on-line) reiserfs resizer.
+ */
+
+#ifdef __KERNEL__
+
+#include <linux/kernel.h>
+#include <linux/vmalloc.h>
+#include <linux/locks.h>
+#include <linux/string.h>
+#include <linux/reiserfs_fs.h>
+#include <linux/reiserfs_fs_sb.h>
+
+#else
+
+#include "nokernel.h"
+
+#endif
+
+int reiserfs_resize (struct super_block * s, unsigned long block_count_new)
+{
+	struct reiserfs_super_block * sb;
+	struct buffer_head ** bitmap, * bh;
+	struct reiserfs_transaction_handle th;
+	unsigned int bmap_nr_new, bmap_nr;
+	unsigned int block_r_new, block_r;
+	
+	struct reiserfs_list_bitmap * jb;
+	struct reiserfs_list_bitmap jbitmap[JOURNAL_NUM_BITMAPS];
+	
+	unsigned long int block_count, free_blocks;
+	int i; 
+	int copy_size ;
+
+	sb = SB_DISK_SUPER_BLOCK(s);
+
+	if (SB_BLOCK_COUNT(s) >= block_count_new) {
+		printk("can\'t shrink filesystem on-line\n");
+		return 1;
+	}
+
+	/* check the device size */
+	bh = bread(s->s_dev, block_count_new - 1, s->s_blocksize);
+	if (!bh) {
+		printk("reiserfs_resize: can\'t read last block\n");
+		return 1;
+	}	
+	brelse(bh);
+
+	/* old disk layout detection; those partitions can be mounted, but
+	 * cannot be resized */
+	if (SB_BUFFER_WITH_SB(s)->b_blocknr *	SB_BUFFER_WITH_SB(s)->b_size 
+		!= REISERFS_DISK_OFFSET_IN_BYTES ) {
+		printk("reiserfs_resize: unable to resize a reiserfs without distributed bitmap (fs version < 3.5.12)\n");
+		return 1;
+	}
+	
+	lock_super(s);
+	
+	/* count used bits in last bitmap block */
+	block_r = le32_to_cpu(SB_BLOCK_COUNT(s)) -
+	        (le16_to_cpu(SB_BMAP_NR(s)) - 1) * s->s_blocksize * 8;
+	
+	/* count bitmap blocks in new fs */
+	bmap_nr_new = block_count_new / ( s->s_blocksize * 8 );
+	block_r_new = block_count_new - bmap_nr_new * s->s_blocksize * 8;
+	if (block_r_new) 
+		bmap_nr_new++;
+	else
+		block_r_new = s->s_blocksize * 8;
+
+	/* save old values */
+	block_count = le32_to_cpu(SB_BLOCK_COUNT(s));
+	bmap_nr     = le16_to_cpu(SB_BMAP_NR(s));
+
+	/* reallocate journal bitmaps */
+	if (reiserfs_allocate_list_bitmaps(s, jbitmap, bmap_nr_new) < 0) {
+		printk("reiserfs_resize: unable to allocate memory for journal bitmaps\n");
+		unlock_super(s) ;
+		return 1 ;
+	}
+	/* the new journal bitmaps are zero filled, now we copy in the bitmap
+	** node pointers from the old journal bitmap structs, and then
+	** transfer the new data structures into the journal struct.
+	**
+	** using the copy_size var below allows this code to work for
+	** both shrinking and expanding the FS.
+	*/
+	copy_size = bmap_nr_new < bmap_nr ? bmap_nr_new : bmap_nr ;
+	copy_size = copy_size * sizeof(struct reiserfs_list_bitmap_node *) ;
+	for (i = 0 ; i < JOURNAL_NUM_BITMAPS ; i++) {
+		struct reiserfs_bitmap_node **node_tmp ;
+		jb = SB_JOURNAL(s)->j_list_bitmap + i ;
+		memcpy(jbitmap[i].bitmaps, jb->bitmaps, copy_size) ;
+
+		/* just in case vfree schedules on us, copy the new
+		** pointer into the journal struct before freeing the 
+		** old one
+		*/
+		node_tmp = jb->bitmaps ;
+		jb->bitmaps = jbitmap[i].bitmaps ;
+		vfree(node_tmp) ;
+	}
+
+	/* allocate additional bitmap blocks, reallocate array of bitmap
+	 * block pointers */
+	if (bmap_nr_new > bmap_nr) {
+		bitmap = reiserfs_kmalloc(sizeof(struct buffer_head *) * bmap_nr_new,
+								   GFP_KERNEL, s);
+		if (!bitmap) {
+			printk("reiserfs_resize: unable to allocate memory.\n");
+			unlock_super(s);
+			return 1;
+		}
+		for (i = 0; i < bmap_nr; i++)
+			bitmap[i] = SB_AP_BITMAP(s)[i];
+		for (i = bmap_nr; i < bmap_nr_new; i++) {
+			bitmap[i] = getblk(s->s_dev, i * s->s_blocksize * 8, s->s_blocksize);
+			if(!bitmap[i]) {
+				printk("reiserfs_resize: getblk() failed");
+				while (--i >= bmap_nr) 
+					brelse(bitmap[i]);
+				reiserfs_kfree(bitmap, 
+					sizeof(struct buffer_head *) * bmap_nr_new, s);
+				unlock_super(s);	
+				return 1;
+			}
+			memset(bitmap[i]->b_data, 0, sb->s_blocksize);
+			set_bit(0, bitmap[i]->b_data);
+
+			mark_buffer_dirty(bitmap[i], 0);
+			mark_buffer_uptodate(bitmap[i], 1);
+			ll_rw_block(WRITE, 1, bitmap + i);
+			wait_on_buffer(bitmap[i]);
+		}	
+		/* free old bitmap blocks array */
+		reiserfs_kfree(SB_AP_BITMAP(s), 
+			sizeof(struct buffer_head *) * bmap_nr, s);
+		SB_AP_BITMAP(s) = bitmap;
+	}
+
+	/* begin transaction */
+	journal_begin(&th, s, 10);
+
+	/* correct last bitmap blocks in old and new disk layout */
+	for (i = block_r; i < s->s_blocksize * 8; i++)
+		clear_bit(i, SB_AP_BITMAP(s)[bmap_nr - 1]->b_data);
+	journal_mark_dirty(&th, s, SB_AP_BITMAP(s)[bmap_nr - 1]);
+
+	for (i = block_r_new; i < s->s_blocksize * 8; i++)
+		set_bit(i, SB_AP_BITMAP(s)[bmap_nr_new - 1]->b_data);
+	journal_mark_dirty(&th, s, SB_AP_BITMAP(s)[bmap_nr_new - 1]);
+ 
+ 	/* update super */
+	free_blocks = le32_to_cpu(SB_FREE_BLOCKS(s));
+	sb->s_free_blocks = cpu_to_le32(free_blocks + (block_count_new 
+		- block_count - (bmap_nr_new - bmap_nr)));
+	sb->s_block_count = cpu_to_le32(block_count_new);
+	sb->s_bmap_nr = cpu_to_le16(bmap_nr_new);
+	s->s_dirt = 1;
+
+	journal_mark_dirty(&th, s, SB_BUFFER_WITH_SB(s));
+	
+	SB_JOURNAL(s)->j_must_wait = 1;
+	journal_end(&th, s, 10);
+
+	unlock_super(s);
+
+	return 0;
+}
+
diff -urN linux/fs/reiserfs/stree.c /tmp/linux/fs/reiserfs/stree.c
--- linux/fs/reiserfs/stree.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/stree.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,2674 @@
+/*
+ *  Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+/*
+ *  Written by Anatoly P. Pinchuk pap@namesys.botik.ru
+ *  Programm System Institute
+ *  Pereslavl-Zalessky Russia
+ */
+
+/*
+ *  This file contains functions dealing with S+tree
+ *
+ * comp_keys
+ * comp_short_keys
+ * bin_search
+ * get_lkey
+ * get_rkey
+ * key_in_buffer
+ * decrement_bcount
+ * decrement_counters_in_path
+ * pathrelse
+ * search_by_key
+ * search_for_position_by_key
+ * comp_items
+ * prepare_for_delete_or_cut
+ * calc_deleted_bytes_number
+ * init_tb_struct
+ * reiserfs_delete_item
+ * reiserfs_delete_object
+ * indirect_to_direct
+ * maybe_indirect_to_direct
+ * reiserfs_cut_from_item
+ * reiserfs_truncate_file
+ * reiserfs_paste_into_item
+ * reiserfs_insert_item
+ * get_buffer_by_range
+ * get_buffers_from_range
+ */
+#ifdef __KERNEL__
+
+#include <linux/sched.h>
+#include <linux/string.h>
+#include <linux/reiserfs_fs.h>
+
+#else
+
+#include "nokernel.h"
+
+#endif
+
+/* Does the buffer contain a disk block which is in the tree. */
+inline int B_IS_IN_TREE (struct buffer_head * p_s_bh)
+{
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( B_BLK_HEAD(p_s_bh)->blk_level > MAX_HEIGHT ) {
+    reiserfs_panic(0, "PAP-1010: B_IS_IN_TREE: block (%b) has too big level (%z)",
+		   p_s_bh, p_s_bh);
+  }
+#endif
+
+  return ( B_BLK_HEAD(p_s_bh)->blk_level != FREE_LEVEL );
+}
+
+
+inline void copy_key (void * to, void * from)
+{
+  memcpy (to, from, KEY_SIZE);
+}
+
+inline void copy_short_key (void * to, void * from)
+{
+  memcpy (to, from, SHORT_KEY_SIZE);
+}
+
+inline void copy_item_head(void * p_v_to, void * p_v_from)
+{
+  memcpy (p_v_to, p_v_from, IH_SIZE);
+}
+
+
+/*
+ Compare keys using all 4 key fields.
+ Returns:  -1 if key1 < key2
+            0 if key1 = key2
+            1 if key1 > key2
+*/
+inline int  comp_keys (void * k1, void * k2)
+{
+  __u32 * p_s_key1, * p_s_key2;
+  int n_key_length = REISERFS_FULL_KEY_LEN;
+
+  p_s_key1 = (__u32 *)k1;
+  p_s_key2 = (__u32 *)k2;
+  for( ; n_key_length--; ++p_s_key1, ++p_s_key2 ) {
+    if ( *p_s_key1 < *p_s_key2 )
+      return -1;
+    if ( *p_s_key1 > *p_s_key2 )
+      return 1;
+  }
+
+  return 0;
+}
+
+
+/*
+ Compare keys using REISERFS_SHORT_KEY_LEN fields.
+ Returns:  -1 if key1 < key2
+            0 if key1 = key2
+            1 if key1 > key2
+*/
+inline int  comp_short_keys (void * k1, void * k2)
+{
+  __u32 * p_s_key1, * p_s_key2;
+  int n_key_length = REISERFS_SHORT_KEY_LEN;
+
+  p_s_key1 = (__u32 *)k1;
+  p_s_key2 = (__u32 *)k2;
+
+  for( ; n_key_length--; ++p_s_key1, ++p_s_key2 ) {
+    if ( *p_s_key1 < *p_s_key2 )
+      return -1;
+    if ( *p_s_key1 > *p_s_key2 )
+      return 1;
+  }
+
+  return 0;
+}
+
+
+
+
+
+
+
+
+/**************************************************************************
+ *  Binary search toolkit function                                        *
+ *  Search for an item in the array by the item key                       *
+ *  Returns:    1 if found,  0 if not found;                              *
+ *        *p_n_pos = number of the searched element if found, else the    *
+ *        number of the first element that is larger than p_v_key.        *
+ **************************************************************************/
+/* For those not familiar with binary search: n_lbound is the leftmost item that it
+ could be, n_rbound the rightmost item that it could be.  We examine the item
+ halfway between n_lbound and n_rbound, and that tells us either that we can increase
+ n_lbound, or decrease n_rbound, or that we have found it, or if n_lbound <= n_rbound that
+ there are no possible items, and we have not found it. With each examination we
+ cut the number of possible items it could be by one more than half rounded down,
+ or we find it. */
+inline	int bin_search (
+              void    * p_v_key,    /* Key to search for.                   */
+	      void    * p_v_base,   /* First item in the array.             */
+	      int       p_n_num,    /* Number of items in the array.        */
+	      int       p_n_width,  /* Item size in the array.
+				       searched. Lest the reader be
+				       confused, note that this is crafted
+				       as a general function, and when it
+				       is applied specifically to the array
+				       of item headers in a node, p_n_width
+				       is actually the item header size not
+				       the item size.                      */
+	      int     * p_n_pos     /* Number of the searched for element. */
+            ) {
+  int   n_rbound, n_lbound, n_j;
+
+  for ( n_j = ((n_rbound = p_n_num - 1) + (n_lbound = 0))/2; n_lbound <= n_rbound; n_j = (n_rbound + n_lbound)/2 )
+    switch( COMP_KEYS(((char * )p_v_base + n_j * p_n_width), p_v_key) )  {
+    case -1: n_lbound = n_j + 1; continue;
+    case  1: n_rbound = n_j - 1; continue;
+    case  0: *p_n_pos = n_j;     return ITEM_FOUND; /* Key found in the array.  */
+    }
+
+  /* bin_search did not find given key, it returns position of key,
+     that is minimal and greater than the given one. */
+  *p_n_pos = n_lbound;
+  return ITEM_NOT_FOUND;
+}
+
+#ifdef CONFIG_REISERFS_CHECK
+extern struct tree_balance * cur_tb;
+extern struct tree_balance init_tb;
+extern int init_item_pos, init_pos_in_item, init_mode;
+#endif
+
+
+
+/* Minimal possible key. It is never in the tree. */
+struct key  MIN_KEY = {0, 0, 0, 0};
+
+/* Maximal possible key. It is never in the tree. */
+struct key  MAX_KEY = {0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff};
+
+
+/* Get delimiting key of the buffer by looking for it in the buffers in the path, starting from the bottom
+   of the path, and going upwards.  We must check the path's validity at each step.  If the key is not in
+   the path, there is no delimiting key in the tree (buffer is first or last buffer in tree), and in this
+   case we return a special key, either MIN_KEY or MAX_KEY. */
+inline	struct  key * get_lkey  (
+	                struct path         * p_s_chk_path,
+                        struct super_block  * p_s_sb
+                      ) {
+  int                   n_position, n_path_offset = p_s_chk_path->path_length;
+  struct buffer_head  * p_s_parent;
+  
+#ifdef CONFIG_REISERFS_CHECK
+  if ( n_path_offset < FIRST_PATH_ELEMENT_OFFSET )
+    reiserfs_panic(p_s_sb,"PAP-5010: get_lkey: illegal offset in the path");
+#endif
+
+  /* While not higher in path than first element. */
+  while ( n_path_offset-- > FIRST_PATH_ELEMENT_OFFSET ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( ! buffer_uptodate(PATH_OFFSET_PBUFFER(p_s_chk_path, n_path_offset)) )
+      reiserfs_panic(p_s_sb, "PAP-5020: get_lkey: parent is not uptodate");
+#endif
+
+    /* Parent at the path is not in the tree now. */
+    if ( ! B_IS_IN_TREE(p_s_parent = PATH_OFFSET_PBUFFER(p_s_chk_path, n_path_offset)) )
+      return &MAX_KEY;
+    /* Check whether position in the parent is correct. */
+    if ( (n_position = PATH_OFFSET_POSITION(p_s_chk_path, n_path_offset)) > B_NR_ITEMS(p_s_parent) )
+       return &MAX_KEY;
+    /* Check whether parent at the path really points to the child. */
+    if ( B_N_CHILD_NUM(p_s_parent, n_position) !=
+	 PATH_OFFSET_PBUFFER(p_s_chk_path, n_path_offset + 1)->b_blocknr )
+      return &MAX_KEY;
+    /* Return delimiting key if position in the parent is not equal to zero. */
+    if ( n_position )
+      return B_N_PDELIM_KEY(p_s_parent, n_position - 1);
+  }
+  /* Return MIN_KEY if we are in the root of the buffer tree. */
+  if ( PATH_OFFSET_PBUFFER(p_s_chk_path, FIRST_PATH_ELEMENT_OFFSET)->b_blocknr ==
+                                      p_s_sb->u.reiserfs_sb.s_rs->s_root_block )
+    return &MIN_KEY;
+  return  &MAX_KEY;
+}
+
+
+/* Get delimiting key of the buffer at the path and its right neighbor. */
+inline	struct  key * get_rkey  (
+	                struct path         * p_s_chk_path,
+                        struct super_block  * p_s_sb
+                      ) {
+  int                   n_position,
+    			n_path_offset = p_s_chk_path->path_length;
+  struct buffer_head  * p_s_parent;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( n_path_offset < FIRST_PATH_ELEMENT_OFFSET )
+    reiserfs_panic(p_s_sb,"PAP-5030: get_rkey: illegal offset in the path");
+#endif
+
+  while ( n_path_offset-- > FIRST_PATH_ELEMENT_OFFSET ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( ! buffer_uptodate(PATH_OFFSET_PBUFFER(p_s_chk_path, n_path_offset)) )
+      reiserfs_panic(p_s_sb, "PAP-5040: get_rkey: parent is not uptodate");
+#endif
+
+    /* Parent at the path is not in the tree now. */
+    if ( ! B_IS_IN_TREE(p_s_parent = PATH_OFFSET_PBUFFER(p_s_chk_path, n_path_offset)) )
+      return &MIN_KEY;
+    /* Check whether position in the parrent is correct. */
+    if ( (n_position = PATH_OFFSET_POSITION(p_s_chk_path, n_path_offset)) > B_NR_ITEMS(p_s_parent) )
+      return &MIN_KEY;
+    /* Check whether parent at the path really points to the child. */
+    if ( B_N_CHILD_NUM(p_s_parent, n_position) !=
+                                        PATH_OFFSET_PBUFFER(p_s_chk_path, n_path_offset + 1)->b_blocknr )
+      return &MIN_KEY;
+    /* Return delimiting key if position in the parent is not the last one. */
+    if ( n_position != B_NR_ITEMS(p_s_parent) )
+      return B_N_PDELIM_KEY(p_s_parent, n_position);
+  }
+  /* Return MAX_KEY if we are in the root of the buffer tree. */
+  if ( PATH_OFFSET_PBUFFER(p_s_chk_path, FIRST_PATH_ELEMENT_OFFSET)->b_blocknr ==
+       p_s_sb->u.reiserfs_sb.s_rs->s_root_block )
+    return &MAX_KEY;
+  return  &MIN_KEY;
+}
+
+
+/* Check whether a key is contained in the tree rooted from a buffer at a path. */
+/* This works by looking at the left and right delimiting keys for the buffer in the last path_element in
+   the path.  These delimiting keys are stored at least one level above that buffer in the tree. If the
+   buffer is the first or last node in the tree order then one of the delimiting keys may be absent, and in
+   this case get_lkey and get_rkey return a special key which is MIN_KEY or MAX_KEY. */
+static  inline  int key_in_buffer (
+                      struct path         * p_s_chk_path, /* Path which should be checked.  */
+                      struct key          * p_s_key,      /* Key which should be checked.   */
+                      struct super_block  * p_s_sb        /* Super block pointer.           */
+		      ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( ! p_s_key || p_s_chk_path->path_length < FIRST_PATH_ELEMENT_OFFSET ||
+       p_s_chk_path->path_length > MAX_HEIGHT )
+    reiserfs_panic(p_s_sb, "PAP-5050: key_in_buffer:  pointer to the key(%p) is NULL or illegal path length(%d)",
+		   p_s_key, p_s_chk_path->path_length);
+  
+  if ( PATH_PLAST_BUFFER(p_s_chk_path)->b_dev == NODEV )
+    reiserfs_panic(p_s_sb, "PAP-5060: key_in_buffer: device must not be NODEV");
+#endif
+
+  if ( COMP_KEYS(get_lkey(p_s_chk_path, p_s_sb), p_s_key) == 1 )
+    return 0;
+  if ( COMP_KEYS(p_s_key, get_rkey(p_s_chk_path, p_s_sb)) != -1 )
+    return 0;
+  return 1;
+}
+
+
+inline void decrement_bcount(
+              struct buffer_head  * p_s_bh
+            ) { 
+  if ( p_s_bh ) {
+    if ( p_s_bh->b_count ) {
+      p_s_bh->b_count--;
+      return;
+    }
+    reiserfs_panic(NULL, "PAP-5070: decrement_bcount: trying to free free buffer %b", p_s_bh);
+  }
+}
+
+
+/* Decrement b_count field of the all buffers in the path. */
+void decrement_counters_in_path (
+              struct path * p_s_search_path
+            ) {
+  int n_path_offset = p_s_search_path->path_length;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( n_path_offset < ILLEGAL_PATH_ELEMENT_OFFSET ||
+       n_path_offset > EXTENDED_MAX_HEIGHT - 1 )
+    reiserfs_panic(NULL, "PAP-5080: decrement_counters_in_path: illegal path offset of %d", n_path_offset);
+#endif
+
+  while ( n_path_offset > ILLEGAL_PATH_ELEMENT_OFFSET )
+    decrement_bcount(PATH_OFFSET_PBUFFER(p_s_search_path, n_path_offset--));
+  p_s_search_path->path_length = ILLEGAL_PATH_ELEMENT_OFFSET;
+}
+
+
+/* Release all buffers in the path. */
+void  pathrelse (
+        struct path * p_s_search_path
+      ) {
+  int n_path_offset = p_s_search_path->path_length;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( n_path_offset < ILLEGAL_PATH_ELEMENT_OFFSET )
+    reiserfs_panic(NULL, "PAP-5090: pathrelse: illegal path offset");
+#endif
+  
+  while ( n_path_offset > ILLEGAL_PATH_ELEMENT_OFFSET ) 
+    brelse(PATH_OFFSET_PBUFFER(p_s_search_path, n_path_offset--));
+
+  p_s_search_path->path_length = ILLEGAL_PATH_ELEMENT_OFFSET;
+}
+
+
+#ifdef SEARCH_BY_KEY_READA
+
+static int search_by_key_reada (struct super_block * s, int blocknr)
+{
+  struct buffer_head * bh;
+  int repeat;
+  
+  repeat = CARRY_ON;
+  if (blocknr == 0)
+    return CARRY_ON;
+
+  bh = reiserfs_getblk (s->s_dev, blocknr, s->s_blocksize, &repeat);
+  
+  if (!buffer_uptodate (bh)) {
+    ll_rw_block (READA, 1, &bh);
+    repeat = SCHEDULE_OCCURRED;
+  }
+  bh->b_count --;
+  return repeat;
+}
+
+#endif
+
+/**************************************************************************
+ * Algorithm   SearchByKey                                                *
+ *             look for item in the Disk S+Tree by its key                *
+ * Input:  p_s_sb   -  super block                                        *
+ *         p_s_key  - pointer to the key to search                        *
+ * Output: true value -  1 - found,  0 - not found                        *
+ *         p_s_search_path - path from the root to the needed leaf        *
+ **************************************************************************/
+
+/* This function fills up the path from the root to the leaf as it
+   descends the tree looking for the key.  It uses reiserfs_bread to
+   try to find buffers in the cache given their block number.  If it
+   does not find them in the cache it reads them from disk.  For each
+   node search_by_key finds using reiserfs_bread it then uses
+   bin_search to look through that node.  bin_search will find the
+   position of the block_number of the next node if it is looking
+   through an internal node.  If it is looking through a leaf node
+   bin_search will find the position of the item which has key either
+   equal to given key, or which is the maximal key less than the given
+   key.  search_by_key returns a path that must be checked for the
+   correctness of the top of the path but need not be checked for the
+   correctness of the bottom of the path */
+int search_by_key(
+                  struct super_block  * p_s_sb,         /* Super block.                           */
+                  struct key          * p_s_key,        /* Key to search.                         */
+                  struct path         * p_s_search_path,/* This structure was allocated and initialized by
+                                                           the calling function. It is filled up by this
+                                                           function.  */
+                  int                 * p_n_repeat,     /* Whether schedule occured. */
+                  int                   n_stop_level,   /* How far down the tree to search.*/
+                  int                   n_bread_par     /* Whether to search even if it requires disk I/O, this is
+                                                           either READ_BLOCKS or DONT_READ_BLOCKS or 0. Hans doesn't
+                                                           know what 0 means, it seems to evaluate to DONT_READ_BLOCKS,
+                                                           but it is bad style to not use the macro.... there is a
+                                                           #define of search by key with no explanation that can allow
+                                                           it to happen.... */
+                  ) {
+    kdev_t                      n_dev           = p_s_sb->s_dev;
+    int                         n_repeat,
+                                n_block_number  = p_s_sb->u.reiserfs_sb.s_rs->s_root_block,
+                                n_block_size    = p_s_sb->s_blocksize;
+    struct buffer_head  *       p_s_bh;
+    struct path_element *       p_s_last_element;
+    int				n_node_level, n_retval;
+    int 			right_neighbor_of_leaf_node;
+
+#ifdef CONFIG_REISERFS_CHECK
+    int n_repeat_counter = 0;
+#endif
+
+    /* As we add each node to a path we increase its count.  This means that we must be careful to
+       release all nodes in a path before we either discard the path struct or re-use the path
+       struct, as we do here. */
+
+    decrement_counters_in_path(p_s_search_path);
+
+    *p_n_repeat = CARRY_ON;
+    right_neighbor_of_leaf_node = 0;
+
+    /* With each iteration of this loop we search through the items in the current node, and
+       calculate the next current node(next path element) for the next iteration of this loop.. */
+    while ( 1 ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+      if ( !(++n_repeat_counter % 50000) )
+	printk ("PAP-5100: search_by_key(pid %u): there were %d searches from the tree_root lokking for key %p\n",
+			  current->pid, n_repeat_counter, p_s_key);
+#endif
+
+      /* prep path to have another element added to it. */
+      p_s_last_element = PATH_OFFSET_PELEMENT(p_s_search_path, ++p_s_search_path->path_length);
+      if (p_s_search_path->path_length >= EXTENDED_MAX_HEIGHT) { 
+	      reiserfs_warning("ak-1000: search_by_key: path grew too long: %k\n", p_s_key); 
+	      --p_s_search_path->path_length; 
+	      pathrelse(p_s_search_path); 
+	      *p_n_repeat |= n_repeat; 
+	      return ITEM_NOT_FOUND; 
+      }
+      
+
+      n_repeat = CARRY_ON;
+
+      if ( n_bread_par == READ_BLOCKS ) { 
+	/* schedule read of right neighbor */
+#ifdef SEARCH_BY_KEY_READA
+	n_repeat |= search_by_key_reada (p_s_sb, right_neighbor_of_leaf_node);
+#endif
+
+	/* Read the next tree node, and set the last element in the path to have a pointer to it. */
+	if ( ! (p_s_bh = p_s_last_element->pe_buffer =
+		reiserfs_bread(n_dev, n_block_number, n_block_size, &n_repeat)) ) {
+	  p_s_search_path->path_length --;
+	  pathrelse(p_s_search_path);
+	  *p_n_repeat |= n_repeat;
+	  return ITEM_NOT_FOUND;	/* IO error */
+	}
+      }
+      else { /* We are looking for the next tree node in cache. */
+	p_s_bh = p_s_last_element->pe_buffer = reiserfs_getblk(n_dev, n_block_number, n_block_size, &n_repeat);
+      }
+
+      *p_n_repeat |= n_repeat;
+
+      /* It is possible that schedule occured. We must check whether the key to search is still in
+	 the tree rooted from the current buffer. If not then repeat search from the root. */
+      if ( n_repeat != CARRY_ON && ((buffer_uptodate (p_s_bh) && !B_IS_IN_TREE (p_s_bh)) ||
+				    (! key_in_buffer(p_s_search_path, p_s_key, p_s_sb))) ) { /* in fact this checks whether path is correct */
+	decrement_counters_in_path(p_s_search_path);
+	
+	/* Get the root block number so that we can repeat the search starting from the root. */
+	n_block_number  = p_s_sb->u.reiserfs_sb.s_rs->s_root_block;
+
+	right_neighbor_of_leaf_node = 0;
+
+	/* repeat search from the root */
+	continue;
+      }
+      
+#ifdef CONFIG_REISERFS_CHECK
+
+      if ( ! key_in_buffer(p_s_search_path, p_s_key, p_s_sb) )
+	reiserfs_panic(p_s_sb, "PAP-5130: search_by_key: key is not in the buffer");
+      if ( cur_tb ) {
+/*	print_tb (init_mode, init_item_pos, init_pos_in_item, &init_tb, "5140");*/
+	reiserfs_panic(p_s_sb, "PAP-5140: search_by_key: schedule occurred in do_balance!");
+      }
+
+#endif
+
+      if ( ! buffer_uptodate(p_s_bh) ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+	if ( n_bread_par != DONT_READ_BLOCKS )
+	  reiserfs_panic(p_s_sb, "PAP-5150: search_by_key: buffer is not uptodate in case of READ_BLOCKS");
+#endif
+
+	return ITEM_NOT_FOUND; /* We can not continue search in the cache. */
+      }
+
+      /* ok, we have acquired next formatted node in the tree */
+      n_node_level = B_BLK_HEAD(p_s_bh)->blk_level;
+
+#ifdef CONFIG_REISERFS_CHECK
+
+      if (n_node_level < n_stop_level)
+	reiserfs_panic (p_s_sb, "vs-5152: search_by_key: tree level is less than stop level (%d)",
+			n_node_level, n_stop_level);
+
+#endif
+
+      n_retval = bin_search (p_s_key, B_N_PITEM_HEAD(p_s_bh, 0), B_NR_ITEMS(p_s_bh),
+		       ( n_node_level == DISK_LEAF_NODE_LEVEL ) ? IH_SIZE : KEY_SIZE, &(p_s_last_element->pe_position));
+      if (n_node_level == n_stop_level)
+	return n_retval;
+
+      /* we are not in the stop level */
+      if (n_retval == ITEM_FOUND)
+	/* item has been found, so we choose the pointer which is to the right of the found one */
+	p_s_last_element->pe_position++;
+      /* if item was not found we choose the position which is to the left of the found item. This
+	 requires no code, bin_search did it already.*/
+
+
+      /* So we have chosen a position in the current node which is an
+	 internal node.  Now we calculate child block number by position in the node. */
+      n_block_number = B_N_CHILD_NUM(p_s_bh, p_s_last_element->pe_position);
+
+#ifdef SEARCH_BY_KEY_READA
+      /* if we are going to read leaf node, then calculate its right neighbor if possible */
+      if (n_node_level == DISK_LEAF_NODE_LEVEL + 1 && p_s_last_element->pe_position < B_NR_ITEMS (p_s_bh))
+	right_neighbor_of_leaf_node = B_N_CHILD_NUM(p_s_bh, p_s_last_element->pe_position + 1);
+#endif
+    }
+}
+
+
+int bin_search_in_dir_item (struct item_head * ih, struct reiserfs_de_head * deh, struct key * key, int * pos_in_item)
+{
+    int rbound, lbound, j;
+
+    lbound = 0;
+    rbound = I_ENTRY_COUNT (ih) - 1;
+
+    for (j = (rbound + lbound) / 2; lbound <= rbound; j = (rbound + lbound) / 2) {
+	if (key->k_offset < deh[j].deh_offset) {
+	    rbound = j - 1;
+	    continue;
+	}
+	if (key->k_offset > deh[j].deh_offset) {
+	    lbound = j + 1;
+	    continue;
+	}
+	/* key found */
+	*pos_in_item = j;
+	return POSITION_FOUND;
+    }
+
+    *pos_in_item = lbound;
+    return POSITION_NOT_FOUND;
+}
+
+
+/* first calls search_by_key, then, if item is not found looks for the entry
+   inside directory item indicated by search_by_key. (We assign a key to each
+   directory item, and place multiple entries in a single directory item.)
+   Fills the path to the entry, and to the entry position in the item */
+int search_by_entry_key (struct super_block * sb, struct key * key, struct path * path, int * pos_in_item, int * repeat)
+{
+    /* search for a directory item using key of entry */
+    if (search_by_key (sb, key, path, repeat, DISK_LEAF_NODE_LEVEL, READ_BLOCKS) == ITEM_FOUND) {
+	*pos_in_item = 0;
+	return POSITION_FOUND;
+    }
+#ifdef CONFIG_REISERFS_CHECK
+    if (!PATH_LAST_POSITION (path))
+	reiserfs_panic (sb, "vs-7000: search_by_entry_key: search_by_key returned item position == 0");
+#endif /* CONFIG_REISERFS_CHECK */
+    PATH_LAST_POSITION (path) --;
+
+#ifdef CONFIG_REISERFS_CHECK
+    {
+	struct item_head * ih = B_N_PITEM_HEAD (PATH_PLAST_BUFFER (path), PATH_LAST_POSITION (path));
+
+	if (!I_IS_DIRECTORY_ITEM (ih) || COMP_SHORT_KEYS (&(ih->ih_key), key)) {
+	    print_block (PATH_PLAST_BUFFER (path), 0, -1, -1);
+	    reiserfs_panic (sb, "vs-7005: search_by_entry_key: found item %h is not directory item or "
+			    "does not belong to the same directory as key %k", ih, key);
+	}
+    }
+#endif /* CONFIG_REISERFS_CHECK */
+
+    /* binary search in directory item by third component of the key */
+    return bin_search_in_dir_item (PATH_PITEM_HEAD (path), B_I_DEH (PATH_PLAST_BUFFER (path), PATH_PITEM_HEAD (path)), key, pos_in_item);
+}
+
+/* Form the path to an item and position in this item which contains file byte defined by p_s_key. If there
+    is no such item corresponding to the key, we point the path to the item with maximal key less than
+    p_s_key, and *p_n_pos_in_item is set to one past the last entry/byte in the item.  If searching for
+    entry in a directory item, and it is not found, *p_n_pos_in_item is set to one entry more than the entry with
+    maximal key which is less than the sought key.  
+
+    Note that if there is no entry in this same node which is one more, then we point to an imaginary entry.
+
+    for direct items, the position is in units of bytes, for
+    indirect items the position is in units of blocknr entries, for directory items the position is in units
+    of directory entries.  */
+int search_for_position_by_key (
+      struct super_block  * p_s_sb,         /* Pointer to the super block.          */
+      struct key          * p_s_key,        /* Key to search.                       */
+      struct path         * p_s_search_path,/* Filled up by this function.          */
+      int		  * p_n_pos_in_item,/* returned value, which is the found position in the item */
+      int                 * p_n_repeat	    /* Whether schedule occured. */
+    ) {
+  struct item_head    * p_s_ih;
+  int                   n_blk_size;
+
+  /* If searching for directory entry. */
+  if ( KEY_IS_DIRECTORY_KEY(p_s_key) )
+    return  search_by_entry_key(p_s_sb, p_s_key, p_s_search_path, p_n_pos_in_item, p_n_repeat);
+
+  /* If not searching for directory entry. */
+
+  /* If item is found. */
+  if ( search_by_key(p_s_sb, p_s_key, p_s_search_path, p_n_repeat, DISK_LEAF_NODE_LEVEL, READ_BLOCKS) == ITEM_FOUND )  {
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( ! B_N_PITEM_HEAD(PATH_PLAST_BUFFER(p_s_search_path),
+			  PATH_LAST_POSITION(p_s_search_path))->ih_item_len )
+      reiserfs_panic(p_s_sb, "PAP-5165: search_for_position_by_key: item length equals zero");
+#endif
+
+    *p_n_pos_in_item = 0;
+    return POSITION_FOUND;
+  }
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( ! PATH_LAST_POSITION(p_s_search_path) )
+    reiserfs_panic(p_s_sb, "PAP-5170: search_for_position_by_key: position equals zero");
+#endif
+
+  /* Item is not found. Set path to the previous item. */
+  p_s_ih = B_N_PITEM_HEAD(PATH_PLAST_BUFFER(p_s_search_path), --PATH_LAST_POSITION(p_s_search_path));
+  n_blk_size = p_s_sb->s_blocksize;
+
+/*#ifdef CONFIG_REISERFS_CHECK */
+  if ( COMP_SHORT_KEYS(&(p_s_ih->ih_key), p_s_key) ) {
+    reiserfs_panic(p_s_sb, "PAP-5180: search_for_position_by_key: found item %h belongs to an other object %k",
+		   p_s_ih, p_s_key);
+  }
+
+  if ( ! I_IS_STAT_DATA_ITEM(p_s_ih) && ((KEY_IS_INDIRECT_KEY(p_s_key) && ! I_IS_INDIRECT_ITEM(p_s_ih)) ||
+					 (KEY_IS_DIRECT_KEY(p_s_key) && ! I_IS_DIRECT_ITEM(p_s_ih))) ) {
+    print_block (PATH_PLAST_BUFFER(p_s_search_path), PRINT_LEAF_ITEMS, 
+		 PATH_LAST_POSITION (p_s_search_path) - 2,
+		 PATH_LAST_POSITION (p_s_search_path) + 2);
+    reiserfs_panic(p_s_sb, "PAP-5190: search_for_position_by_key: found item %h type does not match to the expected one %k",
+		   p_s_key, p_s_ih);
+  }
+/*#endif*/
+
+  /* Needed byte is contained in the item pointed to by the path.*/
+  if ( I_K_KEY_IN_ITEM(p_s_ih, p_s_key, n_blk_size) )  {
+    *p_n_pos_in_item = p_s_key->k_offset - p_s_ih->ih_key.k_offset;
+    if ( I_IS_INDIRECT_ITEM(p_s_ih) )
+      *p_n_pos_in_item /= n_blk_size;
+    return POSITION_FOUND;
+  }
+
+  /* Needed byte is not contained in the item pointed to by the path. Set *p_n_pos_in_item out of the
+     item. */
+  if ( I_IS_INDIRECT_ITEM(p_s_ih) )
+    *p_n_pos_in_item = I_UNFM_NUM(p_s_ih);
+  else
+    *p_n_pos_in_item = p_s_ih->ih_item_len;
+  return POSITION_NOT_FOUND;
+}
+
+
+
+
+
+
+/* Compare given item and item pointed to by the path. */
+int comp_items(
+      struct item_head  * p_s_ih,
+      struct path       * p_s_path
+    ) {
+  struct buffer_head  * p_s_bh;
+  struct item_head    * p_s_path_item;
+
+  /* Last buffer at the path is not in the tree. */
+  if ( ! B_IS_IN_TREE(p_s_bh = PATH_PLAST_BUFFER(p_s_path)) )
+    return 1;
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( p_s_bh->b_dev == NODEV )
+      reiserfs_panic(0, "PAP-5200: comp_items: device is invalid");
+#endif
+
+  /* Last path position is invalid. */
+  if ( PATH_LAST_POSITION(p_s_path) >= B_NR_ITEMS(p_s_bh) )
+    return 1;
+  /* Get item at the path. */
+  p_s_path_item = PATH_PITEM_HEAD(p_s_path);
+  /* Compare keys. */
+  if ( COMP_KEYS(p_s_path_item, p_s_ih) )
+    return 1;
+  /* Compare other items fields. */
+  if ( p_s_path_item->u.ih_free_space != p_s_ih->u.ih_free_space ||
+       p_s_path_item->ih_item_len != p_s_ih->ih_item_len ||
+       p_s_path_item->ih_item_location != p_s_ih->ih_item_location )
+    return 1;
+  /* Items are equal. */
+  return 0;
+}
+
+
+//
+//
+//
+#ifdef __KERNEL__
+
+
+/*  If the path points to a directory or direct item, calculate mode and the size cut, for balance.
+    If the path points to an indirect item, remove some number of its unformatted nodes.
+    In case of file truncate calculate whether this item must be deleted/truncated or last
+    unformatted node of this item will be converted to a direct item.
+    This function returns a determination of what balance mode the calling function should employ. */
+static char  prepare_for_delete_or_cut(
+    struct reiserfs_transaction_handle *th,
+    struct inode * inode,
+    struct path         * p_s_path,
+    struct key          * p_s_item_key,
+    int                 * p_n_pos_in_item,
+    int                 * p_n_removed,      /* Number of unformatted nodes which were removed
+					       from end of the file. */
+    int                 * p_n_cut_size,
+    unsigned long         n_new_file_length, /* MAX_KEY_OFFSET in case of delete. */
+    int		      preserve_mode,
+    int * was_unfm_suspected_recipient
+    ) {
+    struct super_block  * p_s_sb = inode->i_sb;
+    struct item_head    * p_s_ih = PATH_PITEM_HEAD(p_s_path);
+    struct buffer_head  * p_s_bh = PATH_PLAST_BUFFER(p_s_path);
+
+#ifdef CONFIG_REISERFS_CHECK
+    int n_repeat_counter = 0;
+#endif
+
+    /* Stat_data item. */
+    if ( I_IS_STAT_DATA_ITEM(p_s_ih) ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+	if ( n_new_file_length != MAX_KEY_OFFSET )
+	    reiserfs_panic(p_s_sb, "PAP-5210: prepare_for_delete_or_cut: mode must be M_DELETE");
+#endif
+
+	*p_n_cut_size = -(IH_SIZE + p_s_ih->ih_item_len);
+	return M_DELETE;
+    }
+
+    /* Directory item. */
+    if ( I_IS_DIRECTORY_ITEM(p_s_ih) ) {
+	if (p_s_ih->ih_key.k_offset == DOT_OFFSET && n_new_file_length == MAX_KEY_OFFSET) {
+#ifdef CONFIG_REISERFS_CHECK
+	    if (p_s_ih->ih_key.k_uniqueness != DIRENTRY_UNIQUENESS/*DOT_UNIQUENESS*/ || I_ENTRY_COUNT (p_s_ih) != 2)
+		reiserfs_panic(p_s_sb,"PAP-5220: prepare_for_delete_or_cut: "
+			       "empty directory item has uniqueness==%lu and entry count==%d", 
+			       p_s_ih->ih_key.k_uniqueness, I_ENTRY_COUNT (p_s_ih));
+#endif
+	    *p_n_cut_size = -(IH_SIZE + p_s_ih->ih_item_len);
+	    return M_DELETE; /* Delete the directory item containing "." and ".." entry. */
+	}
+
+	if ( I_ENTRY_COUNT(p_s_ih) == 1 )  {
+	    *p_n_cut_size = -(IH_SIZE + p_s_ih->ih_item_len);
+	    return M_DELETE; /* Delete the directory item such as there is one record only in this item. */
+	}
+	*p_n_cut_size = -(DEH_SIZE +
+			  I_DEH_N_ENTRY_LENGTH(p_s_ih, B_I_DEH(p_s_bh,p_s_ih) +
+					       *p_n_pos_in_item, *p_n_pos_in_item));
+	return M_CUT; /* Cut one record from the directory item. */
+    }
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( ! p_s_ih->ih_key.k_offset )
+	reiserfs_panic(p_s_sb, "PAP-5230: prepare_for_delete_or_cut: k_offset is NULL");
+#endif
+
+    /* Direct item. */
+    if ( I_IS_DIRECT_ITEM(p_s_ih) ) {
+	if ( n_new_file_length == MAX_KEY_OFFSET ) { /* Case of delete. */
+	    *p_n_cut_size = -(IH_SIZE + p_s_ih->ih_item_len);
+	    return M_DELETE; /* Delete this item. */
+	}
+	/* Case of truncate. */
+	if ( n_new_file_length < p_s_ih->ih_key.k_offset )  {
+	    *p_n_cut_size = -(IH_SIZE + p_s_ih->ih_item_len);
+	    return M_DELETE; /* Delete this item. */
+	}
+	/* Calculate first position and size for cutting from item. */
+	*p_n_cut_size = -(p_s_ih->ih_item_len -
+			  (*p_n_pos_in_item = n_new_file_length + 1 - p_s_ih->ih_key.k_offset));
+	return M_CUT; /* Cut from this item. */
+    }
+
+    /* Case of an indirect item. */
+    {
+	int                   n_unfm_number,    /* Number of the item unformatted nodes. */
+	    n_counter,
+	    n_repeat,
+	    n_retry,        /* Set to one if there is unformatted node buffer in use. */
+	    n_blk_size;
+	unsigned long       * p_n_unfm_pointer; /* Pointer to the unformatted node number. */
+	struct item_head      s_ih;           /* Item header. */
+	char                  c_mode;           /* Returned mode of the balance. */
+	struct buffer_head  * p_s_un_bh;
+
+
+	n_blk_size = p_s_sb->s_blocksize;
+
+	/* Search for the needed object indirect item until there are no unformatted nodes to be removed. */
+	do  {
+	    /* Copy indirect item header to a temp variable. */
+	    copy_item_head(&s_ih, PATH_PITEM_HEAD(p_s_path));
+	    /* Calculate number of unformatted nodes in this item. */
+	    n_unfm_number = I_UNFM_NUM(&s_ih);
+
+#ifdef CONFIG_REISERFS_CHECK
+	    if ( ! I_IS_INDIRECT_ITEM(&s_ih) || ! n_unfm_number ||
+		 *p_n_pos_in_item + 1 !=  n_unfm_number ) {
+		printk("n_unfm_number = %d *p_n_pos_in_item = %d\n",n_unfm_number, *p_n_pos_in_item);
+		reiserfs_panic(p_s_sb, "PAP-5240: prepare_for_delete_or_cut: illegal item %h", &s_ih);
+	    }
+#endif
+
+	    /* Calculate balance mode and position in the item to remove unformatted nodes. */
+	    if ( n_new_file_length == MAX_KEY_OFFSET ) {/* Case of delete. */
+		*p_n_pos_in_item = 0;
+		*p_n_cut_size = -(IH_SIZE + s_ih.ih_item_len);
+		c_mode = M_DELETE;
+	    }
+	    else  { /* Case of truncate. */
+		if ( n_new_file_length < s_ih.ih_key.k_offset )  {
+		    *p_n_pos_in_item = 0;
+		    *p_n_cut_size = -(IH_SIZE + s_ih.ih_item_len);
+		    c_mode = M_DELETE; /* Delete this item. */
+		}
+		else  {
+		    /* indirect item must be truncated starting from *p_n_pos_in_item-th position */
+		    *p_n_pos_in_item = (n_new_file_length + n_blk_size - s_ih.ih_key.k_offset ) / n_blk_size;
+
+#ifdef CONFIG_REISERFS_CHECK
+		    if ( *p_n_pos_in_item > n_unfm_number ) 
+			reiserfs_panic(p_s_sb, "PAP-5250: prepare_for_delete_or_cut: illegal position in the item");
+#endif
+
+		    /* Either convert last unformatted node of indirect item to direct item or increase
+		       its free space.  */
+		    if ( *p_n_pos_in_item == n_unfm_number )  {
+			*p_n_cut_size = 0; /* Nothing to cut. */
+			return M_CONVERT; /* Maybe convert last unformatted node to the direct item. */
+		    }
+		    /* Calculate size to cut. */
+		    *p_n_cut_size = -(s_ih.ih_item_len - *p_n_pos_in_item * UNFM_P_SIZE);
+
+		    c_mode = M_CUT;     /* Cut from this indirect item. */
+		}
+	    }
+
+#ifdef CONFIG_REISERFS_CHECK
+	    if ( n_unfm_number <= *p_n_pos_in_item ) 
+		reiserfs_panic(p_s_sb, "PAP-5260: prepare_for_delete_or_cut: illegal position in the indirect item");
+#endif
+
+	    /* pointers to be cut */
+	    n_unfm_number -= *p_n_pos_in_item;
+	    /* Set pointer to the last unformatted node pointer that is to be cut. */
+	    p_n_unfm_pointer = (unsigned long *)B_I_PITEM(PATH_PLAST_BUFFER(p_s_path),&s_ih) + I_UNFM_NUM(&s_ih) - 1 - *p_n_removed;
+
+	    /* We go through the unformatted nodes pointers of the indirect item and look for
+	       the unformatted nodes in the cache. If we found some of them we free it and zero
+	       corresponding indirect item entry. If some unformatted node has b_count > 1 we must
+	       not free this unformatted node since it is in use. */
+	    for ( n_retry = 0, n_counter = *p_n_removed;
+		  n_counter < n_unfm_number; n_counter++, p_n_unfm_pointer-- )  {
+		if (comp_items(&s_ih, p_s_path))
+		    break;
+#ifdef CONFIG_REISERFS_CHECK
+		if (p_n_unfm_pointer < (unsigned long *)B_I_PITEM(PATH_PLAST_BUFFER(p_s_path),&s_ih) ||
+		    p_n_unfm_pointer > (unsigned long *)B_I_PITEM(PATH_PLAST_BUFFER(p_s_path),&s_ih) + I_UNFM_NUM(&s_ih) - 1)
+		    reiserfs_panic (p_s_sb, "vs-5265: prepare_for_delete_or_cut: pointer out of range");
+#endif
+		if ( ! *p_n_unfm_pointer )  { /* Hole, nothing to remove. */
+		    if ( ! n_retry )
+			(*p_n_removed)++;
+		    continue;
+		}
+		/* Search for the buffer in cache. */
+		n_repeat = CARRY_ON;
+		p_s_un_bh = reiserfs_get_hash_table(p_s_sb->s_dev, *p_n_unfm_pointer,
+						    n_blk_size, &n_repeat);
+		/* Current item was shifted from buffer pointed to by the path. */
+		if ( n_repeat != CARRY_ON && comp_items(&s_ih, p_s_path) )  {
+		    brelse(p_s_un_bh);
+		    break;
+		}
+
+		/* Block is in use. */
+		/* BUG, find a better test -- CLM */
+		if ( p_s_un_bh && p_s_un_bh->b_count != 1)  {
+		    if ((buffer_journaled(p_s_un_bh) || buffer_journal_dirty(p_s_un_bh)) && p_s_un_bh->b_count == 2) {
+			;
+		    } else {
+
+#ifdef CONFIG_REISERFS_CHECK_ONE_PROCESS
+			reiserfs_panic(p_s_sb, "PAP-5270: prepare_for_delete_or_cut: b_count != 1");
+#endif
+
+			n_retry = 1;
+			brelse(p_s_un_bh);
+			continue;
+		    }
+		}
+      
+		if ( ! n_retry )
+		    (*p_n_removed)++;
+      
+#ifdef CONFIG_REISERFS_CHECK
+		if ( p_s_un_bh && (*p_n_unfm_pointer != p_s_un_bh->b_blocknr || buffer_locked (p_s_un_bh)))
+		    reiserfs_panic(p_s_sb, "PAP-5280: prepare_for_delete_or_cut: blocks numbers are different");	
+#endif
+
+		{
+		    __u32 block_addr = *p_n_unfm_pointer;
+		    *p_n_unfm_pointer = 0;
+		    journal_mark_dirty(th, p_s_sb, PATH_PLAST_BUFFER(p_s_path));
+		    if (p_s_un_bh) {
+			mark_buffer_clean (p_s_un_bh);
+			brelse (p_s_un_bh);
+		    }
+		    reiserfs_free_block(th, p_s_sb, block_addr);
+		    /* non-atomic refile_buffer is allowed */
+		    COMPLETE_BITMAP_DIRTING_AFTER_FREEING (p_s_sb, block_addr / (p_s_sb->s_blocksize * 8));		    
+		}
+
+		inode->i_blocks -= p_s_sb->s_blocksize / 512;
+	    } /* for */
+
+	    /* There is block in use. */
+	    if ( n_retry )  {
+
+#ifdef CONFIG_REISERFS_CHECK
+		if ( *p_n_removed >= n_unfm_number )
+		    reiserfs_panic(p_s_sb, "PAP-5290: prepare_for_delete_or_cut: illegal case");
+		if ( !(++n_repeat_counter % 50000) ) {
+		    printk ("5300: new file length = %ld\n", n_new_file_length);
+		    reiserfs_warning("PAP-5300: prepare_for_delete_or_cut: (pid %u): "
+				     "could not delete item %k in (%d) iterations. Still trying",
+				     current->pid, p_s_item_key, n_repeat_counter);
+		}
+#endif
+
+#ifdef __KERNEL__
+		current->policy |= SCHED_YIELD;
+		schedule();
+#endif
+	    }
+	    /* This loop can be optimized. */
+	} while ( *p_n_removed < n_unfm_number &&
+		  search_for_position_by_key(p_s_sb, p_s_item_key, p_s_path, p_n_pos_in_item, &n_repeat) == POSITION_FOUND );
+
+#ifdef CONFIG_REISERFS_CHECK
+	if ( *p_n_removed < n_unfm_number )
+	    reiserfs_panic(p_s_sb, "PAP-5310: prepare_for_delete_or_cut: indirect item is not found");
+
+	if ( comp_items(&s_ih, p_s_path) ) {
+	    printk("*p_n_removed = %d n_unfm_number = %d\n",*p_n_removed, n_unfm_number);
+	    reiserfs_panic(p_s_sb, "PAP-5312: prepare_for_delete_or_cut: path to item %h has been unexpectedly changed",
+			   &s_ih);
+	}
+#endif
+
+	if (c_mode == M_CUT)
+	    *p_n_pos_in_item *= UNFM_P_SIZE;
+	return c_mode;
+    }
+}
+
+
+/* Calculate bytes number which will be deleted or cutted in the balance. */
+int calc_deleted_bytes_number(
+      struct  tree_balance  * p_s_tb,
+      char                    c_mode
+    ) {
+  int                     n_del_size;
+  struct  item_head     * p_s_ih = PATH_PITEM_HEAD(p_s_tb->tb_path);
+
+  if ( I_IS_STAT_DATA_ITEM(p_s_ih) )
+    return 0;
+
+  if ( I_IS_DIRECTORY_ITEM(p_s_ih) )
+    return EMPTY_DIR_SIZE; /* We delete emty directoris only. */
+
+  n_del_size = ( c_mode == M_DELETE ) ? p_s_ih->ih_item_len : -p_s_tb->insert_size[0];
+
+  if ( I_IS_INDIRECT_ITEM(p_s_ih) )
+    n_del_size = (n_del_size/UNFM_P_SIZE)*
+      (PATH_PLAST_BUFFER(p_s_tb->tb_path)->b_size) - p_s_ih->u.ih_free_space;
+  return n_del_size;
+}
+
+static void init_tb_struct(
+              struct tree_balance * p_s_tb,
+	      struct super_block  * p_s_sb,
+	      struct path         * p_s_path,
+              int                   n_size
+            ) {
+ memset (p_s_tb,'\0',sizeof(struct tree_balance));
+ p_s_tb->tb_sb = p_s_sb;
+ p_s_tb->tb_path = p_s_path;
+ PATH_OFFSET_PBUFFER(p_s_path, ILLEGAL_PATH_ELEMENT_OFFSET) = NULL;
+ PATH_OFFSET_POSITION(p_s_path, ILLEGAL_PATH_ELEMENT_OFFSET) = 0;
+ p_s_tb->insert_size[0] = n_size;
+}
+
+
+
+/* Delete object item. */
+int reiserfs_delete_item(
+      struct reiserfs_transaction_handle *th,
+      struct inode * p_s_inode,
+      struct path         * p_s_path,     /* Path to the deleted item.            */
+      int		  * p_n_pos_in_item,
+      struct key          * p_s_item_key, /* Key to search for the deleted item.  */
+      struct buffer_head  * p_s_un_bh,    /* NULL or unformatted node pointer.    */
+      int                 preserve_mode   /* can be PRESERVE_DIRECT_TO_INDIRECT or NOTHING_SPECIAL */
+    ) {
+  struct super_block * p_s_sb = p_s_inode->i_sb;
+  struct buffer_head *bh ;
+  struct tree_balance   s_del_balance;
+  struct item_head      s_ih;
+  int                   n_repeat,
+                        n_ret_value,
+                        n_del_size,
+                        n_removed;
+
+#ifdef CONFIG_REISERFS_CHECK
+  char                  c_mode;
+  int			n_iter = 0;
+#endif
+
+  init_tb_struct(&s_del_balance, p_s_sb, p_s_path, 0);
+  s_del_balance.preserve_mode = preserve_mode;
+
+  while ( 1 ) {
+    n_removed = 0;
+
+#ifdef CONFIG_REISERFS_CHECK
+    n_iter++;
+    c_mode =
+#endif
+
+    prepare_for_delete_or_cut(th, p_s_inode, p_s_path, p_s_item_key, p_n_pos_in_item, &n_removed, &n_del_size, MAX_KEY_OFFSET, NOTHING_SPECIAL, 0);
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( c_mode != M_DELETE )
+      reiserfs_panic(p_s_sb, "PAP-5320: reiserfs_delete_item: mode must be M_DELETE");
+#endif
+
+    copy_item_head(&s_ih, PATH_PITEM_HEAD(p_s_path));
+    s_del_balance.insert_size[0] = n_del_size;
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( ( ! KEY_IS_STAT_DATA_KEY(p_s_item_key) && ! KEY_IS_DIRECTORY_KEY(p_s_item_key) &&
+	   ! I_K_KEY_IN_ITEM(PATH_PITEM_HEAD(s_del_balance.tb_path), p_s_item_key, p_s_sb->s_blocksize))  ||
+	 p_s_item_key->k_uniqueness != PATH_PITEM_HEAD(s_del_balance.tb_path)->ih_key.k_uniqueness ) {
+      reiserfs_panic(p_s_sb, "PAP-5325: reiserfs_delete_item: (iteration %d): "
+		     "key %k does not correspond to the found item %h", n_iter, p_s_item_key,
+		     PATH_PITEM_HEAD(s_del_balance.tb_path));
+    }
+
+    if ( KEY_IS_DIRECTORY_KEY(p_s_item_key) && (p_s_item_key->k_uniqueness != DIRENTRY_UNIQUENESS/*DOT_DOT_UNIQUENESS*/ &&
+						I_ENTRY_COUNT(PATH_PITEM_HEAD(s_del_balance.tb_path)) != 2) )
+      reiserfs_panic(p_s_sb, "PAP-5327: reiserfs_delete_item(%d): key does not correspond to the item(directory case)", n_iter);
+
+    if ( PATH_LAST_POSITION(s_del_balance.tb_path) >= B_NR_ITEMS(PATH_PLAST_BUFFER(s_del_balance.tb_path)) ) {
+      reiserfs_panic(p_s_sb, "PAP-5330: reiserfs_delete_item: invalid item number (%d) iter = %d, must be < %d. item to delete key %k", 
+		     PATH_LAST_POSITION(s_del_balance.tb_path), n_iter,
+		     B_NR_ITEMS(PATH_PLAST_BUFFER(s_del_balance.tb_path)), p_s_item_key);
+    }
+#endif
+
+    n_ret_value = fix_nodes(th, M_DELETE, &s_del_balance, 0, NULL);
+
+#ifdef CONFIG_REISERFS_CHECK_ONE_PROCESS
+    if ( n_ret_value == PATH_INCORRECT )
+      reiserfs_panic(p_s_sb,"PAP-5340: reiserfs_delete_item: illegal returned value");
+#endif
+
+    if ( n_ret_value != SCHEDULE_OCCURRED && n_ret_value != PATH_INCORRECT )
+      break;
+    /* schedule() occured while make_balance() worked */
+    if ( search_for_position_by_key(p_s_sb, p_s_item_key, p_s_path, p_n_pos_in_item, &n_repeat) == POSITION_NOT_FOUND )
+      reiserfs_panic(p_s_sb, "PAP-5350: reiserfs_delete_item: item to delete does not exist");
+  }
+  if ( n_ret_value == NO_DISK_SPACE || n_ret_value == IO_ERROR ) {
+    unfix_nodes(th, &s_del_balance);
+    return 0;
+  }
+  journal_lock_dobalance(p_s_sb) ;
+ 
+  /* Here n_ret_value equals CARRY_ON. */
+  n_ret_value = calc_deleted_bytes_number(&s_del_balance, M_DELETE);
+
+  if ( p_s_un_bh )  {
+    /* We are deleting direct items in a tail, that we are converting
+       into an unformatted node. */
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( ! I_IS_DIRECT_ITEM(&s_ih) || ! buffer_uptodate(p_s_un_bh) || 
+       ((p_s_un_bh->b_count != 1) && !buffer_journaled(p_s_un_bh)) ) {
+      reiserfs_panic(p_s_sb,"PAP-5370: reiserfs_delete_item: illegal unformatted node buffer %b or item type %h)",
+		     p_s_un_bh, &s_ih);
+    }
+#endif
+
+    /* p_s_un_bh needs to be cleaned and waited on before it is sent
+    ** here.  This should not be a problem, since p_s_un_bh should be
+    ** a newly allocated block.
+    */
+    memcpy(p_s_un_bh->b_data + (s_ih.ih_key.k_offset - 1) % (p_s_sb->s_blocksize),
+	   B_I_PITEM(PATH_PLAST_BUFFER(p_s_path), &s_ih), n_ret_value);
+#ifdef CONFIG_REISERFS_CHECK
+    if ( preserve_mode != PRESERVE_DIRECT_TO_INDIRECT )
+      reiserfs_panic(p_s_sb, "PAP-5380: reiserfs_delete_item: "
+		     "you need to change the code to check converting_tail before preserving");
+#endif /* CONFIG_REISERFS_CHECK */
+
+    journal_mark_dirty(th, p_s_sb, p_s_un_bh) ;
+    bh = PATH_PLAST_BUFFER(p_s_path) ;
+  }
+
+  /* Perform balancing after all resources will be collected at once. */ 
+  do_balance(th, &s_del_balance, 0, NULL, NULL, M_DELETE, REISERFS_KERNEL_MEM, 0/* zeros number */);
+  journal_unlock_dobalance(p_s_sb) ;
+
+  /* Return deleted body length */ 
+  return n_ret_value;
+}
+
+
+/* Summary Of Mechanisms For Handling Collisions Between Processes:
+
+ deletion of the body of the object is performed by iput(), with the
+ result that if multiple processes are operating on a file, the
+ deletion of the body of the file is deferred until the last process
+ that has an open inode performs its iput().
+
+ writes and truncates are protected from collisions by use of
+ semaphores.
+
+ creates, linking, and mknod are protected from collisions with other
+ processes by making the reiserfs_add_entry() the last step in the
+ creation, and then rolling back all changes if there was a collision.
+ - Hans
+*/
+
+/* Delete all items of an object. */
+/* This would work faster if it did not use search by key to access data one node at a time, but instead
+   resembled read more by doing read ahead. */
+void  reiserfs_delete_object(
+	struct reiserfs_transaction_handle *th, 
+        struct inode  * p_s_inode       /* Pointer to the object inode. */
+      ) {
+  struct path           s_search_path;  /* Path to the last object item. */
+  struct key            s_item_key;     /* Key to search for a file item. */  
+  unsigned long         n_obj_size;     /* Object size. */
+  int                   n_repeat,
+                        n_deleted,      /* Number of deleted bytes. */
+                        n_pos_in_item,  /* Found position in the item. */
+    			n_is_last_item = 1;
+
+  struct super_block *  p_s_sb = p_s_inode->i_sb;
+
+  
+  init_path (&s_search_path);
+  /* Copy key of the object stat_data. */
+  copy_key(&s_item_key, INODE_PKEY(p_s_inode));
+
+  /* Get object size. */
+  n_obj_size = p_s_inode->i_size;
+  /* Case of a directory. */
+  if ( S_ISDIR(p_s_inode->i_mode) ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+  /* reiserfs_delete_object is called to delete a directory only for empty directories. */
+    if ( n_obj_size != EMPTY_DIR_SIZE && n_obj_size != 0 )
+      reiserfs_panic (p_s_sb, "PAP-5390: reiserfs_delete_object: bad empty directory sdize (%lu)", n_obj_size);
+#endif
+
+    /* Set key to search for the ".." directory entry. */
+    s_item_key.k_offset = DOT_DOT_OFFSET;
+    s_item_key.k_uniqueness = DIRENTRY_UNIQUENESS/*DOT_DOT_UNIQUENESS*/;
+  }
+  else  {
+    /* Set key to search for the last file byte. */
+    if ( (s_item_key.k_offset = n_obj_size) >= p_s_inode->u.reiserfs_i.i_first_direct_byte )
+      s_item_key.k_uniqueness = TYPE_DIRECT;
+    else
+      s_item_key.k_uniqueness = TYPE_INDIRECT;
+  }
+  /* Delete object body. */
+  while ( n_obj_size )  {
+    /* Search for the last object item. */
+    if ( search_for_position_by_key(p_s_sb, &s_item_key, &s_search_path, &n_pos_in_item, &n_repeat) == POSITION_NOT_FOUND ) {
+      if (  n_is_last_item ) {
+	struct item_head * p_s_ih;
+
+	n_is_last_item = 0;
+	p_s_ih = PATH_PITEM_HEAD(&s_search_path);
+
+	if ( COMP_SHORT_KEYS(&s_item_key, &(p_s_ih->ih_key)) )
+	  reiserfs_panic (p_s_sb, "PAP-5400: reiserfs_delete_object: item to delete doesn't exist");
+
+	if ( I_IS_STAT_DATA_ITEM(p_s_ih) ) {
+	  n_obj_size = 0;
+
+#ifdef REISERFS_INFO
+	  printk("reiserfs_delete_object: file size calculated by last file item(%lu) less than file size in inode(%lu)\n",
+		 n_obj_size, p_s_inode->i_size);
+#endif
+
+	  break;
+	}
+	s_item_key.k_offset = n_obj_size = p_s_ih->ih_key.k_offset + I_BYTES_NUMBER(p_s_ih, p_s_sb->s_blocksize) - 1;
+	n_pos_in_item--;
+
+#ifdef REISERFS_INFO
+	printk("reiserfs_delete_object: file size calculated by last file item(%lu) less than file size in inode(%lu)\n",
+	       n_obj_size, p_s_inode->i_size);
+#endif
+
+      }
+      else {
+	reiserfs_panic (p_s_sb, "PAP-5410: reiserfs_delete_object: item %k to delete doesn't exist", &s_item_key);
+      }
+
+    }
+
+    /* Delete last object item. */
+    n_deleted = reiserfs_delete_item(th, p_s_inode, &s_search_path, &n_pos_in_item, &s_item_key, NULL, NOTHING_SPECIAL);
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( n_deleted <= 0 )
+	    reiserfs_panic(p_s_sb, "reiser-5420: reiserfs_delete_object: this code needs to be fixed to handle ENOSPC");
+    if ( n_deleted > n_obj_size )
+	    reiserfs_panic (p_s_sb, "PAP-5430: reiserfs_delete_object: " 
+                    "reiserfs_delete_item returns too big number");
+#endif
+
+    n_obj_size -= n_deleted;
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( n_obj_size && s_item_key.k_offset < n_deleted )
+	    reiserfs_panic (p_s_sb, "PAP-5440: reiserfs_delete_object: illegal search key offset");
+#endif
+    /* Update key to search for the new last object item. */
+    if ( (s_item_key.k_offset -= n_deleted) < p_s_inode->u.reiserfs_i.i_first_direct_byte )
+      s_item_key.k_uniqueness = TYPE_INDIRECT;
+
+    if (journal_transaction_should_end(th, th->t_blocks_allocated)) {
+      int orig_len_alloc = th->t_blocks_allocated ;
+      struct super_block *orig_super = th->t_super ;
+      p_s_inode->i_size = n_obj_size ;
+      p_s_inode->i_ctime = CURRENT_TIME ;
+      p_s_inode->i_mtime = CURRENT_TIME ;
+      decrement_counters_in_path(&s_search_path);
+      if_in_ram_update_sd(th, p_s_inode) ;
+      journal_end(th, orig_super, orig_len_alloc) ;
+      journal_begin(th, orig_super, orig_len_alloc) ;
+      reiserfs_update_inode_transaction(p_s_inode) ;
+    }
+  }
+
+  /* Set key to search for the object stat_data. */  
+  s_item_key.k_offset = SD_OFFSET;
+  s_item_key.k_uniqueness = SD_UNIQUENESS;
+  /* Search for the object stat_data. */
+  if ( search_by_key(p_s_sb, &s_item_key, &s_search_path, &n_repeat, DISK_LEAF_NODE_LEVEL, READ_BLOCKS) == ITEM_NOT_FOUND ) {
+    print_block (PATH_PLAST_BUFFER (&s_search_path), 0, -1, -1);
+    reiserfs_panic (p_s_sb, "PAP-5450: reiserfs_delete_object: stat_data %k is not found", &s_item_key);
+  }
+
+  /* Delete object stat_data. */
+  if ( reiserfs_delete_item(th, p_s_inode, &s_search_path, &n_pos_in_item, &s_item_key, NULL, NOTHING_SPECIAL) < 0 )
+    reiserfs_panic (p_s_sb, "PAP: 5455: reiserfs_delete_object: reiserfs_delete_item: this code needs to be fixed");
+
+#ifdef CONFIG_REISERFS_CHECK
+  s_item_key.k_offset = MAX_KEY_OFFSET;
+  s_item_key.k_uniqueness = MAX_KEY_UNIQUENESS;
+  /* Try to find item of the deleted object. */
+  if ( search_by_key (p_s_sb, &s_item_key, &s_search_path, &n_repeat, DISK_LEAF_NODE_LEVEL, READ_BLOCKS) == ITEM_FOUND )
+    reiserfs_panic(p_s_sb,"PAP: 5460: reiserfs_delete_object: there is the item of deleted object");
+
+  PATH_LAST_POSITION(&s_search_path)--;
+  if (!COMP_SHORT_KEYS (&(PATH_PITEM_HEAD(&s_search_path)->ih_key), &s_item_key)) {
+    print_block (PATH_PLAST_BUFFER (&s_search_path), PRINT_LEAF_ITEMS,
+		 PATH_LAST_POSITION(&s_search_path) - 2, PATH_LAST_POSITION(&s_search_path) + 2);
+    reiserfs_panic(p_s_sb,"PAP-5470: reiserfs_delete_object: there is the item %h of deleted object %k. Inode key %k",
+		   PATH_PITEM_HEAD(&s_search_path), &s_item_key, INODE_PKEY (p_s_inode));
+  }
+  decrement_counters_in_path(&s_search_path);
+#endif
+
+  p_s_inode->i_size = 0;
+}
+
+
+/*********************** Inode part **************************************/
+int increment_i_read_sync_counter(
+      struct inode  * p_s_inode
+    ) {
+  int n_repeat = CARRY_ON;
+
+#ifdef CONFIG_REISERFS_CHECK
+  int n_repeat_counter = 0;
+#endif
+
+  /* Call schedule while this file is being converted. */
+  while ( p_s_inode->u.reiserfs_i.i_is_being_converted )  {
+#ifdef CONFIG_REISERFS_CHECK
+    if (p_s_inode->u.reiserfs_i.i_read_sync_counter)
+      reiserfs_panic (p_s_inode->i_sb, "PAP-5480: increment_i_read_sync_counter: file is read (synced) already");
+    if ( !(++n_repeat_counter % 15000) )
+      printk ("increment_i_read_sync_counter: (inode=%lu, pid=%d, counter=%d)\n", p_s_inode->i_ino, current->pid, n_repeat_counter);
+#endif
+    n_repeat |= SCHEDULE_OCCURRED;
+#ifdef __KERNEL__
+    current->policy |= SCHED_YIELD;
+    schedule();
+#endif
+  }
+
+  p_s_inode->u.reiserfs_i.i_read_sync_counter++;
+  return n_repeat;
+}
+
+
+void  decrement_i_read_sync_counter(
+        struct inode  * p_s_inode
+      ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( ! p_s_inode->u.reiserfs_i.i_read_sync_counter )
+    reiserfs_panic (p_s_inode->i_sb, "PAP-5490: increment_i_read_sync_counter: read_sync_counter is zero");
+#endif
+
+  p_s_inode->u.reiserfs_i.i_read_sync_counter--;
+}
+
+
+int lock_inode_to_convert(
+      struct inode  * p_s_inode
+    ) {
+  int n_repeat = CARRY_ON;
+
+#ifdef CONFIG_REISERFS_CHECK
+  int n_repeat_counter = 0;
+#endif
+
+  /* Call schedule() while there is read from this file. */
+  while ( p_s_inode->u.reiserfs_i.i_read_sync_counter ) {
+#ifdef CONFIG_REISERFS_CHECK
+    if (p_s_inode->u.reiserfs_i.i_is_being_converted)
+      reiserfs_panic (p_s_inode->i_sb, "PAP-5495: lock_inode_to_convert: file is being truncated (or appended) already");
+    if ( !(++n_repeat_counter % 15000) )
+      printk ("lock_inode_to_convert: (inode=%lu, pid=%d, counter=%d)\n", p_s_inode->i_ino, current->pid, n_repeat_counter);
+#endif
+    n_repeat |= SCHEDULE_OCCURRED;
+#ifdef __KERNEL__
+    current->policy |= SCHED_YIELD;
+    schedule();
+#endif
+  }
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( p_s_inode->u.reiserfs_i.i_is_being_converted || p_s_inode->u.reiserfs_i.i_read_sync_counter )
+    reiserfs_panic (p_s_inode->i_sb, "PAP-5500: lock_inode_to_convert: illegal case");
+#endif
+
+  /* Mark file as ready to convert. */
+  p_s_inode->u.reiserfs_i.i_is_being_converted = 1;
+  return n_repeat;
+}
+
+
+void  unlock_inode_after_convert(
+        struct inode * p_s_inode
+      ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( p_s_inode->u.reiserfs_i.i_is_being_converted != 1 ||
+                                        p_s_inode->u.reiserfs_i.i_read_sync_counter )
+    reiserfs_panic (p_s_inode->i_sb, "PAP-5510: unlock_inode_after_convert: illegal case");
+#endif
+
+  /* Read is possible. */
+  p_s_inode->u.reiserfs_i.i_is_being_converted = 0;
+}
+
+/************ End of the inode part ***************************/
+
+
+
+
+
+
+/* Convert an unformatted node to a direct item. 
+   Returns number of deleted bytes or -1 if io error encountered. */
+static int indirect_to_direct(
+      struct reiserfs_transaction_handle *th,
+      struct inode        * p_s_inode,          /* Pointer to the file inode.                 */
+      struct super_block  * p_s_sb,             /* Pointer to the super block.                */
+      struct path         * p_s_path,           /* Pointer to the path to the indirect item.  */
+      struct key          * p_s_item_key,       /* Key to search for the last file byte.      */
+      unsigned long         n_new_file_size,    /* New file size.                             */
+      char                * p_c_mode
+    ) {
+  struct buffer_head  * p_s_unfm_bh;              /* Pointer to the converted unformatted node
+                                                    buffer.                                   */
+  struct item_head      s_ih;
+  unsigned long         n_unfm_number = 0; 	/* Unformatted node block number              */
+  int                   n_pos_in_item,
+                        n_repeat_or_retval, /* this variable is overloaded to be used for two purposes:
+					       tracking whether schedule occured, and for use as
+					       a temporary variable */
+                        n_block_size = p_s_sb->s_blocksize;
+
+  p_s_sb->u.reiserfs_sb.s_indirect2direct ++;
+  /* Copy item at the path. */
+  copy_item_head(&s_ih, PATH_PITEM_HEAD(p_s_path) );
+  /* Don't read while we are converting the unformatted node. */
+  if ( (n_repeat_or_retval = lock_inode_to_convert(p_s_inode)) )
+    /* Check whether saved item is at the path. */
+    n_repeat_or_retval = comp_items(&s_ih,p_s_path);
+  if ( n_repeat_or_retval == CARRY_ON )
+    /* Calculate last unformatted node number. */
+    n_unfm_number = B_I_POS_UNFM_POINTER(PATH_PLAST_BUFFER(p_s_path), &s_ih, I_UNFM_NUM(&s_ih) - 1);
+
+  /* We get the pointer to the unformatted to be converted into a direct item. */
+  while ( 1 ) {
+    if ( n_repeat_or_retval != CARRY_ON ) {
+      /* Search for the indirect item. */
+      if ( search_for_position_by_key(p_s_sb, p_s_item_key, p_s_path, &n_pos_in_item, &n_repeat_or_retval) == POSITION_NOT_FOUND )
+	reiserfs_panic(p_s_sb, "PAP-5520: indirect_to_direct: item to convert does not exist");
+      copy_item_head(&s_ih, PATH_PITEM_HEAD(p_s_path) );
+      n_unfm_number = B_I_POS_UNFM_POINTER(PATH_PLAST_BUFFER(p_s_path), &s_ih, I_UNFM_NUM(&s_ih) - 1);
+    }
+    p_s_unfm_bh = NULL;
+    if ( n_unfm_number )  {
+      /* Read unformatted node to convert. */
+      n_repeat_or_retval = CARRY_ON;
+      p_s_unfm_bh = reiserfs_bread(p_s_sb->s_dev, n_unfm_number, p_s_sb->s_blocksize, &n_repeat_or_retval);
+      if (!p_s_unfm_bh) {
+	*p_c_mode = M_SKIP_BALANCING;
+	pathrelse (p_s_path);
+	return -1;
+      }
+      /* Current item was shifted from buffer at the path. */
+      if ( n_repeat_or_retval != CARRY_ON && comp_items(&s_ih, p_s_path) )  {
+        brelse(p_s_unfm_bh);
+        continue;
+      }
+
+#if defined(CONFIG_REISERFS_CHECK) && !defined(PACKING_LOCALITY_READ_AHEAD)
+
+      if ( p_s_unfm_bh->b_count != 1 && !buffer_journaled(p_s_unfm_bh)) {
+        reiserfs_panic (p_s_sb, "PAP-5530: indirect_to_direct: (read counter %d, converted %d)"
+			" converted block (%d) must not be in use (b_count==%d)",
+			p_s_inode->u.reiserfs_i.i_read_sync_counter, 
+			p_s_inode->u.reiserfs_i.i_is_being_converted, n_unfm_number, p_s_unfm_bh->b_count);
+      }
+
+#endif
+
+    }
+    break;
+  }
+
+  /* Set direct item header to insert. */
+  s_ih.ih_key.k_offset += (I_UNFM_NUM (&s_ih) - 1) * n_block_size;
+  n_pos_in_item = s_ih.ih_key.k_offset;	/*(s_ih.ih_key.k_offset -= (s_ih.ih_key.k_offset - 1) % n_block_size);*/
+  s_ih.ih_key.k_uniqueness    = TYPE_DIRECT;
+  s_ih.u.ih_free_space          = MAX_US_INT;
+  n_repeat_or_retval = s_ih.ih_item_len = n_new_file_size % n_block_size;
+  PATH_LAST_POSITION(p_s_path)++;
+
+  /* Insert new direct item in the tree. This insert must mark nodes getting a new item as suspected recipient */
+  /* Vladimir, LOOK journal **** was preserve indirect to direct */
+  if ( reiserfs_insert_item(th, p_s_sb, p_s_path, &s_ih,
+                      ( p_s_unfm_bh ) ? p_s_unfm_bh->b_data : NULL, REISERFS_KERNEL_MEM, 0/*zero bytes*/, NOTHING_SPECIAL) < 0 ) {
+    /* No disk memory. So we can not convert last unformatted node to the direct item.
+       In this case we mark that node has just 'n_new_file_size % n_block_size'
+       bytes of the file.*/
+    struct item_head * p_s_ih;
+
+    if ( search_for_position_by_key(p_s_sb, p_s_item_key, p_s_path, &n_pos_in_item, &n_repeat_or_retval) == POSITION_NOT_FOUND )
+      reiserfs_panic(p_s_sb, "PAP-5540: indirect_to_direct: item to convert does not exist");
+    n_repeat_or_retval = (p_s_ih = PATH_PITEM_HEAD(p_s_path))->u.ih_free_space;
+    p_s_ih->u.ih_free_space = n_block_size - n_new_file_size % n_block_size;
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( n_repeat_or_retval > p_s_ih->u.ih_free_space )
+      reiserfs_panic (p_s_sb, "PAP-5550: indirect_to_direct: illegal new ih_free_space");
+#endif
+
+    n_repeat_or_retval = p_s_ih->u.ih_free_space - n_repeat_or_retval;
+    *p_c_mode = M_SKIP_BALANCING;
+
+    /* non-atomic mark_buffer_dirty is allowed here */
+    /* mark_buffer_dirty(PATH_PLAST_BUFFER(p_s_path), 0); journal victim */
+    journal_mark_dirty(th, p_s_sb, PATH_PLAST_BUFFER(p_s_path));
+    unlock_inode_after_convert(p_s_inode);
+    pathrelse(p_s_path);
+  }
+  else {
+    /* We have inserted new direct item and must remove last unformatted node. */
+    *p_c_mode = M_CUT;
+    /* Set position of its first byte to inode (for read needs) */
+    p_s_inode->u.reiserfs_i.i_first_direct_byte = n_pos_in_item;
+    p_s_inode->i_blocks += p_s_sb->s_blocksize / 512;
+  }
+
+  brelse(p_s_unfm_bh);
+  /* We have inserted new direct item and must remove last unformatted node. */
+/*  *p_c_mode = M_CUT;*/
+  return n_repeat_or_retval;
+}
+
+
+
+int maybe_indirect_to_direct (struct reiserfs_transaction_handle *th,
+			      struct inode        * p_s_inode,
+			      struct super_block  * p_s_sb,
+			      struct path         * p_s_path,
+			      struct key          * p_s_item_key,
+			      unsigned long         n_new_file_size,
+			      char                * p_c_mode)
+{
+    int n_block_size = p_s_sb->s_blocksize;
+    int cut_bytes;
+
+    if (n_new_file_size != p_s_inode->i_size)
+	reiserfs_warning ("maybe_indirect_to_direct: new_size %Ld, i_size %Ld\n",
+			  n_new_file_size, p_s_inode->i_size);
+    
+    /* We can store tail of the file in an unformatted node. */ 
+    if ( dont_have_tails (p_s_sb) ||
+	 STORE_TAIL_IN_UNFM(n_new_file_size, n_new_file_size % n_block_size, n_block_size) ) { /* tail too long */
+	/* Change ih->u.ih_free_space in the indirect item defined by path. */
+	struct item_head  * p_s_ih = PATH_PITEM_HEAD(p_s_path);
+	int                 n_old_free_space = p_s_ih->u.ih_free_space;
+	int offset;
+	
+	*p_c_mode = M_SKIP_BALANCING;
+	p_s_ih->u.ih_free_space = n_block_size - n_new_file_size % n_block_size;
+	journal_mark_dirty(th, p_s_sb, PATH_PLAST_BUFFER(p_s_path)) ;
+    
+#ifdef CONFIG_REISERFS_CHECK
+	if ( n_old_free_space >= p_s_ih->u.ih_free_space )
+	    reiserfs_panic (p_s_sb, "PAP-5560: maybe_indirect_to_direct: tail is too small");
+#endif
+	
+	offset = n_new_file_size & (n_block_size - 1);
+	cut_bytes = p_s_ih->u.ih_free_space - n_old_free_space;
+
+	if (cut_bytes) {
+	    __u32 block;
+	    __u32 * item = (__u32 *)B_I_PITEM (PATH_PLAST_BUFFER (p_s_path), p_s_ih);
+	    int unfm_num = I_UNFM_NUM (p_s_ih);
+
+	    block = le32_to_cpu (item [unfm_num - 1]);
+	    if (block) {
+		struct buffer_head * bh;
+
+		bh = bread (p_s_sb->s_dev, block, p_s_sb->s_blocksize);
+		if (bh) {
+		    memset (bh->b_data + offset, 0, cut_bytes);
+		    journal_mark_dirty_nolog (th, p_s_sb, bh);
+		    brelse (bh);
+		}
+	    }
+	}
+	pathrelse(p_s_path);
+	return cut_bytes;
+    }
+    /* Permorm the conversion to a direct_item. */
+    return indirect_to_direct(th, p_s_inode, p_s_sb, p_s_path, p_s_item_key, n_new_file_size, p_c_mode);
+}
+
+
+/* we did indirect_to_direct conversion. And we have inserted direct
+   item successesfully, but there were no disk space to cut unfm
+   pointer being converted. Therefore we have to delete inserted
+   direct item(s) */
+static void indirect_to_direct_roll_back (struct reiserfs_transaction_handle *th, struct inode * inode, struct path * path)
+{
+  struct key tail_key;
+  int tail_len;
+  int pos_in_item;
+  int repeat_or_removed;
+
+
+  copy_key (&tail_key, INODE_PKEY (inode));
+  tail_key.k_offset = inode->i_size + 1;
+  tail_key.k_uniqueness = TYPE_DIRECT;
+  tail_len = tail_key.k_offset % inode->i_sb->s_blocksize - 1;
+  while (tail_len) {
+    /* look for the last byte of the tail */
+    if (search_for_position_by_key (inode->i_sb, &tail_key, path, &pos_in_item, &repeat_or_removed) == POSITION_NOT_FOUND)
+      reiserfs_panic (inode->i_sb, "vs-5615: indirect_to_direct_roll_back: found invalid item");
+#ifdef CONFIG_REISERFS_CHECK
+    if (pos_in_item != PATH_PITEM_HEAD (path)->ih_item_len - 1)
+      reiserfs_panic (inode->i_sb, "vs-5616: indirect_to_direct_roll_back: appended bytes found");
+#endif
+    PATH_LAST_POSITION (path) --;
+	
+    repeat_or_removed = reiserfs_delete_item (th, inode, path, &pos_in_item, &tail_key, 0, NOTHING_SPECIAL);
+#ifdef CONFIG_REISERFS_CHECK
+    if (repeat_or_removed <= 0 || repeat_or_removed > tail_len)
+      reiserfs_panic (inode->i_sb, "vs-5617: indirect_to_direct_roll_back: "
+		      "there was tail %d bytes, removed item length %d bytes",
+		      tail_len, repeat_or_removed);
+#endif
+    tail_len -= repeat_or_removed;
+    tail_key.k_offset -= repeat_or_removed;
+  }
+  printk ("indirect_to_direct_roll_back: indirect_to_direct conversion has been rolled back due to lack of disk space\n");
+  inode->u.reiserfs_i.i_first_direct_byte = NO_BYTES_IN_DIRECT_ITEM;
+  mark_inode_dirty (inode);
+}
+
+
+/* (Truncate or cut entry) or delete object item. */
+int reiserfs_cut_from_item(
+      struct reiserfs_transaction_handle *th,
+      struct inode        * p_s_inode,
+      struct super_block  * p_s_sb,
+      struct path         * p_s_path,
+      int                 * p_n_pos_in_item,
+      struct key          * p_s_item_key,
+      unsigned long         n_new_file_size,
+      int		    preserve_mode	/* can be PRESERVE_RENAMING or NOTHING SPECIAL */
+    ) {
+  /* Every function which is going to call do_balance must first
+     create a tree_balance structure.  Then it must fill up this
+     structure by using the init_tb_struct and fix_nodes functions.
+     After that we can make tree balancing. */
+  struct tree_balance s_cut_balance;
+  int                 n_repeat,
+                      n_cut_size,        /* Amount to be cut. */
+                      /* n_ret_value = CARRY_ON, */
+		      n_fix_ret_value = CARRY_ON, /* return value from fix_nodes and do_balance */
+		      n_count_ret_value = 0,      /* return value from indirect->direct */
+                      n_removed = 0,     /* Number of the removed unformatted nodes. */
+  		      n_is_inode_locked = 0;
+  char                c_mode;            /* Mode of the balance. */
+  int was_unfm_suspected_recipient = 0;
+
+  init_tb_struct(&s_cut_balance, p_s_sb, p_s_path, n_cut_size);
+  s_cut_balance.preserve_mode = preserve_mode;
+
+  /* Repeat this loop until we either cut the item without needing to balance, or we fix_nodes without
+     schedule occuring */
+  while ( 1 ) {
+      /* Determine the balance mode, position of the first byte to be cut, and size to be cut.
+	 In case of the indirect item free unformatted nodes which are pointed to by
+	 the cut pointers. */
+
+    /* Vladimir, LOOK, journal **** first nothing special was preserving indirect to direct */
+    c_mode = prepare_for_delete_or_cut(th, p_s_inode, p_s_path, p_s_item_key, p_n_pos_in_item, &n_removed, &n_cut_size, 
+    				       n_new_file_size,
+				       n_is_inode_locked ? NOTHING_SPECIAL : NOTHING_SPECIAL, &was_unfm_suspected_recipient);
+    if ( c_mode == M_CONVERT )  {
+	/* convert last unformatted node to direct item or adjust its ih_free_space */
+#ifdef CONFIG_REISERFS_CHECK
+      if ( n_fix_ret_value != CARRY_ON )
+        reiserfs_panic (p_s_sb, "PAP-5570: reiserfs_cut_from_item: can not convert twice");
+#endif
+
+      n_count_ret_value = maybe_indirect_to_direct (th, p_s_inode, p_s_sb, p_s_path, p_s_item_key,
+					      n_new_file_size, &c_mode);
+      if (n_count_ret_value == -1)
+	return 0;
+      /* We have cut all item bytes and must stop. */
+      if ( c_mode == M_SKIP_BALANCING )
+        break;
+      n_is_inode_locked = 1;
+      /* So, we have performed the first part of the conversion:
+	 inserting the new direct item.  Now we are removing the last
+	 unformatted node pointer. Set key to search for it. */
+      p_s_item_key->k_uniqueness = TYPE_INDIRECT;
+      n_new_file_size -= n_new_file_size % p_s_sb->s_blocksize;
+      p_s_item_key->k_offset = n_new_file_size + 1;
+      if ( search_for_position_by_key(p_s_sb, p_s_item_key, p_s_path, p_n_pos_in_item, &n_repeat) == POSITION_NOT_FOUND ){
+	print_block (PATH_PLAST_BUFFER (p_s_path), 3, PATH_LAST_POSITION (p_s_path) - 1, PATH_LAST_POSITION (p_s_path) + 1);
+	reiserfs_panic(p_s_sb, "PAP-5580: reiserfs_cut_from_item: item to convert does not exist (%k)", p_s_item_key);
+      }
+      continue;
+    }
+
+    s_cut_balance.insert_size[0] = n_cut_size;
+
+    n_fix_ret_value = fix_nodes(th, c_mode, &s_cut_balance, *p_n_pos_in_item, NULL);
+ 
+#ifdef CONFIG_REISERFS_CHECK_ONE_PROCESS
+    if ( n_fix_ret_value == PATH_INCORRECT )
+      reiserfs_panic(p_s_sb, "PAP-5600: reiserfs_cut_from_item: "
+		     "illegal returned value");
+#endif
+
+    if ( n_fix_ret_value != SCHEDULE_OCCURRED && n_fix_ret_value != PATH_INCORRECT )
+      break;
+
+    /* else schedule() occured while fix_nodes() worked */
+    if ( search_for_position_by_key(p_s_sb, p_s_item_key, p_s_path, p_n_pos_in_item, &n_repeat) == POSITION_NOT_FOUND )
+      reiserfs_panic(p_s_sb, "PAP-5610: reiserfs_cut_from_item: item to delete does not exist");
+  } /* while */
+
+  if ( n_fix_ret_value == NO_DISK_SPACE || n_fix_ret_value == IO_ERROR || n_count_ret_value == NO_DISK_SPACE) {
+    if ( n_is_inode_locked ) {
+      indirect_to_direct_roll_back(th, p_s_inode, p_s_path);
+    }
+    unfix_nodes (th, &s_cut_balance);
+    return 0;
+  }
+
+  if ( c_mode != M_SKIP_BALANCING ) {
+    journal_lock_dobalance(p_s_sb) ;
+
+#ifdef CONFIG_REISERFS_CHECK
+/*    if ( n_ret_value >= calc_deleted_bytes_number(&s_cut_balance, c_mode) )
+      reiserfs_panic (p_s_sb, "PAP-5630: reiserfs_cut_from_item: returned value is too big");*/
+    if (n_fix_ret_value != CARRY_ON)
+      reiserfs_panic (p_s_sb, "PAP-5630: ret_value is other than CARRY_ON");
+    if ( c_mode == M_PASTE || c_mode == M_INSERT )
+      reiserfs_panic (p_s_sb, "PAP-5640: reiserfs_cut_from_item: illegal mode");
+#endif
+      /* Calculate number of bytes that need to be cut from the item.  how could this have been right? */
+      /* it was n_ret_value = calc - n_ret_value.  We know from above that n_ret_value was CARRY_ON
+      ** or we would be reiserfs_panic'ing.  So in the error case (with reiserfs_check off), we were subtracting some number
+      ** of bytes, for no apparent reason.
+      */
+    n_count_ret_value = calc_deleted_bytes_number(&s_cut_balance, c_mode) ;
+
+    if ( c_mode == M_DELETE ) {
+      struct item_head * p_s_ih = B_N_PITEM_HEAD(PATH_PLAST_BUFFER(s_cut_balance.tb_path), PATH_LAST_POSITION(s_cut_balance.tb_path));
+
+      if ( I_IS_DIRECT_ITEM(p_s_ih) && p_s_ih->ih_key.k_offset % p_s_sb->s_blocksize == 1 ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+	if ( p_s_inode->u.reiserfs_i.i_first_direct_byte != p_s_ih->ih_key.k_offset )
+	  reiserfs_panic (p_s_sb, "PAP-5650: reiserfs_cut_from_item: illegal first direct byte position");
+#endif
+
+	p_s_inode->u.reiserfs_i.i_first_direct_byte = NO_BYTES_IN_DIRECT_ITEM;
+	p_s_inode->i_blocks -= p_s_sb->s_blocksize / 512;
+      }
+    }
+
+    if (n_is_inode_locked) {
+      /* we are going to cut last unfm ptr, preserve it first. unfm node block number is on preserve list already */
+#ifdef CONFIG_REISERFS_CHECK
+      if (!I_IS_INDIRECT_ITEM (PATH_PITEM_HEAD (s_cut_balance.tb_path)))
+	reiserfs_panic (p_s_sb, "vs-5652: reiserfs_cut_from_item: item must be indirect %h", PATH_PITEM_HEAD (s_cut_balance.tb_path));
+      if (c_mode == M_DELETE && -(PATH_PITEM_HEAD (s_cut_balance.tb_path)->ih_item_len + IH_SIZE) != s_cut_balance.insert_size[0]) {
+	reiserfs_panic (p_s_sb, "vs-5653: reiserfs_cut_from_item: "
+			"can not complete indirect_to_direct conversion of %h (DELETE, insert_size==%d)",
+			PATH_PITEM_HEAD (s_cut_balance.tb_path), s_cut_balance.insert_size[0]);
+      }
+      if (c_mode == M_CUT && s_cut_balance.insert_size[0] != -UNFM_P_SIZE) {
+	reiserfs_panic (p_s_sb, "vs-5654: reiserfs_cut_from_item: can not complete indirect_to_direct conversion of %h (CUT, insert_size==%d)",
+			PATH_PITEM_HEAD (s_cut_balance.tb_path), s_cut_balance.insert_size[0]);
+      }
+#endif
+      /* we should not preserve indirect item if unformatted node was marked as suspected recipient */
+/*
+      if (!was_unfm_suspected_recipient)
+	preserve_indirect_item (&s_cut_balance);
+*/
+    }
+/*
+    if (preserve_mode == PRESERVE_RENAMING)
+      preserve_entry (&s_cut_balance);
+*/
+    do_balance(th, &s_cut_balance, *p_n_pos_in_item, NULL, NULL, c_mode, REISERFS_KERNEL_MEM, 0/* zero number */);
+    journal_unlock_dobalance(p_s_sb) ;
+    if ( n_is_inode_locked )
+      unlock_inode_after_convert(p_s_inode);
+  } /* ! SKIP_BALANCING */
+
+  return n_count_ret_value;
+}
+
+int reiserfs_file_release(struct inode *p_s_inode, struct file *p_s_filp) {
+  struct path           s_search_path;  /* Path to the current object item. */
+  struct item_head    * p_s_ih;         /* Pointer to an item header. */
+  struct key            s_item_key;     /* Key to search for a previous file item. */
+  unsigned long         n_file_size,    /* Old file size. */
+                        n_new_file_size;/* New file size. */
+  int                   n_deleted,      /* Number of deleted or truncated bytes. */
+                        n_pos_in_item,  /* Found position in an item. */
+  			n_repeat;   
+  int windex ;
+  struct reiserfs_transaction_handle th ;
+  int jbegin_count = JOURNAL_PER_BALANCE_CNT * 3; 
+
+
+  /* only pack when reiserfs_file_write tells us to by setting i_pack_on_close */
+  if (!p_s_inode->u.reiserfs_i.i_pack_on_close || dont_have_tails (p_s_inode->i_sb) ||
+      p_s_inode->u.reiserfs_i.nopack) {
+    return 0 ;
+  }
+  down(&p_s_inode->i_sem) ;
+  p_s_inode->u.reiserfs_i.i_pack_on_close = 0 ;
+  journal_begin(&th, p_s_inode->i_sb, jbegin_count) ;
+  windex = push_journal_writer("reiserfs_release_inode") ;
+  reiserfs_update_inode_transaction(p_s_inode) ;
+  init_path (&s_search_path);
+
+  /* Copy key of the first object item. */
+  copy_key(&s_item_key, INODE_PKEY(p_s_inode));
+
+  /* New file size is the same as the original.  We are calling cut_from_item so it will do all the
+  ** indirect->direct work for us
+  */
+  n_new_file_size = p_s_inode->i_size;
+
+  /* Form key to search for the last file item. */
+  s_item_key.k_offset = MAX_KEY_OFFSET; /* pasted from truncate, I should be able to put the file size here right? */
+  if ( p_s_inode->u.reiserfs_i.i_first_direct_byte != NO_BYTES_IN_DIRECT_ITEM ) {
+    /* already packed, we're done */
+    pop_journal_writer(windex) ;
+    journal_end(&th, p_s_inode->i_sb, jbegin_count) ;
+    up(&p_s_inode->i_sem) ;
+    return 0 ;
+  } else {
+    s_item_key.k_uniqueness = TYPE_INDIRECT;
+  }
+
+  if ( search_for_position_by_key(p_s_inode->i_sb, &s_item_key, &s_search_path, &n_pos_in_item, &n_repeat) == POSITION_FOUND )
+    reiserfs_panic (p_s_inode->i_sb, "PAP-5660: reiserfs_file_release: "
+		      "object item has too big offset");
+
+  /* pasted from truncate.  Why am I doing this? */
+  n_pos_in_item--;
+
+  /* Calculate old size of the file. Pasted from truncate.  I'm keeping this incase the inode is wrong some how,
+  ** but I really don't need it
+  */
+  p_s_ih = PATH_PITEM_HEAD(&s_search_path);
+  if ( I_IS_STAT_DATA_ITEM(p_s_ih) )
+    n_file_size = 0;
+  else
+    n_file_size = p_s_ih->ih_key.k_offset + I_BYTES_NUMBER(p_s_ih,p_s_inode->i_sb->s_blocksize) - 1;
+
+  if ( n_file_size == 0 || n_file_size != n_new_file_size ||
+       (STORE_TAIL_IN_UNFM( I_BYTES_NUMBER(p_s_ih,p_s_inode->i_sb->s_blocksize), n_file_size, p_s_inode->i_sb->s_blocksize))
+  ) {
+    pathrelse(&s_search_path);
+    pop_journal_writer(windex) ;
+    journal_end(&th, p_s_inode->i_sb, jbegin_count) ;
+    /* no conversion done, O_SYNC need not be honored */
+    up(&p_s_inode->i_sem) ;
+    return 0;
+  }
+
+  /* Update key to search for the last file item. */
+  s_item_key.k_offset = n_file_size;
+
+  /* Cut or delete file item. */
+  n_deleted = reiserfs_cut_from_item(&th, p_s_inode, p_s_inode->i_sb, &s_search_path, &n_pos_in_item, &s_item_key, n_new_file_size, 
+                                     NOTHING_SPECIAL);
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( n_deleted > n_file_size ){
+    reiserfs_panic (p_s_inode->i_sb, "PAP-5670: reiserfs_file_release: "
+		    "reiserfs_file_release returns too big number: deleted %d, file_size %lu, item_key %k",
+		    n_deleted, n_file_size, &s_item_key);
+  }
+#endif
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( n_file_size > n_new_file_size )
+    reiserfs_panic (p_s_inode->i_sb, "PAP-5680: reiserfs_file_release: object item did not find");
+#endif
+
+  /* note, FS corruption after crash if I don't update the stat data.  Why? */
+  if_in_ram_update_sd (&th, p_s_inode);
+  pop_journal_writer(windex) ;
+  if (p_s_filp->f_flags & O_SYNC) {
+    journal_end_sync(&th, p_s_inode->i_sb, jbegin_count) ;
+  } else {
+    journal_end(&th, p_s_inode->i_sb, jbegin_count) ;
+  }
+  up(&p_s_inode->i_sem) ;
+  return 0 ;
+}
+
+/* Truncate file to the new size. */
+void  reiserfs_truncate_file(
+        struct  inode * p_s_inode       /* Pointer to the file inode. New size
+                                            already marked in the inode. */
+      ) {
+  struct path           s_search_path;  /* Path to the current object item. */
+  struct item_head    * p_s_ih;         /* Pointer to an item header. */
+  struct key            s_item_key;     /* Key to search for a previous file item. */
+  unsigned long         n_file_size,    /* Old file size. */
+                        n_new_file_size;/* New file size. */
+  int                   n_deleted,      /* Number of deleted or truncated bytes. */
+                        n_pos_in_item,  /* Found position in an item. */
+  			n_repeat;   
+  int windex ;
+  int jbegin_count = JOURNAL_PER_BALANCE_CNT * 3 ;
+  int inode_locked = 0 ;
+  struct reiserfs_transaction_handle th ;
+
+  if ( ! (S_ISREG(p_s_inode->i_mode) || S_ISDIR(p_s_inode->i_mode) || S_ISLNK(p_s_inode->i_mode)) )
+    return;
+
+  journal_begin(&th, p_s_inode->i_sb, jbegin_count) ;
+  windex = push_journal_writer("reiserfs_truncate_file") ;
+  init_path (&s_search_path);
+  reiserfs_update_inode_transaction(p_s_inode) ;
+
+  /* Copy key of the first object item. */
+  copy_key(&s_item_key, INODE_PKEY(p_s_inode));
+
+  /* Get new file size. */
+  n_new_file_size = p_s_inode->i_size;
+
+  /* Form key to search for the last file item. */
+  s_item_key.k_offset = MAX_KEY_OFFSET; /* We don't know old size of the file. */
+  if (p_s_inode->u.reiserfs_i.i_first_direct_byte != NO_BYTES_IN_DIRECT_ITEM ) {
+    s_item_key.k_uniqueness = TYPE_DIRECT;
+    /* we only lock for direct items because reiserfs_cut_from_item might
+    ** try to do an indirect2direct conversion, which also locks the tail
+    **
+    */
+    lock_inode_to_convert(p_s_inode) ;
+    inode_locked = 1;
+  } else {
+    s_item_key.k_uniqueness = TYPE_INDIRECT;
+  }
+  if ( search_for_position_by_key(p_s_inode->i_sb, &s_item_key, &s_search_path, &n_pos_in_item, &n_repeat) == POSITION_FOUND )
+    reiserfs_panic (p_s_inode->i_sb, "PAP-5660: reiserfs_truncate_file: "
+		      "object item has too big offset");
+  n_pos_in_item--;
+
+  /* Calculate old size of the file. */
+  p_s_ih = PATH_PITEM_HEAD(&s_search_path);
+  if ( I_IS_STAT_DATA_ITEM(p_s_ih) )
+    n_file_size = 0;
+  else
+    n_file_size = p_s_ih->ih_key.k_offset + I_BYTES_NUMBER(p_s_ih,p_s_inode->i_sb->s_blocksize) - 1;
+
+  if ( n_file_size == 0 || n_file_size <= n_new_file_size ) {
+#ifdef REISERFS_INFO
+    printk ("reiserfs_truncate_file: old file size = %lu < new file size = %lu\n", n_file_size, n_new_file_size);
+#endif
+    pathrelse(&s_search_path);
+    pop_journal_writer(windex) ;
+    if (inode_locked)
+      unlock_inode_after_convert(p_s_inode) ;
+    journal_end(&th, p_s_inode->i_sb, jbegin_count) ;
+    return;
+  }
+
+  /* Update key to search for the last file item. */
+  s_item_key.k_offset = n_file_size;
+
+  do  {
+    /* Cut or delete file item. */
+    n_deleted = reiserfs_cut_from_item(&th, p_s_inode, p_s_inode->i_sb, &s_search_path, &n_pos_in_item, 
+                                       &s_item_key, n_new_file_size, NOTHING_SPECIAL);
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( n_deleted > n_file_size ){
+      reiserfs_panic (p_s_inode->i_sb, "PAP-5670: reiserfs_truncate_file: "
+		      "reiserfs_truncate_file returns too big number: deleted %d, file_size %lu, item_key %k",
+		      n_deleted, n_file_size, &s_item_key);
+    }
+#endif
+
+    if (inode_locked) {
+      /* the direct item is probably gone, and we can't have the inode
+      ** lock while restarting the transaction.  So, it is 
+      ** easiest to drop the inode lock here, and reaquire below if there
+      ** are still direct bytes
+      */
+      unlock_inode_after_convert(p_s_inode) ;
+      inode_locked = 0 ;
+    }
+
+    /* Change key to search the last file item. */
+    if ( (s_item_key.k_offset = (n_file_size -= n_deleted)) < p_s_inode->u.reiserfs_i.i_first_direct_byte ) {
+      s_item_key.k_uniqueness = TYPE_INDIRECT;
+    }
+    if (journal_transaction_should_end(&th, th.t_blocks_allocated)) {
+      int orig_len_alloc = th.t_blocks_allocated ;
+      p_s_inode->i_ctime = CURRENT_TIME ;
+      p_s_inode->i_mtime = CURRENT_TIME ;
+      decrement_counters_in_path(&s_search_path);
+      if_in_ram_update_sd(&th, p_s_inode) ;
+      journal_end(&th, p_s_inode->i_sb, orig_len_alloc) ;
+      journal_begin(&th, p_s_inode->i_sb, orig_len_alloc) ;
+      reiserfs_update_inode_transaction(p_s_inode) ;
+    }
+    if (s_item_key.k_uniqueness == TYPE_DIRECT) {
+      lock_inode_to_convert(p_s_inode) ;
+      inode_locked = 1;
+    }
+
+    /* While there are bytes to truncate and previous file item is presented in the tree. */
+  } while ( n_file_size > n_new_file_size &&
+	    search_for_position_by_key(p_s_inode->i_sb, &s_item_key, &s_search_path, &n_pos_in_item, &n_repeat) == POSITION_FOUND )  ;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( n_file_size > n_new_file_size )
+    reiserfs_panic (p_s_inode->i_sb, "PAP-5680: reiserfs_truncate_file: object item did not find");
+#endif
+  if (inode_locked) {
+    unlock_inode_after_convert(p_s_inode) ;
+  }
+
+  p_s_inode->i_mtime = p_s_inode->i_ctime = CURRENT_TIME;
+  if_in_ram_update_sd (&th, p_s_inode);
+  pop_journal_writer(windex) ;
+  journal_end(&th, p_s_inode->i_sb, jbegin_count) ;
+}
+
+
+/* Paste bytes to the existing item. Returns bytes number pasted into the item. */
+int reiserfs_paste_into_item(
+	struct reiserfs_transaction_handle *th,
+	struct super_block  * p_s_sb,	   	/* Pointer to the supoer block.	*/
+	struct path         * p_s_search_path,	/* Path to the pasted item.          */
+	int                 * p_n_pos_in_item,	/* Paste position in the item above. */
+	struct key          * p_s_key,        	/* Key to search for the needed item.*/
+	const char          * p_c_body,       	/* Pointer to the bytes to paste.    */
+	int                   n_pasted_size,  	/* Size of pasted bytes.             */
+	int                   n_mem_mode,     	/* Copy from KERNEL or USER buffer.  */
+	int		      n_zeros_num	/* Number of zeros to be pasted.     */
+	) {
+    struct tree_balance s_paste_balance;
+    int                 n_fix_nodes_res,
+      			n_repeat;
+
+    if ( n_pasted_size < 0 )
+      reiserfs_panic(p_s_sb, "PAP-5690: reiserfs_paste_into_item: illegal pasted size");
+
+    init_tb_struct(&s_paste_balance, p_s_sb, p_s_search_path, n_pasted_size);
+    s_paste_balance.preserve_mode = NOTHING_SPECIAL;
+    while ( (n_fix_nodes_res = fix_nodes(th, M_PASTE, &s_paste_balance, *p_n_pos_in_item, NULL)) == SCHEDULE_OCCURRED ||
+	    n_fix_nodes_res == PATH_INCORRECT )  {
+
+#ifdef CONFIG_REISERFS_CHECK_ONE_PROCESS
+      if ( n_fix_nodes_res == PATH_INCORRECT )
+	reiserfs_panic(p_s_sb, "PAP-5700: reiserfs_paste_into_item: illegal returned value");
+#endif
+
+      /* schedule() occurred while fix_balance() worked */
+      if ( search_for_position_by_key (p_s_sb, p_s_key, p_s_search_path, p_n_pos_in_item, &n_repeat) == POSITION_FOUND ) {
+	reiserfs_panic (p_s_sb, "PAP-5710: reiserfs_paste_into_item: entry or pasted byte (%k) exists", p_s_key);
+      }
+#ifdef CONFIG_REISERFS_CHECK
+      {
+	struct item_head * found_ih = B_N_PITEM_HEAD (PATH_PLAST_BUFFER (p_s_search_path),
+						      PATH_LAST_POSITION (p_s_search_path));
+
+	if (I_IS_DIRECT_ITEM (found_ih)) {
+	  if (found_ih->ih_key.k_offset + I_BYTES_NUMBER (found_ih, p_s_sb->s_blocksize) !=
+	      p_s_key->k_offset ||
+	      I_BYTES_NUMBER (found_ih, p_s_sb->s_blocksize) != *p_n_pos_in_item)
+	    reiserfs_panic (p_s_sb, "PAP-5720: reiserfs_paste_into_item: found direct item (offset=%lu, length=%d) or position (%d) does not match to key (offset=%lu)",
+			    found_ih->ih_key.k_offset, found_ih->ih_item_len, *p_n_pos_in_item, p_s_key->k_offset);
+	}
+	if (I_IS_INDIRECT_ITEM (found_ih)) {
+	  if (found_ih->ih_key.k_offset + I_BYTES_NUMBER (found_ih, p_s_sb->s_blocksize) != p_s_key->k_offset || 
+	      I_UNFM_NUM (found_ih) != *p_n_pos_in_item ||
+	      found_ih->u.ih_free_space != 0)
+	    reiserfs_panic (p_s_sb, "PAP-5730: reiserfs_paste_into_item: "
+			  "found indirect item (offset=%lu, unfm pointers=%d, free_space=%d) or position (%d) does not match to key (%lu)",
+			    found_ih->ih_key.k_offset, I_UNFM_NUM (found_ih), found_ih->u.ih_free_space,
+			    *p_n_pos_in_item, p_s_key->k_offset);
+	}
+      }
+#endif
+    }
+
+    /* Perform balancing after all resources are collected by fix_nodes, and accessing
+      them will not risk triggering schedule. */
+    if ( n_fix_nodes_res == CARRY_ON ) {
+      journal_lock_dobalance(p_s_sb) ;
+
+      if ( s_paste_balance.insert_size[0] < 0 )
+	reiserfs_panic (p_s_sb, "PAP-5740: reiserfs_paste_into_item: insert_size = %d\n",s_paste_balance.insert_size[0]);
+
+      do_balance(th, &s_paste_balance, *p_n_pos_in_item, NULL, p_c_body, M_PASTE, n_mem_mode, n_zeros_num);
+      journal_unlock_dobalance(p_s_sb) ;
+      return (n_pasted_size);
+    }
+    unfix_nodes(th, &s_paste_balance);
+    return NO_DISK_SPACE; /* No disk space or io error. */
+}
+
+
+/* Insert new item into the buffer at the path. */
+int reiserfs_insert_item(
+			 struct reiserfs_transaction_handle *th,
+			 struct super_block  * 	p_s_sb,           /* Pointer to the super block.          */
+			 struct path         * 	p_s_path,         /* Path to the inserteded item.         */
+			 struct item_head    * 	p_s_ih,           /* Pointer to the item header to insert.*/
+			 const char          * 	p_c_body,         /* Pointer to the bytes to insert.      */
+			 int                   	n_mem_mode,       /* Copy from KERNEL or USER buffer.     */
+			 int			n_zeros_num,
+			 int			preserve_mode	  /* can be
+								     PRESERVE_INDIRECT_TO_DIRECT or
+								     NOTHING_SPECIAL. if
+								     PRESERVE_INDIRECT_TO_DIRECT,
+								     mark buffers new item gets into
+								     as suspected recipients */
+			 ) {
+    struct tree_balance s_ins_balance;
+    int                 n_fix_nodes_res,
+      			n_repeat;
+
+
+    init_tb_struct(&s_ins_balance, p_s_sb, p_s_path, IH_SIZE + p_s_ih->ih_item_len);
+    s_ins_balance.preserve_mode = preserve_mode;
+
+    p_s_ih->ih_reserved = 0;
+    if (p_c_body == 0)
+      n_zeros_num = p_s_ih->ih_item_len;
+
+
+    while ( (n_fix_nodes_res = fix_nodes(th, M_INSERT, &s_ins_balance, 0, p_s_ih)) == SCHEDULE_OCCURRED ||
+	    n_fix_nodes_res == PATH_INCORRECT ) {
+
+
+#ifdef CONFIG_REISERFS_CHECK_ONE_PROCESS
+      if ( n_fix_nodes_res == PATH_INCORRECT )
+	reiserfs_panic(p_s_sb, "PAP-5750: reiserfs_insert_item: illegal returned value");
+#endif
+
+      /* schedule occurred while fix_nodes() worked */
+      if ( search_by_key(p_s_sb, &(p_s_ih->ih_key), p_s_path, &n_repeat, DISK_LEAF_NODE_LEVEL, READ_BLOCKS) == ITEM_FOUND )
+	reiserfs_panic (p_s_sb, "PAP-5760: reiserfs_insert_item: inserted item exists (%k)", &(p_s_ih->ih_key));
+    }
+    /* make balancing after all resources will be collected at a time */ 
+    if ( n_fix_nodes_res == CARRY_ON ) {
+      journal_lock_dobalance(p_s_sb) ;
+      do_balance (th, &s_ins_balance, 0, p_s_ih, p_c_body, M_INSERT, n_mem_mode, n_zeros_num);
+      journal_unlock_dobalance(p_s_sb) ;
+      return p_s_ih->ih_item_len;
+    }
+
+    unfix_nodes(th, &s_ins_balance);
+    return NO_DISK_SPACE; /* No disk space or io error */
+}
+
+
+#endif // __KERNEL__
+
+
+
+//
+// hopefully we will never need this anymore
+//
+#if 0
+
+
+/*********************** range_read code ***************************************/
+
+/* It is interesting to consider why this code is so
+   complicated.... It seems like it ought to be simpler.*/
+
+
+/*  Get data buffer from cache which contains data (byte or directory record) of some object.
+    This data has minimal possible key >= than *p_range_begin and <= than *p_range_end.
+    In other words get first buffer contains data with key from range
+    [*p_key,*p_range_end].
+    If it is not possible (needed buffer is not in the cache) prepare (not uptodate) buffer
+    at path from root to the needed buffer. Don't read any blocks.
+    Returns:    1) via return value     0 if there is not an needed buffer, 1 otherwise;
+
+                2) via pp_s_buf:        NULL if the needed buffer is not in memory, 
+                                        or pointer to the needed buffer, or pointer to the prepared buffer;
+
+                3) via p_n_objectid:    Corresponding object id if pointer above points to an unformatted node, MAX_KEY_OBJECTID otherwise.
+
+                4) Recalculated head of the range in p_s_range_head */
+
+int get_buffer_by_range(
+      struct super_block      *	p_s_sb,			/* Super block.								*/
+      struct key              *	p_s_range_head,		/* Range begin.								*/
+      struct key              *	p_s_range_end,  	/* Range end.								*/
+      struct buffer_head     **	pp_s_buf,        	/* Returned value; result buffer.					*/
+      unsigned long	      *	p_n_objectid		/* Returned value; corresponding object id if *pp_s_buf points to an
+							   unformatted node, MAX_KEY_OBJECTID in other cases.			*/
+) {			
+
+  int                   n_res,
+    			n_pos_in_buffer,
+    			n_repeat,
+   			n_item_num,
+  			n_pos_in_item;
+  struct path		s_path;
+  struct buffer_head  * p_s_bh;             	/* current buffer                       */
+  struct key	        s_min_key,
+  		      *	p_s_rkey;
+  struct item_head    * p_s_ih,
+			s_ih;
+  unsigned long		n_unfm_pointer;
+
+#ifdef CONFIG_REISERFS_CHECK
+  int			n_repeat_counter = 0;
+#endif
+
+  init_path (&s_path);
+
+repeat:
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( ! (++n_repeat_counter % 10000) ) {
+    reiserfs_panic(p_s_sb, "PAP-5765: get_buffer_by_range: counter(%d) too big. range_head %k", n_repeat_counter, p_s_range_head);
+  }
+#endif
+
+
+  /* Search for the needed buffer in the range. */
+  n_res = search_by_key(p_s_sb, p_s_range_head, &s_path, &n_repeat, DISK_LEAF_NODE_LEVEL, DONT_READ_BLOCKS);
+
+  n_pos_in_buffer = PATH_LAST_POSITION(&s_path);
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( ! key_in_buffer (&s_path, p_s_range_head, p_s_sb) )
+    reiserfs_panic(p_s_sb, " PAP: 5770: get_buffer_by_range: key is not in the buffer");
+#endif
+
+  if ( ! buffer_uptodate(p_s_bh = PATH_PLAST_BUFFER(&s_path)) ) {
+    /* We can not get data buffer from range. Prepare buffer from the cache and recalculate right delimiting key. */
+    p_s_rkey = get_rkey(&s_path, p_s_sb);
+
+    if ( ! COMP_KEYS(p_s_rkey, &MIN_KEY) ) {
+
+#ifdef CONFIG_REISERFS_CHECK_ONE_PROCESS
+      reiserfs_panic(p_s_sb, "PAP-5780: get_buffer_by_range: can not get right delimiting key in case of one process");
+#endif
+
+#ifdef CONFIG_REISERFS_CHECK
+      if ( n_repeat == CARRY_ON )
+	reiserfs_panic(p_s_sb, "PAP-5790: get_buffer_by_range: get_rkey returns KEY_MIN");
+#endif
+
+      goto repeat;  /* We can not recalculate right delimiting key to continue search for the buffers in the range.
+		       Do that by old one. */
+    }
+    *pp_s_buf = p_s_bh;
+
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( ! key_in_buffer(&s_path, p_s_range_head, p_s_sb) )
+      reiserfs_panic(p_s_sb, "PAP-5791: get_buffer_by_range: key is not in the path");
+    if ( s_path.path_length < FIRST_PATH_ELEMENT_OFFSET )
+      reiserfs_panic(p_s_sb, "PAP-5792: get_buffer_by_range: path length is too small");
+#endif
+    
+    copy_key(p_s_range_head, p_s_rkey);
+    
+    s_path.path_length--;
+    decrement_counters_in_path(&s_path);
+    *p_n_objectid = MAX_KEY_OBJECTID;
+    return 1;
+  }
+
+  /* last buffer on the path is uptodate */
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( COMP_KEYS(B_N_PKEY(p_s_bh, 0), p_s_range_head) == 1 || COMP_KEYS(B_PRIGHT_DELIM_KEY(p_s_bh), p_s_range_head ) < 1 ) {
+    reiserfs_panic(p_s_sb, "PAP-5795: range head key %k we looked for is not in the buffer", p_s_range_head);
+  }
+  if ( ! B_IS_ITEMS_LEVEL(p_s_bh) || (n_res == ITEM_NOT_FOUND && ! n_pos_in_buffer) )
+    reiserfs_panic(p_s_sb, "PAP-5800: get_buffer_by_range: last buffer on the path is not leaf or returned position is 0");
+#endif
+
+  p_s_ih = B_N_PITEM_HEAD(p_s_bh, n_pos_in_buffer);
+  n_item_num = B_NR_ITEMS(p_s_bh);
+  /* Now we are defining the data which are placed in buffer p_s_bh and has minimal key
+     more than or equal to *p_range_begin. */
+  if ( n_res == ITEM_FOUND ) { /* Item was found in the tree. */
+    n_pos_in_item = 0;
+    copy_key(&s_min_key,&(p_s_ih->ih_key));
+  }
+
+  else {  /* Item was not found in the tree. */
+    /* Calculate min_key which is the minimal key of the byte or directory entry
+       more or equal than p_range_begin. */
+    n_pos_in_item = MAX_INT;
+    /*  Previous item is item of the same object we are looking for. */
+    if ( ! COMP_SHORT_KEYS(p_s_range_head, &((p_s_ih - 1)->ih_key)) ) {
+      if ( I_IS_DIRECTORY_ITEM(p_s_ih - 1) ) {
+	/* Search in the directory item for the entry that has minimal key more or equal than *p_s_range_head. */
+	bin_search_in_dir_item (p_s_ih - 1, B_I_DEH(p_s_bh, p_s_ih - 1), p_s_range_head, &n_pos_in_item);
+	if ( n_pos_in_item < I_ENTRY_COUNT(p_s_ih - 1) ) {
+	/* Previous item contains needed directory entry. */
+	  p_s_ih--;
+	  PATH_LAST_POSITION(&s_path)--;
+	  n_pos_in_buffer--;
+	  copy_key(&s_min_key, &p_s_ih->ih_key);
+	  s_min_key.k_offset = B_I_DEH(p_s_bh, p_s_ih)[n_pos_in_item].deh_offset;
+	  s_min_key.k_uniqueness = DIRENTRY_UNIQUENESS;
+	}
+	else
+	  /* key *p_s_range_head is greater than last entry in directory item */
+	  n_pos_in_item = MAX_INT;
+      }
+      else {
+	/* key *p_s_range_head is key of regular file */
+	if ( I_K_KEY_IN_ITEM(p_s_ih - 1, p_s_range_head, p_s_bh->b_size) ) {
+	  /* Previous item contains needed byte. */
+	  p_s_ih--;
+	  PATH_LAST_POSITION(&s_path)--;
+	  n_pos_in_buffer--;
+	  copy_key(&s_min_key, p_s_range_head);
+	  n_pos_in_item = p_s_range_head->k_offset - p_s_ih->ih_key.k_offset;
+	  if ( I_IS_INDIRECT_ITEM(p_s_ih) )
+	    n_pos_in_item /= p_s_bh->b_size;
+	}
+      }
+    }
+
+    if ( n_pos_in_item == MAX_INT ) {
+      /* key we looked for is not in the item */
+      if ( n_pos_in_buffer == n_item_num )
+	copy_key(&s_min_key, &MIN_KEY);
+      else {
+ 	n_pos_in_item = 0;
+	copy_key(&s_min_key, &(p_s_ih->ih_key));
+      }
+    }
+  }
+
+  if ( COMP_KEYS(&s_min_key, p_s_range_end) == 1 ) {
+ 
+/******************************************
+    if ( ! key_in_buffer(&s_path, p_s_range_head, p_s_sb) )
+      reiserfs_panic(p_s_sb, "PAP-2: get_buffer_by_range: path length is too small");
+*********************************************/
+
+    *pp_s_buf = NULL;
+    copy_key(p_s_range_head, &s_min_key);
+    decrement_counters_in_path(&s_path);
+    *p_n_objectid = MAX_KEY_OBJECTID;
+    return 0;    /* There is no buffer in the range in the tree. */
+  }
+
+  if ( ! COMP_KEYS(&s_min_key, &MIN_KEY) ) {
+    /* This leaf buffer is not in the range. */
+    p_s_rkey = get_rkey(&s_path, p_s_sb);
+    if ( ! COMP_KEYS(p_s_rkey, &MIN_KEY) ) {
+
+#ifdef CONFIG_REISERFS_CHECK_ONE_PROCESS
+      reiserfs_panic(p_s_sb, "PAP-5810: get_buffer_by_range: can not get right delimiting key in case of one process");
+#endif
+
+#ifdef CONFIG_REISERFS_CHECK
+      if ( n_repeat == CARRY_ON )
+	reiserfs_panic(p_s_sb, "PAP-5820: get_buffer_by_range: get_rkey returns MIN_KEY");
+#endif
+
+      goto repeat; /* We can not recalculate right delimiting key to continue search for the buffer in the range.
+		      Do that by old one. */
+    }
+
+    if ( ! COMP_KEYS(p_s_rkey, &MAX_KEY) ) {
+
+#ifdef CONFIG_REISERFS_CHECK
+      if ( ! key_in_buffer(&s_path, p_s_range_head, p_s_sb) )
+	reiserfs_panic(p_s_sb, "PAP-5830: get_buffer_by_range: key_in_buffer returned 0");
+#endif
+
+      *pp_s_buf = NULL;
+      copy_key(p_s_range_head, &MAX_KEY);
+      decrement_counters_in_path(&s_path);
+      *p_n_objectid = MAX_KEY_OBJECTID;
+      return 0;    /* There is no buffer in the range in the tree. */
+    }
+
+    *pp_s_buf = NULL;
+    copy_key(p_s_range_head, p_s_rkey); /* Reset range head and continue search for the buffer in the range. */
+    decrement_counters_in_path(&s_path);
+    *p_n_objectid = MAX_KEY_OBJECTID;
+    return 1;
+  }
+
+
+  if ( ! I_IS_INDIRECT_ITEM(p_s_ih) ) { /*  We have direct or directory item which contains byte or
+                                          directory record in the range in the buffer p_s_bh. */
+    /* Look for the next indirect item in the buffer */
+    for ( p_s_ih++, n_pos_in_buffer++; n_pos_in_buffer < n_item_num; n_pos_in_buffer++, p_s_ih++ )
+      if ( I_IS_INDIRECT_ITEM(p_s_ih) )
+        break;
+
+    if ( n_pos_in_buffer == n_item_num ) {
+      /* indirect item was not found. */
+      p_s_rkey = get_rkey(&s_path, p_s_sb);
+      if ( ! COMP_KEYS(p_s_rkey, &MIN_KEY) ) {
+
+#ifdef CONFIG_REISERFS_CHECK_ONE_PROCESS
+	reiserfs_panic(p_s_sb, "PAP-5830: get_buffer_by_range: can not get right delimiting key in case of one process");
+#endif
+
+#ifdef CONFIG_REISERFS_CHECK
+	if ( n_repeat == CARRY_ON )
+	  reiserfs_panic(p_s_sb, "PAP-5840: get_buffer_by_range: get_rkey returns MIN_KEY");
+#endif
+	
+	goto repeat; /* We can not recalculate right delimiting key to continue search for the buffers in the range.
+			Do that by old one. */
+      }
+      
+#ifdef CONFIG_REISERFS_CHECK
+      if ( ! key_in_buffer(&s_path, p_s_range_head, p_s_sb) )
+	reiserfs_panic(p_s_sb, "PAP-5850: get_buffer_by_range: key_in_buffer returned 0");
+
+      if ( COMP_KEYS(B_N_PKEY(p_s_bh, 0), p_s_range_head) == 1 || COMP_KEYS(B_PRIGHT_DELIM_KEY(p_s_bh), p_s_range_head ) < 1 ) {
+	reiserfs_panic(p_s_sb, "PAP-5860: get_buffer_by_range: range_head key %k is not in the last buffer on the path",
+		       p_s_range_head);
+      }
+#endif
+
+      copy_key(p_s_range_head, p_s_rkey); /* Reset range_head. */
+    }
+    else /* indirect item was found. */ {
+
+#ifdef CONFIG_REISERFS_CHECK
+      if ( COMP_KEYS(B_N_PKEY(p_s_bh, 0), p_s_range_head) == 1 || COMP_KEYS(B_PRIGHT_DELIM_KEY(p_s_bh), p_s_range_head ) < 1 ) {
+	reiserfs_panic(p_s_sb, "PAP-5870: get_buffer_by_range: range_head key %k is not in the last buffer on the path",
+		       p_s_range_head);
+      }
+#endif
+
+/******************************************
+      if ( ! key_in_buffer(&s_path, p_s_range_head, p_s_sb) )
+	reiserfs_panic(p_s_sb, "PAP-4: get_buffer_by_range: path length is too small");
+*********************************************/
+
+      copy_key(p_s_range_head, &(p_s_ih->ih_key)); /* Reset range head. */
+    }
+
+    *pp_s_buf = p_s_bh;
+
+#ifdef CONFIG_REISERFS_CHECK
+    if ( s_path.path_length < FIRST_PATH_ELEMENT_OFFSET )
+      reiserfs_panic(0, "PAP-5880: get_buffer_by_range: path length is too small (%d)", s_path.path_length);
+#endif
+
+    s_path.path_length--;
+    decrement_counters_in_path(&s_path);
+    *p_n_objectid = MAX_KEY_OBJECTID;
+    return 1;
+  }
+
+  /* Needed byte is located in an unformatted node. Check whether it is in cache.
+      And if not prepare buffer to read it. */
+  n_unfm_pointer = B_I_POS_UNFM_POINTER(p_s_bh, p_s_ih, n_pos_in_item);
+  if ( ! n_unfm_pointer ) { /* This is a hole (nothing to read). */
+    if ( n_pos_in_item + 1 == I_UNFM_NUM(p_s_ih) )
+      if ( n_pos_in_buffer + 1 == B_NR_ITEMS(p_s_bh) ) {
+	p_s_rkey = get_rkey(&s_path, p_s_sb);
+
+	if ( ! COMP_KEYS(p_s_rkey, &MIN_KEY) )
+	  goto repeat;
+
+	copy_key(p_s_range_head, p_s_rkey);
+      }
+      else
+	copy_key(p_s_range_head, &((p_s_ih + 1)->ih_key));
+    else {
+      copy_key(p_s_range_head, &(p_s_ih->ih_key));
+      p_s_range_head->k_offset += (n_pos_in_item + 1)*(p_s_sb->s_blocksize);
+    }
+
+    *pp_s_buf = NULL;
+    decrement_counters_in_path(&s_path);
+    *p_n_objectid = p_s_ih->ih_key.k_objectid;
+    return 1;
+  } /* unformatted node pointer contains 0 */
+
+
+  /* Copy found item header. */
+  copy_item_head(&s_ih, p_s_ih);
+  /* Get unformatted node buffer. */
+  n_repeat = CARRY_ON;
+  p_s_bh = reiserfs_getblk(p_s_sb->s_dev, n_unfm_pointer, p_s_sb->s_blocksize, &n_repeat);
+
+  if ( n_repeat != CARRY_ON  && comp_items(&s_ih, &s_path) ) {
+
+#ifdef CONFIG_REISERFS_CHECK_ONE_PROCESS
+    reiserfs_panic(p_s_sb, "PAP-5890: get_buffer_by_range: item in the path is changed in case of one process");
+#endif
+
+    brelse(p_s_bh);
+    goto repeat;
+  }
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( comp_items(&s_ih, &s_path) )
+    reiserfs_panic(p_s_sb, "PAP-5900: get_buffer_by_range: items must be equal");
+#endif
+
+  if ( n_pos_in_item + 1 == I_UNFM_NUM(p_s_ih) )
+    if ( n_pos_in_buffer + 1 == B_NR_ITEMS(PATH_PLAST_BUFFER(&s_path)) ) {
+      p_s_rkey = get_rkey(&s_path, p_s_sb);
+
+      if ( ! COMP_KEYS(p_s_rkey, &MIN_KEY) ) {
+	brelse(p_s_bh);
+	goto repeat;
+      }
+
+      copy_key(p_s_range_head, p_s_rkey);
+    }
+    else
+     copy_key(p_s_range_head, &((p_s_ih + 1)->ih_key));
+  else {
+    copy_key(p_s_range_head, &(p_s_ih->ih_key));
+    p_s_range_head->k_offset += (n_pos_in_item + 1)*(p_s_sb->s_blocksize);
+  }
+
+  *pp_s_buf = p_s_bh;
+  decrement_counters_in_path(&s_path);
+  *p_n_objectid = s_ih.ih_key.k_objectid;
+  return 1; 
+}
+
+
+int get_buffers_from_range(					/*  Returns length of the array of calculated
+								    buffers which is less or equal than
+								    max_nr_buffers_to_return.         			*/
+      struct  super_block     *	p_s_sb,				/*  Pointer to the super block.				*/
+      struct  key	      *	p_s_range_start,		/*  Minimal range key.                            	*/
+      struct  key	      * p_s_range_end,			/*  Maximal range key.                    		*/
+      struct  buffer_head    **	p_s_range_buffers,		/*  Returned array of pointers to buffer headers.
+								    Must be allocated in calling function.		*/
+      int     			n_max_nr_buffers_to_return	/*  Length of the allocated array.                	*/
+    ) {
+  struct key          * p_s_cur_key = p_s_range_start;
+  struct buffer_head  * p_s_res_buffer;
+  int			n_array_length = 0;
+  unsigned long		n_objectid;
+
+#ifdef CONFIG_REISERFS_CHECK
+  if ( COMP_KEYS(p_s_range_start, p_s_range_end) == 1 ||
+       ! COMP_KEYS(p_s_range_start, &MIN_KEY) || ! COMP_KEYS(p_s_range_start, &MAX_KEY) ||
+       ! COMP_KEYS(p_s_range_end, &MIN_KEY) || ! COMP_KEYS(p_s_range_end, &MAX_KEY) )
+    reiserfs_panic(p_s_sb, "PAP-5890: get_buffers_from_range: illegal range");
+#endif
+
+  /* While p_s_cur_key is in the range. */
+  while ( COMP_KEYS(p_s_cur_key, p_s_range_end) != 1 )  {
+    /* Calculate next buffer from range. */
+    if ( ! get_buffer_by_range(p_s_sb, p_s_cur_key, p_s_range_end, &p_s_res_buffer, &n_objectid) )
+      break; /* There are not more buffers in the range. */
+    p_s_range_buffers[n_array_length++] = p_s_res_buffer;
+    if ( n_array_length == n_max_nr_buffers_to_return )
+      break;
+  }
+  return n_array_length;
+}
+
+#endif
+
+
+
+/* ok, this is not good at all.  Sometimes, we need to search for just an 
+** object id, without knowing the packing locality.  For the moment, this
+** searches through every possible packing locality until it founds the
+** object id in your key.  It starts by making a copy of the key, and setting
+** the k_dir_id to 0.  Every other arg works like search_by_key.
+**
+** After this returns, the last item in the p_s_search_path will have the
+** correct k_dir_id and k_objectid.
+**
+** This function is SLOW.  I mean really really SLOW.  Don't ever call it
+** unless you have no way at all to get the packing locality.
+**
+*/
+int search_by_objectid(
+                  struct super_block  * p_s_sb,         /* Super block.                           */
+                  struct key          * p_s_key,        /* Key to search. packing locality should be set to 0s */
+                  struct path         * p_s_search_path,/* This structure was allocated and initialized by
+                                                           the calling function. It is filled up by this
+                                                           function.  */
+                  int                 * p_n_repeat,     /* Whether schedule occured. */
+                  int                   n_stop_level,   /* How far down the tree to search.*/
+                  int                   n_bread_par     /* Whether to search even if it requires disk I/O, this is
+                                                           either READ_BLOCKS or DONT_READ_BLOCKS or 0. Hans doesn't
+                                                           know what 0 means, it seems to evaluate to DONT_READ_BLOCKS,
+                                                           but it is bad style to not use the macro.... there is a
+                                                           #define of search by key with no explanation that can allow
+                                                           it to happen.... */
+                  ) {
+  struct key cur_key ;
+  struct item_head *ih ;
+  int loop_count = 0 ;
+  int retval ;
+  unsigned long *objectid_map  ;
+  struct reiserfs_super_block *disk_sb ;
+  unsigned long max_objectid ;
+
+  /*  find the max possible objectid to search for */
+  disk_sb = SB_DISK_SUPER_BLOCK(p_s_sb) ;
+  objectid_map = (unsigned long *)(disk_sb + 1) ;
+  max_objectid = objectid_map[disk_sb->s_oid_cursize - 1] ;
+
+  copy_key(&cur_key, p_s_key) ;
+  cur_key.k_dir_id = 0 ;
+  while(1) {
+    retval = search_by_key(p_s_sb, &cur_key, p_s_search_path, p_n_repeat, n_stop_level, n_bread_par) ;    
+    ih = PATH_PITEM_HEAD(p_s_search_path) ;
+    if (retval == ITEM_FOUND) {
+      return retval ;
+    } else if (retval == ITEM_NOT_FOUND) {
+      cur_key.k_dir_id++ ;
+      if (cur_key.k_dir_id > max_objectid) {
+	reiserfs_warning("clm-1001: search_by_objectid: current key dir id %d is > than max object id %lu, giving up\n", cur_key.k_dir_id, max_objectid) ;
+        return retval ;
+      }
+    } else {
+      return retval ;
+    }
+    if ((++loop_count % 10000000) == 0) {
+      reiserfs_warning("clm-1000: search_by_objectid, item not found after %d iterations looking for %k, last attempt %k\n", loop_count, p_s_key, &(ih->ih_key)) ;
+    }
+  }
+  return retval ;
+}
diff -urN linux/fs/reiserfs/super.c /tmp/linux/fs/reiserfs/super.c
--- linux/fs/reiserfs/super.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/super.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,845 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+#ifdef __KERNEL__
+
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <asm/uaccess.h>
+#include <linux/reiserfs_fs.h>
+#include <linux/locks.h>
+#include <linux/init.h>
+
+#else
+
+#include "nokernel.h"
+#include <stdlib.h> // for simple_strtoul
+
+#endif
+
+#define REISERFS_OLD_BLOCKSIZE 4096
+#define REISERFS_SUPER_MAGIC_STRING_OFFSET_NJ 20
+
+
+/* like fs.h:/mark_buffer_dirty but refile_buffer */
+inline void reiserfs_mark_buffer_dirty (struct buffer_head * bh, int flag)
+{
+  if (!test_and_set_bit(BH_Dirty, &bh->b_state))
+    set_writetime(bh, flag);
+}
+
+
+/* like fs.h:/mark_buffer_clean but refile_buffer */
+inline void reiserfs_mark_buffer_clean (struct buffer_head * bh)
+{
+  test_and_clear_bit(BH_Dirty, &bh->b_state);
+}
+
+
+void reiserfs_write_super (struct super_block * s)
+{
+
+  int dirty = 0 ;
+  if (!(s->s_flags & MS_RDONLY)) {
+#if 0 /* journal victim */
+    rs = SB_DISK_SUPER_BLOCK (s);
+    /*
+     * if reiserfs was mounted with read-write permissions make file
+     * system state not valid so that if we crash without doing a
+     * clean umount we know that we must run file system
+     * checker. umount will mark it valid if it does a clean umount
+     */
+    if (le16_to_cpu (rs->s_state) == REISERFS_VALID_FS) {
+      rs->s_state = cpu_to_le16 (REISERFS_ERROR_FS);
+      /* mark_buffer_dirty (SB_BUFFER_WITH_SB (s), 1); */
+      journal_begin(&th, s, 1) ;
+      journal_mark_dirty(&th, s, SB_BUFFER_WITH_SB (s)) ;
+      journal_end(&th, s, 1) ;
+    }
+#endif
+    dirty = flush_old_commits(s, 1) ;
+  }
+  s->s_dirt = dirty;
+}
+
+
+void reiserfs_put_super (struct super_block * s)
+{
+  int i;
+  kdev_t dev = s->s_dev;
+  struct reiserfs_transaction_handle th ;
+
+  unlock_super(s) ; 
+
+  /* change file system state to current state if it was mounted with read-write permissions */
+  if (!(s->s_flags & MS_RDONLY)) {
+    journal_begin(&th, s, 10) ;
+    SB_REISERFS_STATE (s) = le16_to_cpu (s->u.reiserfs_sb.s_mount_state);
+    /* mark_buffer_dirty (SB_BUFFER_WITH_SB (s), 1); */
+    journal_mark_dirty(&th, s, SB_BUFFER_WITH_SB (s));
+  }
+
+  /* note, journal_release can detect a readonly mount, and decide not to
+  ** call journal_end
+  */
+  journal_release(&th, s) ;
+  schedule() ; /* empty task queue one last time */
+  lock_super(s) ;
+
+  /* wait on write completion */
+  for (i = 0; i < SB_BMAP_NR (s); i ++) {
+    /* wait_on_buffer (SB_AP_CAUTIOUS_BITMAP (s)[i]); */
+    /* brelse (SB_AP_CAUTIOUS_BITMAP (s)[i]); */
+    brelse (SB_AP_BITMAP (s)[i]);
+  }
+
+  reiserfs_kfree (SB_AP_BITMAP (s), sizeof (struct buffer_head *) * SB_BMAP_NR (s), s);
+  /* reiserfs_kfree (SB_AP_CAUTIOUS_BITMAP (s), sizeof (struct buffer_head *) * SB_BMAP_NR (s), s); */
+
+
+  brelse (SB_BUFFER_WITH_SB (s));
+
+  print_statistics (s);
+
+  if (s->u.reiserfs_sb.s_kmallocs != 0) {
+    reiserfs_warning ("vs-2004: reiserfs_put_super: aloocated memory left %d\n", s->u.reiserfs_sb.s_kmallocs);
+  }
+
+  s->s_dev = 0;
+
+  fixup_reiserfs_buffers (dev);
+
+  MOD_DEC_USE_COUNT;
+  return;
+}
+
+
+/* super block operations are */
+static struct super_operations reiserfs_sops = 
+{
+  reiserfs_read_inode,
+  reiserfs_write_inode,
+  NULL,				/* put_inode*/
+  reiserfs_delete_inode,
+  reiserfs_notify_change,
+  reiserfs_put_super,
+  reiserfs_write_super,
+  reiserfs_statfs,
+  reiserfs_remount,
+  NULL, 				/* clear_inode */
+  NULL				/* umount_begin */
+};
+
+/* this was (ext2)parse_options */
+static int parse_options (char * options, unsigned long * mount_options, unsigned long * blocks)
+{
+    char * this_char;
+    char * value;
+  
+    *blocks = 0;
+    set_bit (GENERICREAD, mount_options);
+    if (!options)
+	/* use default configuration: complex read, create tails, preserve on */
+	return 1;
+    for (this_char = strtok (options, ","); this_char != NULL; this_char = strtok (NULL, ",")) {
+	if ((value = strchr (this_char, '=')) != NULL)
+	    *value++ = 0;
+	if (!strcmp (this_char, "notail")) {
+	    set_bit (NOTAIL, mount_options);
+	} else if (!strcmp (this_char, "replayonly")) {
+	    set_bit (REPLAYONLY, mount_options);
+	} else if (!strcmp (this_char, "nolog")) {
+	    set_bit (NOLOG, mount_options);
+	} else if (!strcmp (this_char, "hash")) {
+	    if (value && *value) {
+		/* if they specify any hash option, we force detection
+		** to make sure they aren't using the wrong hash
+		*/
+	        if (!strcmp(value, "rupasov")) {
+		    set_bit (FORCE_RUPASOV_HASH, mount_options);
+		    set_bit (FORCE_HASH_DETECT, mount_options);
+		} else if (!strcmp(value, "tea")) {
+		    set_bit (FORCE_TEA_HASH, mount_options);
+		    set_bit (FORCE_HASH_DETECT, mount_options);
+		} else if (!strcmp(value, "detect")) {
+		    set_bit (FORCE_HASH_DETECT, mount_options);
+		} else {
+		    printk("reiserfs: invalid hash function specified\n") ;
+		    return 0 ;
+		}
+	    } else {
+	  	printk("reiserfs: hash option requires a value\n");
+		return 0 ;
+	    }
+	} else if (!strcmp (this_char, "resize")) {
+	    if (!value || !*value){
+	  	printk("reiserfs: resize option requires a value\n");
+		return 0 ;
+	    }
+	    *blocks = simple_strtoul (value, &value, 0);
+	} else {
+	    printk ("reiserfs: Unrecognized mount option %s\n", this_char);
+	    return 0;
+	}
+    }
+    return 1;
+}
+
+
+
+int reiserfs_is_super(struct super_block *s) {
+   return (s->s_dev != 0 && s->s_op == &reiserfs_sops) ;
+}
+int reiserfs_remount (struct super_block * s, int * flags, char * data)
+{
+  struct reiserfs_super_block * rs;
+  struct reiserfs_transaction_handle th ;
+  unsigned long blocks;
+  unsigned long mount_options;
+
+  rs = SB_DISK_SUPER_BLOCK (s);
+
+  if (!parse_options(data, &mount_options, &blocks))
+  	return 0;
+
+  if(blocks && !(s->s_flags & MS_RDONLY)) 
+  	reiserfs_resize(s, blocks);
+	
+  if ((unsigned long)(*flags & MS_RDONLY) == (s->s_flags & MS_RDONLY)) {
+    /* there is nothing to do to remount read-only fs as read-only fs */
+    return 0;
+  }
+  if (*flags & MS_RDONLY) {
+    /* try to remount file system with read-only permissions */
+    if (le16_to_cpu (rs->s_state) == REISERFS_VALID_FS ||
+	s->u.reiserfs_sb.s_mount_state != REISERFS_VALID_FS) {
+      return 0;
+    }
+    /* Mounting a rw partition read-only. */
+    journal_begin(&th, s, 10) ;
+    rs->s_state = cpu_to_le16 (s->u.reiserfs_sb.s_mount_state);
+    /* mark_buffer_dirty (SB_BUFFER_WITH_SB (s), 1); journal victim */
+    journal_mark_dirty(&th, s, SB_BUFFER_WITH_SB (s));
+    s->s_dirt = 0;
+  } else {
+    /* Mount a partition which is read-only, read-write */
+    s->u.reiserfs_sb.s_mount_state = le16_to_cpu (rs->s_state);
+    s->s_flags &= ~MS_RDONLY;
+
+    /* now the filesystem is no longer RDONLY, it is safe to call
+    ** journal_begin
+    */
+    journal_begin(&th, s, 10) ;
+    rs->s_state = cpu_to_le16 (REISERFS_ERROR_FS);
+    /* mark_buffer_dirty (SB_BUFFER_WITH_SB (s), 1); */
+    journal_mark_dirty(&th, s, SB_BUFFER_WITH_SB (s));
+    s->s_dirt = 0;
+    s->u.reiserfs_sb.s_mount_state = REISERFS_VALID_FS ;
+    if (test_bit(NOTAIL, &mount_options)) {
+      set_bit(NOTAIL, &(s->u.reiserfs_sb.s_mount_opt)) ;
+    }
+  }
+  /* this will force a full flush of all journal lists */
+  SB_JOURNAL(s)->j_must_wait = 1 ;
+  journal_end(&th, s, 10) ;
+  return 0;
+}
+
+
+struct key root_key = {REISERFS_ROOT_PARENT_OBJECTID, REISERFS_ROOT_OBJECTID, 0, 0};
+
+static int read_bitmaps (struct super_block * s)
+{
+  int i, repeat, bmp, dl ;
+  struct reiserfs_super_block * rs = SB_DISK_SUPER_BLOCK(s);
+
+  repeat = 0 ;
+  /* read true bitmap block */
+  SB_AP_BITMAP (s) = reiserfs_kmalloc (sizeof (struct buffer_head *) * le16_to_cpu (rs->s_bmap_nr), GFP_KERNEL, s);
+  if (SB_AP_BITMAP (s) == 0)
+    return 1;
+
+  memset (SB_AP_BITMAP (s), 0, sizeof (struct buffer_head *) * le16_to_cpu (rs->s_bmap_nr));
+
+  /* read bitmap blocks */
+				/* reiserfs leaves the first 64k unused
+                                   so that any partition labeling scheme
+                                   currently used will have enough
+                                   space. Then we need one block for the
+                                   super.  -Hans */
+  bmp = (REISERFS_DISK_OFFSET_IN_BYTES / s->s_blocksize) + 1;	/* first of bitmap blocks */
+  SB_AP_BITMAP (s)[0] = reiserfs_bread (s->s_dev, bmp, s->s_blocksize, &repeat);
+  if(!SB_AP_BITMAP(s)[0])
+	  return 1;
+  for (i = 1, bmp = dl = rs->s_blocksize * 8; i < le16_to_cpu (rs->s_bmap_nr); i ++) {
+    SB_AP_BITMAP (s)[i] = reiserfs_bread (s->s_dev, bmp, s->s_blocksize, &repeat);
+    if (!SB_AP_BITMAP (s)[i])
+      return 1;
+	bmp += dl;
+  }
+
+  return 0;
+}
+
+static int read_old_bitmaps (struct super_block * s)
+{
+  int i, repeat ;
+  struct reiserfs_super_block * rs = SB_DISK_SUPER_BLOCK(s);
+  int bmp1 = (REISERFS_OLD_DISK_OFFSET_IN_BYTES / s->s_blocksize) + 1;  /* first of bitmap blocks */
+
+  repeat = 0 ;
+  /* read true bitmap */
+  SB_AP_BITMAP (s) = reiserfs_kmalloc (sizeof (struct buffer_head *) * le16_to_cpu (rs->s_bmap_nr), GFP_KERNEL, s);
+  if (SB_AP_BITMAP (s) == 0)
+    return 1;
+
+  memset (SB_AP_BITMAP (s), 0, sizeof (struct buffer_head *) * le16_to_cpu (rs->s_bmap_nr));
+
+  for (i = 0; i < le16_to_cpu (rs->s_bmap_nr); i ++) {
+    SB_AP_BITMAP (s)[i] = reiserfs_bread (s->s_dev, bmp1 + i, s->s_blocksize, &repeat);
+    if (!SB_AP_BITMAP (s)[i])
+      return 1;
+  }
+	
+  return 0;
+}
+
+
+void check_bitmap (struct super_block * s)
+{
+  int i = 0;
+  int free = 0;
+  char * buf;
+
+  while (i < SB_BLOCK_COUNT (s)) {
+    buf = SB_AP_BITMAP (s)[i / (s->s_blocksize * 8)]->b_data;
+    if (!test_bit (i % (s->s_blocksize * 8), buf))
+      free ++;
+    i ++;
+  }
+
+  if (free != SB_FREE_BLOCKS (s))
+    reiserfs_warning ("vs-4000: check_bitmap: %d free blocks, must be %d\n",
+		      free, SB_FREE_BLOCKS (s));
+}
+
+
+/* support old disk layout */
+static int read_old_super_block (struct super_block * s, int size)
+{
+  struct buffer_head * bh;
+  struct reiserfs_super_block * rs;
+  int repeat ;
+
+  printk("reiserfs_read_super: try to find super block in old location\n");
+  repeat = 0 ;
+  /* there are only 4k-sized blocks in v3.5.10 */
+  if (size != REISERFS_OLD_BLOCKSIZE)
+	  set_blocksize(s->s_dev, REISERFS_OLD_BLOCKSIZE);
+  bh = bread (s->s_dev, 
+  			  REISERFS_OLD_DISK_OFFSET_IN_BYTES / REISERFS_OLD_BLOCKSIZE, 
+ 		      REISERFS_OLD_BLOCKSIZE);
+  if (!bh) {
+    printk("reiserfs_read_super: unable to read superblock on dev %s\n", kdevname(s->s_dev));
+    return 1;
+  }
+
+  rs = (struct reiserfs_super_block *)bh->b_data;
+  if (strncmp (rs->s_magic,  REISERFS_SUPER_MAGIC_STRING, strlen ( REISERFS_SUPER_MAGIC_STRING))) {
+	  /* pre-journaling version check */
+	  if(!strncmp((char*)rs + REISERFS_SUPER_MAGIC_STRING_OFFSET_NJ,
+				  REISERFS_SUPER_MAGIC_STRING, strlen(REISERFS_SUPER_MAGIC_STRING))) {
+		  printk("reiserfs_read_super: a pre-journaling reiserfs filesystem isn't suitable there.\n");
+		  brelse(bh);
+		  return 1;
+	  }
+	  
+    brelse (bh);
+    printk ("reiserfs_read_super: can't find a reiserfs filesystem on dev %s.\n", kdevname(s->s_dev));
+    return 1;
+  }
+
+  if(REISERFS_OLD_BLOCKSIZE != le16_to_cpu (rs->s_blocksize)) {
+  	printk("reiserfs_read_super: blocksize mismatch, super block corrupted\n");
+	brelse(bh);
+	return 1;
+  }	
+
+  s->s_blocksize = REISERFS_OLD_BLOCKSIZE;
+  s->s_blocksize_bits = 0;
+  while ((1 << s->s_blocksize_bits) != s->s_blocksize)
+    s->s_blocksize_bits ++;
+
+  SB_BUFFER_WITH_SB (s) = bh;
+  SB_DISK_SUPER_BLOCK (s) = rs;
+  s->s_op = &reiserfs_sops;
+  return 0;
+}
+
+
+static int read_super_block (struct super_block * s, int size)
+{
+  struct buffer_head * bh;
+  struct reiserfs_super_block * rs;
+  int repeat ;
+
+  repeat = 0 ;
+  bh = bread (s->s_dev, (REISERFS_DISK_OFFSET_IN_BYTES / size), size);
+  if (!bh) {
+    printk("reiserfs_read_super: unable to read superblock on dev %s\n", kdevname(s->s_dev));
+    return 1;
+  }
+
+  rs = (struct reiserfs_super_block *)bh->b_data;
+  if (strncmp (rs->s_magic,  REISERFS_SUPER_MAGIC_STRING, strlen ( REISERFS_SUPER_MAGIC_STRING))) {
+    brelse (bh);
+    printk ("reiserfs_read_super: can't find a reiserfs filesystem on dev %s.\n", kdevname(s->s_dev));
+    return 1;
+  }
+
+  s->s_blocksize = le16_to_cpu (rs->s_blocksize);
+  s->s_blocksize_bits = 0;
+  while ((1 << s->s_blocksize_bits) != s->s_blocksize)
+    s->s_blocksize_bits ++;
+
+  if (size != rs->s_blocksize) {
+    brelse (bh);
+    set_blocksize (s->s_dev, s->s_blocksize);
+    bh = reiserfs_bread (s->s_dev,  (REISERFS_DISK_OFFSET_IN_BYTES / s->s_blocksize), s->s_blocksize, &repeat);
+    if (!bh) {
+      printk("reiserfs_read_super: unable to read superblock on dev %s\n", kdevname(s->s_dev));
+      return 1;
+    }
+
+    rs = (struct reiserfs_super_block *)bh->b_data;
+    if (strncmp (rs->s_magic,  REISERFS_SUPER_MAGIC_STRING, strlen ( REISERFS_SUPER_MAGIC_STRING)) ||
+	le16_to_cpu (rs->s_blocksize) != s->s_blocksize) {
+      brelse (bh);
+      printk ("reiserfs_read_super: can't find a reiserfs filesystem on dev %s.\n", kdevname(s->s_dev));
+      return 1;
+    }
+  }
+  /* must check to be sure we haven't pulled an old format super out of the
+  ** old format's log.  This is a kludge of a check, but it will work.  
+  ** If block we've just read in is inside the journal for that
+  ** super, it can't be valid.
+  */
+  if (bh->b_blocknr >= rs->s_journal_block && 
+      bh->b_blocknr < (rs->s_journal_block + JOURNAL_BLOCK_COUNT)) {
+      brelse(bh) ;
+      printk("super-459: reiserfs_read_super: super found at block %lu is within its own log.  It must not be of this format type.\n", bh->b_blocknr) ;
+      return 1 ;
+  }
+
+  SB_BUFFER_WITH_SB (s) = bh;
+  SB_DISK_SUPER_BLOCK (s) = rs;
+  s->s_op = &reiserfs_sops;
+  return 0;
+}
+
+/* after journal replay, reread all bitmap and super blocks */
+static int reread_meta_blocks(struct super_block *s) {
+  int i ;
+  ll_rw_block(READ, 1, &(SB_BUFFER_WITH_SB(s))) ;
+  wait_on_buffer(SB_BUFFER_WITH_SB(s)) ;
+  if (!buffer_uptodate(SB_BUFFER_WITH_SB(s))) {
+    printk("reread_meta_blocks, error reading the super\n") ;
+    return 1 ;
+  }
+
+  for (i = 0; i < SB_BMAP_NR(s) ; i++) {
+    ll_rw_block(READ, 1, &(SB_AP_BITMAP(s)[i])) ;
+    wait_on_buffer(SB_AP_BITMAP(s)[i]) ;
+    if (!buffer_uptodate(SB_AP_BITMAP(s)[i])) {
+      printk("reread_meta_blocks, error reading bitmap block number %d at %ld\n", i, SB_AP_BITMAP(s)[i]->b_blocknr) ;
+      return 1 ;
+    }
+  }
+  return 0 ;
+
+}
+
+
+// if root directory is empty - we set default - Yura's - hash and
+// warn about it
+// FIXME: we look for only one name in a directory. If tea and yura
+// bith have the same value - we ask user to send report to the
+// mailing list
+__u32 find_hash_out (struct super_block * s)
+{
+    int retval, repeat;
+    struct inode * inode;
+    struct key key;
+    struct path path;
+    struct reiserfs_dir_entry de;
+    struct reiserfs_de_head * deh;
+    __u32 hash = DEFAULT_HASH;
+
+    init_path (&path);
+
+    inode = s->s_root->d_inode;
+
+    while (1) {
+	copy_key (&key, INODE_PKEY (inode));
+	key.k_offset = MAX_KEY_OFFSET;
+	key.k_uniqueness = DIRENTRY_UNIQUENESS;
+	retval = search_by_entry_key (s, &key, &path, &de.de_entry_num, &repeat);
+	if (retval == IO_ERROR)
+	    // FIXME: sigh, still not ready
+	    reiserfs_panic (s, "reiserfs: find_hash: IO error are not handled properly yet");
+	if (retval == POSITION_NOT_FOUND)
+	    de.de_entry_num --;
+	
+	de.de_bh = PATH_PLAST_BUFFER (&path);
+	de.de_ih = PATH_PITEM_HEAD (&path);
+	de.de_deh = B_I_DEH (de.de_bh, de.de_ih);
+	de.de_item_num = PATH_LAST_POSITION (&path);
+	deh = de.de_deh + de.de_entry_num;
+	de.de_entrylen = I_DEH_N_ENTRY_LENGTH (de.de_ih, deh, de.de_entry_num);
+	de.de_namelen = de.de_entrylen - (de_with_sd (deh) ? SD_SIZE : 0);
+	de.de_name = B_I_PITEM (de.de_bh, de.de_ih) + le16_to_cpu (deh->deh_location);
+	if (de.de_name[de.de_namelen - 1] == 0)
+	  de.de_namelen = strlen (de.de_name);
+	//set_de_name_and_namelen (&de);
+	if (le32_to_cpu (de.de_deh[de.de_entry_num].deh_offset) == DOT_DOT_OFFSET) {
+	    /* allow override in this case */
+	    if (reiserfs_rupasov_hash(s)) {
+	        hash = YURA_HASH ;
+	    }
+	    reiserfs_warning("reiserfs: FS seems to be empty, "
+	                     "autodetect is using default hash.\n") ;
+	    break ;
+	}
+	if (GET_HASH_VALUE(yura_hash (de.de_name, de.de_namelen)) == 
+	    GET_HASH_VALUE(keyed_hash (de.de_name, de.de_namelen))) {
+	    reiserfs_warning("reiserfs: could not detect hash function, "
+	                     "please mount with -o hash={tea,rupasov,r5}\n");
+	    hash = UNSET_HASH ;
+	    break;
+	}
+	if (GET_HASH_VALUE(le32_to_cpu(de.de_deh[de.de_entry_num].deh_offset))==
+	    GET_HASH_VALUE(yura_hash (de.de_name, de.de_namelen))) {
+	    hash = YURA_HASH;
+	} else {
+	    hash = TEA_HASH;
+	}
+	break;
+    }
+
+    pathrelse (&path);
+    return hash;
+}
+
+
+// finds out which hash names are sorted with
+static int what_hash (struct super_block * s)
+{
+    __u32 code;
+
+    code = le32_to_cpu (s->u.reiserfs_sb.s_rs->s_hash_function_code);
+
+    /* reiserfs_hash_detect() == true if any of the hash mount options
+    ** were used.  We must check them to make sure the user isn't
+    ** using a bad hash value
+    */
+    if (code == UNSET_HASH || reiserfs_hash_detect(s))
+	code = find_hash_out (s);
+
+    if (code != UNSET_HASH && reiserfs_hash_detect(s)) {
+	/* detection has found the hash, and we must check against the 
+	** mount options 
+	*/
+	if (reiserfs_rupasov_hash(s) && code != YURA_HASH) {
+	    printk("REISERFS: Error, tea hash detected, "
+		   "unable to force rupasov hash\n") ;
+	    code = UNSET_HASH ;
+	} else if (reiserfs_tea_hash(s) && code != TEA_HASH) {
+	    printk("REISERFS: Error, rupasov hash detected, "
+		   "unable to force tea hash\n") ;
+	    code = UNSET_HASH ;
+	} 
+    } else { 
+        /* find_hash_out was not called or could not determine the hash */
+	if (reiserfs_rupasov_hash(s)) {
+	    code = YURA_HASH ;
+	} else if (reiserfs_tea_hash(s)) {
+	    code = TEA_HASH ;
+	} else if (reiserfs_r5_hash(s)) {
+	    code = R5_HASH ;
+	} 
+    }
+
+    /* if we are mounted RW, and we have a new valid hash code, update 
+    ** the super
+    */
+    if (code != UNSET_HASH && 
+	!(s->s_flags & MS_RDONLY) && 
+        code != le32_to_cpu (s->u.reiserfs_sb.s_rs->s_hash_function_code)) {
+        s->u.reiserfs_sb.s_rs->s_hash_function_code = cpu_to_le32(code) ;
+    }
+    return code;
+}
+
+// return pointer to appropriate function
+static hashf_t hash_function (struct super_block * s)
+{
+    switch (what_hash (s)) {
+    case TEA_HASH:
+	reiserfs_warning ("Using tea hash to sort names\n");
+	return keyed_hash;
+    case YURA_HASH:
+	reiserfs_warning ("Using rupasov hash to sort names\n");
+	return yura_hash;
+    case R5_HASH:
+	reiserfs_warning ("Using r5 hash to sort names\n");
+	return r5_hash;
+    }
+    return NULL;
+}
+
+// this is used to set up correct value for old partitions
+int function2code (hashf_t func)
+{
+    if (func == keyed_hash)
+	return TEA_HASH;
+    if (func == yura_hash)
+	return YURA_HASH;
+    if (func == r5_hash)
+	return R5_HASH;
+
+    reiserfs_panic (0, "reiserfs: function2code: unknow hash function detected");
+    return 0;
+}
+
+
+static void print_credits(void) {
+  static int printed = 0 ;
+  if (printed) 
+    return ;
+  printk("Primary Sponsor thresholdnetworks.com\n");
+  printk("Raid Tuning sponsored by emusic.com\n");
+  printk("HSM sponsored by bigstorage.com\n");
+  printk("Alpha port and SMP sponsored by alpha-processor.com, alpha port by www.innovative-software.com and www.quant-x.com.\n");
+  printed = 1 ;
+}
+
+
+struct super_block * reiserfs_read_super (struct super_block * s, void * data, int silent)
+{
+    int size;
+    struct inode *root_inode;
+    kdev_t dev = s->s_dev;
+    int j;
+    extern int *blksize_size[];
+    struct reiserfs_transaction_handle th ;
+    int old_format = 0;
+    unsigned long blocks;
+    int jinit_done = 0 ;
+
+    memset (&s->u.reiserfs_sb, 0, sizeof (struct reiserfs_sb_info));
+
+    if (parse_options ((char *) data, &(s->u.reiserfs_sb.s_mount_opt), &blocks) == 0) {
+	s->s_dev = 0;
+	return NULL;
+    }
+
+    if (blocks) {
+  	printk("reserfs: resize option for remount only\n");
+	return NULL;
+    }	
+
+    MOD_INC_USE_COUNT;
+    lock_super (s);
+
+    if (blksize_size[MAJOR(dev)] && blksize_size[MAJOR(dev)][MINOR(dev)] != 0) {
+	/* as blocksize is set for partition we use it */
+	size = blksize_size[MAJOR(dev)][MINOR(dev)];
+    } else {
+	size = BLOCK_SIZE;
+	set_blocksize (s->s_dev, BLOCK_SIZE);
+    }
+
+    /* read block, containing reiserfs super block (it is stored at REISERFS_FIRST_BLOCK-th 1K block) */
+    if (read_super_block (s, size)) {
+	if(read_old_super_block(s,size)) 
+	    goto error;
+	else
+	    old_format = 1;
+    }
+
+    s->u.reiserfs_sb.s_mount_state = le16_to_cpu (SB_DISK_SUPER_BLOCK (s)->s_state); /* journal victim */
+    s->u.reiserfs_sb.s_mount_state = REISERFS_VALID_FS ;
+
+    /* reiserfs can not be mounted when it propably contains errors */
+#if 0 /* journal victim */
+    if (le16_to_cpu (SB_DISK_SUPER_BLOCK (s)->s_state) != REISERFS_VALID_FS) {
+	printk ("reiserfs_read_super:  mounting unchecked fs, run reiserfsck first\n");
+	goto error;
+    }
+#endif
+    if (old_format ? read_old_bitmaps(s) : read_bitmaps(s)) { 
+	printk ("reiserfs_read_super: unable to read bitmap\n");
+	goto error;
+    }
+
+    if (journal_init(s)) {
+	printk("reiserfs_read_super: unable to initialize journal space\n") ;
+	goto error ;
+    } else {
+	jinit_done = 1 ; /* once this is set, journal_release must be called
+			 ** if we error out of the mount 
+			 */
+    }
+    if (reread_meta_blocks(s)) {
+	printk("reiserfs_read_super: unable to reread meta blocks after journal init\n") ;
+	goto error ;
+    }
+
+    if (replay_only (s))
+	goto error;
+
+    /* VFS should do this for us, but we panic if the MS_RDONLY isn't
+    ** set on a read only device, so I'm doing it again anyway. --clm
+    */
+    if (is_read_only(s->s_dev) && !(s->s_flags & MS_RDONLY)) {
+      printk("clm-4000: Detected readonly device, marking FS readonly\n") ;
+      s->s_flags |= MS_RDONLY ;
+    }
+    /*s->s_op = &reiserfs_sops;*/
+   
+    /* get root directory inode */
+    store_key (s, &root_key);
+    root_inode = iget (s, root_key.k_objectid);
+    forget_key (s, &root_key);
+    if (!root_inode) {
+	printk ("reiserfs_read_super: get root inode failed\n");
+	goto error;
+    }
+
+    s->s_root = d_alloc_root(root_inode, NULL);  
+    if (!s->s_root) {
+	iput(root_inode);
+	goto error;
+    }
+
+    // define and initialize hash function
+    s->u.reiserfs_sb.s_hash_function = hash_function (s);
+    if (s->u.reiserfs_sb.s_hash_function == NULL) {
+        dput(s->s_root) ;
+	goto error ;
+    }
+
+
+    if (!(s->s_flags & MS_RDONLY)) {
+	SB_DISK_SUPER_BLOCK (s)->s_state = cpu_to_le16 (REISERFS_ERROR_FS);
+	/* mark_buffer_dirty (SB_BUFFER_WITH_SB (s), 1); */
+	journal_begin(&th, s, 1) ;
+	journal_mark_dirty(&th, s, SB_BUFFER_WITH_SB (s));
+	journal_end(&th, s, 1) ;
+	s->s_dirt = 0;
+    }
+
+    /*s->u.reiserfs_sb.unpreserve = dont_preserve (s) ? 0 : unpreserve;*/
+    /* we have to do this to make journal writes work correctly */
+    SB_BUFFER_WITH_SB(s)->b_end_io = reiserfs_end_buffer_io_sync ;
+
+    unlock_super (s);
+    print_credits() ;
+    printk("%s\n", reiserfs_get_version_string()) ;
+    return s;
+
+ error:
+    if (jinit_done) { /* kill the commit thread, free journal ram */
+	journal_release_error(NULL, s) ;
+    }
+    if (SB_DISK_SUPER_BLOCK (s)) {
+	for (j = 0; j < le16_to_cpu (SB_DISK_SUPER_BLOCK (s)->s_bmap_nr); j ++) {
+	    if (SB_AP_BITMAP (s))
+		brelse (SB_AP_BITMAP (s)[j]);
+	}
+	if (SB_AP_BITMAP (s))
+	    reiserfs_kfree (SB_AP_BITMAP (s), sizeof (struct buffer_head *) * SB_BMAP_NR (s), s);
+    }
+    if (SB_BUFFER_WITH_SB (s))
+	brelse(SB_BUFFER_WITH_SB (s));
+    s->s_dev = 0;
+    unlock_super(s);
+    MOD_DEC_USE_COUNT;
+
+    return NULL;
+}
+
+
+int reiserfs_statfs (struct super_block * s, struct statfs * buf, int bufsize)
+{
+  struct statfs tmp;
+  struct reiserfs_super_block * rs = SB_DISK_SUPER_BLOCK (s);
+  
+				/* changed to accomodate gcc folks.*/
+  tmp.f_type =  REISERFS_SUPER_MAGIC;
+  tmp.f_bsize = le32_to_cpu (s->s_blocksize);
+  tmp.f_blocks = le32_to_cpu (rs->s_block_count) - le16_to_cpu (rs->s_bmap_nr) - 1;
+  tmp.f_bfree = le32_to_cpu (rs->s_free_blocks);
+  tmp.f_bavail = tmp.f_bfree;
+  tmp.f_files = -1;
+  tmp.f_ffree = -1;
+  tmp.f_namelen = (REISERFS_MAX_NAME_LEN (s->s_blocksize));
+  return copy_to_user (buf, &tmp, bufsize) ? -EFAULT : 0;
+}
+
+#ifdef __KERNEL__
+
+static struct file_system_type reiserfs_fs_type = {
+  "reiserfs", FS_REQUIRES_DEV, reiserfs_read_super, NULL
+};
+
+
+__initfunc(int init_reiserfs_fs(void))
+{
+  return register_filesystem(&reiserfs_fs_type);
+}
+
+#endif
+
+#ifdef MODULE
+EXPORT_NO_SYMBOLS;
+
+int init_module(void)
+{
+	return init_reiserfs_fs();
+}
+
+void cleanup_module(void)
+{
+        unregister_filesystem(&reiserfs_fs_type);
+}
+
+#endif
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
diff -urN linux/fs/reiserfs/symlink.c /tmp/linux/fs/reiserfs/symlink.c
--- linux/fs/reiserfs/symlink.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/symlink.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,140 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+#ifdef __KERNEL__
+
+#include <linux/errno.h>
+#include <linux/sched.h>
+#include <linux/reiserfs_fs.h>
+#include <asm/uaccess.h>
+
+#else
+
+#include "nokernel.h"
+
+#endif
+
+
+static int reiserfs_readlink(struct dentry *, char *, int);
+static struct dentry * reiserfs_follow_link(struct dentry *, struct dentry *, unsigned int);
+
+
+struct inode_operations reiserfs_symlink_inode_operations = {
+	NULL,			/* file-operations */
+	NULL,			/* create */
+	NULL,			/* lookup */
+	NULL,			/* link */
+	NULL,			/* unlink */
+	NULL,			/* symlink */
+	NULL,			/* mkdir */
+	NULL,			/* rmdir */
+	NULL,			/* mknod */
+	NULL,			/* rename */
+	reiserfs_readlink,	/* readlink */
+	reiserfs_follow_link,	/* follow_link */
+	NULL,			/* readpage */
+	NULL,			/* writepage */
+	NULL,			/* bmap */
+	NULL,			/* truncate */
+	NULL,			/* permission */
+	NULL,			/* smap */
+	NULL,			/* updatepage */
+	NULL			/* revalidate */
+	
+};
+
+
+static int read_symlink (struct inode * inode, char * buf, int buflen,
+                         int memmode)
+{
+  struct key key;
+  int repeat, pos_in_item, chars, len = inode->i_size;
+  struct path path;
+  struct item_head * ih;
+  int total_copied = 0 ; /* I should use the koffset instead */
+
+  init_path (&path);
+  copy_key (&key, INODE_PKEY(inode));
+  key.k_offset = 1;
+  key.k_uniqueness = TYPE_DIRECT;
+
+  while (len > 0) {
+    if (search_for_position_by_key (inode->i_sb, &key, &path, &pos_in_item, &repeat) == POSITION_NOT_FOUND) {
+      reiserfs_warning ("vs-17000: read_symlink: symlink item not found");
+      return -EIO ;
+    }
+    ih = PATH_PITEM_HEAD(&path);
+    chars = ih->ih_item_len - pos_in_item;
+
+    if ((chars + total_copied) > buflen) {
+     chars = buflen - total_copied ;
+    }
+    if (memmode == REISERFS_KERNEL_MEM)
+      memcpy (buf, B_I_PITEM(PATH_PLAST_BUFFER(&path),ih) + pos_in_item, chars);
+    else
+      copy_to_user (buf, B_I_PITEM(PATH_PLAST_BUFFER(&path),ih) + pos_in_item, chars);
+    buf += chars;
+    key.k_offset += chars;
+    len -= chars;
+    total_copied += chars ;
+
+    if (total_copied >= buflen) {
+     break ;
+    }
+
+#ifdef CONFIG_REISERFS_CHECK
+    if (len < 0)
+      reiserfs_panic (inode->i_sb, "vs-17005: read_symlink: too many bytes read from symlink (%d). Must be %d", inode->i_size - len,
+		      inode->i_size);
+#endif
+
+    
+  }
+  /* *buf = 0; this is only done in follow_link now. */
+  decrement_counters_in_path(&path);
+  return 0 ;
+}
+
+
+static struct dentry * reiserfs_follow_link (struct dentry * dentry, struct dentry * base, unsigned int follow)
+{
+  struct inode * inode = dentry->d_inode;
+  char * buf;
+  int ret ;
+
+  buf = reiserfs_kmalloc (inode->i_size + 1, GFP_KERNEL, inode->i_sb);
+  if (buf == 0) {
+    dput (base);
+    return ERR_PTR(-ENOMEM);
+  }
+
+  ret = read_symlink (inode, buf, inode->i_size, REISERFS_KERNEL_MEM);
+  if (ret != 0) {
+    reiserfs_kfree (buf, inode->i_size + 1, inode->i_sb);
+    dput (base);
+    return ERR_PTR(-EIO) ;
+  }
+  buf[inode->i_size] = '\0' ;
+
+  UPDATE_ATIME(inode);
+  base = lookup_dentry (buf, base, follow);
+  reiserfs_kfree (buf, inode->i_size + 1, inode->i_sb);
+
+  return base;
+}
+
+
+static int reiserfs_readlink (struct dentry * dentry, char * buffer, int buflen)
+{
+  struct inode * inode = dentry->d_inode;
+
+  if (read_symlink (inode, buffer, buflen, REISERFS_USER_MEM) != 0) {
+    return -EIO ;
+  }
+  return (buflen < inode->i_size ? buflen : inode->i_size) ;
+}
+
+
+
+
+
diff -urN linux/fs/reiserfs/teahash3.c /tmp/linux/fs/reiserfs/teahash3.c
--- linux/fs/reiserfs/teahash3.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/teahash3.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,208 @@
+/*
+ * Keyed 32-bit hash function using TEA in a Davis-Meyer function
+ *   H0 = Key
+ *   Hi = E Mi(Hi-1) + Hi-1
+ *
+ * (see Applied Cryptography, 2nd edition, p448).
+ *
+ * Jeremy Fitzhardinge <jeremy@zip.com.au> 1998
+ * 
+ * Jeremy has agreed to the contents of reiserfs/README. -Hans
+ */
+
+#ifdef __KERNEL__
+#define assert(x)
+#else
+#include <assert.h>
+#include "nokernel.h"
+#endif
+
+#if 1
+/* OK for Intel */
+typedef unsigned long u32;
+typedef const unsigned char u8;
+#else
+#include <inttypes.h>
+typedef uint32_t u32;
+typedef uint8_t u8;
+#endif
+
+#include <linux/config.h>
+
+#define DELTA 0x9E3779B9
+#define FULLROUNDS 10		/* 32 is overkill, 16 is strong crypto */
+#define PARTROUNDS 6		/* 6 gets complete mixing */
+
+/* a, b, c, d - data; h0, h1 - accumulated hash */
+#define TEACORE(rounds)							\
+	do {								\
+		u32 sum = 0;						\
+		int n = rounds;						\
+		u32 b0, b1;						\
+									\
+		b0 = h0;						\
+		b1 = h1;						\
+									\
+		do							\
+		{							\
+			sum += DELTA;					\
+			b0 += ((b1 << 4)+a) ^ (b1+sum) ^ ((b1 >> 5)+b);	\
+			b1 += ((b0 << 4)+c) ^ (b0+sum) ^ ((b0 >> 5)+d);	\
+		} while(--n);						\
+									\
+		h0 += b0;						\
+		h1 += b1;						\
+	} while(0)
+
+u32 keyed_hash(/*u32 k[2], *//*u8*/const char *msg, int len)
+{
+	u32 k[] = { 0x9464a485, 0x542e1a94, 0x3e846bff, 0xb75bcfc3}; 
+
+	u32 h0 = k[0], h1 = k[1];
+	u32 a, b, c, d;
+	u32 pad;
+	int i;
+ 
+#ifdef CONFIG_YRH_HASH
+	int j, pow;
+#endif
+
+	assert(len >= 0 && len < 256);
+
+#ifdef CONFIG_YRH_HASH
+
+	for (pow=1,i=1; i < len; i++) pow = pow * 10; 
+
+        if (len == 1) 
+	  a = msg[0]-48;
+        else
+	  a = (msg[0] - 48) * pow;
+
+	for (i=1; i < len; i++) {
+	    c = msg[i] - 48; 
+	    for (pow=1,j=i; j < len-1; j++) pow = pow * 10; 
+ 	    a = a + c * pow;
+	}
+
+	for (; i < 40; i++) {
+	    c = '0' - 48; 
+	    for (pow=1,j=i; j < len-1; j++) pow = pow * 10; 
+ 	    a = a + c * pow;
+	}
+
+	for (; i < 256; i++) {
+	    c = i; 
+	    for (pow=1,j=i; j < len-1; j++) pow = pow * 10; 
+ 	    a = a + c * pow;
+	}
+
+	a = a << 7;
+	return ((u32)a);
+	
+#endif
+
+	pad = (u32)len | ((u32)len << 8);
+	pad |= pad << 16;
+
+	while(len >= 16)
+	{
+		a = (u32)msg[ 0]      |
+		    (u32)msg[ 1] << 8 |
+		    (u32)msg[ 2] << 16|
+		    (u32)msg[ 3] << 24;
+		b = (u32)msg[ 4]      |
+		    (u32)msg[ 5] << 8 |
+		    (u32)msg[ 6] << 16|
+		    (u32)msg[ 7] << 24;
+		c = (u32)msg[ 8]      |
+		    (u32)msg[ 9] << 8 |
+		    (u32)msg[10] << 16|
+		    (u32)msg[11] << 24;
+		d = (u32)msg[12]      |
+		    (u32)msg[13] << 8 |
+		    (u32)msg[14] << 16|
+		    (u32)msg[15] << 24;
+		
+		TEACORE(PARTROUNDS);
+
+		len -= 16;
+		msg += 16;
+	}
+
+	if (len >= 12)
+	{
+		assert(len < 16);
+
+		a = (u32)msg[ 0]      |
+		    (u32)msg[ 1] << 8 |
+		    (u32)msg[ 2] << 16|
+		    (u32)msg[ 3] << 24;
+		b = (u32)msg[ 4]      |
+		    (u32)msg[ 5] << 8 |
+		    (u32)msg[ 6] << 16|
+		    (u32)msg[ 7] << 24;
+		c = (u32)msg[ 8]      |
+		    (u32)msg[ 9] << 8 |
+		    (u32)msg[10] << 16|
+		    (u32)msg[11] << 24;
+
+		d = pad;
+		for(i = 12; i < len; i++)
+		{
+			d <<= 8;
+			d |= msg[i];
+		}
+	}
+	else if (len >= 8)
+	{
+		assert(len < 12);
+
+		a = (u32)msg[ 0]      |
+		    (u32)msg[ 1] << 8 |
+		    (u32)msg[ 2] << 16|
+		    (u32)msg[ 3] << 24;
+		b = (u32)msg[ 4]      |
+		    (u32)msg[ 5] << 8 |
+		    (u32)msg[ 6] << 16|
+		    (u32)msg[ 7] << 24;
+
+		c = d = pad;
+		for(i = 8; i < len; i++)
+		{
+			c <<= 8;
+			c |= msg[i];
+		}
+	}
+	else if (len >= 4)
+	{
+		assert(len < 8);
+
+		a = (u32)msg[ 0]      |
+		    (u32)msg[ 1] << 8 |
+		    (u32)msg[ 2] << 16|
+		    (u32)msg[ 3] << 24;
+
+		b = c = d = pad;
+		for(i = 4; i < len; i++)
+		{
+			b <<= 8;
+			b |= msg[i];
+		}
+	}
+	else
+	{
+		assert(len < 4);
+
+		a = b = c = d = pad;
+		for(i = 0; i < len; i++)
+		{
+			a <<= 8;
+			a |= msg[i];
+		}
+	}
+
+	TEACORE(FULLROUNDS);
+
+/*	return 0;*/
+	return h0^h1;
+}
diff -urN linux/fs/reiserfs/utils/Makefile /tmp/linux/fs/reiserfs/utils/Makefile
--- linux/fs/reiserfs/utils/Makefile	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/Makefile	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,68 @@
+.EXPORT_ALL_VARIABLES:
+
+TOPDIR = $(shell pwd)
+
+#
+#
+#
+REISERFS_KERNEL_SOURCE = $(TOPDIR)/..
+REISERFS_LIB = ../lib
+
+
+TMPBINDIR = $(TOPDIR)/bin
+INCLUDEDIR = $(TOPDIR)/include
+INCLUDEDIR2 = $(TOPDIR)/../../../include/linux
+
+
+SBIN = /sbin
+MANDIR = /usr/man/man8
+
+IDIRS = -I$(INCLUDEDIR) -I$(INCLUDEDIR2) -I-
+
+
+CFLAGS = -Wall -c -O $(IDIRS)
+LFLAGS = -L$(TMPBINDIR)
+
+#CFLAGS = -Wall -c -g -pg $(IDIRS)
+#LFLAGS =  -pg -L$(TMPBINDIR)
+
+
+ALL_SUB_DIRS = mkreiserfs dumpreiserfs fsck resize_reiserfs
+ALL_PROGS = mkreiserfs resize_reiserfs fsck
+
+all:
+	mkdir -p bin
+	set -e; for i in $(ALL_SUB_DIRS); do $(MAKE) -C $$i ; done
+
+dep:
+	set -e; for i in $(ALL_SUB_DIRS); do $(MAKE) -C $$i dep ; done
+
+clean:
+	set -e; for i in $(ALL_SUB_DIRS); do $(MAKE) -C $$i clean ; done
+
+install:
+	for i in $(ALL_PROGS); do $(MAKE) -C $$i install ; done
+
+uninstall:
+	set -e; for i in $(ALL_PROGS); do $(MAKE) -C $$i uninstall ; done
+
+tags:   
+	:> TAGS
+#	cd mkreiserfs; etags *.[ch] ../lib/*.c ../include/*.h ../../*.c ../../../../include/linux/reiserfs*h
+#	cd fsck; etags *.[ch] ../lib/*.c ../include/*.h ../../../../include/linux/reiserfs*h
+#	cd print_disk_layout; etags *.[ch] ../lib/*.c ../include/*.h ../../../../include/linux/reiserfs*h
+#	cd lib; etags *.[ch] ../lib/*.c ../include/*.h ../../../../include/linux/reiserfs*h
+#	cd include; etags *.[ch] ../lib/*.c ../include/*.h ../../../../include/linux/reiserfs*h
+#	cd emu; etags *.[ch] ../lib/*.c ../include/*.h ../../../../include/linux/reiserfs*h
+#	cd obj; etags ../../*.[ch] ../../../../include/linux/reiserfs*h
+	rm -f TAGS; etags ../*.[ch] ../../../include/linux/reiserfs*h include/*.h lib/*.c dumpreiserfs/*.c mkreiserfs/*.c emu/*.c fsck/*.c
+
+
+
+
+
+
+
+
+
+
diff -urN linux/fs/reiserfs/utils/README /tmp/linux/fs/reiserfs/utils/README
--- linux/fs/reiserfs/utils/README	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/README	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,31 @@
+This contains programs to create (mkreiserfs) and repair
+(reiserfsck) reiserfs file system on a block device. 
+
+Building is simple:
+set up the variable REISERFS_KERNEL_SOURCE in the main Makefile to point
+properly where kernel files of reiserfs are stored and run
+
+make
+
+After that you have binaries in bin/.
+
+You can `make install` to copy programs to /sbin
+and man pages to /usr/man/man8.
+
+
+FSCK NOTE:
+	Normally you do not need fsck even after unclean shutdown. Repairing
+	is performed by replaying of journal on mount.
+	However, it is a good idea to run reiserfsck time by time to make sure that
+	filesystem is kept in consistent state.
+	When it will say that filesystem is broken and --rebuild-tree is
+	recommended you should be aware of the following.
+	Reiserfs stores data on disk in quite complicate manner. This leads
+	that reiserfsck does not look as a simple program. It should
+	still contain bugs. The perfect way to use it is to backup
+	target partition first (I am sorry, if you have desire, time and disk
+	space). If reiserfs will fail, it would be useful to use backuped copy
+	of the partition in the debugging.
+
+Thanks a lot
+
Binary files linux/fs/reiserfs/utils/bin/dumpreiserfs and /tmp/linux/fs/reiserfs/utils/bin/dumpreiserfs differ
Binary files linux/fs/reiserfs/utils/bin/mkreiserfs and /tmp/linux/fs/reiserfs/utils/bin/mkreiserfs differ
Binary files linux/fs/reiserfs/utils/bin/reiserfsck and /tmp/linux/fs/reiserfs/utils/bin/reiserfsck differ
Binary files linux/fs/reiserfs/utils/bin/resize_reiserfs and /tmp/linux/fs/reiserfs/utils/bin/resize_reiserfs differ
diff -urN linux/fs/reiserfs/utils/dumpreiserfs/Makefile /tmp/linux/fs/reiserfs/utils/dumpreiserfs/Makefile
--- linux/fs/reiserfs/utils/dumpreiserfs/Makefile	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/dumpreiserfs/Makefile	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,45 @@
+VPATH = ../bin
+vpath %.c $(REISERFS_KERNEL_SOURCE) $(REISERFS_LIB)
+
+
+# kernel files needed for debugreiserfs/unpack
+KERNEL_C = prints.c
+KERNEL_OBJS = prints.o
+
+# files from utils's lib directory needed for debugreiserfs/unpack
+LIB_C = misc.c version.c io.c reiserfs.c
+LIB_OBJS = misc.o version.o io.o reiserfs.o
+
+
+DUMP_OBJS = dumpreiserfs.o $(LIB_OBJS) $(KERNEL_OBJS)
+UNPACK_OBJS = unpack.o $(LIB_OBJS) $(KERNEL_OBJS)
+
+DUMPFS = $(TMPBINDIR)/dumpreiserfs
+UNPACK = $(TMPBINDIR)/unpackreiserfs
+
+
+all: $(DUMPFS)
+# $(UNPACK)
+
+.c.o:
+	$(CC) $(CFLAGS) $<
+
+$(DUMPFS): $(DUMP_OBJS)
+	$(CC) -O2 $(LFLAGS) -o $(DUMPFS) $(DUMP_OBJS)
+
+$(UNPACK): $(UNPACK_OBJS)
+	$(CC) $(LFLAGS) -o $(UNPACK) $(UNPACK_OBJS)
+
+clean:
+	rm -f *.o $(DUMPFS) $(UNPACK) *~
+
+dep:
+	gcc -MM $(IDIRS) *.c > .depend
+	for i in $(KERNEL_C); do gcc -MM $(IDIRS) $(REISERFS_KERNEL_SOURCE)/$$i >> .depend ; done
+	for i in $(LIB_C); do gcc -MM $(IDIRS) ../lib/$$i >> .depend ; done
+
+
+ifeq (.depend,$(wildcard .depend))
+include .depend
+endif
+
diff -urN linux/fs/reiserfs/utils/dumpreiserfs/README /tmp/linux/fs/reiserfs/utils/dumpreiserfs/README
--- linux/fs/reiserfs/utils/dumpreiserfs/README	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/dumpreiserfs/README	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,14 @@
+This is to be a man page for dumpreiserfs
+
+This program exists only to help to solve problem with reiserfsck.
+
+DUMPREISERFS
+It can be used to dump reiserfs partition out.
+Called with -p it will calculate how many bytes have to be transfereed.
+If -P specified, dumpreiserfs will write the partition metadata out to stdout n
+which should be caugth with |gzip -c > whatever.gz
+
+UNPACKREISERFS deals with the file created in the above way.
+zcat whatever.gz | unpackreiserfs /dev/wherever you want to reiserfs transferred to.
+
+5/10/99
diff -urN linux/fs/reiserfs/utils/dumpreiserfs/dumpreiserfs.c /tmp/linux/fs/reiserfs/utils/dumpreiserfs/dumpreiserfs.c
--- linux/fs/reiserfs/utils/dumpreiserfs/dumpreiserfs.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/dumpreiserfs/dumpreiserfs.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,628 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+#include <stdio.h>
+#include <string.h>
+#include <stdlib.h>
+#include <unistd.h>
+#include <sys/types.h>
+#include <asm/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <errno.h>
+#include <sys/vfs.h>
+#include <netinet/in.h>
+
+//#include "inode.h"
+#include "io.h"
+#include "sb.h"
+#include "misc.h"
+#include "reiserfs_fs.h"
+#include "reiserfs.h"
+
+
+int g_new_internals;
+
+
+#define print_usage_and_exit() die ("Usage: %s [-b block-to-print][-idc] device\n\
+-i Causes to print all items of a leaf\n\
+-d                 content of directory items\n\
+-c                 content of direct items\n\
+-m                 bitmap blocks\n", argv[0]);
+
+
+
+struct reiserfs_fsstat {
+  int nr_internals;
+  int nr_leaves;
+  int nr_files;
+  int nr_directories;
+  int nr_unformatted;
+} g_stat_info;
+int g_comp_number = 0;
+
+/*
+ *  options
+ */
+int opt_print_regular_file_content = 0;/* -c */
+int opt_print_directory_contents = 0;	/* -d */
+int opt_print_leaf_items = 0;		/* -i */
+int opt_print_objectid_map = 0;	/* -o */
+int opt_print_block_map = 0;		/* -m */
+/* when you want print one block specify -b # */
+int opt_block_to_print = -1;
+int opt_pack = 0;	/* -P will produce output that should be |gzip -c > whatever.gz */
+			/* -p will calculate number of bytes needed to transfer the partition */
+int opt_print_journal;
+int opt_pack_all = 0;
+
+struct super_block g_sb;
+
+
+
+int print_mode (void)
+{
+    int mode = 0;
+
+    if (opt_print_leaf_items == 1)
+	mode |= PRINT_LEAF_ITEMS;
+    if (opt_print_directory_contents == 1)
+	mode |= (PRINT_LEAF_ITEMS | PRINT_DIRECTORY_ITEMS);
+    if (opt_print_regular_file_content == 1)
+	mode |= (PRINT_LEAF_ITEMS | PRINT_DIRECT_ITEMS);
+    return mode;
+}
+
+
+void print_disk_tree (int block_nr)
+{
+    struct buffer_head * bh;
+
+    bh = bread (g_sb.s_dev, block_nr, g_sb.s_blocksize);
+    if (B_IS_KEYS_LEVEL (bh)) {
+	int i;
+	struct disk_child * dc;
+
+	g_stat_info.nr_internals ++;
+	print_block (bh, print_mode (), -1, -1);
+      
+	dc = B_N_CHILD (bh, 0);
+	for (i = 0; i <= B_NR_ITEMS (bh); i ++, dc ++)
+	    print_disk_tree (dc->dc_block_number);
+
+    } else if (B_IS_ITEMS_LEVEL (bh)) {
+	g_stat_info.nr_leaves ++;
+	print_block (bh, print_mode (), -1, -1);
+    } else {
+	print_block (bh, print_mode (), -1, -1);
+	die ("print_disk_tree: bad block type");
+    }
+    brelse (bh);
+}
+
+
+
+void print_one_block (int block)
+{
+    struct buffer_head * bh;
+    
+    if (test_bit (block % (g_sb.s_blocksize * 8), 
+		  SB_AP_BITMAP (&g_sb)[block / (g_sb.s_blocksize * 8)]->b_data))
+	printf ("%d is used in true bitmap\n", block);
+    else
+	printf ("%d is free in true bitmap\n", block);
+
+    bh = bread (g_sb.s_dev, block, g_sb.s_blocksize);
+      if (!not_formatted_node (bh->b_data, g_sb.s_blocksize))
+	print_block (bh, PRINT_LEAF_ITEMS | PRINT_DIRECTORY_ITEMS | (opt_print_regular_file_content == 1 ? PRINT_DIRECT_ITEMS : 0), -1, -1);
+    else
+	printf ("Looks like unformatted\n");
+    brelse (bh);
+    return;
+}
+
+
+static char * parse_options (int argc, char * argv [])
+{
+    int c;
+    char * tmp;
+  
+    while ((c = getopt (argc, argv, "b:icdmoMpPaAj")) != EOF) {
+	switch (c) {
+	case 'b':	/* print a single node */
+	    opt_block_to_print = strtol (optarg, &tmp, 0);
+	    if (*tmp)
+		die ("parse_options: bad block size");
+	    break;
+
+	case 'p':	/* calculate number of bytes, that need to be transfered */
+	    opt_pack = 'c'; break;
+	case 'P':	/* dump a partition */
+	    opt_pack = 'p'; break;
+	case 'a':
+	    opt_pack_all = 'c'; break;
+	case 'A':
+	    opt_pack_all = 'p'; break;
+
+	case 'i':	/* print items of a leaf */
+	    opt_print_leaf_items = 1; break;
+
+	case 'd':	/* print directories */
+	    opt_print_directory_contents = 1; break;
+
+	case 'c':	/* print contents of a regular file */
+	    opt_print_regular_file_content = 1; break;
+
+	case 'o':	/* print a objectid map */
+	    opt_print_objectid_map = 1; break;
+
+	case 'm':	/* print a block map */
+	    opt_print_block_map = 1;  break;
+	case 'M':	/* print a block map with details */
+	    opt_print_block_map = 2;  break;
+	case 'j':
+	    opt_print_journal = 1; break; /* print transactions */
+	}
+    }
+    if (optind != argc - 1)
+	/* only one non-option argument is permitted */
+	print_usage_and_exit();
+  
+    return argv[optind];
+}
+
+
+/* journal has permanent location (currently) (after first bitmap
+   block and constant size (JOURNAL_BLOCK_COUNT + 1) */
+int journal_block (struct super_block * s, __u32 block)
+{
+/*
+    if (block > SB_AP_BITMAP (s)[0]->b_blocknr &&
+	block < SB_AP_BITMAP (s)[0]->b_blocknr + JOURNAL_BLOCK_COUNT + 1)
+	return 1;
+*/
+    if (block >= SB_JOURNAL_BLOCK (s) &&
+	block <= SB_JOURNAL_BLOCK (s) + s->u.reiserfs_sb.s_rs->s_orig_journal_size + 1)
+	return 1;
+    return 0;
+}
+
+
+int data_block (struct super_block * s, __u32 block)
+{
+    int i;
+
+    if (block == REISERFS_DISK_OFFSET_IN_BYTES / s->s_blocksize)
+      /* super block, not data block */
+      return 0;
+
+    for (i = 0; i < SB_BMAP_NR (s); i ++)
+	if (block == SB_AP_BITMAP (s)[i]->b_blocknr)
+	    /* bitmap block, not data block */
+	    return 0;
+
+    if (journal_block (s, block))
+	return 0;
+
+    return 1;
+}
+
+
+/* this dumps file sustem to stdout as a such way: 
+   16 bit blocksize
+   32 bit blocknumber
+   16 bit - record length
+   the record of given length
+   ..
+
+   to pack :   print_disk_layout -p /dev/xxx | gzip -c > xxx.gz
+   to unpack : zcat xxx.gz | unpackreiserfs /dev/xxx
+*/
+
+
+static int get_total_block_number (void)
+{
+  int i, j;
+  int retval = 0;
+    
+  retval = 0;
+    
+  if (opt_pack_all)
+    retval = SB_BLOCK_COUNT (&g_sb);
+  else {
+    for (i = 0; i < SB_BMAP_NR (&g_sb); i ++) {
+      for (j = 0; j < g_sb.s_blocksize * 8; j ++)
+	if (i * g_sb.s_blocksize * 8 + j < SB_BLOCK_COUNT (&g_sb) &&
+	    test_bit (j, SB_AP_BITMAP (&g_sb)[i]->b_data))
+	  retval ++;
+    }
+  }
+  return retval;
+}
+
+
+int direct_items = 0, direct_item_total_length = 0;
+int items = 0;
+int unreachable_items = 0;
+
+/* fill direct items with 0s */
+static void zero_direct_items (char * buf)
+{
+    int i;
+    struct item_head * ih;
+
+    if (((struct block_head *)buf)->blk_level != DISK_LEAF_NODE_LEVEL)
+	return;
+
+    /* leaf node found */
+    ih = (struct item_head *)(buf + BLKH_SIZE);
+
+    for (i = 0; i < ((struct block_head *)buf)->blk_nr_item; i ++, ih ++) {
+	if (I_IS_DIRECT_ITEM (ih)) {
+	    /* FIXME: do not zero symlinks */
+	    direct_items ++;
+	    direct_item_total_length += ih->ih_item_len;
+	    memset (buf + ih->ih_item_location, 0, ih->ih_item_len);
+	}
+	items ++;
+	if (ih->u.ih_free_space == 0xffff)
+	    unreachable_items ++;
+    }
+}
+
+
+
+void pack_partition (struct super_block * s)
+{
+    int i, j, k, l;
+    uint32_t blocknumber32;
+    uint16_t reclen16, data16;
+    __u32 done = 0;
+    char * data;
+    long long bytes_to_transfer = 0;
+    struct buffer_head * bh;
+    int total_block_number;
+
+
+    total_block_number = get_total_block_number ();
+    
+
+    /* write filesystem's block size to stdout as 16 bit number */
+    reclen16 = htons (s->s_blocksize);
+    if (opt_pack == 'p' || opt_pack_all == 'p')
+	write (1, &reclen16, sizeof (uint16_t));
+    bytes_to_transfer = sizeof (uint16_t);
+
+    /* go through blocks which are marked used in cautious bitmap */
+    for (i = 0; i < SB_BMAP_NR (s); i ++) {
+	for (j = 0; j < s->s_blocksize; j ++) {
+	    /* make sure, that we are not out of the device */
+	    if (i * s->s_blocksize * 8 + j * 8 >= SB_BLOCK_COUNT (s))
+		goto out_of_bitmap;
+
+	    //if (i * s->s_blocksize * 8 + j * 8 + 8 > SB_BLOCK_COUNT (s))
+	    //die ("build_the_tree: Out of bitmap");
+
+	    if (opt_pack_all == 0)
+		if (SB_AP_BITMAP (s)[i]->b_data[j] == 0) {
+		    /* skip blocks marked free if -a is not specified */
+		    continue;
+		}
+
+	    if (i * s->s_blocksize * 8 + j * 8 == (SB_BLOCK_COUNT (s) & ~(8-1))) {
+		// last byte of bitmap addresses less than 8
+		// blocks. Read them into 32 k buffer
+		l = (SB_BLOCK_COUNT (&g_sb) & 7);
+		bh = getblk (s->s_dev, i * s->s_blocksize + j, 8 * s->s_blocksize);
+		for (k = 0; k < l; k ++) {
+		    struct buffer_head * tmp;
+
+		    tmp = bread (s->s_dev, i * s->s_blocksize * 8 + j * 8 + k, s->s_blocksize);
+		    memcpy (bh->b_data + k * s->s_blocksize, tmp->b_data, s->s_blocksize);
+		    brelse (tmp);
+		}
+		mark_buffer_uptodate (bh, 1);
+	    } else {
+		l = 8;
+		bh = bread (s->s_dev, i * s->s_blocksize + j, 8 * s->s_blocksize);
+	    }
+
+	    /* read 8 blocks at once */
+	    //bh = bread (s->s_dev, i * s->s_blocksize + j, s->s_blocksize * 8);
+
+	    for (k = 0; k < l; k ++) {
+		__u32 block;
+		
+		block = i * s->s_blocksize * 8 + j * 8 + k;
+		
+		if (opt_pack_all == 0 && (SB_AP_BITMAP (s)[i]->b_data[j] & (1 << k)) == 0)
+		    continue;
+		
+		print_how_far (&done, total_block_number);
+		
+		data = bh->b_data + k * s->s_blocksize;
+
+		if (not_formatted_node (data, s->s_blocksize)) {
+		    /* ok, could not find formatted node here. But
+                       this can be commit block, or bitmap which has
+                       to be transferred */
+		    if (!not_data_block (s, block)) {
+			/* this is usual unformatted node. Transfer
+                           its number only to erase previously existed
+                           formatted nodes on the partition we will
+                           apply transferred metadata to */
+	    
+			/* size of following record in network byte order */
+			reclen16 = htons (2);
+
+			/* the record record */
+			data16 = htons (MAX_HEIGHT + 1);/*?*/
+			data = (char *)&data16;
+		    } else {
+			/* write super block and bitmap block must be transferred as are */
+			/* size of record  */
+			reclen16 = htons (s->s_blocksize);
+	    
+			/* the record itself */
+			data = data;
+		    }
+		} else {
+		    /* any kind of formatted nodes gets here (super
+                       block, desc block of journal): FIXME: it would
+                       be useful to be able to find commit blocks */
+		    zero_direct_items (data);
+		    /* FIXME: do other packing */
+		    /* write size of following record */
+		    reclen16 = htons (s->s_blocksize);
+		    
+		    /* the record itself */
+		    data = data;
+
+#if 0
+		    if (blkh->blk_level > DISK_LEAF_NODE_LEVEL) {
+			/* block must look like internal node on the target
+			   partition. But (currently) fsck do not consider internal
+			   nodes, therefore we do not have to transfer contents of
+			   internal nodes */
+	    
+			/* size of following record in network byte order */
+			reclen16 = htons (2);
+	    
+			/* the record itself */
+			data16 = htons (DISK_LEAF_NODE_LEVEL + 1);
+			data = (char *)&data16;	  
+		    } else {
+	    
+			/* leaf node found */
+			ih = (struct item_head *)(blkh + 1);
+	    
+			/* fill direct items with 0s */
+			for (l = 0; l < blkh->blk_nr_item; l ++, ih ++)
+			    if (I_IS_DIRECT_ITEM (ih)) {
+				direct_items ++;
+				direct_item_total_length += ih->ih_item_len;
+				memset ((char *)blkh + ih->ih_item_location, 0, ih->ih_item_len);
+			    }
+	    
+			/* write size of following record */
+			reclen16 = htons (s->s_blocksize);
+	    
+			/* the record itself */
+			data = (char *)blkh;
+		    }
+#endif
+		}
+	  
+		/*fprintf (stderr, "block %d, reclen %d\n", block, ntohs (reclen16));*/
+	
+		/* write block number */
+		blocknumber32 = htonl (block);
+		bytes_to_transfer += sizeof (uint32_t) + sizeof (uint16_t) + ntohs (reclen16);
+		if (opt_pack == 'p' || opt_pack_all == 'p') {
+		    write (1, &blocknumber32, sizeof (uint32_t));
+		    /* write record len */
+		    write (1, &reclen16, sizeof (uint16_t));
+		    /* write the record */
+		    write (1, data, ntohs (reclen16));
+		}
+	    }
+      
+	    bforget (bh);
+	}
+    }
+    
+ out_of_bitmap:
+    fprintf (stderr, "done\n");
+    if (opt_pack == 'c' || opt_pack_all == 'c')
+	fprintf (stderr, "Bytes to transfer %Ld, sequential 0s %d in %d sequeneces (%items (%d unreacable))\n",
+		 bytes_to_transfer, direct_item_total_length, direct_items, items, unreachable_items);
+    else
+	fprintf (stderr, "Bytes dumped %Ld, sequential 0s %d in %d sequeneces\n",
+		 bytes_to_transfer, direct_item_total_length, direct_items);
+    
+    
+}
+
+
+
+
+/* print all valid transactions and found dec blocks */
+static void print_journal (struct super_block * s)
+{
+    struct buffer_head * d_bh, * c_bh;
+    struct reiserfs_journal_desc * desc ;
+    struct reiserfs_journal_commit *commit ;
+    int end_journal;
+    int start_journal;
+    int i, j;
+    int first_desc_block = 0;
+    int wrapped = 0;
+    int valid_transactions = 0;
+
+    start_journal = SB_JOURNAL_BLOCK (s);
+    end_journal = start_journal + JOURNAL_BLOCK_COUNT;
+    printf ("Start scanning from %d\n", start_journal);
+
+    d_bh = 0;
+    desc = 0;
+    for (i = start_journal; i < end_journal; i ++) {
+	d_bh = bread (s->s_dev, i, s->s_blocksize);
+	if (is_desc_block (d_bh)) {
+	    int commit_block;
+
+	    if (first_desc_block == 0)
+		/* store where first desc block found */
+		first_desc_block = i;
+
+	    print_block (d_bh); /* reiserfs_journal_desc structure will be printed */
+	    desc = bh_desc (d_bh);
+
+	    commit_block = d_bh->b_blocknr + desc->j_len + 1;
+	    if (commit_block >= end_journal) {
+		printf ("-- wrapped?");
+		wrapped = 1;
+		break;
+	    }
+
+	    c_bh = bread (s->s_dev, commit_block, s->s_blocksize);
+	    commit = bh_commit (c_bh);
+	    if (does_desc_match_commit (desc, commit)) {
+		printf ("commit block %d (trans_id %ld, j_len %ld) does not match\n", commit_block,
+			commit->j_trans_id, commit->j_len);
+		brelse (c_bh) ;
+		brelse (d_bh);
+		continue;
+	    }
+
+	    valid_transactions ++;
+	    printf ("(commit block %d) - logged blocks (", commit_block);
+	    for (j = 0; j < desc->j_len; j ++) {
+		if (j < JOURNAL_TRANS_HALF) {
+		    printf (" %ld", desc->j_realblock[j]);
+		} else {
+		    printf (" %ld", commit->j_realblock[i - JOURNAL_TRANS_HALF]);
+		}
+	    }
+	    printf ("\n");
+	    i += desc->j_len + 1;
+	    brelse (c_bh);
+	}
+	brelse (d_bh);
+    }
+    
+    if (wrapped) {
+	c_bh = bread (s->s_dev, first_desc_block - 1, s->s_blocksize);
+	commit = bh_commit (c_bh);
+	if (does_desc_match_commit (desc, commit)) {
+	    printf ("No! commit block %d (trans_id %ld, j_len %ld) does not match\n", first_desc_block - 1,
+		    commit->j_trans_id, commit->j_len);
+	} else {
+	    printf ("Yes! (commit block %d) - logged blocks (\n", first_desc_block - 1);
+	    for (j = 0; j < desc->j_len; j ++) {
+		if (j < JOURNAL_TRANS_HALF) {
+		    printf (" %ld", desc->j_realblock[j]);
+		} else {
+		    printf (" %ld", commit->j_realblock[i - JOURNAL_TRANS_HALF]);
+		}
+	    }
+	    printf ("\n");
+	}
+	brelse (c_bh) ;
+	brelse (d_bh);
+    }
+
+    printf ("%d valid transactions found\n", valid_transactions);
+
+    {
+	struct buffer_head * bh;
+	struct reiserfs_journal_header * j_head;
+
+	bh = bread (s->s_dev, s->u.reiserfs_sb.s_rs->s_journal_block + s->u.reiserfs_sb.s_rs->s_orig_journal_size,
+		    s->s_blocksize);
+	j_head = (struct reiserfs_journal_header *)(bh->b_data);
+
+	printf ("#######################\nJournal header:\n"
+		"j_last_flush_trans_id %ld\n"
+		"j_first_unflushed_offset %ld\n"
+		"j_mount_id %ld\n", j_head->j_last_flush_trans_id, j_head->j_first_unflushed_offset,
+		j_head->j_mount_id);
+	brelse (bh);
+    }
+}
+
+
+int main (int argc, char * argv[])
+{
+    char * file_name;
+    int dev, i;
+
+
+    fprintf (stderr, "\n\n<-----------DUMPREISERFS, 2000----------->\n%s\n",
+	     reiserfs_get_version_string());
+ 
+    file_name = parse_options (argc, argv);
+
+
+    dev = open (file_name, O_RDONLY);
+    if (dev == -1)
+	die ("dumpreiserfs: Can not open device %s: %s\n", file_name, strerror (errno));
+    g_sb.s_dev = dev;
+
+    if (uread_super_block (&g_sb))
+	die ("dumpreiserfs: no reiserfs found on %s", file_name);
+    if (uread_bitmaps (&g_sb))
+	die ("dumpreiserfs: read_bitmap failed");
+
+    if (opt_pack || opt_pack_all) {
+	pack_partition (&g_sb);
+    } else {
+	/* dump file system to stdout */
+	if (opt_block_to_print != -1) {
+	    print_one_block (opt_block_to_print);
+	    goto end;
+	}
+
+	print_block (SB_BUFFER_WITH_SB (&g_sb));
+
+	if (opt_print_journal)
+	    print_journal (&g_sb);
+    
+	if (opt_print_objectid_map == 1)
+	    print_objectid_map (&g_sb);
+    
+	if (opt_print_block_map) {
+	    print_bmap (&g_sb, opt_print_block_map == 1 ? 1 : 0);
+	}
+
+	if (opt_print_regular_file_content || opt_print_directory_contents ||
+	    opt_print_leaf_items) {
+	    print_disk_tree (SB_ROOT_BLOCK (&g_sb));
+
+	    /* print the statistic */
+	    printf ("File system uses %d internal + %d leaves + %d unformatted nodes = %d blocks\n", 
+		    g_stat_info.nr_internals, g_stat_info.nr_leaves, g_stat_info.nr_unformatted, 
+		    g_stat_info.nr_internals + g_stat_info.nr_leaves + g_stat_info.nr_unformatted);
+	}
+    }
+
+
+ end:
+    /* brelse bitmaps */
+    if (SB_AP_BITMAP (&g_sb)) {
+	for (i = 0; i < SB_BMAP_NR (&g_sb); i ++) {
+	    brelse (SB_AP_BITMAP (&g_sb)[i]);
+	}
+	freemem (SB_AP_BITMAP (&g_sb));
+    }
+
+    /* brelse buffer containing super block */
+    brelse (SB_BUFFER_WITH_SB (&g_sb));
+
+    check_and_free_buffer_mem ();
+
+    return 0;
+}
+/* end of main */
+
+
+
diff -urN linux/fs/reiserfs/utils/dumpreiserfs/unpack.c /tmp/linux/fs/reiserfs/utils/dumpreiserfs/unpack.c
--- linux/fs/reiserfs/utils/dumpreiserfs/unpack.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/dumpreiserfs/unpack.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,156 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+#include <stdio.h>
+#include <errno.h>
+#include <malloc.h>
+#include <string.h>
+#include <fcntl.h>
+#include <netinet/in.h>
+#include <unistd.h>
+#include <sys/stat.h>
+
+/*uint32_t htonl (uint32_t hostlong);
+uint32_t ntohl (uint32_t netlong);*/
+
+int opt_skip_unfms = 0;
+int opt_do_not_write = 0;
+
+int waiting_read (int fd, char * buf, int count)
+{
+    int rd, done = 0;
+
+    while (count) {
+	rd = read (fd, buf, count);
+	if (rd < 1)
+	    return rd;
+	buf += rd;
+	count -= rd;
+	done += rd;
+    }
+    return done;
+}
+
+int main (int argc, char ** argv)
+{
+    uint16_t blocksize, reclen16;
+    uint32_t blocknumber32;
+    int c;
+    char * buf;
+    int fd;
+    int res;
+    struct stat st;
+  
+    if (argc < 2) {
+	printf ("Usage: gunzip -c | unpack [-s][-n] /dev/dest\n");
+	return 0;
+    }
+
+    while ((c = getopt (argc, argv, "sn")) != EOF) {
+	switch (c) {
+	case 's': /* skip writing of unformatted nodes */
+	    opt_skip_unfms = 1;
+	    break;
+	case 'n':
+	    opt_do_not_write = 1;
+	    break;
+	default:
+	    printf ("Usage: gunzip -c | unpack [-s] /dev/dest\n");
+	    return 0;
+	}
+    }
+
+    /* get file system's block size */
+    read (0, &blocksize, sizeof (uint16_t));
+    blocksize = ntohs (blocksize);
+    fprintf (stderr, "blocksize = %d\n", blocksize);
+
+    buf = (char *)malloc (blocksize);
+    if (!buf) {
+	perror ("malloc failed");
+	return 1;
+    }
+
+    /* we need to skip the below:
+       reiserfs: read_bitmaps: 0 blocks differ in true and cautious bitmaps
+       reiserfs: read_bitmaps: 1 blocks differ in true and cautious bitmaps
+    */
+
+/*  
+    read (0, buf, strlen ("reiserfs: read_bitmaps: 0 blocks differ in true and cautious bitmaps\n"));
+    if (strncmp (buf, "reiserfs", strlen ("reiserfs"))) {
+    fprintf (stderr, "Bad signature 1\n");
+    return 1;
+    }
+*/
+    /*  
+	read (0, buf, strlen ("reiserfs: read_bitmaps: 1 blocks differ in true and cautious bitmaps\n"));
+	if (strncmp (buf, "reiserfs", strlen ("reiserfs"))) {
+	fprintf (stderr, "Bad signature 2\n");
+	return 1;
+	}*/
+
+    if (is_mounted (argv[optind])) {
+	/* check forced on clean filesystem, maybe we can rebuild it (if it is mounted read-only). Later. */
+	die ("unpack: '%s' contains a mounted file system\n", argv[optind]);
+    }
+
+    if (stat (argv[optind], &st) == -1)
+	die ("unpack: stat failed: %s", strerror (errno));
+    if (!S_ISBLK (st.st_mode))
+	die ("unpck: %s is not a block device", argv[optind]);
+
+    fd = open (argv[optind], O_CREAT | O_RDWR);
+    if (fd == -1) {
+	perror ("open failed");
+	return 1;
+    }
+
+    while ((res = waiting_read (0, (char *)&blocknumber32, sizeof (uint32_t))) == sizeof (uint32_t)) {
+	/* read block number from stdin */
+/*
+  if (blocknumber32 == 0) {
+  printf ("exit\n");
+  exit (0);
+  }
+*/
+	blocknumber32 = ntohl (blocknumber32);
+
+	/* read 16 bit record length */
+	if (waiting_read (0, (char *)&reclen16, sizeof (uint16_t)) != sizeof (uint16_t)) {
+	    perror ("read reclen failed");
+	    return 1;
+	}
+	reclen16 = ntohs (reclen16);
+
+	fprintf (stderr, "%d reclen %d\n", blocknumber32, reclen16);
+
+	/* read the record itself */
+	if ((res = waiting_read (0, buf, reclen16)) != reclen16) {
+	    fprintf (stderr, "read record failed (%d %d)\n", res, reclen16);
+	    return 1;
+	}
+
+
+	/* the only one requirement to this block: does not look like
+           leaf node. If you unpacked damaged partition already you
+           might consider using -s to save time */
+	if ((opt_skip_unfms && reclen16 == 2) || opt_do_not_write == 1)
+	    continue;
+
+
+	/* write to argv[1] */
+	if (reiserfs_llseek (fd, (loff_t)blocknumber32 * (loff_t)blocksize, SEEK_SET) == (loff_t)-1) {
+	    perror ("llseek failed");
+	    return 1;
+	}
+	if (write (fd, buf, reclen16) != reclen16) {
+	    perror ("write failed");
+	    return 1;
+	}
+    }
+
+    fprintf (stderr, "done\n");
+    return 0;
+}
diff -urN linux/fs/reiserfs/utils/fsck/Makefile /tmp/linux/fs/reiserfs/utils/fsck/Makefile
--- linux/fs/reiserfs/utils/fsck/Makefile	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/fsck/Makefile	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,52 @@
+VPATH = ../bin
+vpath %.c $(REISERFS_KERNEL_SOURCE) $(REISERFS_LIB)
+
+# kernel files needed for reiserfsck
+KERNEL_C = fix_node.c do_balan.c lbalance.c ibalance.c stree.c prints.c buffer.c hashes.c
+KERNEL_OBJS = fix_node.o do_balan.o lbalance.o ibalance.o stree.o prints.o buffer.o hashes.o
+
+# files from utils's lib directory needed for reiserfsck
+LIB_C = misc.c version.c io.c reiserfs.c
+LIB_OBJS = misc.o version.o io.o reiserfs.o
+
+# fsck's files
+FSCK_C = main.c pass1.c pass2.c semantic.c pass4.c noname.c ubitmap.c info.c check.c ufile.c ustree.c uobjectid.c segments.c check_tree.c journal.c lost+found.c
+FSCK_OBJS = main.o pass1.o pass2.o semantic.o pass4.o noname.o ubitmap.o info.o check.o ufile.o ustree.o uobjectid.o segments.o check_tree.o journal.o lost+found.o $(LIB_OBJS) $(KERNEL_OBJS)
+
+
+FSCK = $(TMPBINDIR)/reiserfsck
+
+.c.o:
+	$(CC)  -DREISERFS_FSCK $(CFLAGS) $<
+
+all: $(FSCK)
+
+$(FSCK): $(FSCK_OBJS)
+	$(CC) $(LFLAGS) -o $(FSCK) $(FSCK_OBJS)
+
+clean:
+	rm -f *.o $(FSCK) *~ TAGS .depend
+
+dep:
+	gcc -MM $(IDIRS) *.c ../../*.c > .depend
+
+install:
+	cp -f $(FSCK) $(SBIN)
+	if [ -d $(MANDIR) ] ; then cp reiserfsck.8 $(MANDIR) ; gzip -f -9 $(MANDIR)/reiserfsck.8 ; fi
+
+uninstall:
+	rm -f $(MANDIR)/reiserfsck.8.gz $(SBIN)/reiserfsck
+
+
+ifeq (.depend,$(wildcard .depend))
+include .depend
+endif
+
+
+
+
+
+
+
+
+
diff -urN linux/fs/reiserfs/utils/fsck/check.c /tmp/linux/fs/reiserfs/utils/fsck/check.c
--- linux/fs/reiserfs/utils/fsck/check.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/fsck/check.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,484 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+#include "fsck.h"
+#include "reiserfs.h"
+
+
+int check_file_system ()
+{
+  return 0;
+}
+
+/* this goes through buffers checking delimiting keys
+ */
+
+struct buffer_head * g_left = 0;
+struct buffer_head * g_right = 0;
+struct key * g_dkey = 0;
+
+
+static void check_directory_item (struct item_head * ih, struct buffer_head * bh)
+{
+  int i;
+  struct reiserfs_de_head * deh;
+
+  for (i = 0, deh = B_I_DEH (bh, ih); i < I_ENTRY_COUNT (ih) - 1; i ++)
+    if (deh[i].deh_offset > deh[i + 1].deh_offset)
+      die ("check_directory_item: entries are not sorted properly");
+}
+
+
+static void check_items (struct buffer_head * bh)
+{
+  int i;
+  struct item_head * ih;
+
+  for (i = 0, ih = B_N_PITEM_HEAD (bh, i); i < B_NR_ITEMS (bh); i ++, ih) {
+    if (I_IS_DIRECTORY_ITEM (ih))
+      check_directory_item (ih, bh);
+  }
+}
+
+
+static void compare_neighboring_leaves_in_pass1 (void)
+{
+  struct key * left = B_N_PKEY (g_left, B_NR_ITEMS (g_left) - 1);
+
+
+  if (comp_keys (left, B_N_PKEY (g_right, 0)) != SECOND_GREATER)
+    die ("compare_neighboring_leaves_in_pass1: left key is greater, that the right one");
+
+  if (/*comp_keys (B_PRIGHT_DELIM_KEY (g_left), g_dkey) == FIRST_GREATER ||*/
+      comp_keys (g_dkey, B_N_PKEY (g_right, 0)) != KEYS_IDENTICAL) {
+    reiserfs_panic (0, "compare_neighboring_leaves_in_pass1: left's rdkey %k, dkey %k, first key in right %k",
+		    B_PRIGHT_DELIM_KEY (g_left), g_dkey, B_N_PKEY (g_right, 0));
+  }
+  
+  check_items (g_left);
+
+/*&&&&&&&&&&&&&&&&&&&&&&&&&&
+  for (i = 0, ih = B_N_PITEM_HEAD (g_left, i); i < B_NR_ITEMS (g_left); i ++, ih ++)
+    if (is_item_accessed (ih) == YES)
+      die ("compare_neighboring_leaves_in_pass1: item marked as accessed in g_left");
+  for (i = 0, ih = B_N_PITEM_HEAD (g_right, i); i < B_NR_ITEMS (g_right); i ++, ih ++)
+    if (is_item_accessed (ih) == YES)
+      die ("compare_neighboring_leaves_in_pass1: item marked as accessed in g_right");
+&&&&&&&&&&&&&&&&&&&&&&&&&&&*/
+    
+}
+
+
+static void is_there_unaccessed_items (struct buffer_head * bh)
+{
+  int i;
+  struct item_head * ih;
+
+  ih = B_N_PITEM_HEAD (bh, 0);
+  for (i = 0; i < B_NR_ITEMS (bh); i ++, ih ++) {
+    if (is_objectid_used (ih->ih_key.k_objectid) == 0)
+      die ("is_there_unaccessed_items: %lu is not marked as used", ih->ih_key.k_objectid);
+      
+    if (is_item_accessed (ih) == 0) {
+      print_block (bh, 1, -1, -1);
+      die ("is_there_unaccessed_items: unaccessed item found");
+    }
+  }
+}
+
+
+static void compare_neighboring_leaves_after_all (void)
+{
+  struct key * left = B_N_PKEY (g_left, B_NR_ITEMS (g_left) - 1);
+  struct key * right = B_N_PKEY (g_right, 0);
+
+  /*
+  if (comp_keys (left, B_PRIGHT_DELIM_KEY (g_left)) != SECOND_GREATER)
+    die ("compare_neighboring_leaves_after_all: invalid right delimiting key");
+  */
+  if (comp_keys (left, B_N_PKEY (g_right, 0)) != SECOND_GREATER)
+    die ("compare_neighboring_leaves_after_all: left key is greater, that the right one");
+
+  if (/*comp_keys (B_PRIGHT_DELIM_KEY (g_left), g_dkey) != KEYS_IDENTICAL ||*/
+      comp_keys (g_dkey, B_N_PKEY (g_right, 0)) != KEYS_IDENTICAL) {
+    reiserfs_panic (0, "compare_neighboring_leaves_after all: invalid delimiting keys from left to right (%k %k %k)",
+		    B_PRIGHT_DELIM_KEY (g_left), g_dkey, B_N_PKEY (g_right, 0));
+  }
+
+  if (comp_short_keys (left, right) == KEYS_IDENTICAL) {
+    if (KEY_IS_DIRECT_KEY (left) || KEY_IS_INDIRECT_KEY (left))
+      if (right->k_offset != left->k_offset + I_BYTES_NUMBER (B_N_PITEM_HEAD (g_left, B_NR_ITEMS (g_left) - 1), g_sb.s_blocksize))
+	die ("compare_neighboring_leaves_after all: hole between items or items are overlapped");
+  }
+
+  is_there_unaccessed_items (g_left);
+  
+}
+
+
+typedef	void (check_function_t)(void);
+
+static void reiserfsck_check_tree (int dev, int block, int size, check_function_t comp_func)
+{
+  struct buffer_head * bh;
+
+  bh = bread (dev, block, size);
+
+  if (!B_IS_IN_TREE (bh)) {
+    reiserfs_panic (0, "reiserfsck_check_tree: buffer (%b %z) not in tree", bh, bh);
+  }
+
+  if (not_formatted_node (bh->b_data, bh->b_size))
+    die ("Not formatted node");
+  if (!is_block_used (bh->b_blocknr))
+    die ("Not marked as used");
+  if (is_leaf_node (bh->b_data) && is_leaf_bad (bh))
+    die ("Bad leaf");
+  if (is_internal_node (bh->b_data) && is_internal_bad (bh))
+    die ("bad internal");
+
+#if 0
+    || !is_block_used (bh->b_blocknr) ||
+      (is_leaf_node (bh->b_data) && is_leaf_bad (bh)) ||
+      (is_internal_node (bh->b_data) && is_internal_bad (bh)))
+    die ("reiserfsck_check_tree: bad node in the tree");
+#endif
+
+  if (B_IS_KEYS_LEVEL (bh)) {
+    int i;
+    struct disk_child * dc;
+
+    dc = B_N_CHILD (bh, 0);
+    for (i = 0; i <= B_NR_ITEMS (bh); i ++, dc ++) {
+      reiserfsck_check_tree (dev, dc->dc_block_number, size, comp_func);
+      g_dkey = B_N_PDELIM_KEY (bh, i);
+    }
+  } else if (B_IS_ITEMS_LEVEL (bh)) {
+    g_right = bh;
+    if (g_left != 0 && g_dkey != 0) {
+      comp_func ();
+      brelse (g_left);
+    }
+    g_left = g_right;
+    return;
+  } else {
+    print_block (bh, 0, -1, -1);
+    reiserfs_panic (0, "reiserfsck_check_tree: bad block type");
+  }
+  brelse (bh);
+}
+
+static void reiserfsck_check_cached_tree (int dev, int block, int size)
+{
+  struct buffer_head * bh;
+
+  bh = find_buffer (dev, block, size);
+  if (bh == 0)
+    return;
+  if (!buffer_uptodate (bh)) {
+    die ("reiserfsck_check_cached_tree: found notuptodate buffer");
+  }
+  bh->b_count ++;
+
+  if (!B_IS_IN_TREE (bh)) {
+    die ("reiserfsck_check_cached_tree: buffer (%b %z) not in tree", bh, bh);
+  }
+
+  if (not_formatted_node (bh->b_data, bh->b_size) || !is_block_used (bh->b_blocknr) ||
+      (is_leaf_node (bh->b_data) && is_leaf_bad (bh)) ||
+      (is_internal_node (bh->b_data) && is_internal_bad (bh)))
+    die ("reiserfsck_check_cached_tree: bad node in the tree");
+  if (B_IS_KEYS_LEVEL (bh)) {
+    int i;
+    struct disk_child * dc;
+
+    dc = B_N_CHILD (bh, 0);
+    for (i = 0; i <= B_NR_ITEMS (bh); i ++, dc ++) {
+      reiserfsck_check_cached_tree (dev, dc->dc_block_number, size);
+      g_dkey = B_N_PDELIM_KEY (bh, i);
+    }
+  } else if (B_IS_ITEMS_LEVEL (bh)) {
+    /*    g_right = bh;
+    if (g_left != 0 && g_dkey != 0) {
+      comp_func ();
+      brelse (g_left);
+    }
+    g_left = g_right;*/
+    brelse (bh);
+    return;
+  } else {
+    print_block (bh, 0, -1, -1);
+    reiserfs_panic (0, "reiserfsck_check_cached_tree: bad block type");
+  }
+  brelse (bh);
+}
+
+
+void reiserfsck_tree_check (check_function_t how_to_compare_neighbors)
+{
+  g_left = 0;
+  g_dkey = 0;
+  reiserfsck_check_tree (g_sb.s_dev, SB_ROOT_BLOCK (&g_sb), g_sb.s_blocksize, how_to_compare_neighbors);
+  brelse (g_right);
+}
+
+
+void reiserfsck_check_pass1 ()
+{
+/*  if (opt_check == 1)*/
+    reiserfsck_tree_check (compare_neighboring_leaves_in_pass1);
+}
+
+void check_cached_tree ()
+{
+  reiserfsck_check_cached_tree (g_sb.s_dev, SB_ROOT_BLOCK (&g_sb), g_sb.s_blocksize);
+}
+
+void reiserfsck_check_after_all ()
+{
+  reiserfsck_tree_check (compare_neighboring_leaves_after_all);
+}
+
+
+#if 0
+/* returns 1 if buf looks like a leaf node, 0 otherwise */
+static int is_leaf (char * buf, int blocksize)
+{
+  struct block_head * blkh;
+  struct item_head * ih;
+  int used_space;
+  int prev_location;
+  int i;
+
+  blkh = (struct block_head *)buf;
+  ih = (struct item_head *)(buf + BLKH_SIZE) + blkh->blk_nr_item - 1;
+  used_space = BLKH_SIZE + IH_SIZE * blkh->blk_nr_item + (blocksize - ih->ih_item_location);
+  if (used_space != blocksize - blkh->blk_free_space)
+    return 0;
+  ih = (struct item_head *)(buf + BLKH_SIZE);
+  prev_location = blocksize;
+  for (i = 0; i < blkh->blk_nr_item; i ++, ih ++) {
+    if (ih->ih_item_location >= blocksize || ih->ih_item_location < IH_SIZE * blkh->blk_nr_item)
+      return 0;
+    if (ih->ih_item_len < 1 || ih->ih_item_len > MAX_ITEM_LEN (blocksize))
+      return 0;
+    if (prev_location - ih->ih_item_location != ih->ih_item_len)
+      return 0;
+    prev_location = ih->ih_item_location;
+  }
+
+  return 1;
+}
+
+
+/* returns 1 if buf looks like an internal node, 0 otherwise */
+static int is_internal (char * buf, int blocksize)
+{
+  struct block_head * blkh;
+  int used_space;
+
+  blkh = (struct block_head *)buf;
+  used_space = BLKH_SIZE + KEY_SIZE * blkh->blk_nr_item + DC_SIZE * (blkh->blk_nr_item + 1);
+  if (used_space != blocksize - blkh->blk_free_space)
+    return 0;
+  return 1;
+}
+
+
+/* sometimes unfomatted node looks like formatted, if we check only
+   block_header. This is the reason, why it is so complicated. We
+   believe only when free space and item locations are ok 
+   */
+int not_formatted_node (char * buf, int blocksize)
+{
+  struct block_head * blkh;
+
+  blkh = (struct block_head *)buf;
+
+  if (blkh->blk_level == FREE_LEVEL ||
+      blkh->blk_level < DISK_LEAF_NODE_LEVEL || blkh->blk_level > MAX_HEIGHT)
+    /* blk_level is out of range */
+    return 1;
+
+  if (blkh->blk_nr_item < 1 || blkh->blk_nr_item > (blocksize - BLKH_SIZE) / IH_SIZE)
+    /* item number is out of range */
+    return 1;
+
+  if (blkh->blk_free_space > blocksize - BLKH_SIZE - IH_SIZE)
+    /* free space is out of range */
+    return 1;
+
+  /* check format of nodes, such as we are not sure, that this is formatted node */
+  if (blkh->blk_level == DISK_LEAF_NODE_LEVEL)
+    return (is_leaf (buf, blocksize) == 1) ? 0 : 1;
+  return (is_internal (buf, blocksize) == 1) ? 0 : 1;
+}
+#endif
+
+
+
+int is_internal_node (char * buf)
+{
+  struct block_head * blkh;
+  
+  blkh = (struct block_head *)buf;
+  if (blkh->blk_level != DISK_LEAF_NODE_LEVEL)
+    return 1;
+  return 0;
+}
+
+int is_leaf_node (char * buf)
+{
+  struct block_head * blkh;
+
+  blkh = (struct block_head *)buf;
+  if (blkh->blk_level == DISK_LEAF_NODE_LEVEL)
+    return 1;
+  return 0;
+}
+
+static int is_bad_sd (struct item_head * ih, char * item)
+{
+  struct stat_data * sd = (struct stat_data *)item;
+
+  if (!S_ISDIR (sd->sd_mode) && !S_ISREG(sd->sd_mode) &&
+      !S_ISCHR (sd->sd_mode) && !S_ISBLK(sd->sd_mode) &&
+      !S_ISLNK (sd->sd_mode) && !S_ISFIFO(sd->sd_mode) &&
+      !S_ISSOCK(sd->sd_mode)) {
+    if (opt_verbose)
+      reiserfs_warning ("file %k unexpected mode encountered 0%o\n", &ih->ih_key, sd->sd_mode);
+  }
+  return 0;
+}
+
+
+
+static int is_bad_directory (struct item_head * ih, char * item, int blocksize)
+{
+  int i;
+  int namelen;
+  struct reiserfs_de_head * deh = (struct reiserfs_de_head *)item;
+  __u32 prev_offset = 0;
+  __u16 prev_location = 0xffff;
+
+  for (i = 0; i < I_ENTRY_COUNT (ih); i ++) {
+    namelen = I_DEH_N_ENTRY_FILE_NAME_LENGTH (ih, deh + i, i);
+    if (namelen > REISERFS_MAX_NAME_LEN (blocksize)) {
+      return 1;
+    }
+    if (deh[i].deh_offset <= prev_offset) {
+      return 1;
+    }
+    prev_offset = deh[i].deh_offset;
+
+    if (deh[i].deh_location >= prev_location) {
+      return 1;
+    }
+  }
+
+  return 0;
+}
+
+
+#include <sys/ioctl.h>
+#include <sys/mount.h>
+
+
+int blocks_on_device (int dev, int blocksize)
+{
+int size;
+
+  if (ioctl (dev, BLKGETSIZE, &size) >= 0) {
+    return  size / (blocksize / 512);
+  }
+  if (ioctl (dev, BLKGETSIZE, &size) >= 0) {
+    return  size / (blocksize / 512);
+  } else {
+    struct stat stat_buf;
+    memset(&stat_buf, '\0', sizeof(struct stat));
+    if(fstat(dev, &stat_buf) >= 0) {
+      return stat_buf.st_size / (blocksize / 512);
+    } else {
+      die ("can not calculate device size\n");
+    }
+  }
+  return 0;
+}
+
+
+/* change incorrect block adresses by 0. Do not consider such item as incorrect */
+static int is_bad_indirect (struct item_head * ih, char * item, int dev, int blocksize)
+{
+  int i;
+  int bad = 0;
+  int blocks;
+
+  if (ih->ih_item_len % UNFM_P_SIZE) {
+    if (opt_verbose)
+      reiserfs_warning ("indirect item of %h of invalid length");
+    return 1;
+  }
+  blocks = blocks_on_device (dev, blocksize);
+  
+  for (i = 0; i < I_UNFM_NUM (ih); i ++) {
+    __u32 * ind = (__u32 *)item;
+
+    if (ind[i] >= blocks) {
+      bad ++;
+      ind[i] = 0;
+      continue;
+    }
+  }
+  return 0;
+}
+
+
+int is_bad_item (struct item_head * ih, char * item, int blocksize, int dev)
+{
+  if (I_IS_STAT_DATA_ITEM (ih))
+    return is_bad_sd (ih, item);
+
+  if (I_IS_DIRECTORY_ITEM (ih))
+    return is_bad_directory (ih, item, blocksize);
+
+  if (I_IS_INDIRECT_ITEM (ih))
+    return is_bad_indirect (ih, item, dev, blocksize);
+
+  return 0;
+}
+
+
+/* only directory item can be fatally bad */
+int is_leaf_bad (struct buffer_head * bh)
+{
+  int i;
+  struct item_head * ih;
+
+  if (!is_leaf_node (bh->b_data))
+    return 0;
+  for (i = 0, ih = B_N_PITEM_HEAD (bh,  0); i < B_NR_ITEMS (bh); i ++, ih ++)
+    if (is_bad_item (ih, B_I_PITEM (bh, ih), bh->b_size, bh->b_dev))
+      return 1;
+  return 0;
+}
+
+int is_internal_bad (struct buffer_head * bh)
+{
+  struct key * key;
+  int i;
+  
+  if (!is_internal_node (bh->b_data))
+    return 0;
+  for (i = 0; i < B_NR_ITEMS (bh); i ++) {
+    key = B_N_PDELIM_KEY (bh, i);
+    if (//key->k_dir_id >= key->k_objectid ||
+	(key->k_uniqueness != 500 && key->k_uniqueness != (__u32)-1 && key->k_uniqueness != (__u32)-2 &&
+	 key->k_uniqueness != 0))
+      return 1;
+  }
+  return 0;
+
+}
+
+
diff -urN linux/fs/reiserfs/utils/fsck/check_tree.c /tmp/linux/fs/reiserfs/utils/fsck/check_tree.c
--- linux/fs/reiserfs/utils/fsck/check_tree.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/fsck/check_tree.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,698 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+#include "fsck.h"
+#include "reiserfs.h"
+
+
+//
+//
+//  check S+ tree of the file system 
+//
+// check_fs_tree stops and recommends to run fsck --rebuild-tree when:
+// 1. read fails
+// 2. node of wrong level found in the tree
+// 3. something in the tree points to wrong block number
+//      out of filesystem boundary is pointed by tree
+//      to block marked as free in bitmap
+//      the same block is pointed from more than one place
+//      not data blocks (journal area, super block, bitmaps)
+// 4. bad formatted node found
+// 5. delimiting keys are incorrect
+//      
+
+
+
+/* to make sure, that no blocks are pointed to from more than one
+   place we use additional bitmap (control_bitmap). If we see pointer
+   to a block we set corresponding bit to 1. If it is set already -
+   run fsck with --rebuild-tree */
+static char ** control_bitmap;
+/* will compare with what does super_block say */
+int used_blocks = 0;
+
+
+/* 1 if block is not marked as used in the bitmap */
+static int is_block_free (struct super_block * s, blocknr_t block)
+{
+    int i, j;
+    char * bitmap;
+
+    i = block / (s->s_blocksize * 8);
+    j = block % (s->s_blocksize * 8);
+
+    if (opt_fsck_mode == FSCK_DEFAULT)
+	bitmap = SB_AP_BITMAP (s)[i]->b_data;
+    else
+	bitmap = g_new_bitmap[i];
+    return !test_bit (j, bitmap);
+    
+}
+
+
+/* we have seen this block in the tree, mark corresponding bit in the
+   control bitmap */
+static void we_met_it (struct super_block * s, blocknr_t block)
+{
+    int i, j;
+    
+    used_blocks ++;
+    i = block / (s->s_blocksize * 8);
+    j = block % (s->s_blocksize * 8);
+    return set_bit (j, control_bitmap [i]);
+}
+
+
+/* have we seen this block somewhere in the tree before? */
+static int did_we_meet_it (struct super_block * s, blocknr_t block)
+{
+    int i, j;
+    
+    i = block / (s->s_blocksize * 8);
+    j = block % (s->s_blocksize * 8);
+    return test_bit (j, control_bitmap [i]);
+}
+
+
+static void init_control_bitmap (struct super_block * s)
+{
+    int i;
+
+    control_bitmap = getmem (sizeof (char *) * SB_BMAP_NR (s));
+    for (i = 0; i < SB_BMAP_NR (s); i ++) {
+	control_bitmap[i] = getmem (s->s_blocksize);
+	memset (control_bitmap[i], 0, s->s_blocksize);
+    }
+    
+    /* skipped and super block */
+    for (i = 0; i <= SB_BUFFER_WITH_SB (s)->b_blocknr; i ++)
+	we_met_it (s, i);
+    
+    /* bitmaps */
+    for (i = 0; i < SB_BMAP_NR (s); i ++)
+	we_met_it (s, SB_AP_BITMAP (s)[i]->b_blocknr);
+
+    for (i = 0; i < get_journal_size (s) + 1; i ++)
+	we_met_it (s, i + get_journal_start (s));
+
+
+    /* unused space of last bitmap is filled by 1s */
+    for (i = SB_BMAP_NR (s) * s->s_blocksize * 8; --i >= SB_BLOCK_COUNT (s); ) {
+	we_met_it (s, i);
+	used_blocks --;
+    }
+}
+
+
+#if 0
+static void print_bmap_block (int i, char * data, int silent)
+{
+    int j, k;
+    int bits = g_sb.s_blocksize * 8;
+    int zeros = 0, ones = 0;
+  
+    printf ("#%d: ", i);
+
+    if (test_bit (0, data)) {
+	/* first block addressed by this bitmap block is used */
+	ones ++;
+	if (!silent)
+	    printf ("Busy (%d-", i * bits);
+	for (j = 1; j < bits; j ++) {
+	    while (test_bit (j, data)) {
+		ones ++;
+		if (j == bits - 1) {
+		    if (!silent)
+			printf ("%d)\n", j + i * bits);
+		    goto end;
+		}
+		j++;
+	    }
+	    if (!silent)
+		printf ("%d) Free(%d-", j - 1 + i * bits, j + i * bits);
+
+	    while (!test_bit (j, data)) {
+		zeros ++;
+		if (j == bits - 1) {
+		    if (!silent)
+			printf ("%d)\n", j + i * bits);
+		    goto end;
+		}
+		j++;
+	    }
+	    if (!silent)
+		printf ("%d) Busy(%d-", j - 1 + i * bits, j + i * bits);
+
+	    j --;
+	end:
+	}
+    } else {
+	/* first block addressed by this bitmap is free */
+	zeros ++;
+	if (!silent)
+	    printf ("Free (%d-", i * bits);
+	for (j = 1; j < bits; j ++) {
+	    k = 0;
+	    while (!test_bit (j, data)) {
+		k ++;
+		if (j == bits - 1) {
+		    if (!silent)
+			printf ("%d)\n", j + i * bits);
+		    zeros += k;
+		    goto end2;
+		}
+		j++;
+	    }
+	    zeros += k;
+	    if (!silent)
+		printf ("%d) Busy(%d-", j - 1 + i * bits, j + i * bits);
+	    
+	    k = 0;
+	    while (test_bit (j, data)) {
+		ones ++;
+		if (j == bits - 1) {
+		    if (!silent)
+			printf ("%d)\n", j + i * bits);
+		    ones += k;
+		    goto end2;
+		}
+		j++;
+	    }
+	    ones += k;
+	    if (!silent)
+		printf ("%d) Busy(%d-", j - 1 + i * bits, j + i * bits);
+	
+	    j --;
+	end2:
+	}
+    }
+
+    printf ("used %d, free %d\n", ones, zeros);
+}
+#endif
+
+static void show_diff (int n, char * disk, char * control, int bits)
+{
+    int i;
+    int last_diff = 0;
+    int from = 0, num = 0;
+    
+    for (i = 0; i < bits; i ++) {
+	if (test_bit (i, disk) && !test_bit (i, control)) {
+	    if (last_diff == 1) {
+		num ++;
+		continue;
+	    } else if (last_diff == 2) {
+		printf ("Block [%d-%d] free in disk bitmap, used in control\n", from, from + num - 1);
+	    }
+	    num = 1;
+	    from = n * bits + i;
+	    last_diff = 1;
+	    continue;
+	}
+	if (!test_bit (i, disk) && test_bit (i, control)) {
+	    if (last_diff == 2) {
+		num ++;
+		continue;
+	    } else if (last_diff == 1) {
+		printf ("Block [%d-%d] used in disk bitmap, free in control\n", from, from + num - 1);
+	    }
+	    num = 1;
+	    from = n * bits + i;
+	    last_diff = 2;
+	    continue;
+	}
+	/* the same bits */
+	if (last_diff == 1)
+	    printf ("Block [%d-%d] used in disk bitmap, free in control\n", from, from + num - 1);
+	if (last_diff == 2)
+	    printf ("Block [%d-%d] free in disk bitmap, used in control\n", from, from + num - 1);
+	    
+	num = 0;
+	from = 0;
+	last_diff = 0;
+	continue;
+    }
+}
+
+static void compare_bitmaps (struct super_block * s)
+{
+    int i, wrong_bitmap = 0;
+    char * bitmap;
+
+    printf ("Comparing bitmaps..");
+
+    if (SB_FREE_BLOCKS (s) != SB_BLOCK_COUNT (s) - used_blocks) {
+	printf ("\nUsed blocks %d, super block version %d",
+		used_blocks, SB_BLOCK_COUNT (s) - SB_FREE_BLOCKS (s));
+	wrong_bitmap = 1;
+    }
+
+    for (i = 0; i < SB_BMAP_NR (s); i ++) {
+	if (opt_fsck_mode == FSCK_DEFAULT)
+	    /* we are read-only checking the partition, check this
+               bitmap */
+	    bitmap = SB_AP_BITMAP(s)[i]->b_data;
+	else
+	    /* we are re-building the tree, bitmap for check is here */
+	    bitmap = g_new_bitmap [i];
+
+	if (memcmp (bitmap, control_bitmap[i], s->s_blocksize)) {
+	    printf ("\nbitmap %d does not match to the correct one", i);
+	    if (opt_verbose) {
+		printf ("\nSee diff");
+		show_diff (i, bitmap, control_bitmap[i], s->s_blocksize * 8);
+	    }
+	    wrong_bitmap = 1;
+	}
+    }
+    if (wrong_bitmap)
+	reiserfs_panic (s, "\nRun reiserfsck with --rebuild-tree (or rewrite correct bitmap)\n");
+    
+    printf ("ok\n");
+}
+
+
+
+
+
+
+/* is this block legal to be pointed to by some place of the tree? */
+static int bad_block_number (struct super_block * s, blocknr_t block)
+{
+    if (block >= SB_BLOCK_COUNT (s)) {
+	reiserfs_warning ("block out of filesystem boundary found\n");
+	return 1;
+    }
+
+    if (not_data_block (s, block)) {
+	reiserfs_warning ("not data block is used in the tree\n");
+	return 1;
+    }
+
+    if (is_block_free (s, block)) {
+	reiserfs_warning ("block %lu is not marked as used in the disk bitmap\n",
+			  block);
+	return 1;
+    }
+
+    if (did_we_meet_it (s, block)) {
+	reiserfs_warning ("block %lu is in tree already\n", block);
+	return 1;
+    }
+
+    we_met_it (s, block);
+    return 0;
+}
+
+
+/* 1 if some of fields in the block head of bh look bad */
+static int bad_block_head (struct buffer_head * bh)
+{
+    struct block_head * blkh;
+
+    blkh = B_BLK_HEAD (bh);
+    if (__le16_to_cpu (blkh->blk_nr_item) > (bh->b_size - BLKH_SIZE) / IH_SIZE) {
+	reiserfs_warning ("block %lu has wrong blk_nr_items (%z)\n", 
+			  bh->b_blocknr, bh);
+	return 1;
+    }
+    if (__le16_to_cpu (blkh->blk_free_space) > 
+	bh->b_size - BLKH_SIZE - IH_SIZE * __le16_to_cpu (blkh->blk_nr_item)) {
+	reiserfs_warning ("block %lu has wrong blk_free_space %z\n", 
+			  bh->b_blocknr, bh);
+	return 1;
+    }
+    return 0;
+}
+
+
+/* 1 if it does not look like reasonable stat data */
+static int bad_stat_data (struct buffer_head * bh, struct item_head * ih)
+{
+    if (!is_objectid_used (ih->ih_key.k_objectid)) {
+	// FIXME: this could be cured right here
+	reiserfs_warning ("%lu is marked free, but used by an object");
+	return 1;
+    }
+    return 0;
+}
+
+
+/* it looks like we can check item length only */
+static int bad_direct_item (struct buffer_head * bh, struct item_head * ih)
+{
+    return 0;
+}
+
+
+/* each unformatted node pointer*/
+static int bad_indirect_item (struct super_block * s, struct buffer_head * bh,
+			      struct item_head * ih)
+{
+    int i;
+    __u32 * ind = (__u32 *)B_I_PITEM (bh, ih);
+
+    if (__le16_to_cpu (ih->ih_item_len) % 4)
+	return 1;
+    for (i = 0; i < I_UNFM_NUM (ih); i ++) {
+	/* check unformatted node pointer and mark it used in the
+           control bitmap */
+	if (ind[i] && bad_block_number (s, __le32_to_cpu (ind[i])))
+	    return 1;
+    }
+    /* delete this check for 3.6 */
+    if (ih->u.ih_free_space > s->s_blocksize - 1)
+	reiserfs_warning ("%h has wrong wong ih_free_space\n");
+    return 0;
+}
+
+
+/* check entry count and locations of all names */
+static int bad_directory_item (struct buffer_head * bh, struct item_head * ih)
+{
+    int i;
+    struct reiserfs_de_head * deh;
+
+
+    if (I_ENTRY_COUNT (ih) > __le16_to_cpu (ih->ih_item_len) / (DEH_SIZE + 1))
+	return 1;
+
+    deh = B_I_DEH (bh, ih);
+    for (i = 0; i < I_ENTRY_COUNT (ih); i ++, deh ++) {
+	if (__le16_to_cpu (deh->deh_location) >= __le16_to_cpu (ih->ih_item_len))
+	    return 1;
+	if (i && __le16_to_cpu (deh->deh_location) >= __le16_to_cpu ((deh-1)->deh_location))
+	    return 1;
+	if ((ih->ih_key.k_objectid != REISERFS_ROOT_OBJECTID && deh_dir_id (deh) == 0) ||
+	    deh_offset (deh) == 0 || deh_objectid (deh) == 0 || 
+	    deh_dir_id (deh) == deh_objectid (deh))
+	    return 1;
+    }
+    return 0;
+}
+
+
+static int bad_item (struct super_block * s, struct buffer_head * bh, int i)
+{
+    struct item_head * ih;
+
+    ih = B_N_PITEM_HEAD (bh, i);
+
+    if (I_IS_STAT_DATA_ITEM (ih))
+	return bad_stat_data (bh, ih);
+
+    if (I_IS_DIRECT_ITEM (ih))
+	return bad_direct_item (bh, ih);
+
+    if (I_IS_INDIRECT_ITEM (ih))
+	return bad_indirect_item (s, bh, ih);
+    
+    return bad_directory_item (bh, ih);
+}
+
+
+/* 1 if i-th and (i-1)-th items can not be neighbors */
+static int bad_pair (struct super_block * s, struct buffer_head * bh, int i)
+{
+    struct item_head * ih;
+
+    ih = B_N_PITEM_HEAD (bh, i);
+
+    if (comp_keys (&((ih - 1)->ih_key), &ih->ih_key) != -1)
+	return 1;
+
+    if (I_IS_STAT_DATA_ITEM (ih))
+	/* left item must be of another object */
+	if (comp_short_keys (&((ih - 1)->ih_key), &ih->ih_key) != -1)
+	    return 1;
+    
+    if (I_IS_DIRECT_ITEM (ih)) {
+	/* left item must be indirect or stat data item of the same
+	   file */
+	if (comp_short_keys (&((ih - 1)->ih_key), &ih->ih_key) != 0)
+	    return 1;
+	if (!((I_IS_STAT_DATA_ITEM (ih - 1) && ih->ih_key.k_offset == 1) ||
+	      (I_IS_INDIRECT_ITEM (ih - 1) && 
+	       (ih - 1)->ih_key.k_offset + I_BYTES_NUMBER (ih - 1, s->s_blocksize) == 
+	       ih->ih_key.k_offset)))
+	    return 1;
+    }
+
+    if (I_IS_INDIRECT_ITEM (ih) || I_IS_DIRECTORY_ITEM (ih)) {
+	/* left item must be stat data of the same object */
+	if (comp_short_keys (&((ih - 1)->ih_key), &ih->ih_key) != 0)
+	    return 1;
+	if (!I_IS_STAT_DATA_ITEM (ih - 1))
+	    return 1;
+    }
+    
+    return 0;
+}
+ 
+
+/* 1 if block head or any of items is bad */
+static int bad_leaf (struct super_block * s, struct buffer_head * bh)
+{
+    int i;
+
+    if (bad_block_head (bh))
+	return 1;
+    
+    for (i = 0; i < B_NR_ITEMS (bh); i ++) {
+	if (bad_item (s, bh, i)) {
+	    reiserfs_warning ("block %lu has invalid item %d: %h\n",
+			      bh->b_blocknr, i, B_N_PITEM_HEAD (bh, i));
+	    return 1;
+	}
+	
+	if (i && bad_pair (s, bh, i)) {
+	    reiserfs_warning ("block %lu has wrong order of items\n", 
+			      bh->b_blocknr);
+	    return 1;
+	}
+    }
+    return 0;
+}
+
+
+/* 1 if bh does not look like internal node */
+static int bad_internal (struct super_block * s, struct buffer_head * bh)
+{
+    return 0;
+}
+
+
+/* bh must be formatted node. blk_level must be tree_height - h + 1 */
+static int bad_node (struct super_block * s, struct buffer_head * bh,
+		     int level)
+{
+    if (B_LEVEL (bh) != level) {
+	reiserfs_warning ("node with wrong level found in the tree\n");
+	return 1;
+    }
+
+    if (bad_block_number (s, bh->b_blocknr))
+	return 1;
+    
+    if (B_IS_ITEMS_LEVEL (bh))
+	return bad_leaf (s, bh);
+
+    return bad_internal (s, bh);
+}
+
+
+/* internal node bh must point to block */
+static int get_pos (struct buffer_head * bh, blocknr_t block)
+{
+    int i;
+
+    for (i = 0; i <= B_NR_ITEMS (bh); i ++) {
+	if (B_N_CHILD (bh, i)->dc_block_number == block)
+	    return i;
+    }
+    die ("get_pos: position for block %lu not found", block);
+    return 0;
+}
+
+
+/* path[h] - leaf node */
+static struct key * lkey (struct buffer_head ** path, int h)
+{
+    int pos;
+
+    while (h > 0) {
+	pos = get_pos (path[h - 1], path[h]->b_blocknr);
+	if (pos)
+	    return B_N_PDELIM_KEY(path[h - 1], pos - 1);
+	h --;
+    }
+    return 0;
+}
+
+
+/* path[h] - leaf node */
+static struct key * rkey (struct buffer_head ** path, int h)
+{
+    int pos;
+
+    while (h > 0) {
+	pos = get_pos (path[h - 1], path[h]->b_blocknr);
+	if (pos != B_NR_ITEMS (path[h - 1]))
+	    return B_N_PDELIM_KEY (path[h - 1], pos);
+	h --;
+    }
+    return 0;
+}
+
+
+/* are all delimiting keys correct */
+static int bad_path (struct buffer_head ** path)
+{
+    int h = 0 ;
+    struct key * dk;
+    
+    while (path[h]) 
+	h ++;    
+    h-- ;
+    dk = lkey (path, h);
+    if (dk && comp_keys (dk, B_N_PKEY (path[h], 0)))  {
+	reiserfs_warning("\nleft keys don't match %k, %k\n", dk, B_N_PKEY(path[h], 0)) ;
+	return 1;
+    } 
+    /*
+    dk = rkey (path, h);
+    if (dk && comp_keys (dk, B_PRIGHT_DELIM_KEY (path[h]))) {
+	reiserfs_warning("\nright keys don't match %k, %k\n", dk, B_PRIGHT_DELIM_KEY(path[h])) ;
+	return 1;
+    } 
+    */
+    return 0;
+}
+
+
+static inline blocknr_t first_child (struct buffer_head * bh)
+{
+    return B_N_CHILD (bh, 0)->dc_block_number;
+}
+
+
+static inline blocknr_t last_child (struct buffer_head * bh)
+{
+    return B_N_CHILD (bh, B_NR_ITEMS (bh))->dc_block_number;
+}
+
+
+static inline blocknr_t next_child (struct buffer_head * child,
+				    struct buffer_head * parent)
+{
+    int i;
+    
+    for (i = 0; i < B_NR_ITEMS (parent); i ++) {
+	if (B_N_CHILD (parent, i)->dc_block_number == child->b_blocknr)
+	    return B_N_CHILD (parent, i + 1)->dc_block_number;
+    }
+    die ("next_child: no child found: should not happen");
+    return 0;
+}
+
+
+/* h == 0 for root level. block head's level == 1 for leaf level  */
+static inline int h_to_level (struct super_block * s, int h)
+{
+    return SB_TREE_HEIGHT (s) - h - 1;
+}
+
+
+static inline int leaf_level (struct buffer_head * bh)
+{
+    return B_LEVEL(bh) == DISK_LEAF_NODE_LEVEL;
+}
+
+
+static void print (int cur, int total)
+{
+  printf ("/%3d (of %3d)", cur, total);fflush (stdout);
+}
+
+
+/* erase /XXX(of XXX) */
+static void erase (void)
+{
+    printf ("\b\b\b\b\b\b\b\b\b\b\b\b\b");
+    printf ("             ");
+    printf ("\b\b\b\b\b\b\b\b\b\b\b\b\b");
+    fflush (stdout);
+}
+
+
+/* pass the S+ tree of filesystem */
+void check_fs_tree (struct super_block * s)
+{
+    struct buffer_head * path[MAX_HEIGHT] = {0,};
+    int total[MAX_HEIGHT] = {0,};
+    int cur[MAX_HEIGHT] = {0,};
+    int h = 0;
+    blocknr_t block = SB_ROOT_BLOCK (s);
+
+    uread_bitmaps (s);
+
+    init_control_bitmap (s);
+
+    printf ("Checking S+tree..");
+
+    while ( 1 ) {
+	if (path[h])
+	    die ("check_fs_tree: empty slot expected");
+	
+	if (h)
+	    print (cur[h - 1], total[h - 1]);
+
+	path[h] = bread (s->s_dev, block, s->s_blocksize);
+	if (path[h] == 0 || bad_node (s, path[h], h_to_level (s, h)))
+	    reiserfs_panic (s, "Run reiserfsck with --rebuild-tree\n");
+
+ 	if (leaf_level (path[h])) {
+	    if (bad_path (path))
+		reiserfs_panic (s, "Run reiserfsck with --rebuild-tree\n");
+
+	    brelse (path[h]);
+	    if (h)
+	      erase ();
+
+	    while (h && path[h]->b_blocknr == last_child (path[h - 1])) { 
+		path[h] = 0;
+		h --;
+/*		check_internal (path[h]);*/
+		brelse (path[h]);
+		if (h)
+		  erase ();
+	    }
+
+	    if (h == 0) {
+		path[h] = 0;
+		break;
+	    }
+
+	    cur[h - 1] ++;
+	    block = next_child (path[h], path[h-1]);
+	    path[h] = 0;
+	    continue; 
+	}
+	total[h] = B_NR_ITEMS (path[h]) + 1;
+	cur[h] = 1;
+	block = first_child (path[h]);
+	h ++;
+    }
+
+    /* S+ tree is correct (including all objects have correct
+       sequences of items) */
+    printf ("ok\n");
+    
+    /* compare created bitmap with the original */
+    compare_bitmaps (s);
+
+}
+
+
diff -urN linux/fs/reiserfs/utils/fsck/info.c /tmp/linux/fs/reiserfs/utils/fsck/info.c
--- linux/fs/reiserfs/utils/fsck/info.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/fsck/info.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,122 @@
+
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+#include "fsck.h"
+
+struct fsck_stat g_fsck_info = {0, };
+
+
+void add_event (int event)
+{
+  switch (event) {
+    /* tree building (pass 1 and 2) info */
+  case GOOD_LEAVES:
+    g_fsck_info.fs_good_leaves ++; break;
+  case UNINSERTABLE_LEAVES:
+    g_fsck_info.fs_uninsertable_leaves ++; break;
+  case REWRITTEN_FILES:
+    g_fsck_info.fs_rewritten_files ++; break;
+  case LEAVES_USED_BY_INDIRECT_ITEMS:
+    g_fsck_info.fs_leaves_used_by_indirect_items ++; break;
+  case UNFM_OVERWRITING_UNFM:
+    g_fsck_info.fs_unfm_overwriting_unfm ++; break;
+  case INDIRECT_TO_DIRECT:
+    g_fsck_info.fs_indirect_to_direct ++; break;
+
+    /* pass 3 info (semantic) */
+  case FIXED_SIZE_DIRECTORIES:
+    g_fsck_info.fs_fixed_size_directories ++; break;
+  case INCORRECT_REGULAR_FILES:
+    /* file has incorrect sequence of items (incorrect items are truncated) */
+    g_fsck_info.fs_incorrect_regular_files ++; break;
+  case FIXED_SIZE_FILES:
+    g_fsck_info.fs_fixed_size_files ++; break;
+
+    /* pass 4 info */
+  case UNACCESSED_ITEMS:
+    g_fsck_info.fs_unaccessed_items ++; break;
+  case FIXED_RIGHT_DELIM_KEY:
+    g_fsck_info.fs_fixed_right_delim_key ++; break;
+
+    /* file system info */
+  case STAT_DATA_ITEMS:
+    g_fsck_info.fs_stat_data_items ++; break;
+  case REGULAR_FILES:
+    g_fsck_info.fs_regular_files ++; break;
+  case DIRECTORIES:
+    g_fsck_info.fs_directories ++; break;
+  case SYMLINKS:
+    g_fsck_info.fs_symlinks ++; break;
+  case OTHERS:
+    g_fsck_info.fs_others ++; break;
+  }
+}
+
+
+int get_event (int event)
+{
+  switch (event) {
+  case GOOD_LEAVES:
+    return g_fsck_info.fs_good_leaves;
+  case UNINSERTABLE_LEAVES:
+    return g_fsck_info.fs_uninsertable_leaves;
+  case REGULAR_FILES:
+    return g_fsck_info.fs_regular_files;
+  case INCORRECT_REGULAR_FILES:
+    return g_fsck_info.fs_incorrect_regular_files;
+  case DIRECTORIES:
+    return g_fsck_info.fs_directories;
+  case FIXED_SIZE_DIRECTORIES:
+    return g_fsck_info.fs_fixed_size_directories;
+  case STAT_DATA_ITEMS:
+    return g_fsck_info.fs_stat_data_items;
+  }
+  return 0;
+}
+
+/* outputs information about inconsistencies */
+void output_information ()
+{
+  FILE * fp;
+  char buf[160];
+/*
+  if (opt_verbose == 0)
+    return;
+*/
+  fp = stderr;
+
+/*  time (&t);
+  fputs ("**** This is reiserfsck log file: created ", fp); fputs (ctime (&t), fp); fputs ("\n", fp);*/
+  fputs ("Building S+ tree info\n", fp);
+  sprintf (buf, "\tGood leaves: %d\n", g_fsck_info.fs_good_leaves); fputs (buf, fp);
+  sprintf (buf, "\tBad leaves: %d\n", g_fsck_info.fs_uninsertable_leaves); fputs (buf, fp);
+  sprintf (buf, "\tRewritten files: %d\n", g_fsck_info.fs_rewritten_files); fputs (buf, fp);
+  sprintf (buf, "\tLeaves pointed by indirect item: %d\n", g_fsck_info.fs_leaves_used_by_indirect_items); fputs (buf, fp);
+  sprintf (buf, "\tUnformatted nodes overwritten by direct items\nand then by other unformatted node: %d\n",
+	   g_fsck_info.fs_unfm_overwriting_unfm); fputs (buf, fp);
+  sprintf (buf, "\tIndirect_to_direct conversions: %d\n", g_fsck_info.fs_indirect_to_direct); fputs (buf, fp);
+
+  fputs ("Semantic pass info\n", fp);
+  sprintf (buf, "\tFiles with fixed size: %d\n", g_fsck_info.fs_fixed_size_files); fputs (buf, fp);
+  sprintf (buf, "\tDirectories with fixed size: %d\n", g_fsck_info.fs_fixed_size_directories); fputs (buf, fp);
+  sprintf (buf, "\tEntries pointing to nowhere (deleted): %d\n", g_fsck_info.fs_deleted_entries); fputs (buf, fp);
+
+  fputs ("Pass 4 info\n", fp);
+  sprintf (buf, "\tUnaccessed items found (and deleted): %d\n", g_fsck_info.fs_unaccessed_items); fputs (buf, fp);
+  sprintf (buf, "\tFixed right delimiting keys: %d\n", g_fsck_info.fs_fixed_right_delim_key); fputs (buf, fp);
+  sprintf (buf, "\tStat datas: %d\n", g_fsck_info.fs_stat_data_items); fputs (buf, fp);
+
+
+  fputs ("File system info\n", fp);
+  sprintf (buf, "\tFiles found: %d\n", g_fsck_info.fs_regular_files); fputs (buf, fp);
+  sprintf (buf, "\tDirectories found: %d\n", g_fsck_info.fs_directories); fputs (buf, fp);
+  sprintf (buf, "\tSymlinks found: %d\n", g_fsck_info.fs_symlinks); fputs (buf, fp);
+  sprintf (buf, "\tOthers: %d\n", g_fsck_info.fs_others); fputs (buf, fp);
+
+  /*fclose (fp);*/
+}
+
+
+
+
diff -urN linux/fs/reiserfs/utils/fsck/journal.c /tmp/linux/fs/reiserfs/utils/fsck/journal.c
--- linux/fs/reiserfs/utils/fsck/journal.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/fsck/journal.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,536 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+#include "fsck.h"
+#include <limits.h>
+#include "reiserfs.h"
+
+
+/* compares description block with commit block.  returns 1 if they differ, 0 if they are the same */
+static int journal_compare_desc_commit(struct super_block *p_s_sb, struct reiserfs_journal_desc *desc, 
+			               struct reiserfs_journal_commit *commit) {
+  if (commit->j_trans_id != desc->j_trans_id || commit->j_len != desc->j_len || commit->j_len > JOURNAL_TRANS_MAX || 
+      commit->j_len <= 0 
+  ) {
+    return 1 ;
+  }
+  return 0 ;
+}
+
+
+//
+// set up start journal block and journal size
+// make journal unreplayable by kernel replay routine
+//
+void reset_journal (struct super_block * s)
+{
+    int i ;
+    struct buffer_head *bh ;
+    int done = 0;
+    int len;
+    int start;
+
+    /* first block of journal */
+    s->u.reiserfs_sb.s_rs->s_journal_block = get_journal_start (s);
+    start = s->u.reiserfs_sb.s_rs->s_journal_block;
+
+    /* journal size */
+    s->u.reiserfs_sb.s_rs->s_orig_journal_size = get_journal_size (s);
+    len = s->u.reiserfs_sb.s_rs->s_orig_journal_size + 1;
+
+    printf ("Resetting journal - "); fflush (stdout);
+
+    for (i = 0 ; i < len ; i++) {
+	print_how_far (&done, len);
+	bh = getblk (s->s_dev, start + i, s->s_blocksize) ;
+	memset(bh->b_data, 0, s->s_blocksize) ;
+	mark_buffer_dirty(bh,0) ;
+	mark_buffer_uptodate(bh,0) ;
+	bwrite (bh);
+	brelse(bh) ;
+    }
+    printf ("\n"); fflush (stdout);
+    
+#if 0 /* need better way to make journal unreplayable */
+
+
+    /* have journal_read to replay nothing: look for first non-desc
+       block and set j_first_unflushed_offset to it */
+    {   
+	int offset;
+	struct buffer_head * bh, *jh_bh;
+	struct reiserfs_journal_header * j_head;
+	struct reiserfs_journal_desc * desc;
+
+
+	jh_bh = bread (s->s_dev, s->u.reiserfs_sb.s_rs->s_journal_block + s->u.reiserfs_sb.s_rs->s_orig_journal_size,
+		       s->s_blocksize);
+	j_head = (struct reiserfs_journal_header *)(jh_bh->b_data);
+
+	for (offset = 0; offset < s->u.reiserfs_sb.s_rs->s_orig_journal_size; offset ++) {
+	    bh = bread (s->s_dev, s->u.reiserfs_sb.s_rs->s_journal_block + offset, s->s_blocksize);
+	    desc = (struct reiserfs_journal_desc *)((bh)->b_data);
+	    if (memcmp(desc->j_magic, JOURNAL_DESC_MAGIC, 8)) {
+		/* not desc block found */
+		j_head->j_first_unflushed_offset = offset;
+		brelse (bh);
+		break;
+	    }
+	    brelse (bh);
+	}
+
+	mark_buffer_uptodate (jh_bh, 1);
+	mark_buffer_dirty (jh_bh, 1);
+	bwrite (jh_bh);
+	brelse (jh_bh);
+    }
+#endif
+}
+
+//
+// end of stolen from ./fs/reiserfs/journal.c
+//
+
+
+#define bh_desc(bh) ((struct reiserfs_journal_desc *)((bh)->b_data))
+#define bh_commit(bh) ((struct reiserfs_journal_commit *)((bh)->b_data))
+
+
+
+
+
+static int desc_block (struct buffer_head * bh)
+{
+    struct reiserfs_journal_desc * desc = (struct reiserfs_journal_desc *)bh->b_data;
+    if (!memcmp(desc->j_magic, JOURNAL_DESC_MAGIC, 8))
+	return 1;
+    return 0;
+}
+
+static int next_expected_desc (struct super_block * s, struct buffer_head * d_bh)
+{
+    int offset;
+    struct reiserfs_journal_desc * desc;
+
+    desc = (struct reiserfs_journal_desc *)d_bh->b_data;
+    offset = d_bh->b_blocknr - get_journal_start (s);
+    return get_journal_start (s) + ((offset + desc->j_len + 1 + 1) % JOURNAL_BLOCK_COUNT);
+}
+
+
+static int is_valid_transaction (struct super_block * s, struct buffer_head * d_bh)
+{
+    struct buffer_head * c_bh;
+    int offset;
+    struct reiserfs_journal_desc *desc  = (struct reiserfs_journal_desc *)d_bh->b_data;
+    struct reiserfs_journal_commit *commit ;
+
+
+    offset = d_bh->b_blocknr - get_journal_start (s);
+    
+    /* ok, we have a journal description block, lets see if the transaction was valid */
+    c_bh = bread (s->s_dev, next_expected_desc (s, d_bh) - 1,
+		  s->s_blocksize) ;
+    
+    commit = (struct reiserfs_journal_commit *)c_bh->b_data ;
+    if (journal_compare_desc_commit (s, desc, commit)) {
+/*	printf ("desc and commit block do not match\n");*/
+	brelse (c_bh) ;
+	return 0;
+    }
+    brelse (c_bh);
+    return 1;
+}
+
+
+int next_desc (struct super_block * s, int this)
+{
+    int j;
+    struct buffer_head * bh;
+    int retval;
+
+    j = this + 1;
+    do {
+	bh = bread (s->s_dev, (j % JOURNAL_BLOCK_COUNT), s->s_blocksize);
+	if (!desc_block (bh)) {
+	    j ++;
+	    brelse (bh);
+	    continue;
+	}
+/*	printf ("desc block found %lu, trans_id %ld, len %ld\n",
+		bh->b_blocknr, bh_desc(bh)->j_trans_id, bh_desc(bh)->j_len);*/
+	retval = (j % JOURNAL_BLOCK_COUNT);
+	brelse (bh);
+	break;
+    } while (1);
+
+    return retval;
+}
+
+
+void replay_all (struct super_block * s)
+{
+    int first_journal_block = get_journal_start (s);
+    int journal_size = get_journal_size (s);
+    struct buffer_head * d_bh, * c_bh;
+    struct reiserfs_journal_desc *desc ;
+    struct reiserfs_journal_commit *commit ;
+    int i;
+    int the_most_old_transaction = INT_MAX;
+    int the_most_young_transaction = 0;
+    int valid_transactions = 0;
+    int last_replayed;
+    int start_replay = 0;
+
+
+    /* look for oldest valid transaction */
+    printf ("Looking for the oldest transaction to start with %4d", valid_transactions);
+    for (i = first_journal_block; i < first_journal_block + journal_size; i ++) {
+	d_bh = bread (s->s_dev, i, s->s_blocksize);
+	if (desc_block (d_bh)) {
+	    desc = (struct reiserfs_journal_desc *)d_bh->b_data;
+	    /*printf ("block %ld is desc block of the transaction (trans_id %ld, len %ld, mount_id %ld) - ", 
+		    d_bh->b_blocknr, desc->j_trans_id, desc->j_len, desc->j_mount_id);*/
+	    if (!is_valid_transaction (s, d_bh)) {
+		i += desc->j_len + 1;
+		brelse (d_bh);
+		continue;
+	    }
+	    valid_transactions ++;
+	    printf ("\b\b\b\b    \b\b\b\b%4d", valid_transactions); fflush (stdout);
+	    
+	    /*printf ("good\n");*/
+	    if (the_most_old_transaction > desc->j_trans_id) {
+		the_most_old_transaction = desc->j_trans_id;
+		start_replay = d_bh->b_blocknr;
+	    }
+	    if (the_most_young_transaction < desc->j_trans_id) {
+		the_most_young_transaction = desc->j_trans_id;
+		start_replay = d_bh->b_blocknr;
+	    }
+	    i += desc->j_len + 1;
+	}
+	brelse (d_bh);
+	continue;
+    }
+
+    printf ("\b\b\b\b     \b\b\b\bok\n"
+	    "%d valid trans found. Will replay from %d to %d\n", valid_transactions,
+	    the_most_old_transaction, the_most_young_transaction);
+
+
+    printf ("Replaying transaction..%4d left..\b\b\b\b\b\b\b", valid_transactions);
+
+    /* replay all valid transaction */
+    last_replayed = 0;
+
+    while (1) {
+	d_bh = bread (s->s_dev, start_replay, s->s_blocksize);
+	if (!desc_block (d_bh)) {
+/*	    printf ("No desc block found at the expected place %lu\n", d_bh->b_blocknr);*/
+	    brelse (d_bh);
+	    start_replay = next_desc (s, start_replay);
+	    continue;
+	}
+
+	desc = bh_desc (d_bh);
+
+	if (!is_valid_transaction (s, d_bh)) {
+/*	    printf ("skip invalid transaction %ld (length %ld) starting from %lu\n", desc->j_trans_id, desc->j_len, d_bh->b_blocknr);*/
+	    brelse (d_bh);
+	    start_replay = next_desc (s, start_replay);
+	    continue;
+	}
+	
+	if (desc->j_trans_id < last_replayed) {
+	    /* we found transaction that has been replayed already */
+	    brelse (d_bh);
+/*	    printf ("Found transaction %ld. last replayed %d\n", desc->j_trans_id, last_replayed);*/
+	    break;
+	}
+/*	printf ("Replay transaction %ld (length %ld)-", desc->j_trans_id, desc->j_len);*/
+
+
+	/* replay transaction */
+	{
+	    int trans_offset = d_bh->b_blocknr - get_journal_start (s);
+	    struct buffer_head * log_bh, * in_place;
+
+
+	    c_bh = bread (s->s_dev, get_journal_start (s) + ((trans_offset + desc->j_len + 1) % JOURNAL_BLOCK_COUNT), 
+			  s->s_blocksize) ;
+	
+	    desc = bh_desc (d_bh);
+	    commit = bh_commit (c_bh);
+	    if (journal_compare_desc_commit(s, desc, commit))
+		die ("read_journal: invalid transaction");
+
+	    for (i = 0; i < desc->j_len; i ++) {
+		/* read from log record */
+		log_bh = bread (s->s_dev, get_journal_start (s) + (trans_offset + 1 + i) % JOURNAL_BLOCK_COUNT,
+				s->s_blocksize);
+		if (log_bh->b_blocknr == 8199)
+		    printf ("block 8199 put in-placen\n");
+		/* write in-place */
+		if (i < JOURNAL_TRANS_HALF) {
+		    in_place = getblk(s->s_dev, desc->j_realblock[i], s->s_blocksize) ;
+		} else {
+		    in_place = getblk(s->s_dev, commit->j_realblock[i - JOURNAL_TRANS_HALF], s->s_blocksize) ;
+		}
+		if (log_bh->b_blocknr == 8199) {
+		    printf ("Put 8199 to %lu\n", in_place->b_blocknr);
+		}
+		memcpy (in_place->b_data, log_bh->b_data, s->s_blocksize);
+		mark_buffer_dirty (in_place, 0);
+		mark_buffer_uptodate (in_place, 1);
+		bwrite (in_place);
+		brelse (in_place);
+		brelse (log_bh);
+	    }
+	    brelse (c_bh);
+	}
+	valid_transactions --;
+	printf ("\b\b\b\b    \b\b\b\b%4d", valid_transactions); fflush (stdout);
+	last_replayed = desc->j_trans_id;
+	start_replay = next_expected_desc (s, d_bh);
+	brelse (d_bh);
+    }
+    printf (" left .. ok\n");
+}
+
+
+//
+// these duplicate the same from fsck/check_tree.c
+//
+static inline blocknr_t first_child (struct buffer_head * bh)
+{
+    return B_N_CHILD (bh, 0)->dc_block_number;
+}
+
+
+static inline blocknr_t last_child (struct buffer_head * bh)
+{
+    return B_N_CHILD (bh, B_NR_ITEMS (bh))->dc_block_number;
+}
+
+
+static inline blocknr_t next_child (struct buffer_head * child,
+				    struct buffer_head * parent)
+{
+    int i;
+    
+    for (i = 0; i < B_NR_ITEMS (parent); i ++) {
+	if (B_N_CHILD (parent, i)->dc_block_number == child->b_blocknr)
+	    return B_N_CHILD (parent, i + 1)->dc_block_number;
+    }
+    die ("next_child: no child found: should not happen");
+    return 0;
+}
+
+
+/* h == 0 for root level. block head's level == 1 for leaf level  */
+static inline int h_to_level (struct super_block * s, int h)
+{
+    return SB_TREE_HEIGHT (s) - h - 1;
+}
+
+
+static inline int leaf_level (struct buffer_head * bh)
+{
+    return B_LEVEL(bh) == DISK_LEAF_NODE_LEVEL;
+}
+
+
+static void print (int cur, int total)
+{
+  printf ("/%3d (of %3d)", cur, total);fflush (stdout);
+}
+
+
+/* erase /XXX(of XXX) */
+static void erase (void)
+{
+    printf ("\b\b\b\b\b\b\b\b\b\b\b\b\b");
+    printf ("             ");
+    printf ("\b\b\b\b\b\b\b\b\b\b\b\b\b");
+    fflush (stdout);
+}
+
+
+/* the simplest scanning for free block., This should be rare */
+__u32 alloc_block (void)
+{
+    int i, j;
+    int bits = g_sb.s_blocksize * 8;
+    int start = get_journal_start (&g_sb) + get_journal_size (&g_sb) + 1;
+
+    for (i = 0; i < SB_BMAP_NR (&g_sb); i ++) {
+	j = find_next_zero_bit (g_new_bitmap[i], bits, start);
+	if (j < bits) {
+	    mark_block_used (j + i * bits);
+	    return j + i * bits;
+	}
+	start = 0;
+    }
+    die ("allocate_block: no free blocks");
+    return 0;
+	
+}
+
+struct buffer_head * copy_contents (struct buffer_head * from)
+{
+    struct buffer_head * bh;
+    __u32 new;
+
+    new = alloc_block ();
+    bh = getblk (from->b_dev, new, from->b_size);
+    memcpy (bh->b_data, from->b_data, bh->b_size);
+    mark_buffer_uptodate (bh, 1);
+    mark_buffer_dirty (bh, 1);
+    bwrite (bh);
+    return bh;
+}
+
+
+static void update_pointer (struct buffer_head * parent, __u32 new, __u32 old)
+{
+    int i;
+       
+    for (i = 0; i <= B_NR_ITEMS (parent); i ++) {
+	if (B_N_CHILD (parent, i)->dc_block_number == old) {
+	    B_N_CHILD (parent, i)->dc_block_number = new;
+	    mark_buffer_dirty (parent, 1);
+	    return;
+	}
+    }
+    die ("update_pointer: old pointer not found");
+}
+
+
+static int block_from_journal (struct super_block * s, __u32 block)
+{
+    if(block && block < get_journal_start (s)) {
+	printf ("not data block (%d) got into tree. Should not appear, but fixable\n", block);
+	return 0;
+    }
+    if (block >= get_journal_start (s) && block <= get_journal_start (s) + get_journal_size (s))
+	/* <= must_journal_end due to journal header */
+	return 1;
+    return 0;
+}
+
+
+/* sometimes indirect items point to blocks from journal. Replace them
+   with data blocks. I believe this is rare case */
+static void correct_indirect_items (struct super_block * s, struct buffer_head * bh)
+{
+    int i, j;
+    struct item_head * ih;
+    __u32 * unfm;
+
+    ih = B_N_PITEM_HEAD (bh, 0);
+    for (i = 0; i < B_NR_ITEMS (bh); i ++, ih ++) {
+	if (!I_IS_INDIRECT_ITEM (ih))
+	    continue;
+	unfm = (__u32 *)B_I_PITEM (bh, ih);
+	for (j = 0; j < I_UNFM_NUM (ih); j++) {
+	    if (block_from_journal (s, unfm[j])) {
+		struct buffer_head * from, * to;
+
+		from = bread (bh->b_dev, unfm[j], bh->b_size);
+		to = copy_contents (from);
+		unfm[j] = to->b_blocknr;
+		mark_buffer_dirty (bh, 1);
+		brelse (from);
+		brelse (to);
+	    }
+	}
+    }
+}
+
+
+
+/* sometimes, (hopefully very rare) we have to use journal blocks to
+   complete tree building. In this case we have to find all those
+   blocks and replace them with data blocks (Those must exist to this
+   time. We have to look such blocks also when start of  */
+void release_journal_blocks (struct super_block * s)
+{
+    struct buffer_head * path[MAX_HEIGHT] = {0,};
+    int total[MAX_HEIGHT] = {0,};
+    int cur[MAX_HEIGHT] = {0,};
+    int h = 0;
+
+
+    blocknr_t block = SB_ROOT_BLOCK (s);
+
+    printf ("%d blocks from journal area [%d %d] has been used to perform repairing. Will release them. This may take a while\nScanning tree..",
+	    from_journal, get_journal_start (s), 
+	    get_journal_start (s) + get_journal_size (s));
+
+
+    while ( 1 ) {
+	if (path[h])
+	    die ("release_journal_blocks: empty slot expected");
+	
+	if (h)
+	    print (cur[h - 1], total[h - 1]);
+
+	path[h] = bread (s->s_dev, block, s->s_blocksize);
+	if (path[h] == 0)
+	    die ("release_journal_blocks: bread failed");
+
+	if (block_from_journal (s, path[h]->b_blocknr)) {
+	    /* copy block to newly allocated, adjust pointer in the
+               parent, replace on the path */
+	    struct buffer_head * bh;
+	    __u32 old = path[h]->b_blocknr;
+
+	    bh = copy_contents (path[h]);
+	    brelse (path[h]);
+	    path[h] = bh;
+	    if (h) {
+		/* adjust corresponding dc_child_num in the parent*/
+		update_pointer (path[h - 1], bh->b_blocknr, old);
+	    } else {
+		/* change pointer from super block */
+		SB_ROOT_BLOCK (s) = bh->b_blocknr;
+	    }
+	}
+
+ 	if (leaf_level (path[h])) {
+	    /* correct unformatted node pointers if they point to the
+               journal area */
+	    correct_indirect_items (s, path[h]);
+
+	    brelse (path[h]);
+	    if (h)
+	      erase ();
+
+	    while (h && path[h]->b_blocknr == last_child (path[h - 1])) { 
+		path[h] = 0;
+		h --;
+		brelse (path[h]);
+		if (h)
+		  erase ();
+	    }
+
+	    if (h == 0) {
+		path[h] = 0;
+		break;
+	    }
+
+	    cur[h - 1] ++;
+	    block = next_child (path[h], path[h-1]);
+	    path[h] = 0;
+	    continue; 
+	}
+	total[h] = B_NR_ITEMS (path[h]) + 1;
+	cur[h] = 1;
+	block = first_child (path[h]);
+	h ++;
+    }
+    
+    printf ("ok\n");
+}
diff -urN linux/fs/reiserfs/utils/fsck/lost+found.c /tmp/linux/fs/reiserfs/utils/fsck/lost+found.c
--- linux/fs/reiserfs/utils/fsck/lost+found.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/fsck/lost+found.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,348 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+#include "fsck.h"
+#include "reiserfs.h"
+
+
+//
+// this will probably eventually replace pass4 in fsck
+//
+
+
+//
+// FIXME: there is no way to know what hash function is used to order
+// names in directory
+//
+#define MAX_GEN_NUMBER  127
+#define SET_GENERATION_NUMBER(offset,gen_number) (GET_HASH_VALUE(offset)|(gen_number))
+static __u32 get_third_component (char * name, int len)
+{
+    __u32 res;
+
+    if (!len || (len == 1 && name[0] == '.'))
+	return DOT_OFFSET;
+    if (len == 2 && name[0] == '.' && name[1] == '.')
+	return DOT_DOT_OFFSET;
+
+    res = g_sb.u.reiserfs_sb.s_hash_function (name, len);
+    res = GET_HASH_VALUE(res);
+    if (res == 0)
+	res = 128;
+    return res + MAX_GEN_NUMBER;
+    
+}
+
+
+//
+// looks for name in the directory dir, return 1 if name found, 0
+// otherwise
+//
+static objectid_t find_entry (struct key * dir, char * name)
+{
+    struct key entry_key;
+    struct path path;
+    int retval;
+    int i;
+
+    init_path (&path);
+
+    entry_key.k_dir_id = dir->k_dir_id;
+    entry_key.k_objectid = dir->k_objectid;
+    entry_key.k_offset = get_third_component (name, strlen (name));
+    entry_key.k_uniqueness = DIRENTRY_UNIQUENESS;
+    
+    while (1) {
+	struct buffer_head * bh;
+	struct item_head * ih;
+	struct reiserfs_de_head * deh;
+
+	retval = usearch_by_key (&g_sb, &entry_key, &path, 0, DISK_LEAF_NODE_LEVEL, 
+				 READ_BLOCKS, comp_keys);
+	if (retval == ITEM_NOT_FOUND)
+	    PATH_LAST_POSITION (&path) --;
+	
+	bh = PATH_PLAST_BUFFER (&path);
+	ih = PATH_PITEM_HEAD (&path);
+	deh = B_I_DEH (bh, ih);
+	for (i = ih->u.ih_entry_count - 1; i >= 0; i --) {
+	    if (strlen (name) != I_DEH_N_ENTRY_LENGTH (ih, deh + i, i))
+		continue;
+	    if (!memcmp (B_I_E_NAME (i, bh, ih), name, strlen (name))) {
+		pathrelse (&path);
+		return deh[i].deh_objectid;;
+	    }
+	}
+	pathrelse (&path);
+	if (ih->ih_key.k_offset == DOT_OFFSET)
+	    return 0;
+	entry_key.k_offset --;
+    }
+    die ("find_entry: endless loop broken");
+    return 0;
+}
+    
+
+//
+// add name pointing to 'key' to the directory 'dir'. FIXME: this will
+// not work if there is a name in directory with the same value of
+// hash function
+//
+static void add_entry (struct key * dir, char * name, struct key * key)
+{
+    struct path path;
+    struct key entry_key;
+    char * entry;
+    struct reiserfs_de_head * deh;
+    int retval;
+
+    init_path (&path);
+    
+    entry_key.k_dir_id = dir->k_dir_id;
+    entry_key.k_objectid = dir->k_objectid;
+    entry_key.k_offset = SET_GENERATION_NUMBER (get_third_component (name, strlen (name)), 0);
+    entry_key.k_uniqueness = DIRENTRY_UNIQUENESS;
+
+    retval = usearch_by_entry_key (&g_sb, &entry_key, &path);
+    if (retval == ENTRY_FOUND) {
+	printf ("add_entry: can not add name %s", name);
+	pathrelse (&path);
+	return;
+    }
+
+    entry = getmem (DEH_SIZE + strlen (name));
+    deh = (struct reiserfs_de_head *)entry;
+    deh->deh_location = 0;
+    deh->deh_offset = entry_key.k_offset;
+    deh->deh_state = 0;
+    mark_de_visible (deh);
+    /* put key (ino analog) to de */
+    deh->deh_dir_id = key->k_dir_id;
+    deh->deh_objectid = key->k_objectid;
+    memcpy ((char *)(deh + 1), name, strlen (name));
+   
+    reiserfsck_paste_into_item (&path, entry, DEH_SIZE + strlen (name));
+}
+
+
+/* mkreiserfs should have created this */
+static objectid_t make_lost_found_dir (void)
+{
+    int retval;
+    struct path path;
+    struct key key;
+    struct stat_data sd;
+    struct item_head ih;
+    char empty [EMPTY_DIR_SIZE];
+    objectid_t lost_found;
+
+    lost_found = find_entry (&root_key, "lost+found");
+    if (lost_found)
+	return lost_found;
+
+    key.k_dir_id = REISERFS_ROOT_OBJECTID;
+    key.k_objectid = get_unused_objectid (&g_sb);
+    key.k_offset = SD_OFFSET;
+    key.k_uniqueness = SD_UNIQUENESS;
+
+    /* stat data */
+    make_dir_stat_data (&key, &ih, &sd);
+
+    retval = usearch_by_key (&g_sb, &key, &path, 0, DISK_LEAF_NODE_LEVEL, READ_BLOCKS, comp_keys);
+    if (retval != KEY_NOT_FOUND)
+	die ("make_lost_found_dir: can not create stat data of \'lost+found\'");
+
+    reiserfsck_insert_item (&path, &ih, (char *)&sd);
+
+    
+    /* empty dir item */
+    ih.ih_key.k_offset = DOT_OFFSET;
+    ih.ih_key.k_uniqueness = DIRENTRY_UNIQUENESS;
+    ih.ih_item_len = EMPTY_DIR_SIZE;
+    ih.u.ih_entry_count = 2;
+
+    make_empty_dir_item (empty, key.k_dir_id, key.k_objectid, 
+			 REISERFS_ROOT_PARENT_OBJECTID, REISERFS_ROOT_OBJECTID);
+
+    retval = usearch_by_key (&g_sb, &(ih.ih_key), &path, 0, DISK_LEAF_NODE_LEVEL, READ_BLOCKS, comp_keys);
+    if (retval != KEY_NOT_FOUND)
+	die ("make_lost_found_dir: can not create empty dir body of \'lost+found\'");
+
+    reiserfsck_insert_item (&path, &ih, empty);
+
+    add_entry (&root_key, "lost+found", &key);
+
+    {
+	struct stat_data * root;
+
+	/* update root directory */
+	if (usearch_by_key (&g_sb, &root_key, &path, 0, DISK_LEAF_NODE_LEVEL,
+			    READ_BLOCKS, comp_keys) != ITEM_FOUND)
+	    die ("make_lost_found_dir: can not find root directory");
+	root = B_I_STAT_DATA (PATH_PLAST_BUFFER (&path), PATH_PITEM_HEAD (&path));
+	root->sd_size += DEH_SIZE + strlen ("lost+found");
+	mark_buffer_dirty (PATH_PLAST_BUFFER (&path), 1);
+	pathrelse (&path);
+    }
+
+    return key.k_objectid;
+}
+
+
+
+char lost_name[80];
+
+static void link_lost (struct key * lost_found, struct buffer_head * bh)
+{
+    int i;
+    struct item_head * ih;
+
+
+    ih = B_N_PITEM_HEAD (bh, 0);
+    for (i = 0; i < B_NR_ITEMS (bh); i ++, ih++) {
+	if (I_IS_STAT_DATA_ITEM (ih) && ih->ih_reserved == 0xffff && S_ISDIR (B_I_STAT_DATA (bh, ih)->sd_mode)) {
+	    struct key key;
+	    
+	    sprintf (lost_name, "%u_%u", ih->ih_key.k_dir_id, ih->ih_key.k_objectid);
+	    /* entry in lost+found directory will point to this key */
+	    key.k_dir_id = ih->ih_key.k_dir_id;
+	    key.k_objectid = ih->ih_key.k_objectid;
+	    add_entry (lost_found, lost_name, &key);
+	}
+    }
+}
+
+
+
+static inline blocknr_t first_child (struct buffer_head * bh)
+{
+    return B_N_CHILD (bh, 0)->dc_block_number;
+}
+
+
+static inline blocknr_t last_child (struct buffer_head * bh)
+{
+    return B_N_CHILD (bh, B_NR_ITEMS (bh))->dc_block_number;
+}
+
+
+static inline blocknr_t next_child (struct buffer_head * child,
+				    struct buffer_head * parent)
+{
+    int i;
+    
+    for (i = 0; i < B_NR_ITEMS (parent); i ++) {
+	if (B_N_CHILD (parent, i)->dc_block_number == child->b_blocknr)
+	    return B_N_CHILD (parent, i + 1)->dc_block_number;
+    }
+    die ("next_child: no child found: should not happen");
+    return 0;
+}
+
+
+/* h == 0 for root level. block head's level == 1 for leaf level  */
+static inline int h_to_level (struct super_block * s, int h)
+{
+    return SB_TREE_HEIGHT (s) - h - 1;
+}
+
+
+static inline int leaf_level (struct buffer_head * bh)
+{
+    return B_LEVEL(bh) == DISK_LEAF_NODE_LEVEL;
+}
+
+
+static void print (int cur, int total)
+{
+  printf ("/%3d (of %3d)", cur, total);fflush (stdout);
+}
+
+
+/* erase /XXX(of XXX) */
+static void erase (void)
+{
+    printf ("\b\b\b\b\b\b\b\b\b\b\b\b\b");
+    printf ("             ");
+    printf ("\b\b\b\b\b\b\b\b\b\b\b\b\b");
+    fflush (stdout);
+}
+
+
+void pass4 (struct super_block * s)
+{
+    struct buffer_head * path[MAX_HEIGHT] = {0,};
+    int total[MAX_HEIGHT] = {0,};
+    int cur[MAX_HEIGHT] = {0,};
+    int h = 0;
+    blocknr_t block = SB_ROOT_BLOCK (s);
+    struct key lost_found;
+
+
+    if (opt_stop_point != STOP_DEFAULT || opt_lost_found == NO_LOST_FOUND)
+	return ;
+
+    if (!s->u.reiserfs_sb.s_hash_function) {
+	s->u.reiserfs_sb.s_hash_function = keyed_hash;
+	s->u.reiserfs_sb.s_rs->s_hash_function_code = __cpu_to_le32 (TEA_HASH);
+	reiserfs_warning ("pass4: do not know which hash is used to sort names, using default\n");
+    }	
+
+    /* create lost+found directory (if it is not there) */
+    lost_found.k_dir_id = root_key.k_objectid;
+    lost_found.k_objectid = make_lost_found_dir ();
+    lost_found.k_offset = lost_found.k_uniqueness = 0;
+
+
+    printf ("Looking for lost files..");
+
+    while ( 1 ) {
+	if (path[h])
+	    die ("pass4: empty slot expected");
+	
+	if (h)
+	    print (cur[h - 1], total[h - 1]);
+
+	path[h] = bread (s->s_dev, block, s->s_blocksize);
+	if (path[h] == 0)
+	    die ("pass4: bread failed");
+
+ 	if (leaf_level (path[h])) {
+	    link_lost (&lost_found, path[h]);
+
+	    brelse (path[h]);
+	    if (h)
+	      erase ();
+
+	    while (h && path[h]->b_blocknr == last_child (path[h - 1])) { 
+		path[h] = 0;
+		h --;
+		brelse (path[h]);
+		if (h)
+		  erase ();
+	    }
+
+	    if (h == 0) {
+		path[h] = 0;
+		break;
+	    }
+
+	    cur[h - 1] ++;
+	    block = next_child (path[h], path[h-1]);
+	    path[h] = 0;
+	    continue; 
+	}
+	total[h] = B_NR_ITEMS (path[h]) + 1;
+	cur[h] = 1;
+	block = first_child (path[h]);
+	h ++;
+    }
+
+    /* we have passed whole tree once again. Something have been added
+       to the lost+found directory. Do semantic pass for it */
+
+    printf ("Checking lost+found directory.."); fflush (stdout);
+    check_semantic_tree (&lost_found, &root_key, 0);
+    printf ("ok\n");
+    
+}
diff -urN linux/fs/reiserfs/utils/fsck/main.c /tmp/linux/fs/reiserfs/utils/fsck/main.c
--- linux/fs/reiserfs/utils/fsck/main.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/fsck/main.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,653 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+#include "fsck.h"
+#include <stdio.h>
+#include <getopt.h>
+#include <sys/mount.h>
+
+#include "reiserfs.h"
+
+//
+// by default journal will be replayed via mount -o replay-only
+// --replay-whole-journal
+// --no-replay-journal - journal replaying will be skipped
+// --scan-whole-partition - will scan whole parititon looking for the leaves
+//
+
+
+#define print_usage_and_exit() die ("Usage: %s [-aprv] [--rebuild-tree]\n"\
+"[--scan-whole-partition] [--no-journal-replay]\n"\
+"[--replay-whole-journal] device\n"\
+"\n"\
+"Long options:\n"\
+"\tModes:\n"\
+"\t--check                  consistency checking (default)\n"\
+"\t--rebuild-tree           force fsck to rebuild filesystem from scratch\n"\
+"\t                         (takes a long time)\n"\
+"\t--search-key             will search for key you specify\n"\
+"\n"\
+"\tArea to scan:\n"\
+"\t--scan-used-part-only    scan what is marked used in bitmap (default)\n"\
+"\t--scan-whole-partition   scan whole partition\n"\
+"\n"\
+"\tJournal replay options:\n"\
+"\t--replay-by-mount        replay by calling mount -o replay-only (default)\n"\
+"\t--no-journal-replay      skip journal replaying\n"\
+"\t--replay-whole-journal   replay all valid transaction found in the journal\n"\
+"\n"\
+"\tStop point specifying\n"\
+"\t--do-not-stop            (default)\n"\
+"\t--stop-after-replay\n"\
+"\t--stop-after-pass1\n"\
+"\t--stop-after-pass2\n"\
+"\t--stop-after-semantic-pass\n"\
+"Short options:\n"\
+"\t-v verbose mode\n"\
+"\t-a supress progress information\n"\
+"\t-y\n"\
+"\t-p do nothing, exist for compatibility with fsck(8)\n"\
+"\t-r\n", argv[0]);
+
+
+int opt_verbose = 0;
+int opt_fsck = 0; /* called with -a by fsck - the front-end for the
+		     various file system checkers */
+
+
+//
+// fsck has three modes: default one - is check, other two are rebuild
+// and find items
+//
+int opt_fsck_mode = FSCK_DEFAULT;
+
+/* in mode FSCK_FIND_ITEM keu for search is stored here */
+struct key key_to_find;
+
+//
+// replay journal modes
+//
+#define REPLAY_DEFAULT 0
+#define REPLAY_ALL 1
+#define NO_REPLAY 2
+int opt_journal_replay = REPLAY_DEFAULT;
+
+
+//
+// fsck may stop after any of its phases: after journal replay or
+// after any of passes. Default is do not stop
+//
+int opt_stop_point = STOP_DEFAULT;
+
+
+//
+// 
+//
+int opt_what_to_scan = SCAN_USED_PART;
+
+
+//
+//
+//
+int opt_lost_found = NO_LOST_FOUND;
+
+
+
+/* fsck is called with one non-optional argument - file name of device
+   containing reiserfs. This function parses other options, sets flags
+   based on parsing and returns non-optional argument */
+static char * parse_options (int argc, char * argv [])
+{
+    int c;
+
+    while (1) {
+	static struct option options[] = {
+	    // mode options
+	    {"check", no_argument, &opt_fsck_mode, FSCK_DEFAULT},
+	    {"rebuild-tree", no_argument, &opt_fsck_mode, FSCK_REBUILD},
+	    {"search-key", no_argument, &opt_fsck_mode, FSCK_FIND_ITEM},
+
+	    // journal replay options
+	    {"replay-by-mount", no_argument, &opt_journal_replay, REPLAY_DEFAULT},
+	    {"no-journal-replay", no_argument, &opt_journal_replay, NO_REPLAY},
+	    {"replay-whole-journal", no_argument, &opt_journal_replay, REPLAY_ALL},
+
+	    // stop point options
+	    {"do-not-stop", no_argument, &opt_stop_point, STOP_DEFAULT},
+	    {"stop-after-replay", no_argument, &opt_stop_point, STOP_AFTER_REPLAY},
+	    {"stop-after-pass1", no_argument, &opt_stop_point, STOP_AFTER_PASS1},
+	    {"stop-after-pass2", no_argument, &opt_stop_point, STOP_AFTER_PASS2},
+	    {"stop-after-semantic-pass", no_argument, &opt_stop_point, STOP_AFTER_SEMANTIC},
+
+	    // scanned area option
+	    {"scan-used-part-only", no_argument, &opt_what_to_scan, SCAN_USED_PART},
+	    {"scan-whole-partition", no_argument, &opt_what_to_scan, SCAN_WHOLE_PARTITION},
+
+	    // lost+found
+	    {"no-lost+found", no_argument, &opt_lost_found, NO_LOST_FOUND},
+	    {"lost+found",  no_argument, &opt_lost_found, DO_LOST_FOUND},
+	    {0, 0, 0, 0}
+	};
+	int option_index;
+      
+	c = getopt_long (argc, argv, "yapv", options, &option_index);
+	if (c == -1)
+	    break;
+
+	switch (c) {
+	case 0:
+	    switch (option_index) {
+	    case 0: /* check */
+	    case 1: /* rebuild */
+	    case 2: /* find */
+		break;
+
+	    case 3: /* replay by mount */
+	    case 4: /* no journal replay */
+	    case 5: /* replay whole journal */
+		break;
+
+	    case 6: /* do not stop */
+	    case 7: /* stop after replay */
+	    case 8: /* stop after pass 1 */
+	    case 9: /* stop after pass 2 */
+	    case 10: /* stop after semantic */
+		break;
+	    case 11: /* scan used part of partition */
+	    case 12: /* scan whole partition */
+		break;
+		
+	    }
+	    break;
+
+	    //case 'y':
+	case 'p': /* these do nothing */
+	case 'r':
+	    break;
+
+	case 'a':
+	    opt_fsck = 1;
+	    break;
+
+	case 'v':
+	    /* output fsck statistics to stdout on exit */
+	    opt_verbose = 1;
+	    break;
+
+	default:
+	    print_usage_and_exit();
+	}
+    }
+
+  if (optind != argc - 1)
+    /* only one non-option argument is permitted */
+    print_usage_and_exit();
+  
+  return argv[optind];
+}
+
+
+struct super_block g_sb;
+struct buffer_head * g_sbh;
+struct reiserfs_super_block * g_old_rs;
+struct key root_key = {REISERFS_ROOT_PARENT_OBJECTID, REISERFS_ROOT_OBJECTID, 0, 0};
+
+
+static void reset_super_block (struct super_block * s)
+{
+    __u32 * oids;
+
+    g_old_rs = (struct reiserfs_super_block *)getmem (s->s_blocksize);
+    memcpy (g_old_rs, SB_BUFFER_WITH_SB (s)->b_data, s->s_blocksize);
+
+    /* reset few fields in */
+    SB_FREE_BLOCKS (s) = SB_BLOCK_COUNT (s);
+    SB_TREE_HEIGHT (s) = ~0;
+    SB_ROOT_BLOCK (s) = ~0;
+    s->u.reiserfs_sb.s_mount_state = REISERFS_ERROR_FS;
+
+    // reset objectid map
+    s->u.reiserfs_sb.s_rs->s_oid_cursize = 2;
+    oids = (__u32 *)(s->u.reiserfs_sb.s_rs + 1);
+    if (oids[0] != 1) {
+	printf ("reset_super_block: invalid objectid map\n");
+	oids[0] = 1;
+    }
+    oids[1] = 2;
+
+    // hash functino will be found in semantic pass
+    s->u.reiserfs_sb.s_hash_function = 0;
+
+    mark_buffer_dirty (SB_BUFFER_WITH_SB (s), 0);
+}
+
+static void update_super_block (void)
+{
+    SB_REISERFS_STATE (&g_sb) = REISERFS_VALID_FS;
+    
+    reset_journal (&g_sb);
+
+    mark_buffer_dirty (SB_BUFFER_WITH_SB (&g_sb), 0);
+}
+
+
+char ** g_disk_bitmap;
+char ** g_new_bitmap;
+char ** g_uninsertable_leaf_bitmap;
+char ** g_formatted;
+char ** g_unformatted;
+int g_blocks_to_read;
+
+
+/* read bitmaps (new or old format), create data blocks for new
+   bitmap, mark non-data blocks in it (skipped, super block, journal
+   area, bitmaps) used, create other auxiliary bitmaps */
+static void init_bitmaps (struct super_block * s)
+{
+    int i, j;
+
+    /* read disk bitmap */
+    if (uread_bitmaps (s))
+	die ("init_bitmap: unable to read bitmap");
+
+    g_disk_bitmap = getmem (sizeof (char *) * SB_BMAP_NR (s));
+    for (i = 0; i < SB_BMAP_NR (s); i ++) {
+	g_disk_bitmap[i] = SB_AP_BITMAP (s)[i]->b_data;
+
+	if (opt_what_to_scan == SCAN_WHOLE_PARTITION)
+	    /* mark all blocks busy */
+	    memset (g_disk_bitmap[i], 0xff, s->s_blocksize);
+    }
+
+
+    /* g_blocks_to_read is used to report progress */
+    if (opt_what_to_scan == SCAN_WHOLE_PARTITION)
+	/* all blocks will be scanned */
+	g_blocks_to_read = SB_BLOCK_COUNT (s);
+    else {
+	/* blocks marked used in bitmap will be scanned */
+	g_blocks_to_read = 0;
+	for (i = 0; i < SB_BMAP_NR (s); i ++) {
+	    for (j = 0; j < s->s_blocksize * 8; j ++)
+		if (i * s->s_blocksize * 8 + j < SB_BLOCK_COUNT (s) &&
+		    test_bit (j, SB_AP_BITMAP (s)[i]->b_data))
+		    g_blocks_to_read ++;
+	}
+    }
+
+    /* this bitmap will contain valid bitmap when fsck will have done */
+    g_new_bitmap = getmem (sizeof (char *) * SB_BMAP_NR (s));
+    for (i = 0; i < SB_BMAP_NR (s); i ++)
+	g_new_bitmap[i] = getmem (s->s_blocksize);
+
+    /* mark skipped blocks and super block used */
+    for (i = 0; i <= SB_BUFFER_WITH_SB (s)->b_blocknr; i ++)
+	mark_block_used (i);
+
+    /* mark bitmap blocks as used */
+    for (i = 0; i < SB_BMAP_NR (s); i ++)
+	mark_block_used (SB_AP_BITMAP (s)[i]->b_blocknr);
+
+    /* mark journal area as used */
+    for (i = 0; i < JOURNAL_BLOCK_COUNT + 1; i ++)
+	mark_block_used (i + get_journal_start (s));
+
+    /* fill by 1s the unused part of last bitmap */
+    if (SB_BLOCK_COUNT (s) % (s->s_blocksize * 8))
+	for (j = SB_BLOCK_COUNT (s) % (s->s_blocksize * 8); j < s->s_blocksize * 8; j ++)
+	    set_bit (j, g_new_bitmap[SB_BMAP_NR (s) - 1]);
+
+    /* allocate space for bitmap of uninsertable leaves */
+    g_uninsertable_leaf_bitmap = getmem (sizeof (char *) * SB_BMAP_NR (s));
+    for (i = 0; i < SB_BMAP_NR (s); i ++) {
+	g_uninsertable_leaf_bitmap[i] = getmem (s->s_blocksize);
+	memset (g_uninsertable_leaf_bitmap[i], 0xff, s->s_blocksize);
+    }
+
+    /* bitmap of formatted nodes */
+    g_formatted = getmem (sizeof (char *) * SB_BMAP_NR (s));
+    for (i = 0; i < SB_BMAP_NR (s); i ++) {
+	g_formatted[i] = getmem (s->s_blocksize);
+	memset (g_formatted[i], 0, s->s_blocksize);
+    }
+    /* bitmap of unformatted nodes */
+    g_unformatted = getmem (sizeof (char *) * SB_BMAP_NR (s));
+    for (i = 0; i < SB_BMAP_NR (s); i ++) {
+	g_unformatted[i] = getmem (s->s_blocksize);
+	memset (g_unformatted[i], 0, s->s_blocksize);
+    }
+}
+
+
+/* write bitmaps and brelse them */
+static void update_bitmap (struct super_block * s)
+{
+    int i;
+
+    /* journal area could be used, reset it */
+    for (i = 0; i < get_journal_start (s) + get_journal_size (s) + 1; i ++)
+	if (!is_block_used (i))
+	    mark_block_used (i);
+
+    for (i = 0; i < SB_BMAP_NR (s); i ++) {
+
+	/* copy newly built bitmap to cautious bitmap */
+	memcpy (SB_AP_BITMAP (s)[i]->b_data, g_new_bitmap[i], s->s_blocksize);
+	mark_buffer_dirty (SB_AP_BITMAP (s)[i], 0);
+	bwrite (SB_AP_BITMAP (s)[i]);
+    
+
+	freemem (g_new_bitmap[i]);
+	/* g_disk_bitmap[i] points to corresponding cautious bitmap's b_data */
+	freemem (g_uninsertable_leaf_bitmap[i]);
+    }
+
+    freemem (g_disk_bitmap);
+    freemem (g_new_bitmap);
+    freemem (g_uninsertable_leaf_bitmap);
+
+}
+
+
+static void release_bitmap (void)
+{
+    int i;
+
+    for (i = 0; i < SB_BMAP_NR (&g_sb); i ++) {
+	brelse (SB_AP_BITMAP (&g_sb)[i]);
+    }
+}
+
+static void release_super_block (void)
+{
+    bwrite (SB_BUFFER_WITH_SB (&g_sb));
+    freemem (SB_AP_BITMAP (&g_sb));
+    brelse (SB_BUFFER_WITH_SB (&g_sb));
+
+    freemem (g_old_rs);
+}
+
+
+
+static void mount_replay (char * devname1)
+{
+    int retval;
+    char * tmpdir;
+
+    printf ("Replaying journal.."); fflush (stdout);
+
+    tmpdir = tmpnam (0);
+    if (!tmpdir || mkdir (tmpdir, 0644))
+	die ("replay_journal: tmpnam or mkdir failed: %s", strerror (errno));
+ 
+    retval = mount (devname1, tmpdir, "reiserfs", MS_MGC_VAL, "replayonly");
+    if (retval != -1 || errno != EINVAL) {
+	printf ("\nMAKE SURE, THAT YOUR KERNEL IS ABLE TO MOUNT REISERFS\n");
+	die ("replay_journal: mount returned unexpected value: %s", 
+	     strerror (errno));
+    }
+
+    if (rmdir (tmpdir) == -1)
+	die ("replay_journal: rmdir failed: %s", strerror (errno));
+
+    printf ("ok\n"); fflush (stdout);
+}
+
+
+static inline int nothing_todo (struct super_block * s)
+{
+    if (opt_fsck)
+	return 1;
+    return 0;
+}
+
+
+void write_dirty_blocks (void)
+{
+    flush_buffers ();
+}
+
+
+#define WARNING \
+"Don't run this program unless something is broken.  You may want\n\
+to backup first.  Some types of random FS damage can be recovered\n\
+from by this program, which basically throws away the internal nodes\n\
+of the tree and then reconstructs them.  This program is for use only\n\
+by the desperate, and is of only beta quality.  Email\n\
+reiserfs@devlinux.com with bug reports. \n"
+
+/* 
+   warning #2
+   you seem to be running this automatically.  you are almost
+   certainly doing it by mistake as a result of some script that
+   doesn't know what it does.  doing nothing, rerun without -p if you
+   really intend to do this.  */
+
+void warn_what_will_be_done (void)
+{
+    char answer [10];
+
+    /* warn about fsck mode */
+    switch (opt_fsck_mode) {
+    case FSCK_DEFAULT:
+	printf ("Will read-only check consistency of the partition\n");
+	break;
+
+    case FSCK_REBUILD:
+	printf (WARNING);
+	break;
+
+    case FSCK_FIND_ITEM:
+	printf ("Will look for the item with key\n");
+	break;
+    }
+
+    /* warn about replay */
+    switch (opt_journal_replay) {
+    case REPLAY_DEFAULT:
+	printf ("Will replay just like mounting would\n");
+	break;
+	
+    case REPLAY_ALL:
+	printf ("Will replay all valid transactions\n"); break;
+
+    case NO_REPLAY:
+	printf ("Will not replay journal\n"); break;
+    }
+
+    /* warn about stop point */
+    switch (opt_stop_point) {
+    case STOP_AFTER_REPLAY:
+	printf ("Will stop after journal replay\n"); break;
+    case STOP_AFTER_PASS1:
+	printf ("Will stop after pass 1\n"); break;
+
+    case STOP_AFTER_PASS2:
+	printf ("Will stop after pass 2\n"); break;
+
+    case STOP_AFTER_SEMANTIC:
+	printf ("Will stop after semantic pass\n"); break;
+    }
+
+
+    /* warn about scanned area */
+    if (opt_what_to_scan == SCAN_WHOLE_PARTITION)
+	printf ("Will scan whole partition\n");
+    
+    printf ("Do you want to run this "
+	    "program?[N/Yes] (note need to type Yes):");
+    if (fgets (answer, 5, stdin) == 0 || strcmp ("Yes\n", answer)) {
+	exit (0);
+    }
+
+    if (opt_fsck_mode == FSCK_FIND_ITEM) {
+	printf ("Specify key to search:");
+	if (scanf ("%d %d %d %d", &(key_to_find.k_dir_id), &(key_to_find.k_objectid),
+		   &(key_to_find.k_offset), &(key_to_find.k_uniqueness)) != 4)
+	    die ("parse_options: specify a key through stdin");
+    }
+}
+
+
+void end_fsck (char * file_name)
+{
+    update_super_block ();
+    update_bitmap (&g_sb);
+    release_bitmap ();
+    release_super_block ();
+    
+    if (opt_verbose == 1)
+	output_information ();
+
+    printf ("Syncing.."); fflush (stdout);
+    
+    write_dirty_blocks ();
+    sync ();
+    
+    printf ("done\n"); fflush (stdout);
+    
+    if (opt_verbose == 1)
+	printf ("Checking mem..");
+    
+    free_overwritten_unfms ();
+    check_and_free_buffer_mem ();
+
+    if (opt_verbose == 1)
+	printf ("done\n");
+
+    if (opt_fsck == 1)
+	printf("ReiserFS : done checking %s\n", file_name);
+    else
+	printf ("Ok\n");
+    exit (0);
+}
+
+
+static void open_device (char * file_name, int flag)
+{
+    g_sb.s_dev = open (file_name, flag);
+    if (g_sb.s_dev == -1)
+	die ("reiserfsck: can not open '%s': %s", file_name, strerror (errno));
+}  
+
+static void reopen_read_only (char * file_name)
+{
+    close (g_sb.s_dev);
+    open_device (file_name, O_RDONLY);
+}
+
+static void reopen_read_write (char * file_name)
+{
+    close (g_sb.s_dev);
+    open_device (file_name, O_RDWR);
+}
+
+
+/* ubitmap.c: */extern int from_journal;
+
+
+int main (int argc, char * argv [])
+{
+    char * file_name;
+ 
+    file_name = parse_options (argc, argv);
+
+    if (opt_fsck == 0)
+	printf ("\n\n<-----------REISERFSCK, 2000----------->\n%s\n",
+		reiserfs_get_version_string());
+
+    if (is_mounted (file_name))
+	/* do not work on mounted filesystem for now */
+	die ("reiserfsck: '%s' contains a mounted file system\n", file_name);
+
+    warn_what_will_be_done (); /* and ask confirmation Yes */
+
+
+    if (opt_journal_replay == REPLAY_DEFAULT)
+	mount_replay (file_name);
+
+    open_device (file_name, O_RDONLY);
+    
+    if (uread_super_block (&g_sb))
+	die ("reiserfsck: no reiserfs found");
+
+    if (opt_journal_replay == REPLAY_ALL) {
+	/* read-write permissions are needed */
+	reopen_read_write (file_name);
+	replay_all (&g_sb);
+	reopen_read_only (file_name);
+    }
+
+
+    if (nothing_todo (&g_sb)) {
+	/* this should work when fsck is called by fsck -a */
+	printf ("%s: clean, %d/%d %ldk blocks\n", file_name,
+		SB_BLOCK_COUNT (&g_sb) - SB_FREE_BLOCKS(&g_sb), SB_BLOCK_COUNT (&g_sb), g_sb.s_blocksize / 1024);
+	brelse (SB_BUFFER_WITH_SB (&g_sb));
+	return 0;
+    }
+
+
+    if (opt_fsck_mode == FSCK_DEFAULT) {
+	check_fs_tree (&g_sb);
+	release_bitmap ();
+	release_super_block ();
+	check_and_free_buffer_mem ();
+	exit (0);
+    }
+
+    if (opt_stop_point == STOP_AFTER_REPLAY) {
+	release_super_block ();
+	check_and_free_buffer_mem ();
+	exit (0);	
+    }
+	
+
+    if (opt_fsck_mode == FSCK_REBUILD) {
+	reopen_read_write (file_name);
+
+	if (opt_fsck == 1)
+	    printf ("ReiserFS : checking %s\n",file_name);
+	else
+	    printf ("Rebuilding..\n");
+
+	reset_super_block (&g_sb);
+	init_bitmaps (&g_sb);
+	
+	/* make file system invalid unless fsck done */
+	SB_REISERFS_STATE (&g_sb) = REISERFS_ERROR_FS;
+	bwrite (SB_BUFFER_WITH_SB (&g_sb));
+	/* 1,2. building of the tree */
+	build_the_tree ();
+
+	/* 3. semantic pass */
+	semantic_pass ();
+
+	/* if --lost+found is set - link unaccessed directories to
+           lost+found directory */
+	pass4 (&g_sb);
+
+	/* 4. look for unaccessed items in the leaves */
+	check_unaccessed_items ();
+
+
+	if (from_journal)
+	    /* blocks from journal area could get into tree, fix that */
+	    release_journal_blocks (&g_sb);
+
+	end_fsck (file_name);
+    }
+
+
+    if (opt_fsck_mode == FSCK_FIND_ITEM) {
+	init_bitmaps (&g_sb);
+	build_the_tree ();
+	release_bitmap ();
+	release_super_block ();
+	check_and_free_buffer_mem ();
+	exit (0);
+    }
+
+
+    return 0;
+}
diff -urN linux/fs/reiserfs/utils/fsck/noname.c /tmp/linux/fs/reiserfs/utils/fsck/noname.c
--- linux/fs/reiserfs/utils/fsck/noname.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/fsck/noname.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,309 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+/*#include <stdio.h>*/
+/*#include <string.h>*/
+/*#include <sys/types.h>*/
+/*#include <asm/bitops.h>
+#include "../include/reiserfs_fs.h"
+#include "../include/reiserfs_fs_sb.h"
+#include "../include/reiserfslib.h"*/
+#include "fsck.h"
+
+
+void get_max_buffer_key (struct buffer_head * bh, struct key * key)
+{
+  struct item_head * ih;
+
+  ih = B_N_PITEM_HEAD (bh, B_NR_ITEMS (bh) - 1);
+  copy_key (key, &(ih->ih_key));
+
+  if (KEY_IS_DIRECTORY_KEY (key)) {
+    /* copy 3-rd and 4-th key components of the last entry */
+    key->k_offset = B_I_DEH (bh, ih)[I_ENTRY_COUNT (ih) - 1].deh_offset;
+    key->k_uniqueness = DIRENTRY_UNIQUENESS;
+  } else if (!KEY_IS_STAT_DATA_KEY (key))
+    /* get key of the last byte, which is contained in the item */
+    key->k_offset += I_BYTES_NUMBER (ih, bh->b_size) - 1;
+
+}
+
+
+#if 0
+int are_items_mergeable (struct item_head * left, struct item_head * right, int bsize)
+{
+  if (comp_keys (&left->ih_key, &right->ih_key) != SECOND_GREATER) {
+    print_key (&(left->ih_key));
+    print_key (&(right->ih_key));
+    die ("are_items_mergeable: second key is not greater");
+  }
+
+  if (comp_short_keys (&left->ih_key, &right->ih_key) != KEYS_IDENTICAL)
+    return NO;
+
+  if (I_IS_DIRECTORY_ITEM (left)) {
+    if (!I_IS_DIRECTORY_ITEM (right))
+      die ("are_items_mergeable: right item must be of directory type");
+    return 1;
+  }
+
+  if ((I_IS_DIRECT_ITEM (left) && I_IS_DIRECT_ITEM (right)) || 
+      (I_IS_INDIRECT_ITEM (left) && I_IS_INDIRECT_ITEM (right)))
+    return (left->ih_key.k_offset + I_BYTES_NUMBER (left, bsize) == right->ih_key.k_offset) ? 1 : 0;
+
+  return 0;
+}
+
+
+static void decrement_key (struct key * key)
+{
+  unsigned long * key_field = (unsigned long *)key + REISERFS_FULL_KEY_LEN - 1;
+  int i;
+
+  for (i = 0; i < REISERFS_FULL_KEY_LEN; i ++, key_field--)
+    if (*key_field) {
+      (*key_field)--;
+      break;
+    }
+
+  if (i == REISERFS_FULL_KEY_LEN)
+    die ("decrement_key: zero key found");
+}
+
+
+/* get left neighbor of the leaf node */
+static struct buffer_head * get_left_neighbor (struct path * path)
+{
+  struct key key;
+  struct path path_to_left_neighbor;
+  struct buffer_head * bh;
+
+  copy_key (&key, B_N_PKEY (PATH_PLAST_BUFFER (path), 0));
+  decrement_key (&key);  
+
+  reiserfsck_search_by_key (&g_sb, &key, &path_to_left_neighbor, comp_keys);
+  if (PATH_LAST_POSITION (&path_to_left_neighbor) == 0) {
+    pathrelse (&path_to_left_neighbor);
+    return 0;
+  }
+  bh = PATH_PLAST_BUFFER (&path_to_left_neighbor);
+  bh->b_count ++;
+  pathrelse (&path_to_left_neighbor);
+  return bh;
+}
+
+
+int is_left_mergeable (struct path * path)
+{
+  struct item_head * right;
+  struct buffer_head * bh;
+  int retval;
+  
+  right = B_N_PITEM_HEAD (PATH_PLAST_BUFFER (path), 0);
+
+  bh = get_left_neighbor (path);
+  if (bh == 0) {
+    return 0;
+  }
+  retval = are_items_mergeable (B_N_PITEM_HEAD (bh, B_NR_ITEMS (bh) - 1), right, bh->b_size);
+  brelse (bh);
+  return retval;
+}
+
+
+static struct buffer_head * get_right_neighbor (struct path * path)
+{
+  struct key key;
+  struct key * rkey;
+  struct path path_to_right_neighbor;
+  struct buffer_head * bh;
+  struct key maxkey = {0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff};
+
+  rkey = get_right_dkey (path);
+  if (rkey == 0)
+    copy_key (&key, &maxkey);
+  else
+    copy_key (&key, rkey);
+
+  reiserfsck_search_by_key (&g_sb, &key, &path_to_right_neighbor, comp_keys);
+  if (PATH_PLAST_BUFFER (&path_to_right_neighbor) == PATH_PLAST_BUFFER (path)) {
+    pathrelse (&path_to_right_neighbor);
+    return 0;
+  }
+  bh = PATH_PLAST_BUFFER (&path_to_right_neighbor);
+  bh->b_count ++;
+  pathrelse (&path_to_right_neighbor);
+  return bh;
+}
+
+
+int is_right_mergeable (struct path * path)
+{
+  struct item_head * left;
+  struct buffer_head * bh;
+  int retval;
+  
+  left = B_N_PITEM_HEAD (PATH_PLAST_BUFFER (path), B_NR_ITEMS (PATH_PLAST_BUFFER (path)) - 1);
+
+  bh = get_right_neighbor (path);
+  if (bh == 0) {
+    return 0;
+  }
+  retval = are_items_mergeable (left, B_N_PITEM_HEAD (bh, 0), bh->b_size);
+  brelse (bh);
+  return retval;
+}
+
+#endif /*0*/
+
+
+#if 0
+/* retunrs 1 if buf looks like a leaf node, 0 otherwise */
+static int is_leaf (char * buf)
+{
+  struct block_head * blkh;
+  struct item_head * ih;
+  int used_space;
+  int prev_location;
+  int i;
+
+  blkh = (struct block_head *)buf;
+  ih = (struct item_head *)(buf + BLKH_SIZE) + blkh->blk_nr_item - 1;
+  used_space = BLKH_SIZE + IH_SIZE * blkh->blk_nr_item + (g_sb.s_blocksize - ih->ih_item_location);
+  if (used_space != g_sb.s_blocksize - blkh->blk_free_space)
+    return 0;
+  ih = (struct item_head *)(buf + BLKH_SIZE);
+  prev_location = g_sb.s_blocksize;
+  for (i = 0; i < blkh->blk_nr_item; i ++, ih ++) {
+    if (ih->ih_item_location >= g_sb.s_blocksize || ih->ih_item_location < IH_SIZE * blkh->blk_nr_item)
+      return 0;
+    if (ih->ih_item_len < 1 || ih->ih_item_len > MAX_ITEM_LEN (g_sb.s_blocksize))
+      return 0;
+    if (prev_location - ih->ih_item_location != ih->ih_item_len)
+      return 0;
+    prev_location = ih->ih_item_location;
+  }
+
+  return 1;
+}
+
+
+/* retunrs 1 if buf looks like an internal node, 0 otherwise */
+static int is_internal (char * buf)
+{
+  struct block_head * blkh;
+  int used_space;
+
+  blkh = (struct block_head *)buf;
+  used_space = BLKH_SIZE + KEY_SIZE * blkh->blk_nr_item + DC_SIZE * (blkh->blk_nr_item + 1);
+  if (used_space != g_sb.s_blocksize - blkh->blk_free_space)
+    return 0;
+  return 1;
+}
+
+
+/* sometimes unfomatted node looks like formatted, if we check only
+   block_header. This is the reason, why it is so complicated. We
+   believe only when free space and item locations are ok 
+   */
+int not_formatted_node (char * buf)
+{
+  struct block_head * blkh;
+
+  blkh = (struct block_head *)buf;
+
+  if (blkh->blk_level < DISK_LEAF_NODE_LEVEL || blkh->blk_level > MAX_HEIGHT)
+    /* blk_level is out of range */
+    return 1;
+
+  if (blkh->blk_nr_item < 1 || blkh->blk_nr_item > (g_sb.s_blocksize - BLKH_SIZE) / IH_SIZE)
+    /* item number is out of range */
+    return 1;
+
+  if (blkh->blk_free_space > g_sb.s_blocksize - BLKH_SIZE - IH_SIZE)
+    /* free space is out of range */
+    return 1;
+
+  /* check format of nodes, such as we are not sure, that this is formatted node */
+  if (blkh->blk_level == DISK_LEAF_NODE_LEVEL)
+    return (is_leaf (buf) == 1) ? 0 : 1;
+  return (is_internal (buf) == 1) ? 0 : 1;
+}
+
+
+int is_internal_node (char * buf)
+{
+  struct block_head * blkh;
+  
+  blkh = (struct block_head *)buf;
+  if (blkh->blk_level != DISK_LEAF_NODE_LEVEL)
+    return 1;
+  return 0;
+}
+
+#endif /*0*/
+
+/*
+int ready_preserve_list (struct tree_balance * tb, struct buffer_head * bh)
+{
+  return 0;
+}
+
+
+void	preserve_shifted (
+			  struct tree_balance * tb,
+			  struct buffer_head **bh,
+			  struct buffer_head * parent,
+			  int position,
+			  struct buffer_head * dest)
+{
+  return;
+}
+*/
+
+#if 0
+
+char * strs[] =
+{"0%",".",".",".",".","20%",".",".",".",".","40%",".",".",".",".","60%",".",".",".",".","80%",".",".",".",".","100%"};
+
+char progress_to_be[1024];
+char current_progress[1024];
+
+void str_to_be (char * buf, int prosents)
+{
+  int i;
+  prosents -= prosents % 4;
+  buf[0] = 0;
+  for (i = 0; i <= prosents / 4; i ++)
+    strcat (buf, strs[i]);
+}
+
+
+void print_how_far (unsigned long * passed, unsigned long total)
+{
+  int n;
+
+  if (*passed == 0)
+    current_progress[0] = 0;
+
+  if (*passed >= total) {
+    printf/*die*/ ("print_how_far: total %lu has been reached already. cur=%lu\n", total, ++(*passed));
+    return;
+  }
+
+  (*passed) ++;
+  n = ((double)((double)(*passed) / (double)total) * (double)100);
+
+  str_to_be (progress_to_be, n);
+
+  if (strlen (current_progress) != strlen (progress_to_be))
+    printf ("%s", progress_to_be + strlen (current_progress));
+
+  strcat (current_progress, progress_to_be + strlen (current_progress));
+
+
+  fflush (stdout);
+}
+#endif
+
diff -urN linux/fs/reiserfs/utils/fsck/pass1.c /tmp/linux/fs/reiserfs/utils/fsck/pass1.c
--- linux/fs/reiserfs/utils/fsck/pass1.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/fsck/pass1.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,566 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+#include "fsck.h"
+#include <stdlib.h>
+#include "reiserfs.h"
+
+
+void build_the_tree (void);
+int is_item_accessed (struct item_head * ih);
+void mark_item_unaccessed (struct item_head * ih);
+void mark_item_accessed (struct item_head * ih, struct buffer_head * bh);
+
+
+/* allocates buffer head and copy buffer content */
+static struct buffer_head * make_buffer (int dev, int blocknr, int size, char * data)
+{
+  struct buffer_head * bh;
+
+  bh = getblk (dev, blocknr, size);
+  if (buffer_uptodate (bh))
+    die ("make_buffer: uptodate buffer found");
+  memcpy (bh->b_data, data, size);
+  set_bit (BH_Uptodate, (char *)&bh->b_state);
+  return bh;
+}
+
+
+static void find_a_key (struct key * key, struct buffer_head * bh)
+{
+  int i;
+  struct item_head * ih;
+  struct reiserfs_de_head * deh;
+
+  for (i = 0; i < B_NR_ITEMS (bh); i ++) {
+    ih = B_N_PITEM_HEAD (bh, i);
+    if (comp_short_keys (key, &(ih->ih_key)))
+      continue;
+
+    if (key->k_offset == MAX_KEY_OFFSET && key->k_uniqueness == MAX_KEY_OFFSET) {
+      /* look for all items of this file */
+      reiserfs_warning ("\nblock %d contains item of this file %k (item %d)\n", bh->b_blocknr, key, i);
+      return;
+    }
+
+    /* key is specified precisely */
+    if (!comp_keys (key, &(ih->ih_key))) {
+      reiserfs_warning ("\nblock %d contains key %k (item %d)\n", bh->b_blocknr, key, i);
+      return;
+    }
+    if (KEY_IS_DIRECTORY_KEY (key) && I_IS_DIRECTORY_ITEM (ih)) {
+      int j;
+      
+      deh = B_I_DEH (bh, ih);
+      for (j = 0; j < I_ENTRY_COUNT (ih); j ++, deh ++)
+	if (deh->deh_offset == key->k_offset) {
+	  reiserfs_warning ("\nblock %d contains key %k (item_pos %d, pos_in_item %d)\n", bh->b_blocknr, key, i, j);
+	  return;
+	}
+    }
+  }
+}
+
+
+/* analyse contents of indirect items. If it points to used blocks or
+   to uninsertable node, which has to be inserted by items - we free
+   those slots (putting 0-s), if not - mark pointed blocks as used */
+static void handle_indirect_items (struct buffer_head * bh)
+{
+  int i, j;
+  struct item_head * ih;
+
+  for (i = 0, ih = B_N_PITEM_HEAD (bh, 0); i < B_NR_ITEMS (bh); i ++, ih ++) {
+    if (I_IS_INDIRECT_ITEM(ih)) {
+      __u32 * unp;
+      
+      /* check each pointer to unformatted node, if it is in the tree already, put 0 here */
+      unp = (__u32 *)B_I_PITEM (bh, ih);
+      for (j = 0; j < ih->ih_item_len / UNFM_P_SIZE; j ++) {
+	if (unp[j] >= SB_BLOCK_COUNT (&g_sb) || /* invalid data block */
+	    !was_block_used (unp[j]) ||	/* block is marked free in on
+					   disk bitmap */
+	    is_block_used (unp[j]) ||	/* that is either it looked
+					   like leaf or other indirect
+					   item contains this pointer
+					   already */
+	    is_block_uninsertable (unp[j])) {	/* block contains leaf
+						   node, its insertion
+						   has been postponed */
+	  unp[j] = 0;
+	  mark_buffer_dirty (bh, 0);
+	  continue;
+	}
+	/* ok, mark that block is in tree and that it is unformatted node */
+	mark_block_used (unp[j]);
+
+	/* this is for check only */
+	mark_block_unformatted (unp[j]);
+      }
+    }
+  }
+}
+
+int g_unaccessed_items = 0;
+
+int is_item_accessed (struct item_head * ih)
+{
+  return (ih->ih_reserved == 0) ? 1 : 0;
+}
+
+
+void mark_item_unaccessed (struct item_head * ih)
+{
+  g_unaccessed_items ++;
+  ih->ih_reserved = MAX_US_INT;
+}
+
+
+void mark_item_accessed (struct item_head * ih, struct buffer_head * bh)
+{
+  g_unaccessed_items --;
+  ih->ih_reserved = 0;
+  mark_buffer_dirty (bh, 0);
+}
+
+
+/* used when leaf is inserted into tree by pointer
+   1. set sd_nlinks to 0 in all stat data items
+   2. mark all items as unaccessed
+   */
+static void reset_nlinks (struct buffer_head * bh)
+{ 
+  int i;
+  struct item_head * ih;
+
+  ih = B_N_PITEM_HEAD (bh, 0);
+  for (i = 0; i < B_NR_ITEMS (bh); i ++, ih ++) {
+    mark_item_unaccessed (ih);
+    if (I_IS_STAT_DATA_ITEM (ih)) {
+      add_event (STAT_DATA_ITEMS);
+      B_I_STAT_DATA (bh, ih)->sd_nlink = 0;
+    }
+  }
+
+  mark_buffer_dirty (bh, 0);
+}
+
+
+/* ubitmap.c */
+extern int from_journal;
+
+static void insert_pointer (struct buffer_head * bh, struct path * path)
+{
+  struct tree_balance tb;
+  struct item_head * ih;
+  char * body;
+  int memmode;
+  int zeros_number;
+  int retval;
+
+  init_tb_struct (&tb, &g_sb, path, 0x7fff);
+  tb.preserve_mode = NOTHING_SPECIAL;
+
+  /* fix_nodes & do_balance must work for internal nodes only */
+  ih = 0;
+  retval = fix_nodes (0, M_INTERNAL, &tb, PATH_LAST_POSITION (path), ih);
+  if (retval != CARRY_ON)
+    die ("insert_pointer: no free space on device (retval == %d, used blocks from journal %d",
+	 retval, from_journal);
+
+  /* child_pos: we insert after position child_pos: this feature of the insert_child */
+  /* there is special case: we insert pointer after
+     (-1)-st key (before 0-th key) in the parent */
+  if (PATH_LAST_POSITION (path) == 0 && path->pos_in_item == 0)
+    PATH_H_B_ITEM_ORDER (path, 0) = -1;
+  else {
+    if (PATH_H_PPARENT (path, 0) == 0)
+      PATH_H_B_ITEM_ORDER (path, 0) = 0;
+/*    PATH_H_B_ITEM_ORDER (path, 0) = PATH_H_PPARENT (path, 0) ? PATH_H_B_ITEM_ORDER (path, 0) : 0;*/
+  }
+
+  ih = 0;
+  body = (char *)bh;
+  memmode = 0;
+  zeros_number = 0;
+
+  do_balance (0, &tb, path->pos_in_item, ih, body, M_INTERNAL, memmode, zeros_number);
+
+  /* mark as used block itself and pointers to unformatted nodes */
+  mark_block_used (bh->b_blocknr);
+
+  /* this is for check only */
+  mark_block_formatted (bh->b_blocknr);
+  reset_nlinks (bh);
+  handle_indirect_items (bh);
+
+  /* statistic */
+  add_event (GOOD_LEAVES);
+
+}
+
+
+/* return 1 if left and right can be joined. 0 otherwise */
+int balance_condition_fails (struct buffer_head * left, struct buffer_head * right)
+{
+  if (B_FREE_SPACE (left) >= B_CHILD_SIZE (right) - 
+      (are_items_mergeable (B_N_PITEM_HEAD (left, B_NR_ITEMS (left) - 1), B_N_PITEM_HEAD (right, 0), left->b_size) ? IH_SIZE : 0))
+    return 1;
+  return 0;
+}
+
+
+/* return 1 if new can be joined with last node on the path or with
+   its right neighbor, 0 otherwise */
+int balance_condition_2_fails (struct buffer_head * new, struct path * path)
+{
+  struct buffer_head * bh;
+  struct key * right_dkey;
+  int pos, used_space;
+  struct path path_to_right_neighbor;
+
+  bh = PATH_PLAST_BUFFER (path);
+
+
+  if (balance_condition_fails (bh, new))
+    /* new node can be joined with last buffer on the path */
+    return 1;
+
+  /* new node can not be joined with its left neighbor */
+
+  right_dkey = uget_rkey (path);
+  if (right_dkey == 0)
+    /* there is no right neighbor */
+    return 0;
+  
+  pos = PATH_H_POSITION (path, 1);
+  if (pos == B_NR_ITEMS (bh = PATH_H_PBUFFER (path, 1))) {
+    /* we have to read parent of right neighbor. For simplicity we
+       call search_by_key, which will read right neighbor as well */
+    init_path (&path_to_right_neighbor);
+    if (usearch_by_key (&g_sb, right_dkey, &path_to_right_neighbor, 0, 
+			DISK_LEAF_NODE_LEVEL, 0, comp_keys) != ITEM_FOUND)
+      die ("get_right_neighbor_free_space: invalid right delimiting key");
+    used_space =  B_CHILD_SIZE (PATH_PLAST_BUFFER (&path_to_right_neighbor));
+    pathrelse (&path_to_right_neighbor);
+  }
+  else
+    used_space = B_N_CHILD (bh, pos + 1)->dc_size;
+  
+
+  if (B_FREE_SPACE (new) >= used_space - 
+      (are_items_mergeable (B_N_PITEM_HEAD (new, B_NR_ITEMS (new) - 1), (struct item_head *)right_dkey, new->b_size) ? IH_SIZE : 0))
+    return 1;
+  
+  return 0;
+}
+
+
+
+/* inserts pointer to leaf into tree if possible. If not, marks node as uninsrtable */
+static void try_to_insert_pointer_to_leaf (struct buffer_head * new_bh)
+{
+  struct path path;
+  struct buffer_head * bh;			/* last path buffer */
+  struct key * first_bh_key, last_bh_key;	/* first and last keys of new buffer */
+  struct key last_path_buffer_last_key, * right_dkey;
+  int ret_value;
+
+  path.path_length = ILLEGAL_PATH_ELEMENT_OFFSET;
+
+  if (is_block_used (new_bh->b_blocknr))
+    /* block could get into tree already if its number was used by
+       some indirect item */
+    goto cannot_insert;
+
+  first_bh_key = B_N_PKEY (new_bh, 0);
+
+  /* try to find place in the tree for the first key of the coming node */
+/*
+  if (KEY_IS_DIRECTORY_KEY (first_bh_key))
+    ret_value = search_by_entry_key (&g_sb, first_bh_key, &path);
+  else if (KEY_IS_STAT_DATA_KEY (first_bh_key)) {
+    ret_value = reiserfsck_search_by_key (&g_sb, first_bh_key, &path, comp_keys);
+    path.pos_in_item = 0;
+  } else
+    ret_value = search_by_position (&g_sb, first_bh_key, &path);
+  if (ret_value == KEY_FOUND || ret_value == ENTRY_FOUND || ret_value == BYTE_FOUND)
+    goto cannot_insert;
+*/
+  ret_value = usearch_by_key (&g_sb, first_bh_key, &path, 0, DISK_LEAF_NODE_LEVEL, READ_BLOCKS, comp_keys);
+  if (ret_value == KEY_FOUND)
+    goto cannot_insert;
+
+
+  /* get max key in the new node */
+  get_max_buffer_key (new_bh, &last_bh_key);
+  bh = PATH_PLAST_BUFFER (&path);
+  if (comp_keys ((unsigned long *)B_N_PKEY (bh, 0), (unsigned long *)&last_bh_key) == FIRST_GREATER) {
+    /* new buffer falls before the leftmost leaf */
+    if (balance_condition_fails (new_bh, bh))
+      goto cannot_insert;
+
+    if (uget_lkey (&path) != 0 || PATH_LAST_POSITION (&path) != 0)
+      die ("try_to_insert_pointer_to_leaf: bad search result");
+
+    path.pos_in_item = 0;
+    goto insert;
+  }
+
+  /* get max key of buffer, that is in tree */
+  get_max_buffer_key (bh, &last_path_buffer_last_key);
+  if (comp_keys (&last_path_buffer_last_key, first_bh_key) != SECOND_GREATER)
+    /* first key of new buffer falls in the middle of node that is in tree */
+    goto cannot_insert;
+
+  right_dkey = uget_rkey (&path);
+  if (right_dkey && comp_keys (right_dkey, &last_bh_key) != FIRST_GREATER) {
+    goto cannot_insert;
+  }
+
+  if (balance_condition_2_fails (new_bh, &path))
+    goto cannot_insert;
+
+insert:
+  insert_pointer (new_bh, &path);
+
+  goto out;
+
+cannot_insert:
+  /* statistic */
+  add_event (UNINSERTABLE_LEAVES);
+  mark_block_uninsertable (new_bh->b_blocknr);
+
+out:
+  pathrelse (&path);
+  brelse (new_bh);
+  return;
+}
+
+
+
+
+static int tree_is_empty (void)
+{
+  return (SB_ROOT_BLOCK (&g_sb) == ~0) ? 1 : 0;
+}
+
+
+static void make_single_leaf_tree (struct buffer_head * bh)
+{
+  /* tree is empty, make tree root */
+  SB_ROOT_BLOCK (&g_sb) = bh->b_blocknr;
+  SB_TREE_HEIGHT (&g_sb) = 2;
+
+  mark_block_used (bh->b_blocknr);
+
+  /* this is for check only */
+  mark_block_formatted (bh->b_blocknr);
+  
+  /* set stat data nlinks fields to 0, mark all items as unaccessed, analyse contents of indirect
+     items */
+  reset_nlinks (bh);
+  handle_indirect_items (bh);
+
+  /* statistic */
+  add_event (GOOD_LEAVES);
+
+  brelse (bh);
+}
+
+/*
+int g_found_internals, g_freed_internals, g_allocated, g_freed, g_new_internals, g_really_freed;
+*/
+
+/* reads the device by set of 8 blocks, takes leaves and tries to
+   insert them into tree */
+
+void build_the_tree (void)
+{
+    int i, j, k, l;
+    struct buffer_head * bbh, * bh;
+    __u32 handled_blocks = 0;
+    struct si * saved_items = 0;
+
+
+    if ( opt_fsck == 0 )
+	fprintf (stderr, "Pass 1 - ");
+
+    for (i = 0; i < SB_BMAP_NR (&g_sb); i ++)
+	for (j = 0; j < g_sb.s_blocksize; j ++) {
+	    /* make sure, that we are not out of the device */
+	    if (i * g_sb.s_blocksize * 8 + j * 8 >= SB_BLOCK_COUNT (&g_sb))
+		goto out_of_bitmap;
+
+	    //if (i * g_sb.s_blocksize * 8 + j * 8 + 8 > SB_BLOCK_COUNT (&g_sb))
+	    //    die ("build_the_tree: Out of bitmap");
+
+	    if (SB_AP_BITMAP (&g_sb)[i]->b_data[j] == 0) {
+		/* all blocks are free */
+		if (opt_what_to_scan == SCAN_USED_PART)
+		    continue;
+	    }
+
+	    if (i * g_sb.s_blocksize * 8 + j * 8 == (SB_BLOCK_COUNT (&g_sb) & ~(8-1))) {
+		// last byte of bitmap addresses less than 8
+		// blocks. Read them into 32 k buffer
+		l = (SB_BLOCK_COUNT (&g_sb) & 7);
+		bbh = getblk (g_sb.s_dev, i * g_sb.s_blocksize + j, 8 * g_sb.s_blocksize);
+		for (k = 0; k < l; k ++) {
+		    struct buffer_head * tmp;
+
+		    tmp = bread (g_sb.s_dev, i * g_sb.s_blocksize * 8 + j * 8 + k, g_sb.s_blocksize);
+		    memcpy (bbh->b_data + k * g_sb.s_blocksize, tmp->b_data, g_sb.s_blocksize);
+		    brelse (tmp);
+		}
+		mark_buffer_uptodate (bbh, 1);
+	    } else {
+		l = 8;
+		bbh = bread (g_sb.s_dev, i * g_sb.s_blocksize + j, 8 * g_sb.s_blocksize);
+	    }
+	    
+	    
+	    for (k = 0; k < l; k ++) {
+		unsigned long block;
+
+		if ((SB_AP_BITMAP (&g_sb)[i]->b_data[j] & (1 << k)) == 0) {
+		    /* k-th block is free */
+		    if (opt_what_to_scan == SCAN_USED_PART)
+			continue;
+		}
+
+		if ( opt_fsck == 0 )
+		    print_how_far (&handled_blocks, g_blocks_to_read);
+
+		block = i * g_sb.s_blocksize * 8 + j * 8 + k;
+		if (not_data_block (&g_sb, block)) {
+		    /* skip not data area of the filesystem (journal,
+                       bitmaps, reserved space) */
+		    continue;
+		}
+
+		if (not_formatted_node (bbh->b_data + k * g_sb.s_blocksize, g_sb.s_blocksize))
+		    continue;
+		if (is_internal_node (bbh->b_data + k * g_sb.s_blocksize) == 1 && opt_fsck_mode == FSCK_REBUILD) {
+/*		    g_found_internals ++;*/
+		    if (!is_block_used (block)) {
+/*			g_freed_internals ++;*/
+			reiserfs_free_internal_block (&g_sb, i * g_sb.s_blocksize * 8 + j * 8 + k);
+		    } else
+			/* block is used in new tree already. There was an
+			   indirect item, pointing to it. We keep information
+			   about it for check only */
+			/*mark_formatted_pointed_by_indirect (block)*/;
+
+		    continue;
+		}
+	  
+		/* leaf node found */
+		bh = make_buffer (g_sb.s_dev, block, g_sb.s_blocksize, bbh->b_data + k * g_sb.s_blocksize);
+
+		/* */
+		if (opt_fsck_mode == FSCK_FIND_ITEM) {
+		    find_a_key (&key_to_find, bh);
+		    brelse (bh);
+		    continue;
+		}
+
+	  
+		if (is_block_used (block)) {
+		    /* block is used in new tree already. There was an indirect
+		       item, pointing to it. We keep information about it for
+		       check only */
+		    /*	  mark_formatted_pointed_by_indirect (block);*/
+
+		    add_event (LEAVES_USED_BY_INDIRECT_ITEMS);
+		    /* Rather than try to find UNP to this block we save its
+		       items and will put them into tree at the end of pass 1 */
+		    for_all_items_in_node (save_item, &saved_items, bh);
+		    brelse (bh);
+		    continue;
+		}
+
+		if (is_leaf_bad (bh)) {
+		    /* leaf is bad: directory item structure corrupted, or something else */
+		    /*	  mark_formatted_pointed_by_indirect (block);*/
+		    if (opt_verbose)
+			reiserfs_warning ("\nbuild_the_tree: bad leaf encountered: %lu\n", bh->b_blocknr);
+		    add_event (LEAVES_USED_BY_INDIRECT_ITEMS);
+		    /* Save good items only to put them into tree at the end of this pass */
+		    for_all_items_in_node (save_item, &saved_items, bh);
+		    brelse (bh);
+		    continue;
+		}
+
+		if (tree_is_empty () == 1) {
+		    make_single_leaf_tree (bh);
+		    continue;
+		}
+
+		/* if the leaf node can not be inserted into tree by pointer,
+		   we postpone its insertion at the end of the pass 1 */
+		try_to_insert_pointer_to_leaf (bh);
+
+/*
+		if (opt_check == 1)
+		    reiserfsck_check_pass1 ();
+*/
+	    }
+
+	    bforget (bbh);
+	}
+
+
+ out_of_bitmap:
+
+/*
+    check_bitmaps (&g_sb);
+
+    printf ("bitmap scanning completed: allocated %d, freed %d *really freed %d, new internals %d, found internals %d (freed %d)\n",
+	    g_allocated, g_freed, g_really_freed, g_new_internals, g_found_internals, g_freed_internals);fflush (stdout);
+*/
+
+    if (opt_fsck_mode == FSCK_FIND_ITEM)
+	return;
+
+    /* this checks what has been built (if -c option is set) */
+/*    reiserfsck_check_pass1 ();*/
+
+    /* put saved items into tree. These items were in leaves, those
+       could not be inserted into tree because some indirect items point
+       to those leaves. Rather than lookup for corresponding unfm
+       pointers in the tree, we save items of those leaves and put them
+       into tree separately */
+    if ( opt_fsck == 0 )
+	printf ("\nPass 1a - ");
+    put_saved_items_into_tree (saved_items);
+    if ( opt_fsck == 0 )
+	printf ("done\n");
+
+    /* end of pass 1 */
+    if ( opt_fsck == 0 )
+	printf ("\n");
+
+    /* this works only if -c specified  */
+    reiserfsck_check_pass1 ();
+
+
+    if (opt_stop_point == STOP_AFTER_PASS1)
+	return;
+
+    /* pass 2 */
+    take_bad_blocks_put_into_tree ();
+
+    reiserfsck_check_pass1 ();
+
+}
+
+
+
+
+
+
+
+
+
+
diff -urN linux/fs/reiserfs/utils/fsck/pass2.c /tmp/linux/fs/reiserfs/utils/fsck/pass2.c
--- linux/fs/reiserfs/utils/fsck/pass2.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/fsck/pass2.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,235 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+#include "fsck.h"
+
+#include "reiserfs.h"
+
+void for_all_items_in_node (action_on_item_t action, struct si ** si, struct buffer_head * bh)
+{
+  int i;
+  struct item_head * ih;
+
+  for (i = 0, ih = B_N_PITEM_HEAD (bh, 0); i < B_NR_ITEMS (bh); i ++, ih ++)
+    action (si, ih, B_I_PITEM (bh,ih));
+#if 0
+  int j;
+
+  for (i = B_NR_ITEMS (bh) / 2, j = i + 1; ; i --, j ++) {
+    if (i >= 0) {
+      ih = B_N_PITEM_HEAD (bh, i);
+      action (si, ih, B_I_PITEM (bh,ih));
+    }
+    if (j < B_NR_ITEMS (bh)) {
+      ih = B_N_PITEM_HEAD (bh, j);
+      action (si, ih, B_I_PITEM (bh,ih));
+    }
+
+/*    check_buffer_queues ();*/
+
+    if (i <= 0 && j >= B_NR_ITEMS (bh) - 1)
+      break;
+  }
+#endif
+}
+
+
+/* insert sd item if it does not exist, overwrite it otherwise */
+static void put_sd_item_into_tree (struct item_head * comingih, char * item)
+{
+  struct item_head ih;
+  struct path path;
+  struct buffer_head * path_bh;
+  int path_item_num;
+  struct stat_data * psd;
+
+  copy_key (&(ih.ih_key), &(comingih->ih_key));
+  if (usearch_by_key (&g_sb, &(ih.ih_key), &path, 0, DISK_LEAF_NODE_LEVEL, READ_BLOCKS, comp_keys) == ITEM_FOUND) {
+    /* overwrite stat data in the tree */
+    path_bh = PATH_PLAST_BUFFER (&path);
+    path_item_num = PATH_LAST_POSITION (&path);
+    psd = B_N_STAT_DATA (path_bh, path_item_num);
+    if (psd->sd_nlink != 0)
+      die ("put_sd_item_into_tree: all stat data in the tree (at this moment) must have nllinks == 0 (not %d)",
+	   psd->sd_nlink);
+    if (psd->sd_mtime > ((struct stat_data *)item)->sd_mtime) {
+      /* new sd is newer than the found one */
+      memcpy (psd, item, SD_SIZE);
+      psd->sd_nlink = 0;
+      psd->sd_first_direct_byte = NO_BYTES_IN_DIRECT_ITEM;
+      mark_buffer_dirty (PATH_PLAST_BUFFER (&path), 0);
+    }
+    pathrelse (&path);
+  } else {
+    struct stat_data sd;
+
+    ih.ih_item_len = SD_SIZE;
+    ih.u.ih_free_space = MAX_US_INT;
+    mark_item_unaccessed (&ih);
+    memcpy (&sd, item, SD_SIZE);
+    sd.sd_nlink = 0;
+    sd.sd_first_direct_byte = NO_BYTES_IN_DIRECT_ITEM;
+    reiserfsck_insert_item (&path, &ih, (const char *)&sd);
+
+    add_event (STAT_DATA_ITEMS);
+  }
+}
+
+
+/* Keyed 32-bit hash function using TEA in a Davis-Meyer function */
+/*
+static unsigned long get_third_component (char * name, int len)
+{
+  if (!len || (len == 1 && name[0] == '.'))
+    return DOT_OFFSET;
+  if (len == 2 && name[0] == '.' && name[1] == '.')
+    return DOT_DOT_OFFSET;
+  return keyed_hash (name, len);
+}
+*/
+
+static int reiserfsck_find_entry (struct key * key, struct reiserfs_de_head * deh, struct path * path)
+{
+  struct key entry_key;
+
+  copy_key (&entry_key, key);
+  entry_key.k_offset = deh->deh_offset;
+  entry_key.k_uniqueness = DIRENTRY_UNIQUENESS;
+
+  return usearch_by_entry_key (&g_sb, &entry_key, path);
+}
+
+
+/* this tries to put each item entry to the tree, if there is no items of
+   the directory, insert item containing 1 entry */
+static void put_directory_item_into_tree (struct item_head * comingih, char * item)
+{
+  /*  struct item_head * ih;*/
+  struct reiserfs_de_head * deh;
+  int i, retval;
+  struct path path;
+  int size;
+  char * buf, * entry;
+  struct item_head tmpih;
+
+  /*ih = B_N_PITEM_HEAD (bh, item_num);*/
+  deh = (struct reiserfs_de_head *)item;/*B_I_DEH (bh, comingih);*/
+
+  for (i = 0; i < I_ENTRY_COUNT (comingih); i ++, deh ++) {
+    entry = item + deh->deh_location;
+    retval = reiserfsck_find_entry (&(comingih->ih_key), deh, &path);
+    switch (retval) {
+    case ENTRY_FOUND:
+      pathrelse (&path);
+      break;
+
+    case ENTRY_NOT_FOUND:
+      /* paste_into_item accepts entry to paste as buffer, beginning
+         with entry header and body, that follows it */
+      buf = getmem (size = I_DEH_N_ENTRY_LENGTH (comingih, deh, i) + DEH_SIZE);
+      memcpy (buf, deh, DEH_SIZE);
+      ((struct reiserfs_de_head *)buf)->deh_location = 0;
+      memcpy (buf + DEH_SIZE, entry, size - DEH_SIZE);
+
+      reiserfsck_paste_into_item (&path, buf, size);
+
+      freemem (buf);
+      break;
+
+    case DIRECTORY_NOT_FOUND:
+      buf = getmem (size = I_DEH_N_ENTRY_LENGTH (comingih, deh, i) + DEH_SIZE);
+      memcpy (buf, deh, DEH_SIZE);
+      ((struct reiserfs_de_head *)buf)->deh_location = DEH_SIZE;
+      memcpy (buf + DEH_SIZE, entry, size - DEH_SIZE);
+      copy_key (&(tmpih.ih_key), &(comingih->ih_key));
+      tmpih.ih_item_len = size;
+      tmpih.u.ih_entry_count = 1;
+      mark_item_unaccessed (&tmpih);
+      
+      reiserfsck_insert_item (&path, &tmpih, buf);
+
+      freemem (buf);
+      break;
+
+    case REGULAR_FILE_FOUND:
+      /* this should never happen. */
+      goto end;
+    }
+
+    /*&&&&&&&&&&&&&&&&&*/
+/*    reiserfsck_check_pass1 ();*/
+    /*&&&&&&&&&&&&&&&&&*/
+  }
+ end:
+
+}
+
+
+/* If item is item of regular file (direct or indirect item) - this
+   file is in tree (with first byte) - write to it. If this file is in
+   tree (without first byte) - delete what we have in tree, create
+   file again keeping what we already had in tree this file is not in
+   tree - create hole at the beginning of file if necessary and write
+   to file */
+void put_regular_file_item_into_tree (struct item_head * ih, char * item)
+{
+  reiserfsck_file_write (ih, item);
+}
+
+
+void insert_item_separately (struct si ** si, struct item_head * ih, char * item)
+{
+  if (I_IS_STAT_DATA_ITEM (ih)) {
+    put_sd_item_into_tree (ih, item);
+  } else if (I_IS_DIRECTORY_ITEM (ih)) {
+    put_directory_item_into_tree (ih, item);
+  } else {
+    put_regular_file_item_into_tree (ih, item);
+  }
+
+  
+}
+
+
+/* uninsertable blocks are marked by 0s in
+   g_uninsertable_leaf_bitmap during the pass 1. They still must be not in the tree */
+void take_bad_blocks_put_into_tree (void)
+{
+  struct buffer_head * bh;
+  int i, j;
+  __u32 bb_counter = 0;
+
+  if ( opt_fsck == 0 )
+    fprintf (stderr, "Pass 2 - ");
+
+
+  for (i = 0; i < SB_BMAP_NR (&g_sb); i ++) {
+    j = find_first_zero_bit (g_uninsertable_leaf_bitmap[i], g_sb.s_blocksize * 8);
+    while (j < g_sb.s_blocksize * 8) {
+      bh = bread (g_sb.s_dev, i * g_sb.s_blocksize * 8 + j, g_sb.s_blocksize);
+
+      if (is_block_used (bh->b_blocknr))
+	die ("take_bad_blocks_put_into_tree: block %d can not be in tree", bh->b_blocknr);
+      /* this must be leaf */
+      if (not_formatted_node (bh->b_data, g_sb.s_blocksize) || is_internal_node (bh->b_data)) {
+	reiserfs_panic (0, "take_bad_blocks_put_into_tree: buffer (%b %z) must contain leaf", bh, bh);
+      }
+
+      for_all_items_in_node (insert_item_separately, 0, bh);
+
+      if ( opt_fsck == 0 )
+	print_how_far (&bb_counter, get_event (UNINSERTABLE_LEAVES));
+
+      brelse (bh);
+
+      j = find_next_zero_bit (g_uninsertable_leaf_bitmap[i], g_sb.s_blocksize * 8, j + 1);
+    }
+  }
+
+  if (bb_counter != get_event (UNINSERTABLE_LEAVES))
+    die ("take_bad_blocks_put_into_tree: found bad block %d, must be %d", 
+	 bb_counter, get_event (UNINSERTABLE_LEAVES));
+
+  if ( opt_fsck == 0 )
+    printf ("\n");
+}
diff -urN linux/fs/reiserfs/utils/fsck/pass4.c /tmp/linux/fs/reiserfs/utils/fsck/pass4.c
--- linux/fs/reiserfs/utils/fsck/pass4.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/fsck/pass4.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,92 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+#include "fsck.h"
+
+
+static void get_next_key (struct path * path, int i, struct key * key)
+{
+    struct buffer_head * bh = PATH_PLAST_BUFFER (path);
+    struct key maxkey = {0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff};
+    struct key * rkey;
+
+    if (i < B_NR_ITEMS (bh) - 1) {
+	copy_key (key, B_N_PKEY (bh, i + 1));
+	return;
+    }
+
+    rkey = uget_rkey (path);
+    if (rkey) {
+	copy_key (key, rkey);
+	if (comp_keys (key, B_PRIGHT_DELIM_KEY (bh)) != KEYS_IDENTICAL) {
+	    add_event (FIXED_RIGHT_DELIM_KEY);
+	    copy_key (B_PRIGHT_DELIM_KEY (bh), key);
+	    mark_buffer_dirty (bh, 0);
+	}
+    } else {
+	if (comp_keys (&maxkey, B_PRIGHT_DELIM_KEY (bh)) != KEYS_IDENTICAL) {
+	    /*printf ("get_next_key: Hmm, max key not found in the tree\n");*/
+	    copy_key (B_PRIGHT_DELIM_KEY (bh), &maxkey);
+	    mark_buffer_dirty (bh, 0);
+	}
+	copy_key (key, &maxkey);
+    }
+}
+
+/* this is pass 4 */
+int check_unaccessed_items (void)
+{
+    struct key key;
+    struct path path;
+    int i;
+    struct buffer_head * bh;
+    struct item_head * ih;
+    __u32 passed = 0;
+
+    if (opt_stop_point != STOP_DEFAULT)
+	return 0;
+
+    path.path_length = ILLEGAL_PATH_ELEMENT_OFFSET;
+    copy_key (&key, &g_root_directory_key);
+
+    if ( opt_fsck == 0 )
+	fprintf (stderr, "Pass 4 - ");
+
+    while (/*reiserfsck_*/usearch_by_key (&g_sb, &key, &path, 0, DISK_LEAF_NODE_LEVEL, READ_BLOCKS, comp_keys) == ITEM_FOUND) {
+	bh = PATH_PLAST_BUFFER (&path);
+	for (i = PATH_LAST_POSITION (&path), ih = PATH_PITEM_HEAD (&path); i < B_NR_ITEMS (bh); i ++, ih ++) {
+	    if (is_item_accessed (ih) == 0) {
+
+		get_next_key (&path, i, &key);
+
+		add_event (UNACCESSED_ITEMS);
+		if (I_IS_STAT_DATA_ITEM (ih))
+		    g_fsck_info.fs_stat_data_items --;
+	
+		PATH_LAST_POSITION (&path) = i;
+		reiserfsck_delete_item (&path);
+
+		goto cont;
+	    }
+	    if ((I_IS_STAT_DATA_ITEM (ih)) && opt_fsck == 0) {
+		print_how_far (&passed, get_event (STAT_DATA_ITEMS));
+	    }
+	}
+	get_next_key (&path, i - 1, &key);
+	pathrelse (&path);
+
+/*fu_check ();*/
+
+    cont:
+    }
+    if (key.k_dir_id != MAX_UL_INT || key.k_objectid != MAX_UL_INT ||
+	key.k_offset != MAX_UL_INT || key.k_uniqueness != MAX_UL_INT) {
+	reiserfs_panic (0, "check_unaccessed_items: invalid exit key %k", &key);
+    }
+    pathrelse (&path);
+
+    if ( opt_fsck == 0 )
+	printf ("\n");
+
+    return 0;
+}
diff -urN linux/fs/reiserfs/utils/fsck/reiserfsck.8 /tmp/linux/fs/reiserfs/utils/fsck/reiserfsck.8
--- linux/fs/reiserfs/utils/fsck/reiserfsck.8	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/fsck/reiserfsck.8	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,64 @@
+.\" -*- nroff -*-
+.\" Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+.\" 
+.TH REISERFSCK 8 "February 1999" "Reiserfs utilities"
+.SH NAME
+reiserfsck \- check a Linux Reiserfs file system
+.SH SYNOPSIS
+.B reiserfsck 
+[
+.I mode
+] [ 
+.I area to scan
+] [
+.I howto replay journal
+] [
+.I stop point
+] [
+.I \-aprv
+]
+.I device
+.SH DESCRIPTION
+.B reiserfsck
+is used to perform a consistency check for the Linux Reiserfs file
+system.
+.SH Non-optional argument
+.TP
+.I device
+is the special file corresponding to the device (e.g /dev/hdXX for
+IDE disk partition or /dev/sdXX for SCSI disk partition).
+.SH Long options by categories
+.TP
+.I mode
+can be --check (default) or --rebuild-tree. First option makes fsck to read-only check consistency of the filesystem. The second one causes rebuild the filesystem tree using everything what can be found on the partition. Normally, you do not need this, but if you do - please backup whole partition first or at least the most important data. 
+.TP
+.I area to scan
+can be --scan-used-part-only (default) or --scan-whole-partition. In the default case only used part of the partition will be scanned running with --rebuild-tree set. Otherwise - whole partition will be scanned. It is very unlikely, that you will ever use it, however.
+.TP
+.I howto replay journal
+can be --replay-by-mount (default, will call mount -o replay-only, the kernel must support reiserfs for that), --replay-whole-journal - fsck will replay all valid transactions found in the journal, and --no-replay-journal.
+.SH Short options
+.TP
+.I -a 
+Suppress output information. As reiserfsck can not be ran in interactive more - this option makes no effect on that
+.TP
+.I -p
+does nothing
+.TP
+.I -r
+does nothing
+.TP
+.I -v
+Verbose mode.
+.\" .SH AUTHOR
+.\" This version of
+.\" .B reiserfsck
+.\" has been written by Hans Reiser <reiser@idiom.com>.
+.SH BUGS
+There is no signal handling.
+Please, report bugs to Hans Reiser <reiser@idiom.com>.
+.SH AVAILABILITY
+.B reiserfsck
+sources are included into reiserfs patch and appear in the kernel source tree after applying under directory ./fs/reiserfs/utils/fsck.
+.SH SEE ALSO
+.BR mkreiserfs (8)
diff -urN linux/fs/reiserfs/utils/fsck/segments.c /tmp/linux/fs/reiserfs/utils/fsck/segments.c
--- linux/fs/reiserfs/utils/fsck/segments.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/fsck/segments.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,223 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+/*#include <stdio.h>
+#include <string.h>*/
+/*#include <asm/bitops.h>
+#include "../include/reiserfs_fs.h"
+#include "../include/reiserfs_fs_sb.h"
+#include "../include/reiserfslib.h"*/
+#include "fsck.h"
+
+
+/* there is a situation, when we overwrite contents of unformatted
+   node with direct item. One unformatted node can be overwritten
+   several times by direct items */
+/*
+struct overwritten_unfm_segment {
+  int ous_begin;
+  int ous_end;
+  struct overwritten_unfm_segment * ous_next;  
+};
+*/
+struct overwritten_unfm {
+  unsigned long ou_unfm_ptr;	/* block number of unfm node */
+  unsigned long ou_dir_id;
+  unsigned long ou_objectid; 	/* key corresponding to an unfm node */
+  unsigned long ou_offset;
+
+  struct overwritten_unfm_segment * ou_segments;	/* list of segmens, than have been overwritten in ths unfm node */
+};
+
+struct overwritten_unfm ** g_overwritten_unfms;
+int g_overwritten_unfms_amount;	/* number of unformatted nodes, which contain direct items */
+
+
+/* adds segment to the single linked list of segments sorted by begin
+   field. Retuns pointer to first element of list */
+static struct overwritten_unfm_segment * add_segment (struct overwritten_unfm_segment * first, int begin, int end)
+{
+  struct overwritten_unfm_segment * new, * next, * prev;
+
+  new = getmem (sizeof (struct overwritten_unfm_segment));
+  new->ous_begin = begin;
+  new->ous_end = end;
+  new->ous_next = 0;
+
+  next = first;
+  prev = 0;
+  while (next) {
+    if (next->ous_begin > begin)
+      break;
+    prev = next;
+    next = next->ous_next;
+  }
+
+  if (prev == 0) {
+    /* insert into head of list */
+    first = new;
+  } else {
+    prev->ous_next = new;
+  }
+  new->ous_next = next;
+  return first;
+}
+
+
+/* input parameter 
+   `list_head` - first element of overlapping segments sorted by left edge
+   `unoverwritten_segment` - returned by previous call of get_unoverwritten_segment or (-2,-2) if called first time
+   */
+/* returns
+   1 and segment unoverwritten by elements of list `list_head`
+   0 if there isno such segment
+   */
+int get_unoverwritten_segment (struct overwritten_unfm_segment * list_head, struct overwritten_unfm_segment * unoverwritten_segment)
+{
+  int end;
+
+  /* look for segment, which has begin field greater than end of previous interval */
+  while (list_head->ous_begin <= unoverwritten_segment->ous_end) {
+    list_head = list_head->ous_next;
+  }
+  /* look for the end of the continuous region covered by otrezkami */
+  end = list_head->ous_end;
+  while (list_head->ous_next) {
+    if (list_head->ous_next->ous_begin > end + 1)
+      /* intreval found */
+      break;
+    if (list_head->ous_next->ous_end > end)
+      end = list_head->ous_next->ous_end;
+    list_head = list_head->ous_next;
+  }
+  /* ok, between segment and segment->next we have an interval (segment->next != 0) */
+  if (list_head->ous_next != 0) {
+    unoverwritten_segment->ous_begin = end + 1;
+    unoverwritten_segment->ous_end = list_head->ous_next->ous_begin - 1;
+    return 1;
+  }
+  return 0;
+}
+
+
+void print_segments (struct overwritten_unfm_segment * list_head)
+{
+  struct overwritten_unfm_segment * cur;
+
+  cur = list_head;
+  while (cur) {
+    printf ("%s%d %d%s", cur == list_head ? "(" : "", cur->ous_begin, cur->ous_end, cur->ous_next ? ", " : ")\n");
+    cur = cur->ous_next;
+  }
+}
+
+
+/* this prepare list of segments to extracting of unoverwritten segments */
+struct overwritten_unfm_segment * find_overwritten_unfm (unsigned long unfm, int length, struct overwritten_unfm_segment * segment_to_init)
+{
+  int i;
+
+  for (i = 0; i < g_overwritten_unfms_amount && g_overwritten_unfms[i] != 0; i ++)
+    if (g_overwritten_unfms[i]->ou_unfm_ptr == unfm) {
+      if (g_overwritten_unfms[i]->ou_segments == 0)
+	die ("find_overwritten_unfm: no segment found");
+      g_overwritten_unfms[i]->ou_segments = add_segment (g_overwritten_unfms[i]->ou_segments, -1, -1);
+      add_segment (g_overwritten_unfms[i]->ou_segments, length, length);
+      segment_to_init->ous_begin = -2;
+      segment_to_init->ous_end = -2;
+      return g_overwritten_unfms[i]->ou_segments;
+    }
+  return 0;
+}
+
+struct overwritten_unfm * look_for_overwritten_unfm (__u32 unfm)
+{
+  int i;
+
+  for (i = 0; i < g_overwritten_unfms_amount && g_overwritten_unfms[i] != 0; i ++)
+    if (g_overwritten_unfms[i]->ou_unfm_ptr == unfm)
+      return g_overwritten_unfms[i];
+    return 0;
+}
+
+#define GROW_BY 10
+struct overwritten_unfm * add_overwritten_unfm (unsigned long unfm, struct item_head * direct_ih)
+{
+  int i;
+
+  for (i = 0; i < g_overwritten_unfms_amount && g_overwritten_unfms[i] != 0; i ++) {
+    if (g_overwritten_unfms[i]->ou_unfm_ptr == unfm)
+      return g_overwritten_unfms[i];
+  }
+
+  if (i == g_overwritten_unfms_amount) {
+    g_overwritten_unfms = expandmem (g_overwritten_unfms, sizeof (struct overwritten_unfm *) * i, 
+				     sizeof (struct overwritten_unfm *) * GROW_BY);
+    g_overwritten_unfms_amount += GROW_BY;
+  }
+  g_overwritten_unfms[i] = getmem (sizeof (struct overwritten_unfm));
+  g_overwritten_unfms[i]->ou_unfm_ptr = unfm;
+  g_overwritten_unfms[i]->ou_dir_id = direct_ih->ih_key.k_dir_id;
+  g_overwritten_unfms[i]->ou_objectid = direct_ih->ih_key.k_objectid;
+  g_overwritten_unfms[i]->ou_offset = direct_ih->ih_key.k_offset - (direct_ih->ih_key.k_offset - 1) % g_sb.s_blocksize;
+  return g_overwritten_unfms[i];
+}
+
+
+void save_unfm_overwriting (unsigned long unfm, struct item_head * direct_ih)
+{
+  struct overwritten_unfm * ov_unfm;
+
+  /* add new overwritten unfm or return existing one */
+  ov_unfm = add_overwritten_unfm (unfm, direct_ih);
+  ov_unfm->ou_segments = add_segment (ov_unfm->ou_segments, (direct_ih->ih_key.k_offset - 1) % g_sb.s_blocksize,
+				      (direct_ih->ih_key.k_offset - 1) % g_sb.s_blocksize + direct_ih->ih_item_len - 1);
+}
+
+
+void free_overwritten_unfms (void)
+{
+  int i;
+
+  for (i = 0; i < g_overwritten_unfms_amount && g_overwritten_unfms[i]; i ++) {
+    /* free all segments */
+    while (g_overwritten_unfms[i]->ou_segments) {
+      struct overwritten_unfm_segment * tmp;
+
+      tmp = g_overwritten_unfms[i]->ou_segments->ous_next;
+      freemem (g_overwritten_unfms[i]->ou_segments);
+      g_overwritten_unfms[i]->ou_segments = tmp;
+    }
+    /* free struct overwritten_unfm */
+    freemem (g_overwritten_unfms[i]);
+  }
+
+  /* free array of pointers to overwritten unfms */
+  if (g_overwritten_unfms)
+    freemem (g_overwritten_unfms);
+}
+
+#if 0
+static int formatted_pointed_by_indirect;
+static __u32 * stored;
+static int length;
+void mark_formatted_pointed_by_indirect (__u32 block)
+{
+  if (stored == 0 || length == formatted_pointed_by_indirect) {
+    stored = expandmem (stored, sizeof (__u32) * length, sizeof (__u32) * 1000);
+    length += 1000;
+  }
+  stored [formatted_pointed_by_indirect ++] = block;
+}
+
+int is_formatted_pointed_by_indirect (__u32 block)
+{
+  int i;
+
+  for (i = 0; i < formatted_pointed_by_indirect; i ++)
+    if (stored [i] == block)
+      return 1;
+
+  return 0;
+}
+#endif
diff -urN linux/fs/reiserfs/utils/fsck/semantic.c /tmp/linux/fs/reiserfs/utils/fsck/semantic.c
--- linux/fs/reiserfs/utils/fsck/semantic.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/fsck/semantic.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,532 @@
+/*
+ * Copyright 2000 Hans Reiser, licensing governed by reiserfs/README
+ */
+#include "fsck.h"
+#include <time.h>
+
+
+
+
+/* path is path to stat data */
+static void check_regular_file (struct path * path, struct stat_data * sd)
+{
+    int mark_passed_items;
+    struct key key;
+    unsigned long size;
+    struct buffer_head * bh = PATH_PLAST_BUFFER (path);/* contains stat data */
+    struct item_head * ih = PATH_PITEM_HEAD (path);/* stat data item */
+
+    if (sd->sd_nlink == 0) {
+
+/*    print_how_far (&stat_datas, get_event (STAT_DATA_ITEMS));*/
+	if ((sd->sd_mode & S_IFMT) == S_IFREG)
+	    add_event (REGULAR_FILES);
+	else if ((sd->sd_mode & S_IFMT) == S_IFLNK)
+	    add_event (SYMLINKS);
+	else
+	    add_event (OTHERS);
+	sd->sd_nlink ++;
+	mark_item_accessed (ih, bh);
+	mark_objectid_as_used (ih->ih_key.k_objectid);
+
+	copy_key (&key, &(ih->ih_key));
+	if (are_file_items_correct (&key, &size, mark_passed_items = 1, path, &sd) != 1) {
+	    /* unpassed items will be deleted in pass 4 as they left unaccessed */
+	    add_event (INCORRECT_REGULAR_FILES);
+	}
+	/* are_file_items_correct could perform indirect_to_direct, bh could be changed */
+	bh = PATH_PLAST_BUFFER (path);
+	/* set correct size */
+	if (sd->sd_size != size) {
+	    add_event (FIXED_SIZE_FILES);
+	    sd->sd_size = size;
+	    mark_buffer_dirty (bh, 0);
+	}
+	/* set first direct byte field of stat data (if it is set incorrect) */
+	if (size == 0 || KEY_IS_INDIRECT_KEY(&key)) {
+	    /* there are no direct items in file */
+	    if (sd->sd_first_direct_byte != NO_BYTES_IN_DIRECT_ITEM) {
+		sd->sd_first_direct_byte = NO_BYTES_IN_DIRECT_ITEM;
+		mark_buffer_dirty (bh, 0);
+	    }
+	} else {
+	    /* there is at least one direct item */
+	    if (sd->sd_first_direct_byte != key.k_offset - (key.k_offset % g_sb.s_blocksize - 1)) {
+		sd->sd_first_direct_byte = key.k_offset - (key.k_offset % g_sb.s_blocksize - 1);
+		mark_buffer_dirty (bh, 0);
+	    }
+	}
+    } else {
+	if (is_item_accessed (ih) == 0)
+	    die ("check_regular_file: stat data item must be accessed");
+	sd->sd_nlink ++;
+	mark_buffer_dirty (bh, 0);
+    }
+}
+
+
+static int is_rootdir_key (struct key * key)
+{
+    if (comp_keys (key, &root_key))
+	return 0;
+    return 1;
+}
+
+static int is_rootdir_entry_key (struct key * key)
+{
+    if (comp_short_keys (key, &root_key))
+	return 0;
+    return 1;
+}
+
+
+/* when root direcotry can not be found */
+static void create_root_directory (struct path * path)
+{
+    struct item_head ih;
+    struct stat_data sd;
+
+    /* insert stat data item */
+    copy_key (&(ih.ih_key), &root_key);
+    ih.ih_item_len = SD_SIZE;
+    ih.u.ih_free_space = MAX_US_INT;
+    mark_item_unaccessed (&ih);
+
+    sd.sd_mode = S_IFDIR + 0755;
+    sd.sd_nlink = 0;
+    sd.sd_uid = 0;
+    sd.sd_gid = 0;
+    sd.sd_size = EMPTY_DIR_SIZE;
+    sd.sd_atime = sd.sd_ctime = sd.sd_mtime = time (NULL);
+    sd.u.sd_blocks = 0;
+    sd.sd_first_direct_byte = MAX_UL_INT;
+  
+    reiserfsck_insert_item (path, &ih, (char *)(&sd));
+}
+
+
+static void paste_dot_and_dot_dot (struct path * path)
+{
+    char dir[EMPTY_DIR_SIZE];
+    struct reiserfs_de_head * deh;
+    struct key key;
+  
+    copy_key (&key, &root_key);
+
+    deh = (struct reiserfs_de_head *)dir;
+    deh[0].deh_offset = DOT_OFFSET;
+    deh[0].deh_dir_id = root_key.k_dir_id;
+    deh[0].deh_objectid = root_key.k_objectid;
+    deh[0].deh_state = 0;
+    set_bit (DEH_Visible, &(deh[0].deh_state));
+    dir[DEH_SIZE] = '.';
+    reiserfsck_paste_into_item (path, dir, DEH_SIZE + 1);
+
+    key.k_offset = DOT_DOT_OFFSET;
+    key.k_uniqueness = DIRENTRY_UNIQUENESS;
+    if (usearch_by_entry_key (&g_sb, &key, path) == ENTRY_FOUND) {
+	reiserfs_warning ("paste_dot_and_dot_dot: \"..\" found\n");
+	pathrelse (path);
+	return;
+    }
+    deh[0].deh_offset = DOT_DOT_OFFSET;
+    deh[0].deh_dir_id = 0;
+    deh[0].deh_objectid = root_key.k_dir_id;
+    deh[0].deh_state = 0;
+    set_bit (DEH_Visible, &(deh[0].deh_state));
+    dir[DEH_SIZE] = '.';
+    dir[DEH_SIZE + 1] = '.';
+
+    reiserfsck_paste_into_item (path, dir, DEH_SIZE + 2);
+}
+
+
+static void insert_dot_and_dot_dot (struct path * path)
+{
+    struct item_head ih;
+    char dir[EMPTY_DIR_SIZE];
+    struct reiserfs_de_head * deh;
+   
+    copy_key (&(ih.ih_key), &root_key);
+    ih.ih_key.k_offset = DOT_OFFSET;
+    ih.ih_key.k_uniqueness = DIRENTRY_UNIQUENESS;
+    ih.ih_item_len = EMPTY_DIR_SIZE;
+    ih.u.ih_entry_count = 2;
+    mark_item_unaccessed (&ih);
+
+    deh = (struct reiserfs_de_head *)dir;
+    deh[0].deh_offset = DOT_OFFSET;
+    deh[0].deh_dir_id = root_key.k_dir_id;
+    deh[0].deh_objectid = root_key.k_objectid;
+    deh[0].deh_location = ih.ih_item_len - strlen (".");
+    deh[0].deh_state = 0;
+    set_bit (DEH_Visible, &(deh[0].deh_state));
+
+    deh[1].deh_offset = DOT_DOT_OFFSET;
+    deh[1].deh_dir_id = 0;
+    deh[1].deh_objectid = root_key.k_dir_id;
+    deh[1].deh_location = deh[0].deh_location - strlen ("..");
+    deh[1].deh_state = 0;
+    set_bit (DEH_Visible, &(deh[1].deh_state));
+    dir[DEH_SIZE * 2] = '.';
+    dir[DEH_SIZE * 2 + 1] = '.';
+    dir[DEH_SIZE * 2 + 2] = '.';
+
+    reiserfsck_insert_item (path, &ih, dir);
+}
+
+
+/* returns buffer, containing found directory item.*/
+char * get_next_directory_item (struct path * path, struct key * key, struct key * parent, struct item_head * ih)
+{
+    char * dir_item;
+    struct key * rdkey;
+    struct buffer_head * bh;
+    struct reiserfs_de_head * deh;
+    int i;
+    int retval;
+
+    if ((retval = usearch_by_entry_key (&g_sb, key, path)) != ENTRY_FOUND) {
+	if (key->k_offset != DOT_OFFSET)
+	    die ("get_next_directory_item: entry not found");
+
+	/* first directory item not found */
+	if (is_rootdir_entry_key (key)) {
+	    /* add "." and ".." to the root directory */
+	    if (retval == ENTRY_NOT_FOUND)
+		paste_dot_and_dot_dot (path);
+	    else if (retval == DIRECTORY_NOT_FOUND)
+		insert_dot_and_dot_dot (path);
+	    else
+		die ("get_next_directory_item: invalid return value");
+	    usearch_by_entry_key (&g_sb, key, path);
+	} else {
+	    /* it is ok for directories but the root one that "." is not found */
+	    pathrelse (path);
+	    return 0;
+	}
+    }
+    /* leaf containing directory item */
+    bh = PATH_PLAST_BUFFER (path);
+
+    memcpy (ih, PATH_PITEM_HEAD (path), IH_SIZE);
+
+    /* make sure, that ".." exists as well */
+    if (key->k_offset == DOT_OFFSET) {
+	if (I_ENTRY_COUNT (ih) < 2) {
+	    pathrelse (path);
+	    return 0;
+	}
+	deh = B_I_DEH (bh, ih) + 1;
+	if (I_DEH_N_ENTRY_FILE_NAME_LENGTH (ih, deh, 1) != strlen ("..") ||
+	    memcmp ("..", B_I_E_NAME (1, bh, ih), 2)) {
+	    printf ("\nget_next_directory_item: \"..\" not found\n");
+	    pathrelse (path);
+	    return 0;
+	}
+    }
+
+    deh = B_I_DEH (bh, ih);
+
+    /* mark hidden entries as visible, reset ".." correctly */
+    for (i = 0; i < I_ENTRY_COUNT (ih); i ++, deh ++) {
+	if (de_hidden (deh)) {
+	    if (opt_verbose)
+		reiserfs_warning ("\nget_next_directory_item: hidden entry %d\n", i);
+
+	    mark_de_visible (deh);
+	    mark_buffer_dirty (bh, 0);
+	}
+	if (deh->deh_offset == DOT_DOT_OFFSET) {
+	    /* set ".." so that it points to the correct parent directory */
+	    if (comp_short_keys (&(deh->deh_dir_id), parent) && 
+		deh->deh_objectid != REISERFS_ROOT_PARENT_OBJECTID) {
+		if (opt_verbose)
+		    reiserfs_warning ("\nget_next_directory_item: \"..\" fixed\n");
+		deh->deh_dir_id = key->k_dir_id;
+		deh->deh_objectid = key->k_objectid;
+		mark_buffer_dirty (bh, 0);
+	    }
+	}
+    }
+
+    /* copy directory item to the temporary buffer */
+    dir_item = getmem (ih->ih_item_len); 
+    memcpy (dir_item, B_I_PITEM (bh, ih), ih->ih_item_len);
+
+    /* next item key */
+    if (PATH_LAST_POSITION (path) == (B_NR_ITEMS (PATH_PLAST_BUFFER (path)) - 1) &&
+	(rdkey = uget_rkey (path)))
+	copy_key (key, rdkey);
+    else {
+	key->k_dir_id = 0;
+	key->k_objectid = 0;
+    }
+
+    mark_item_accessed (PATH_PITEM_HEAD (path), PATH_PLAST_BUFFER (path));
+    return dir_item;
+}
+
+
+static void get_object_key (struct reiserfs_de_head * deh, struct key * key, struct key * entry_key, struct item_head * ih)
+{
+    key->k_dir_id = deh->deh_dir_id;
+    key->k_objectid = deh->deh_objectid;
+    key->k_offset = SD_OFFSET;
+    key->k_uniqueness = SD_UNIQUENESS;
+
+    entry_key->k_dir_id = ih->ih_key.k_dir_id;
+    entry_key->k_objectid = ih->ih_key.k_objectid;
+    entry_key->k_offset = deh->deh_offset;
+    entry_key->k_uniqueness = DIRENTRY_UNIQUENESS;
+}
+
+
+static void reiserfsck_cut_entry (struct key * key)
+{
+    struct path path;
+
+    if (usearch_by_entry_key (&g_sb, key, &path) != ENTRY_FOUND || key->k_offset == DOT_OFFSET)
+	die ("reiserfsck_cut_entry: entry not found");
+
+    if (I_ENTRY_COUNT (PATH_PITEM_HEAD (&path)) == 1)
+	reiserfsck_delete_item (&path);
+    else {
+	struct reiserfs_de_head * deh = B_I_DEH (PATH_PLAST_BUFFER (&path), PATH_PITEM_HEAD (&path)) + path.pos_in_item;
+	reiserfsck_cut_from_item (&path, -(DEH_SIZE + I_DEH_N_ENTRY_LENGTH (PATH_PITEM_HEAD (&path), deh, path.pos_in_item)));
+    }
+}
+
+
+
+/* check recursively the semantic tree. Returns 0 if entry points to
+   good object, and -1 or -2 if this entry must be deleted (stat data
+   not found or directory does have any items).  Hard links are not
+   allowed, but if directory rename has been interrupted by the system
+   crash, it is possible, that fsck will find two entries (not "..") 
+   pointing to the same directory. In this case fsck keeps only the
+   first one. */
+#define OK 0
+#define STAT_DATA_NOT_FOUND -1
+#define DIRECTORY_HAS_NO_ITEMS -2
+
+static __u32 stat_datas = 0;
+
+
+static hashf_t code2function (int code)
+{
+    switch (code) {
+    case TEA_HASH:
+	return keyed_hash;
+    case YURA_HASH:
+	return yura_hash;
+    case R5_HASH:
+	return r5_hash;
+    }
+    printf ("code2function: unknown code %d of hash function, using default\n", code);
+    return keyed_hash;
+}
+
+
+static char * hash_name (int code)
+{
+    static char name[20];
+
+    switch (code) {
+    case TEA_HASH:
+        strcpy (name, "tea");
+        break;
+    case YURA_HASH:
+        strcpy (name, "rupasov");
+        break;
+    case R5_HASH:
+        strcpy (name, "rrr5");
+        break;
+    case UNSET_HASH:
+        strcpy (name, "code");
+        break;
+    default:
+        strcpy (name, "unknown");
+        break;
+    }
+    return name;
+}
+
+
+int check_semantic_tree (struct key * key, struct key * parent, int is_dot_dot)
+{
+    struct path path;
+    struct stat_data * sd;
+
+    if (!KEY_IS_STAT_DATA_KEY (key))
+	die ("check_semantic_tree: key must be key of a stat data");
+
+    /* look for stat data of an object */
+    if (usearch_by_key (&g_sb, key, &path, 0, DISK_LEAF_NODE_LEVEL, READ_BLOCKS, comp_keys) == ITEM_NOT_FOUND) {
+	if (is_rootdir_key (key)) {
+	    /* stat data of the root directory not found. Make it */
+	    create_root_directory (&path);
+	    usearch_by_key (&g_sb, key, &path, 0, DISK_LEAF_NODE_LEVEL, READ_BLOCKS, comp_keys);
+	} else {
+	    pathrelse (&path);
+	    return STAT_DATA_NOT_FOUND;
+	}
+    }
+
+    sd = B_N_STAT_DATA (PATH_PLAST_BUFFER (&path), PATH_LAST_POSITION (&path));
+    if ((sd->sd_nlink == 0) && ( opt_fsck == 0 ))
+	print_how_far (&stat_datas, get_event (STAT_DATA_ITEMS));
+
+    if ((sd->sd_mode & S_IFMT) != S_IFDIR) {
+	/* object is not a directory (regular, symlink, device file) */
+	/*if ((sd->sd_mode & S_IFMT) == S_IFLNK)
+	  printf ("Symlink found\n");*/
+
+	check_regular_file (&path, sd);
+	pathrelse (&path);
+	return OK;
+    }
+
+    /* object is directory */
+    sd->sd_nlink ++;
+    mark_buffer_dirty (PATH_PLAST_BUFFER (&path), 0);
+    if (sd->sd_nlink == 1) {
+	char * dir_item;
+	struct item_head ih;
+	struct key item_key, entry_key, object_key;
+	unsigned long dir_size = 0;
+
+	/*print_how_far (&stat_datas, get_event (STAT_DATA_ITEMS));*/
+
+	if (key->k_objectid == REISERFS_ROOT_OBJECTID)
+	    // empty root directory has sd_nlink == 3
+	    sd->sd_nlink ++;
+
+	add_event (DIRECTORIES);
+	copy_key (&item_key, key);
+	item_key.k_offset = DOT_OFFSET;
+	item_key.k_uniqueness = DIRENTRY_UNIQUENESS;
+	pathrelse (&path);
+	while ((dir_item = get_next_directory_item (&path, &item_key, parent, &ih)) != 0) {
+	    /* dir_item is copy of the item in separately allocated memory */
+	    int i;
+	    int retval;
+	    struct reiserfs_de_head * deh = (struct reiserfs_de_head *)dir_item + path.pos_in_item;
+
+/*&&&&&&&&&&&&&&&*/
+	    if (dir_size == 0) {
+		if (deh->deh_offset != DOT_OFFSET || (deh + 1)->deh_offset != DOT_DOT_OFFSET) {
+		    die ("check_semantic_tree: Directory without \".\" or \"..\"");
+	        }
+	    }
+/*&&&&&&&&&&&&&&&*/
+
+	    for (i = path.pos_in_item; i < I_ENTRY_COUNT (&ih); i ++, deh ++) {
+		int hash_code;
+		char * name;
+		int namelen;
+		int got_dot ;
+
+		got_dot = 0 ;
+		name = dir_item + deh_location (deh);
+		if (de_with_sd (deh))
+		    die ("check_semantic_tree: entry marked as having stat data");
+		namelen = (i ? deh_location (deh - 1) : ih_item_len (&ih)) - deh_location (deh);
+		if (deh_offset (deh) == DOT_OFFSET || deh_offset (deh) == DOT_DOT_OFFSET) {
+		    got_dot = 1 ;
+		    
+		} else if (GET_HASH_VALUE(deh_offset (deh)) == GET_HASH_VALUE(yura_hash (name, namelen))) {
+		    hash_code = YURA_HASH;
+		} else if (GET_HASH_VALUE(deh_offset (deh)) == GET_HASH_VALUE(r5_hash (name, namelen))) {
+		    hash_code = R5_HASH;
+		} else if (GET_HASH_VALUE(deh_offset (deh)) == GET_HASH_VALUE(keyed_hash (name, namelen))){
+		    hash_code = TEA_HASH;
+		} else {
+		    hash_code = TEA_HASH; // to avoid gcc's warning
+		    die ("check_semantic_tree: unknown hash is used");
+		}
+
+		if (!got_dot && !g_sb.u.reiserfs_sb.s_hash_function) {
+		    g_sb.u.reiserfs_sb.s_hash_function = code2function (hash_code);
+		    g_sb.u.reiserfs_sb.s_rs->s_hash_function_code = __cpu_to_le32 (hash_code);
+		} else if (!got_dot && hash_code != __le32_to_cpu (g_sb.u.reiserfs_sb.s_rs->s_hash_function_code))
+		    printf ("check_semantic_tree: \"%s\" hash is used. Should be \"%s\" only\n", hash_name (hash_code), 
+			    hash_name (__le32_to_cpu (g_sb.u.reiserfs_sb.s_rs->s_hash_function_code)));
+
+		get_object_key (deh, &object_key, &entry_key, &ih);
+		retval = check_semantic_tree (&object_key, key,
+					      (deh->deh_offset == DOT_OFFSET ||deh->deh_offset == DOT_DOT_OFFSET) ? 1 : 0);
+		if (retval != OK) {
+		    if (entry_key.k_offset == DOT_DOT_OFFSET && object_key.k_objectid == REISERFS_ROOT_PARENT_OBJECTID) {
+			/* ".." of root directory can not be found */
+			if (retval != STAT_DATA_NOT_FOUND)
+			    die ("check_semantic_tree: stat data of parent directory of root directory found");
+			dir_size += DEH_SIZE + strlen ("..");
+			continue;
+		    }
+		    add_event (DELETED_ENTRIES);
+		    reiserfsck_cut_entry (&entry_key);
+		} else {
+		    /* OK */
+		    dir_size += DEH_SIZE + I_DEH_N_ENTRY_LENGTH (&ih, deh, i);
+		}
+	    }
+
+	    freemem (dir_item);
+
+	    if (comp_short_keys (&item_key, key) != KEYS_IDENTICAL) {
+		pathrelse (&path);
+		break;
+	    }
+	    pathrelse (&path);
+	}
+
+	if (dir_size == 0)
+	    return DIRECTORY_HAS_NO_ITEMS;
+
+	if (usearch_by_key (&g_sb, key, &path, 0, DISK_LEAF_NODE_LEVEL, READ_BLOCKS, comp_keys) != ITEM_FOUND)
+	    die ("check_semantic_tree: stat data not found");
+
+	mark_objectid_as_used (PATH_PITEM_HEAD (&path)->ih_key.k_objectid);
+
+	if (dir_size != (sd = B_N_STAT_DATA (PATH_PLAST_BUFFER (&path), PATH_LAST_POSITION (&path)))->sd_size) {
+	    add_event (FIXED_SIZE_DIRECTORIES);
+	    sd->sd_size = dir_size;
+	}
+	/* stat data of a directory is accessed */
+	mark_item_accessed (PATH_PITEM_HEAD (&path), PATH_PLAST_BUFFER (&path));
+    } else {
+	/* we have accessed directory stat data not for the first time. we
+	   can come here only from "." or "..". Other names must be removed
+	   to avoid creation of hard links */
+	if (!is_dot_dot) {
+	    sd->sd_nlink --;
+	    if (opt_verbose)
+		reiserfs_warning ("\ncheck_semantic_tree: more than one name (neither \".\" nor \"..\") of a directory. Removed\n");
+	    pathrelse (&path);
+	    return STAT_DATA_NOT_FOUND;
+	}
+    }
+    pathrelse (&path);
+
+
+    return OK;
+}
+
+
+struct key g_root_directory_key = {REISERFS_ROOT_PARENT_OBJECTID, REISERFS_ROOT_OBJECTID, 0, 0};
+struct key g_parent_root_directory_key = {0, REISERFS_ROOT_PARENT_OBJECTID, 0, 0};
+
+void semantic_pass (void)
+{
+    if (opt_stop_point == STOP_AFTER_PASS1 || opt_stop_point == STOP_AFTER_PASS2)
+	return;
+
+    if ( opt_fsck == 0 )
+	fprintf (stderr, "Pass 3 (semantic) - ");
+    check_semantic_tree (&g_root_directory_key, &g_parent_root_directory_key, 0);
+    if ( opt_fsck == 0 )
+	printf ("\n");
+}
+
+
diff -urN linux/fs/reiserfs/utils/fsck/ubitmap.c /tmp/linux/fs/reiserfs/utils/fsck/ubitmap.c
--- linux/fs/reiserfs/utils/fsck/ubitmap.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/fsck/ubitmap.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,453 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+#include "fsck.h"
+#include "reiserfs.h"
+
+/* g_disk_bitmap initially contains copy of disk bitmaps
+   (cautious version of it);
+
+   g_new_bitmap initially has marked only super block, bitmap blocks
+   and bits after the end of bitmap
+
+   in pass 1 we go through g_disk_bitmap. 
+
+   If block does not look like formatted node, we skip it.
+
+   If block contains internal node, put 0 in g_disk_bitmap if block is
+   not used in new tree yet.
+
+   If block contains leaf and is used already (by an indirect item
+   handled already to this time) save all items. They will be inserted
+   into tree after pass 1.
+
+   If block looking like leaf is not used in the new tree, try to
+   insert in into tree. If it is not possible, mark block in
+   g_uninsertable_leaf_bitmap. Blocks marked in this bitmap will be inserted into tree in pass 2. They can not be
+
+  This means, that in pass 1 when we have
+   found block containing the internal nodes we mark it in
+   g_disk_bitmap as free (reiserfs_free_internal_block). When block
+   gets into new tree it is marked in g_new_bitmap (mark_block_used)
+   When collecting resources for do_balance, we mark new blocks with
+   mark_block_used. After do_balance we unmark unused new blocks in
+   g_new_bitmap (bitmap.c:/reiserfs_free_block)
+
+   Allocating of new blocks: look for 0 bit in g_disk_bitmap
+   (find_zero_bit_in_bitmap), make sure, that g_new_bitmap contains 0
+   at the corresponding bit (is_block_used).
+      
+ */
+
+
+
+int was_block_used (unsigned long block)
+{
+  int i, j;
+
+  if (block >= SB_BLOCK_COUNT (&g_sb))
+    die ("was_block_used: %d is too big (%d)\n", block, SB_BLOCK_COUNT (&g_sb));
+
+  if (opt_what_to_scan == SCAN_WHOLE_PARTITION)
+      /* this function is used to set 0 into indirect item entry when
+         it points to a block which was marked free in the
+         bitmap. When we scan whole partition we must gather as much
+         as possible. So, take it */
+      return 1;
+
+  i = block / (g_sb.s_blocksize * 8);
+  j = block % (g_sb.s_blocksize * 8);
+  return test_bit (j, g_disk_bitmap[i]);
+}
+
+
+/* is blocks used (marked by 1 in new bitmap) in the tree which is being built (as leaf, internal,
+   bitmap, or unformatted node) */
+int is_block_used (unsigned long block)
+{
+  int i, j;
+
+  if(g_new_bitmap == 0)
+    return 0;
+  if (block >= SB_BLOCK_COUNT (&g_sb)) {
+    printf ("is_block_used: %ld is too big (%d)\n", block, SB_BLOCK_COUNT (&g_sb));
+    return 1;
+  }
+
+  i = block / (g_sb.s_blocksize * 8);
+  j = block % (g_sb.s_blocksize * 8);
+  return test_bit (j, g_new_bitmap[i]);
+}
+
+
+void mark_block_used (unsigned long block)
+{
+  int i, j;
+
+  if (is_block_used (block))
+    die ("mark_block_used: (%lu) used already", block);
+
+  i = block / (g_sb.s_blocksize * 8);
+  j = block % (g_sb.s_blocksize * 8);
+  set_bit (j, g_new_bitmap[i]);
+  SB_FREE_BLOCKS (&g_sb)--;
+}
+
+/*%%%%%%%%%%%%%%%%%%%%%%*/
+int is_block_formatted (unsigned long block)
+{
+  int i, j;
+
+  i = block / (g_sb.s_blocksize * 8);
+  j = block % (g_sb.s_blocksize * 8);
+  return test_bit (j, g_formatted[i]);
+}
+int is_block_unformatted (unsigned long block)
+{
+  int i, j;
+
+  i = block / (g_sb.s_blocksize * 8);
+  j = block % (g_sb.s_blocksize * 8);
+  return test_bit (j, g_unformatted[i]);
+}
+void mark_block_formatted (unsigned long block)
+{
+  int i, j;
+
+  if (is_block_formatted (block) || is_block_unformatted (block))
+    die ("mark_block_formatted: (%lu) used already", block);
+
+  i = block / (g_sb.s_blocksize * 8);
+  j = block % (g_sb.s_blocksize * 8);
+  set_bit (j, g_formatted[i]);
+}
+void mark_block_unformatted (unsigned long block)
+{
+  int i, j;
+
+  if (is_block_formatted (block) || is_block_unformatted (block))
+    die ("mark_block_unformatted: (%lu) used already", block);
+
+
+  i = block / (g_sb.s_blocksize * 8);
+  j = block % (g_sb.s_blocksize * 8);
+  set_bit (j, g_unformatted[i]);
+}
+void unmark_block_formatted (unsigned long block)
+{
+  int i, j;
+
+  if (!is_block_formatted (block) || is_block_unformatted (block))
+    die ("unmark_block_formatted: (%lu) used already", block);
+
+  i = block / (g_sb.s_blocksize * 8);
+  j = block % (g_sb.s_blocksize * 8);
+  clear_bit (j, g_formatted[i]);
+}
+void unmark_block_unformatted (unsigned long block)
+{
+  int i, j;
+
+  if (is_block_formatted (block) || !is_block_unformatted (block))
+    die ("unmark_block_unformatted: (%lu) used already", block);
+
+  i = block / (g_sb.s_blocksize * 8);
+  j = block % (g_sb.s_blocksize * 8);
+  clear_bit (j, g_unformatted[i]);
+}
+/*%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%*/
+
+/* uninsertable block is marked by bit clearing */
+void mark_block_uninsertable (unsigned long block)
+{
+  int i, j;
+
+  if (is_block_used (block))
+    die ("mark_block_uninsertable: (%lu) used already", block);
+
+  i = block / (g_sb.s_blocksize * 8);
+  j = block % (g_sb.s_blocksize * 8);
+  clear_bit (j, g_uninsertable_leaf_bitmap[i]);
+}
+
+int is_block_uninsertable (unsigned long block)
+{
+  int i, j;
+  
+  if (is_block_used (block))
+    die ("is_block_uninsertable: (%lu) used already", block);
+
+  i = block / (g_sb.s_blocksize * 8);
+  j = block % (g_sb.s_blocksize * 8);
+  return !test_bit (j, g_uninsertable_leaf_bitmap[i]);
+}
+
+static inline void get_bit_address (struct super_block * s, unsigned long block, int * bmap_nr, int * offset)
+{
+  *bmap_nr = block / (s->s_blocksize << 3);
+  *offset = block % (s->s_blocksize << 3);
+  return;
+}
+
+static inline int find_prev_zero_bit (void * addr, int offset)
+{
+  char * start;			/* byte pointer to starting byte of search */
+  int bit_offset;		/* bit offset within starting byte of starting point */
+  char mask;
+
+  start = (char *)addr + (offset >> 3);
+  bit_offset = (offset % 8);
+
+  mask = (unsigned int)0xff >> (7 - bit_offset);
+  while (start >= (char *)addr) {
+    if ((*start & mask) != mask) {
+      /* there is at least one 0 bit in current byte */
+      for (; bit_offset >= 0; bit_offset --) {
+	if (!((1 << bit_offset) & *start))
+	  return ((start - (char *)addr) << 3) + bit_offset;
+      }
+      die ("find_prev_zero_bit: must be at least 1 zero bit");
+    }
+    bit_offset = 7;
+    mask = (unsigned int)0xff;
+    start --;
+  }
+  /* there is no zero bit when we go from offset to the left up to addr */
+  return -1;
+
+}
+
+
+/* beginning from offset-th bit in bmap_nr-th bitmap block,
+   find_forward finds the closest zero bit. It returns 1 and zero
+   bit address (bitmap, offset) if zero bit found or 1 if there is no
+   zero bits in forward direction */
+static int find_forward (struct super_block * s, int * bmap_nr, int * offset)
+{
+  int i, j;
+  struct buffer_head * bh;
+
+  for (i = *bmap_nr; i < SB_BMAP_NR (s); i ++, *offset = 0) {
+    /* get corresponding bitmap block */
+    bh = SB_AP_BITMAP (s)[i];/*g_disk_bitmap[i];*/
+    while (*offset < (s->s_blocksize << 3)) {
+      j = find_next_zero_bit ((unsigned long *)bh->b_data, s->s_blocksize << 3, *offset);
+      if (j < (s->s_blocksize << 3)) {
+	*bmap_nr = i;
+	*offset = j;
+	
+	/* we found free block in disk bitmap, make sure, that it is
+           not used in new built tree yet */
+	if (is_block_used (i * (s->s_blocksize << 3) + j)) {
+	  (*offset) ++;
+	  continue;
+	}
+	return 1;
+      }
+      break; /* while */
+    }
+  }	/* for */
+
+  /* zero bit not found */
+  return 0;
+}
+
+
+/* this does the same as find_forward does, but in backward direction */
+static int find_backward (struct super_block * s, int * bmap_nr, int * offset)
+{
+  int i, j;
+  struct buffer_head * bh;
+
+  for (i = *bmap_nr; i > -1; i --, *offset = (s->s_blocksize << 3) - 1) {
+    /* get corresponding bitmap block */
+    bh = SB_AP_BITMAP (s)[i];/*g_disk_bitmap[i];*/
+    
+    /* at first we start from position, in next bitmap block we start from 0th position */
+    while (*offset > -1) {
+      j = find_prev_zero_bit ((unsigned long *)bh->b_data, *offset);
+      if (j != -1) {
+	*bmap_nr = i;
+	*offset = j;
+	
+	/* we found free block in disk bitmap, make sure, that it is not used in new built tree yet */
+	if (is_block_used (i * (s->s_blocksize << 3) + j)) {
+	  (*offset) --;
+	  continue;
+	}
+	return 1;
+      }
+      break;	/* from while */
+    }
+    
+    /* in previous bitmap block we start from the end */
+/*    *offset = (s->s_blocksize << 3) - 1;*/
+  }	/* for */
+  
+  /* zero bit not found */
+  return 0;
+}
+
+
+static unsigned long find_zero_bit_in_bitmap (struct super_block * s, unsigned long search_start)
+{
+  int bmap_nr, offset;
+
+  /* get bit location (bitmap number and bit offset) of search_start block */
+  get_bit_address (s, search_start, &bmap_nr, &offset);
+
+  /* first we are going to the right (as elevator_direction requires) */
+  if (find_forward (s, &bmap_nr, &offset) == 0) {
+    /* there wasn't a free block with number greater than our
+       starting point, so we are going to do find_backward */
+    get_bit_address (s, search_start, &bmap_nr, &offset);
+    if (find_backward (s, &bmap_nr, &offset) == 0)
+      return 0;
+  }
+
+  /* ok, mark block in new bitmap */
+  mark_block_used (bmap_nr * (s->s_blocksize << 3) + offset);
+  return (bmap_nr * (s->s_blocksize << 3)) + offset;
+}
+
+
+/* mark block free in bitmap we use to build the tree */
+void reiserfs_free_internal_block (struct super_block * s, unsigned long block)
+{
+  int i, j;
+
+  i = block / (s->s_blocksize * 8);
+  j = block % (s->s_blocksize * 8);
+
+  if (test_bit (j, SB_AP_BITMAP (s)[i]->b_data) == 0)
+    die ("reiserfs_free_internal_block: Block %lu is free", block);
+
+  clear_bit (j, SB_AP_BITMAP (s)[i]->b_data);
+  g_old_rs->s_free_blocks ++;
+}
+
+
+/* try to find 'to_free' internal nodes and mark corresponding blocks
+   free. Return number of freed blocks */
+static int try_to_free_unused_internal_blocks (int to_free)
+{
+    int i, j, k;
+    int freed = 0;
+    struct buffer_head * bh;
+    int block;
+
+    /* just to do not waste time: onthe partition sent by Petru there
+       is no internal nodes */
+    return 0;
+
+    printf ("Trying to find internal nodes which are not used in new tree..");fflush (stdout);
+    for (i = 0; i < SB_BMAP_NR (&g_sb); i ++)
+	for (j = 0; j < g_sb.s_blocksize; j ++) {
+	    if (i * g_sb.s_blocksize * 8 + j * 8 == SB_BLOCK_COUNT (&g_sb))
+		goto out_of_bitmap;
+	    for (k = 0; k < 8; k ++) {
+		block = i * g_sb.s_blocksize * 8 + j * 8 + k;
+		if (is_block_used (block/*i * g_sb.s_blocksize * 8 + j * 8 + k*/))
+		    continue;
+		bh = bread (g_sb.s_dev, i * g_sb.s_blocksize * 8 + j * 8 + k, g_sb.s_blocksize);
+		if (not_formatted_node (bh->b_data, g_sb.s_blocksize)) {
+		    brelse (bh);
+		    continue;
+		}
+		/* this node is formatted node. we can free internal node  */
+		if (is_internal_node (bh->b_data)) {
+		    reiserfs_free_internal_block (&g_sb, bh->b_blocknr);
+		    printf (".");fflush (stdout);
+		    freed ++;
+		    if (freed == to_free) {
+			brelse (bh);
+			goto out_of_bitmap;
+		    }
+		}
+		brelse (bh);
+	    }
+	}
+ out_of_bitmap:
+    printf ("\n");
+    return freed;
+}
+
+
+int from_journal;
+
+int reiserfs_new_blocknrs (struct reiserfs_transaction_handle *th, struct super_block * s, 
+			   unsigned long * free_blocknrs, unsigned long start, int amount_needed, int notused)
+{
+    while (amount_needed --) {
+	*free_blocknrs = find_zero_bit_in_bitmap (s, start);
+	if (*free_blocknrs == 0) {
+	    /* if we still did not take journal space lets try to find
+               internal nodes and free them */
+	    if (from_journal == 0 && try_to_free_unused_internal_blocks (10))
+		/* got some space */
+		continue;
+
+	    /* ok, no free space on device. There are no internal
+	       nodes which could be freed. This is especially very
+	       likely when you specify --scan-whole-partition. Take
+	       journal space! */
+
+	    /* take blocks starting from must journal start but not
+               over the journal partition had before recovering */
+	    if (from_journal == get_journal_size (s))
+		/* whole journal is used already */
+		die ("Journal space is used. No idea where to get free space");
+
+	    if (from_journal == 0)
+		printf ("Start using journal space\n");
+	    *free_blocknrs = from_journal + get_journal_start (s);
+	    from_journal ++;
+	}
+
+	free_blocknrs ++;
+    }
+    
+    return CARRY_ON;
+}
+
+
+int reiserfs_new_unf_blocknrs(struct reiserfs_transaction_handle *th, struct super_block * s, unsigned long * free_blocknrs,
+				 unsigned long search_start, int amount_needed, int for_preserve_list) {
+  return reiserfs_new_blocknrs(th, s, free_blocknrs, search_start, amount_needed, for_preserve_list) ;
+}
+
+
+
+struct buffer_head * reiserfsck_get_new_buffer (unsigned long start)
+{
+  unsigned long blocknr = 0;
+  struct buffer_head * bh;
+
+  reiserfs_new_blocknrs (0, &g_sb, &blocknr, start, 1, 0);
+  
+  bh = getblk (g_sb.s_dev, blocknr, g_sb.s_blocksize);
+  if (buffer_uptodate (bh))
+    die ("reiserfsck_get_new_buffer: found uptodate buffer for new blocknr");
+
+  return bh;
+}
+
+
+/* free block in new bitmap */
+void reiserfs_free_block (struct reiserfs_transaction_handle *th, struct super_block * s, unsigned long block)
+{
+  int i, j;
+
+  i = block / (s->s_blocksize * 8);
+  j = block % (s->s_blocksize * 8);
+
+  if (test_bit (j, g_new_bitmap[i]) == 0)
+    die ("reiserfs_free_block: Block %lu is free", block);
+
+  clear_bit (j, g_new_bitmap[i]);
+  SB_FREE_BLOCKS (&g_sb)++;
+}
+
+
+
diff -urN linux/fs/reiserfs/utils/fsck/ufile.c /tmp/linux/fs/reiserfs/utils/fsck/ufile.c
--- linux/fs/reiserfs/utils/fsck/ufile.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/fsck/ufile.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,1066 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+#include "fsck.h"
+
+#if 0
+static int is_bad_sd (struct item_head * ih, char * item)
+{
+  struct stat_data * sd = (struct stat_data *)item;
+
+  if (!S_ISDIR (sd->sd_mode) && !S_ISREG(sd->sd_mode) &&
+      !S_ISCHR (sd->sd_mode) && !S_ISBLK(sd->sd_mode) &&
+      !S_ISLNK (sd->sd_mode)) {
+    reiserfs_warning ("is_bad_sd: \
+stat data item (%h) has sd_mode 0%o. Skipped\n", ih, sd->sd_mode);
+    return 1;
+  }
+  if ((sd->sd_first_direct_byte != NO_BYTES_IN_DIRECT_ITEM &&
+       sd->sd_first_direct_byte >= sd->sd_size) ||
+      sd->sd_size > MAX_INT) {
+    reiserfs_warning ("is_bad_sd: \
+stat data item (%h) has sd_size %d, first direct byte %d\n", ih, sd->sd_size,
+		      sd->sd_first_direct_byte);
+    return 1;
+  }
+  if (sd->sd_nlink > 100) {
+    reiserfs_warning ("is_bad_sd: \
+stat data item (%h) has sd_nlink %d\n", sd->sd_nlink);
+    return 1;
+  }
+  return 0;
+}
+
+
+static int is_bad_directory (struct item_head * ih, char * item)
+{
+  int i;
+  int namelen;
+  struct reiserfs_de_head * deh = (struct reiserfs_de_head *)item;
+  __u32 prev_offset = 0;
+  __u16 prev_location = 0xffff;
+
+  for (i = 0; i < I_ENTRY_COUNT (ih); i ++) {
+    namelen = I_DEH_N_ENTRY_FILE_NAME_LENGTH (ih, deh + i, i);
+    if (namelen > REISERFS_MAX_NAME_LEN (g_sb.s_blocksize)) {
+      reiserfs_warning ("is_bad_directory: dir item %h has too long name (%d)\n", ih, namelen);
+      return 1;
+    }
+    if (deh[i].deh_offset <= prev_offset) {
+      reiserfs_warning ("is_bad_directory: dir item %h has invalid header array \
+(offsets: prev %u, %d-th cur %u)\n", ih, prev_offset, i, deh[i].deh_offset);
+      return 1;
+    }
+    prev_offset = deh[i].deh_offset;
+
+    if (deh[i].deh_location >= prev_location) {
+      reiserfs_warning ("is_bad_directory: dir item %h has invalid header array \
+(locations: prev %u, %d-th cur %u)\n", ih, prev_location, i, deh[i].deh_location);
+      return 1;
+    }
+  }
+
+  return 0;
+}
+
+
+/* change incorrect block adresses by 0. Do not consider such item as incorrect */
+static int is_bad_indirect (struct item_head * ih, char * item)
+{
+  int i;
+
+  for (i = 0; i < I_UNFM_NUM (ih); i ++) {
+    __u32 * ind = (__u32 *)item;
+
+    if (ind[i] >= SB_BLOCK_COUNT (&g_sb)) {
+      /*reiserfs_warning ("is_bad_indirect: block address (%lu) in indirect item. Super block block count == %u\n",
+	      ind[i], SB_BLOCK_COUNT (&g_sb));*/
+      ind[i] = 0;
+      continue;
+    }
+    if (is_block_used (ind[i])) {
+      ind[i] = 0;
+      continue;
+    }
+  }
+  return 0;
+}
+
+
+int is_bad_item (struct item_head * ih, char * item)
+{
+  if (I_IS_STAT_DATA_ITEM (ih))
+    return is_bad_sd (ih, item);
+
+  if (I_IS_DIRECTORY_ITEM (ih))
+    return is_bad_directory (ih, item);
+
+  if (I_IS_INDIRECT_ITEM (ih))
+    return is_bad_indirect (ih, item);
+
+  return 0;
+}
+#endif /* 0 */
+
+int is_bad_item (struct item_head *, char *, int, int);
+
+/* append item to end of list. Set head if it is 0. For indirect item
+   set wrong unformatted node pointers to 0 */
+void save_item (struct si ** head, struct item_head * ih, char * item)
+{
+    struct si * si, * cur;
+    int i;
+
+    if (is_bad_item (ih, item, g_sb.s_blocksize, g_sb.s_dev)) {
+	return;
+    }
+
+    if (I_IS_INDIRECT_ITEM (ih))
+	for (i = 0; i < I_UNFM_NUM (ih); i ++) {
+	    __u32 * ind = (__u32 *)item;
+
+	    if (ind[i] >= SB_BLOCK_COUNT (&g_sb) ||
+		!was_block_used (ind[i]) ||
+		is_block_used (ind[i]) ||
+		is_block_uninsertable (ind[i])) {
+		ind[i] = 0;
+		continue;
+	    }
+	}
+
+    si = getmem (sizeof (*si));
+    si->si_dnm_data = getmem (ih->ih_item_len);
+    memcpy (&(si->si_ih), ih, IH_SIZE);
+    memcpy (si->si_dnm_data, item, ih->ih_item_len);
+
+    // changed by XB
+    si->last_known = NULL;
+
+    if (*head == 0)
+	*head = si;
+    else {
+	cur = *head;
+	// changed by XB
+	//    while (cur->si_next)
+	//      cur = cur->si_next;
+
+	{
+	  int count = 0;
+	  int speedcount = 0;
+	  
+	  while (cur->si_next) {
+	    if (cur->last_known!=NULL) {
+	      cur = cur->last_known; // speed up to the end if the chain
+	      speedcount++;
+	    } else {
+	      cur = cur->si_next;
+	      count++;
+	    }
+	  }
+	  
+	  if ((*head)!=cur) // no self referencing loop please
+	    (*head)->last_known = cur;
+	  
+	}
+	
+	cur->si_next = si;
+    }
+    return;
+}
+
+
+/* this item is in tree. All unformatted pointer are correct. Do not
+   check them */
+static void save_item_2 (struct si ** head, struct item_head * ih, char * item)
+{
+    struct si * si, * cur;
+
+    if (is_bad_item (ih, item, g_sb.s_blocksize, g_sb.s_dev)) {
+	return;
+    }
+
+    si = getmem (sizeof (*si));
+    si->si_dnm_data = getmem (ih->ih_item_len);
+    memcpy (&(si->si_ih), ih, IH_SIZE);
+    memcpy (si->si_dnm_data, item, ih->ih_item_len);
+
+
+    if (*head == 0)
+	*head = si;
+    else {
+	cur = *head;
+	while (cur->si_next)
+	    cur = cur->si_next;
+	cur->si_next = si;
+    }
+    return;
+}
+
+
+
+static struct si * save_and_delete_file_item (struct si * si, struct path * path)
+{
+    struct buffer_head * bh = PATH_PLAST_BUFFER (path);
+    struct item_head * ih = PATH_PITEM_HEAD (path);
+
+    save_item_2 (&si, ih, B_I_PITEM (bh, ih));
+
+    reiserfsck_delete_item (path);
+    return si;
+}
+
+
+static struct si * remove_saved_item (struct si * si)
+{
+  struct si * tmp = si->si_next;
+
+  freemem (si->si_dnm_data);
+  freemem (si);
+  return tmp;
+}
+
+
+void put_saved_items_into_tree (struct si * si)
+{
+  while (si) {
+    insert_item_separately (&si, &(si->si_ih), si->si_dnm_data);
+/*    reiserfsck_file_write (&(si->si_ih), si->si_dnm_data);*/
+    si = remove_saved_item (si);
+  }
+}
+
+
+/* path points to an item or behind last item of the node */
+/*
+static int next_item_of_other_object (struct key * key, struct path * path)
+{
+  struct key * next_key;
+
+  if (PATH_LAST_POSITION (path) < B_NR_ITEMS (PATH_PLAST_BUFFER (path)))
+    next_key = B_N_PKEY (PATH_PLAST_BUFFER (path), PATH_LAST_POSITION (path));
+  else
+    next_key = get_right_dkey (path);
+
+  if (next_key == 0 || comp_short_keys (key, next_key) != KEYS_IDENTICAL)
+    return YES;
+  return NO;
+}
+*/
+
+
+static int do_items_have_the_same_type (struct key * key1, struct key * key2)
+{
+  return (key1->k_uniqueness == key2->k_uniqueness) ? 1 : 0;
+}
+
+static int are_items_in_the_same_node (struct path * path)
+{
+  return (PATH_LAST_POSITION (path) < B_NR_ITEMS (PATH_PLAST_BUFFER (path)) - 1) ? 1 : 0;
+}
+
+
+static struct key * get_next_key (struct path * path)
+{
+  if (PATH_LAST_POSITION (path) < B_NR_ITEMS (PATH_PLAST_BUFFER (path)) - 1)
+    return B_N_PKEY (PATH_PLAST_BUFFER (path), PATH_LAST_POSITION (path) + 1);
+  return uget_rkey (path);
+}
+
+
+#if 0
+/* whether last unfm pointer must be and can be converted to direct item */
+static int can_indirect_item_be_converted (struct item_head * ih)
+{
+  unsigned long file_size = ih->ih_key.k_offset + I_BYTES_NUMBER (ih, g_sb.s_blocksize) - 1;
+  unsigned long tail_size = g_sb.s_blocksize - ih->u.ih_free_space;
+
+  if (!STORE_TAIL_IN_UNFM (file_size, tail_size, g_sb.s_blocksize) &&
+      I_IS_INDIRECT_ITEM (ih)/* && tail_size <= MAX_DIRECT_ITEM_LEN (g_sb.s_blocksize)*/)
+    return 1;
+  return 0;
+}
+#endif
+
+int do_make_tails ()
+{
+  return 1;/*SB_MAKE_TAIL_FLAG (&g_sb) == MAKE_TAILS ? YES : NO;*/
+}
+
+
+static void cut_last_unfm_pointer (struct path * path, struct item_head * ih)
+{
+  ih->u.ih_free_space = 0;
+  if (I_UNFM_NUM (ih) == 1)
+    reiserfsck_delete_item (path);
+  else
+    reiserfsck_cut_from_item (path, -UNFM_P_SIZE);
+}
+
+
+static unsigned long indirect_to_direct (struct path * path)
+{
+  struct buffer_head * bh = PATH_PLAST_BUFFER (path);
+  struct item_head * ih = PATH_PITEM_HEAD (path);
+  unsigned long unfm_ptr;
+  struct buffer_head * unfm_bh = 0;
+  struct item_head ins_ih;
+  char * buf;
+  int len;
+  unsigned long offset;
+
+
+  add_event (INDIRECT_TO_DIRECT);
+
+  unfm_ptr = B_I_POS_UNFM_POINTER (bh, ih, I_UNFM_NUM (ih) - 1);
+
+
+  /* direct item to insert */
+  ins_ih.ih_key.k_dir_id = ih->ih_key.k_dir_id;
+  ins_ih.ih_key.k_objectid = ih->ih_key.k_objectid;
+  ins_ih.ih_key.k_offset = ih->ih_key.k_offset + (I_UNFM_NUM (ih) - 1) * bh->b_size;
+  offset = ins_ih.ih_key.k_offset;
+  ins_ih.ih_key.k_uniqueness = TYPE_DIRECT;
+  ins_ih.ih_item_len = g_sb.s_blocksize - ih->u.ih_free_space;
+  len = ins_ih.ih_item_len;
+  ins_ih.u.ih_free_space = MAX_US_INT;
+  ins_ih.ih_reserved = 0;
+
+  /* get buffer filled with 0s */
+  buf = getmem (len);
+  if (unfm_ptr) {
+    unfm_bh = bread (bh->b_dev, unfm_ptr, bh->b_size);
+    memcpy (buf, unfm_bh->b_data, ins_ih.ih_item_len);
+    brelse (unfm_bh);
+  }
+
+
+  path->pos_in_item = I_UNFM_NUM (ih) - 1;
+  cut_last_unfm_pointer (path, ih);
+
+  /* insert direct item */
+  if (usearch_by_key (&g_sb, &(ins_ih.ih_key), path, 0, DISK_LEAF_NODE_LEVEL, READ_BLOCKS, comp_keys) == ITEM_FOUND)
+    die ("indirect_to_direct: key must be not found");
+  reiserfsck_insert_item (path, &ins_ih, (const char *)(buf));
+
+
+  freemem (buf);
+  
+  /* put to stat data offset of first byte in direct item */
+  return offset;
+}
+
+
+/* when it returns, key->k_offset is offset of the last item of file */
+int are_file_items_correct (struct key * key, unsigned long * size, int mark_passed_items, 
+			    struct path * path_to_sd, struct stat_data ** sd)
+{
+    struct path path;
+    int retval;
+    struct item_head * ih;
+    struct key * next_key;
+    int symlink = 0;
+
+    if (sd && ((*sd)->sd_mode & S_IFMT) == S_IFLNK)
+	symlink = 1;
+
+    *size = 0;
+    key->k_offset = 1;
+    key->k_uniqueness = TYPE_DIRECT;
+    path.path_length = ILLEGAL_PATH_ELEMENT_OFFSET;
+
+    do {
+	retval = usearch_by_position (&g_sb, key, &path);
+	if (retval == BYTE_FOUND && path.pos_in_item != 0)
+	    die ("are_file_items_correct: all bytes we look for must be found at position 0");
+
+	switch (retval) {
+	case BYTE_FOUND:/**/
+	    ih = PATH_PITEM_HEAD (&path);
+	    key->k_uniqueness = ih->ih_key.k_uniqueness;
+	    if (mark_passed_items == 1) {
+		mark_item_accessed (ih, PATH_PLAST_BUFFER (&path));
+	    }
+	    next_key = get_next_key (&path);
+	    if (next_key == 0 || comp_short_keys (key, next_key) != KEYS_IDENTICAL || 
+		(!KEY_IS_INDIRECT_KEY (next_key) && !KEY_IS_DIRECT_KEY (next_key))) {
+		/* next item does not exists or is of another object, therefore all items of file are correct */
+		*size = key->k_offset + I_BYTES_NUMBER (ih, g_sb.s_blocksize) - 1;
+
+		/* here is a problem: if file system being repaired
+                   was full enough, then we should avoid
+                   indirect_to_direct conversions. This is because
+                   unformatted node we have to free will not get into
+                   pool of free blocks, but new direct item is very
+                   likely of big size, therefore it may require
+                   allocation of new blocks. So, skip it for now */
+		if (symlink && I_IS_INDIRECT_ITEM (ih)) {
+/*
+		if (0 && mark_passed_items == 1 && 
+		    do_make_tails () == 1 && can_indirect_item_be_converted (ih) == 1) {
+*/	    
+		    struct key sd_key;
+		    unsigned long first_direct_byte;
+
+		    first_direct_byte = indirect_to_direct (&path);
+		    /* we have to research stat data of object after converting */
+		    pathrelse (path_to_sd);
+		    copy_key (&sd_key, key);
+		    sd_key.k_offset = SD_OFFSET;
+		    sd_key.k_uniqueness = SD_UNIQUENESS;
+		    if (usearch_by_key (&g_sb, &(sd_key), path_to_sd, 0, DISK_LEAF_NODE_LEVEL, READ_BLOCKS, comp_keys) != ITEM_FOUND)
+			die ("are_file_items_correct: stat data not found");
+		    *sd = B_N_STAT_DATA (PATH_PLAST_BUFFER (path_to_sd), PATH_LAST_POSITION (path_to_sd));
+		    /* last item of the file is direct item */
+		    key->k_offset = first_direct_byte;
+		    key->k_uniqueness = TYPE_DIRECT;
+		} else
+		    pathrelse (&path);
+		return 1;
+	    }
+	    /* next item is item of this file */
+	    if ((I_IS_INDIRECT_ITEM (ih) &&
+		 ih->ih_key.k_offset + g_sb.s_blocksize * I_UNFM_NUM (ih) != next_key->k_offset) ||
+		(I_IS_DIRECT_ITEM (ih) && ih->ih_key.k_offset + ih->ih_item_len != next_key->k_offset)) {
+		/* next item has incorrect offset (hole or overlapping) */
+		*size = key->k_offset + I_BYTES_NUMBER (ih, g_sb.s_blocksize) - 1;
+		pathrelse (&path);
+		return 0;
+	    }
+	    if (do_items_have_the_same_type (&(ih->ih_key), next_key) == 1 && are_items_in_the_same_node (&path) == 1) {
+		/* two indirect items or two direct items in the same leaf */
+		*size = key->k_offset + I_BYTES_NUMBER (ih, g_sb.s_blocksize) - 1;
+		pathrelse (&path);
+		return 0;
+	    }
+	    /* items are of different types or are in different nodes */
+	    if (ih->ih_key.k_offset + I_BYTES_NUMBER (ih, g_sb.s_blocksize) != next_key->k_offset) {
+		/* indirect item free space is not set properly */
+		if (!I_IS_INDIRECT_ITEM (ih) || ih->u.ih_free_space == 0)
+		    die ("are_file_items_correct: item must be indirect and must have invalid free space (%d)",
+			 ih->u.ih_free_space);
+	
+		ih->u.ih_free_space = 0;
+		mark_buffer_dirty (PATH_PLAST_BUFFER (&path), 0);
+		if (ih->ih_key.k_offset + I_BYTES_NUMBER (ih, g_sb.s_blocksize) != next_key->k_offset)
+		    die ("are_file_items_correct: invalid offset");
+	    }
+	    /* next item exists */
+	    key->k_offset = next_key->k_offset;
+	    pathrelse (&path);
+	    break;
+
+	case BYTE_NOT_FOUND:
+	    if (key->k_offset != 1)
+		die ("are_file_items_correct: byte can be not found only when it is first byte of file");
+	    pathrelse (&path);
+	    return 0;
+      
+	case FILE_NOT_FOUND:
+	    if (key->k_offset != 1)
+		die ("are_file_items_correct: there is no items of this file, byte 0 found though");
+	    pathrelse (&path);
+	    return 1;
+
+	case DIRECTORY_FOUND:
+	    pathrelse (&path);
+	    return 0;
+	}
+    } while (1);
+
+    die ("are_file_items_correct: code can not reach here");
+    return 0;
+}
+
+
+/* file must have correct sequence of items and tail must be stored in
+   unformatted pointer */
+static int make_file_writeable (struct item_head * ih)
+{
+  struct key key;
+  struct key * rkey;
+  struct path path;
+  struct item_head * path_ih;
+  struct si * si = 0;
+  unsigned long size;
+  int mark_passed_items;
+  int retval;
+
+  copy_key (&key, &(ih->ih_key));
+
+  if ((retval = are_file_items_correct (&key, &size, mark_passed_items = 0, 0, 0)) == 1)
+    /* this file looks good (or there is no any items of it) */
+    return 1;
+
+  if (retval == -1) {
+    /* there is an object with this key and it is directory */
+    return -1;
+  }
+
+  /* rewrite file */
+
+
+  /* look for all items of file, store them and delete */
+  key.k_offset = 1;
+  while (1) {
+    usearch_by_key (&g_sb, &key, &path, 0, DISK_LEAF_NODE_LEVEL, READ_BLOCKS, comp_keys_3);
+    if (PATH_LAST_POSITION (&path) == B_NR_ITEMS (PATH_PLAST_BUFFER (&path))) {
+      rkey = uget_rkey (&path);
+      if (rkey && comp_short_keys (&key, rkey) == KEYS_IDENTICAL) {
+	/* file continues in the right neighbor */
+	copy_key (&key, rkey);
+	pathrelse (&path);
+	continue;
+      }
+      /* there is no more items of file */
+      pathrelse (&path);
+      break;
+    }
+    path_ih = PATH_PITEM_HEAD (&path);
+    if (comp_short_keys (&key, &(path_ih->ih_key)) != KEYS_IDENTICAL) {
+      pathrelse (&path);
+      break;
+    }
+    si = save_and_delete_file_item (si, &path);
+  }
+
+  /* put all items back into tree */
+  put_saved_items_into_tree (si);
+
+  add_event (REWRITTEN_FILES);
+
+/*%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%*/
+  copy_key (&key, &(ih->ih_key));
+  size = 0;
+  if (are_file_items_correct (&key, &size, mark_passed_items = 0, 0, 0) == 0) {
+    die ("file still incorrect\n");
+  }
+/*%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%*/
+
+  return 1;
+
+}
+
+
+/* this inserts __first__ indirect item (having k_offset == 1 and only
+   one unfm pointer) into tree */
+static int create_first_item_of_file (struct item_head * ih, char * item, struct path * path, int *pos_in_coming_item)
+{
+  unsigned long unfm_ptr;
+  struct buffer_head * unbh;
+  struct item_head indih;
+  int retval;
+
+  if (ih->ih_key.k_offset > g_sb.s_blocksize) {
+    /* insert indirect item containing 0 unfm pointer */
+    unfm_ptr = 0;
+    indih.u.ih_free_space = 0;
+    retval = 0;
+  } else {
+    if (I_IS_DIRECT_ITEM (ih)) {
+      /* copy direct item to new unformatted node. Save information about it */
+      
+      unbh = reiserfsck_get_new_buffer (PATH_PLAST_BUFFER (path)->b_blocknr);
+      unfm_ptr = unbh->b_blocknr;
+
+/* this is for check only */
+mark_block_unformatted (unfm_ptr);
+      memcpy (unbh->b_data + ih->ih_key.k_offset - 1, item, ih->ih_item_len);
+
+      save_unfm_overwriting (unfm_ptr, ih);
+
+      indih.u.ih_free_space = g_sb.s_blocksize - ih->ih_item_len - (ih->ih_key.k_offset - 1);
+      mark_buffer_dirty (unbh, 0);
+      mark_buffer_uptodate (unbh, 0);
+      brelse (unbh);
+      retval = ih->ih_item_len;
+    } else {
+      /* take first unformatted pointer from an indirect item */
+      unfm_ptr = *(unsigned long *)item;/*B_I_POS_UNFM_POINTER (bh, ih, 0);*/
+      if (!is_block_used (unfm_ptr) && !is_block_uninsertable (unfm_ptr)) {
+	mark_block_used (unfm_ptr);
+/* this is for check only */
+mark_block_unformatted (unfm_ptr);
+      } else {
+	unfm_ptr = 0;
+      }
+      indih.u.ih_free_space = (ih->ih_item_len == UNFM_P_SIZE) ? ih->u.ih_free_space : 0;
+      retval = g_sb.s_blocksize - indih.u.ih_free_space;
+      (*pos_in_coming_item) ++;
+    }
+  }
+  copy_key (&(indih.ih_key), &(ih->ih_key));
+  indih.ih_key.k_offset = 1;
+  indih.ih_key.k_uniqueness = TYPE_INDIRECT;
+  indih.ih_item_len = UNFM_P_SIZE;
+  mark_item_unaccessed (&indih);
+  reiserfsck_insert_item (path, &indih, (const char *)&unfm_ptr);
+  return retval;
+}
+
+
+/* path points to first part of tail. Function copies file tail into unformatted node and returns
+   its block number. If we are going to overwrite direct item then keep free space (keep_free_space
+   == YES). Else (we will append file) set free space to 0 */
+/* we convert direct item that is on the path to indirect. we need a number of free block for
+   unformatted node. reiserfs_new_blocknrs will start from block number returned by this function */
+static unsigned long block_to_start (struct path * path)
+{
+  struct buffer_head * bh;
+  struct item_head * ih;
+
+  bh = PATH_PLAST_BUFFER (path);
+  ih = PATH_PITEM_HEAD (path);
+  if (ih->ih_key.k_offset == 1 || PATH_LAST_POSITION (path) == 0)
+    return bh->b_blocknr;
+
+  ih --;
+  return (B_I_POS_UNFM_POINTER (bh, ih, I_UNFM_NUM (ih) - 1)) ?: bh->b_blocknr;
+}
+
+
+static void direct2indirect (unsigned long unfm, struct path * path, int keep_free_space)
+{
+  struct item_head * ih;
+  struct key key;
+  struct buffer_head * unbh;
+  struct unfm_nodeinfo ni;
+  int copied = 0;
+  
+  copy_key (&key, &(PATH_PITEM_HEAD (path)->ih_key));
+
+  if (key.k_offset % g_sb.s_blocksize != 1) {
+    /* look for first part of tail */
+    pathrelse (path);
+    key.k_offset -= (key.k_offset % g_sb.s_blocksize - 1);
+    if (usearch_by_key (&g_sb, &key, path, 0, DISK_LEAF_NODE_LEVEL, READ_BLOCKS, comp_keys) != ITEM_FOUND)
+      die ("direct2indirect: can not find first part of tail");
+  }
+
+  unbh = reiserfsck_get_new_buffer (unfm ?: block_to_start (path));
+
+  /* delete parts of tail coping their contents to new buffer */
+  do {
+    ih = PATH_PITEM_HEAD (path);
+    memcpy (unbh->b_data + copied, B_I_PITEM (PATH_PLAST_BUFFER (path), ih), ih->ih_item_len);
+
+    save_unfm_overwriting (unbh->b_blocknr, ih);
+
+    copied += ih->ih_item_len;
+    key.k_offset += ih->ih_item_len;
+    reiserfsck_delete_item (path);
+  } while (/*reiserfsck_*/usearch_by_key (&g_sb, &key, path, 0, DISK_LEAF_NODE_LEVEL, READ_BLOCKS, comp_keys) == ITEM_FOUND);
+
+  pathrelse (path);
+
+  /* paste or insert pointer to the unformatted node */
+  key.k_offset -= copied;
+  ni.unfm_nodenum = unbh->b_blocknr;
+  ni.unfm_freespace = (keep_free_space == 1) ? (g_sb.s_blocksize - copied) : 0;
+
+/* this is for check only */
+mark_block_unformatted (ni.unfm_nodenum);
+
+  if (usearch_by_position (&g_sb, &key, path) == FILE_NOT_FOUND) {
+    struct item_head insih;
+
+    copy_key (&(insih.ih_key), &key);
+    insih.ih_key.k_uniqueness = TYPE_INDIRECT;
+    insih.u.ih_free_space = ni.unfm_freespace;
+    mark_item_unaccessed (&insih);
+    insih.ih_item_len = UNFM_P_SIZE;
+    reiserfsck_insert_item (path, &insih, (const char *)&(ni.unfm_nodenum));
+  } else {
+    ih = PATH_PITEM_HEAD (path);
+    if (!I_IS_INDIRECT_ITEM (ih) || ih->ih_key.k_offset + I_BYTES_NUMBER (ih, g_sb.s_blocksize) != key.k_offset)
+      die ("direct2indirect: incorrect item found");
+    reiserfsck_paste_into_item (path, (const char *)&ni, UNFM_P_SIZE);
+  }
+
+  mark_buffer_dirty (unbh, 0);
+  mark_buffer_uptodate (unbh, 0);
+  brelse (unbh);
+
+  if (usearch_by_position (&g_sb, &key, path) != BYTE_FOUND || !I_IS_INDIRECT_ITEM (PATH_PITEM_HEAD (path)))
+    die ("direct2indirect: position not found");
+  return;
+}
+
+
+
+
+static int append_to_unformatted_node (struct item_head * comingih, struct item_head * ih, char * item, struct path * path)
+{
+  struct buffer_head * bh, * unbh;
+  int end_of_data = g_sb.s_blocksize - ih->u.ih_free_space;
+  int offset = comingih->ih_key.k_offset % g_sb.s_blocksize - 1;
+  int zero_number = offset - end_of_data;
+  __u32 unfm_ptr;
+
+  /* append to free space of the last unformatted node of indirect item ih */
+  if (ih->u.ih_free_space < comingih->ih_item_len)
+    die ("reiserfsck_append_file: there is no enough free space in unformatted node");
+
+  bh = PATH_PLAST_BUFFER (path);
+
+  unfm_ptr = B_I_POS_UNFM_POINTER (bh, ih, I_UNFM_NUM (ih) - 1);
+  if (unfm_ptr == 0 || unfm_ptr >= SB_BLOCK_COUNT (&g_sb)) {
+    unbh = reiserfsck_get_new_buffer (bh->b_blocknr);
+    B_I_POS_UNFM_POINTER (bh, ih, I_UNFM_NUM (ih) - 1) = unbh->b_blocknr;
+    mark_block_unformatted (unbh->b_blocknr);
+    mark_buffer_dirty (bh, 0);
+  } else {
+    unbh = bread (g_sb.s_dev, unfm_ptr, g_sb.s_blocksize);
+    if (!is_block_used (unfm_ptr))
+      die ("append_to_unformatted_node:  unused block %d", unfm_ptr);
+
+  }
+  memset (unbh->b_data + end_of_data, 0, zero_number);
+  memcpy (unbh->b_data + offset, item, comingih->ih_item_len);
+
+  save_unfm_overwriting (unbh->b_blocknr, comingih);
+
+  ih->u.ih_free_space -= (zero_number + comingih->ih_item_len);
+  memset (unbh->b_data + offset + comingih->ih_item_len, 0, ih->u.ih_free_space);
+  mark_buffer_uptodate (unbh, 0);
+  mark_buffer_dirty (unbh, 0);
+  brelse (unbh);
+  pathrelse (path);
+  return comingih->ih_item_len;
+}
+
+
+static void adjust_free_space (struct buffer_head * bh, struct item_head * ih, struct item_head * comingih)
+{
+  if (I_IS_INDIRECT_ITEM (comingih)) {
+    ih->u.ih_free_space = 0;
+  } else {
+    if (comingih->ih_key.k_offset < ih->ih_key.k_offset + g_sb.s_blocksize * I_UNFM_NUM (ih))
+      /* append to the last unformatted node */
+      ih->u.ih_free_space = g_sb.s_blocksize - ih->ih_key.k_offset % g_sb.s_blocksize + 1;
+    else
+      ih->u.ih_free_space = 0;
+  }
+
+  mark_buffer_dirty (bh, 0);
+}
+
+
+/* this appends file with one unformatted node pointer (since balancing algorithm limitation). This
+   pointer can be 0, or new allocated block or pointer from indirect item that is being inserted
+   into tree */
+int reiserfsck_append_file (struct item_head * comingih, char * item, int pos, struct path * path)
+{
+  struct unfm_nodeinfo ni;
+  struct buffer_head * unbh;
+  int retval;
+/*  int keep_free_space;*/
+  struct item_head * ih = PATH_PITEM_HEAD (path);
+
+  if (!I_IS_INDIRECT_ITEM (ih))
+    die ("reiserfsck_append_file: can not append to non-indirect item");
+
+  if (ih->ih_key.k_offset + I_BYTES_NUMBER (ih, g_sb.s_blocksize) != comingih->ih_key.k_offset) {
+    adjust_free_space (PATH_PLAST_BUFFER (path), ih, comingih);
+  }
+
+  if (I_IS_DIRECT_ITEM (comingih)) {
+    if (comingih->ih_key.k_offset < ih->ih_key.k_offset + g_sb.s_blocksize * I_UNFM_NUM (ih)) {
+      /* direct item fits to free space of indirect item */
+      return append_to_unformatted_node (comingih, ih, item, path);
+    }
+
+    unbh = reiserfsck_get_new_buffer (PATH_PLAST_BUFFER (path)->b_blocknr);
+    /* this is for check only */
+    mark_block_unformatted (unbh->b_blocknr);
+    memcpy (unbh->b_data + comingih->ih_key.k_offset % unbh->b_size - 1, item, comingih->ih_item_len);
+
+    save_unfm_overwriting (unbh->b_blocknr, comingih);
+
+    mark_buffer_dirty (unbh, 0);
+    mark_buffer_uptodate (unbh, 0);
+
+    ni.unfm_nodenum = unbh->b_blocknr;
+    ni.unfm_freespace = g_sb.s_blocksize - comingih->ih_item_len - (comingih->ih_key.k_offset % unbh->b_size - 1);
+    brelse (unbh);
+    retval = comingih->ih_item_len;
+  } else {
+    /* coming item is indirect item */
+    if (comingih->ih_key.k_offset + pos * g_sb.s_blocksize != ih->ih_key.k_offset + I_BYTES_NUMBER (ih, g_sb.s_blocksize))
+      die ("reiserfsck_append_file: can not append indirect item (%lu) to position (%lu + %lu)",
+	   comingih->ih_key.k_offset, ih->ih_key.k_offset, I_BYTES_NUMBER (ih, g_sb.s_blocksize));
+
+    /* take unformatted pointer from an indirect item */
+    ni.unfm_nodenum = *(unsigned long *)(item + pos * UNFM_P_SIZE);/*B_I_POS_UNFM_POINTER (bh, ih, pos);*/
+    if (!is_block_used (ni.unfm_nodenum) && !is_block_uninsertable (ni.unfm_nodenum)) {
+      mark_block_used (ni.unfm_nodenum);
+
+      /* this is for check only */
+      mark_block_unformatted (ni.unfm_nodenum);
+    } else {
+      ni.unfm_nodenum = 0;
+    }
+    ni.unfm_freespace = ((pos == (I_UNFM_NUM (comingih) - 1)) ? comingih->u.ih_free_space : 0);
+    retval = g_sb.s_blocksize - ni.unfm_freespace;
+  }
+
+  reiserfsck_paste_into_item (path, (const char *)&ni, UNFM_P_SIZE);
+  return retval;
+}
+
+
+int must_there_be_a_hole (struct item_head * comingih, struct path * path)
+{
+  struct item_head * ih = PATH_PITEM_HEAD (path);
+  int keep_free_space;
+
+  if (I_IS_DIRECT_ITEM (ih)) {
+    direct2indirect (0, path, keep_free_space = 1);
+    ih = PATH_PITEM_HEAD (path);
+  }
+
+  path->pos_in_item = I_UNFM_NUM (ih);
+  if (ih->ih_key.k_offset + (I_UNFM_NUM (ih) + 1) * g_sb.s_blocksize <= comingih->ih_key.k_offset)
+    return 1;
+
+  return 0;
+}
+
+
+int reiserfs_append_zero_unfm_ptr (struct path * path)
+{
+  struct unfm_nodeinfo ni;
+  int keep_free_space;
+
+  ni.unfm_nodenum = 0;
+  ni.unfm_freespace = 0;
+
+  if (I_IS_DIRECT_ITEM (PATH_PITEM_HEAD (path)))
+    /* convert direct item to indirect */
+    direct2indirect (0, path, keep_free_space = 0);
+
+  reiserfsck_paste_into_item (path, (const char *)&ni, UNFM_P_SIZE);
+  return 0;
+}
+
+
+/* write direct item to unformatted node */
+static int overwrite_by_direct_item (struct item_head * comingih, char * item, struct path * path)
+{
+  unsigned long unfm_ptr;
+  struct buffer_head * unbh, * bh;
+  struct item_head * ih;
+  int offset;
+
+  bh = PATH_PLAST_BUFFER (path);
+  ih = PATH_PITEM_HEAD (path);
+  unfm_ptr = B_I_POS_UNFM_POINTER (bh, ih, path->pos_in_item);
+  if (unfm_ptr == 0 || unfm_ptr >= SB_BLOCK_COUNT (&g_sb)) {
+    unbh = reiserfsck_get_new_buffer (PATH_PLAST_BUFFER (path)->b_blocknr);
+    B_I_POS_UNFM_POINTER (bh, ih, path->pos_in_item) = unbh->b_blocknr;
+/* this is for check only */
+mark_block_unformatted (unbh->b_blocknr);
+    mark_buffer_dirty (bh, 0);
+  }
+  else {
+    unbh = bread (g_sb.s_dev, unfm_ptr, bh->b_size);
+    if (!is_block_used (unfm_ptr))
+      die ("overwrite_by_direct_item: unused block %d", unfm_ptr);
+  }
+
+  offset = comingih->ih_key.k_offset % bh->b_size - 1;
+  if (offset + comingih->ih_item_len > MAX_DIRECT_ITEM_LEN (bh->b_size))
+    die ("overwrite_by_direct_item: direct item too long (offset=%lu, length=%u)", comingih->ih_key.k_offset, comingih->ih_item_len);
+
+  memcpy (unbh->b_data + offset, item, comingih->ih_item_len);
+
+  save_unfm_overwriting (unbh->b_blocknr, comingih);
+
+  if (path->pos_in_item == I_UNFM_NUM (ih) - 1 && (bh->b_size - ih->u.ih_free_space) < (offset + comingih->ih_item_len)) {
+    ih->u.ih_free_space = bh->b_size - (offset + comingih->ih_item_len);
+    mark_buffer_dirty (bh, 0);
+  }
+  mark_buffer_dirty (unbh, 0);
+  mark_buffer_uptodate (unbh, 0);
+  brelse (unbh);
+  return comingih->ih_item_len;
+}
+
+
+
+void overwrite_unfm_by_unfm (unsigned long unfm_in_tree, unsigned long coming_unfm, int bytes_in_unfm)
+{
+  struct overwritten_unfm_segment * unfm_os_list;/* list of overwritten segments of the unformatted node */
+  struct overwritten_unfm_segment unoverwritten_segment;
+  struct buffer_head * bh_in_tree, * coming_bh;
+
+  if (!test_bit (coming_unfm % (g_sb.s_blocksize * 8), SB_AP_BITMAP (&g_sb)[coming_unfm / (g_sb.s_blocksize * 8)]->b_data))
+    /* block (pointed by indirect item) is free, we do not have to keep its contents */
+    return;
+
+  /* coming block is marked as used in disk bitmap. Put its contents to block in tree preserving
+     everything, what has been overwritten there by direct items */
+  unfm_os_list = find_overwritten_unfm (unfm_in_tree, bytes_in_unfm, &unoverwritten_segment);
+  if (unfm_os_list) {
+    add_event (UNFM_OVERWRITING_UNFM);
+    bh_in_tree = bread (g_sb.s_dev, unfm_in_tree, g_sb.s_blocksize);
+    coming_bh = bread (g_sb.s_dev, coming_unfm, g_sb.s_blocksize);
+    
+    while (get_unoverwritten_segment (unfm_os_list, &unoverwritten_segment)) {
+      if (unoverwritten_segment.ous_begin < 0 || unoverwritten_segment.ous_end > bytes_in_unfm - 1 ||
+	  unoverwritten_segment.ous_begin > unoverwritten_segment.ous_end)
+	die ("overwrite_unfm_by_unfm: invalid segment found (%d %d)", unoverwritten_segment.ous_begin, unoverwritten_segment.ous_end);
+
+      memcpy (bh_in_tree->b_data + unoverwritten_segment.ous_begin, coming_bh->b_data + unoverwritten_segment.ous_begin,
+	      unoverwritten_segment.ous_end - unoverwritten_segment.ous_begin + 1);
+      mark_buffer_dirty (bh_in_tree, 0);
+    }
+
+    brelse (bh_in_tree);
+    brelse (coming_bh);
+  }
+}
+
+
+/* put unformatted node pointers from incoming item over the in-tree ones */
+static int overwrite_by_indirect_item (struct item_head * comingih, unsigned long * coming_item, struct path * path, int * pos_in_coming_item)
+{
+  struct buffer_head * bh = PATH_PLAST_BUFFER (path);
+  struct item_head * ih = PATH_PITEM_HEAD (path);
+  int written;
+  unsigned long * item_in_tree;
+  int src_unfm_ptrs, dest_unfm_ptrs, to_copy;
+  int i;
+
+
+  item_in_tree = (unsigned long *)B_I_PITEM (bh, ih) + path->pos_in_item;
+  coming_item += *pos_in_coming_item;
+
+  dest_unfm_ptrs = I_UNFM_NUM (ih) - path->pos_in_item;
+  src_unfm_ptrs = I_UNFM_NUM (comingih) - *pos_in_coming_item;
+  
+  if (dest_unfm_ptrs >= src_unfm_ptrs) {
+    /* whole coming item (comingih) fits into item in tree (ih) starting with path->pos_in_item */
+    written = I_BYTES_NUMBER (comingih, g_sb.s_blocksize) - *pos_in_coming_item * g_sb.s_blocksize;
+    *pos_in_coming_item = I_UNFM_NUM (comingih);
+    to_copy = src_unfm_ptrs;
+    if (dest_unfm_ptrs == src_unfm_ptrs)
+      ih->u.ih_free_space = comingih->u.ih_free_space;/*??*/
+  } else {
+    /* only part of coming item overlaps item in the tree */
+    *pos_in_coming_item += dest_unfm_ptrs;
+    written = dest_unfm_ptrs * g_sb.s_blocksize;
+    to_copy = dest_unfm_ptrs;
+    ih->u.ih_free_space = 0;
+  }
+  
+  for (i = 0; i < to_copy; i ++) {
+    if (!is_block_used (coming_item[i]) && !is_block_uninsertable (coming_item[i])) {
+      if (item_in_tree[i]) {
+	/* do not overwrite unformatted pointer. We must save everything what is there already from
+           direct items */
+	overwrite_unfm_by_unfm (item_in_tree[i], coming_item[i], g_sb.s_blocksize);
+      } else {
+	item_in_tree[i] = coming_item[i];
+	mark_block_used (coming_item[i]);
+/* this is for check only */
+mark_block_unformatted (coming_item[i]);
+      }
+    }
+  }
+  mark_buffer_dirty (bh, 0);
+  return written;
+}
+
+
+int reiserfsck_overwrite_file (struct item_head * comingih, char * item, struct path * path, int * pos_in_coming_item)
+{
+  __u32 unfm_ptr;
+  int written = 0;
+  int keep_free_space;
+  struct item_head * ih = PATH_PITEM_HEAD (path);
+
+  if (comp_short_keys (ih, &(comingih->ih_key)) != KEYS_IDENTICAL)
+    die ("reiserfsck_overwrite_file: found [%lu %lu], new item [%lu %lu]", ih->ih_key.k_dir_id, ih->ih_key.k_objectid,
+	 comingih->ih_key.k_dir_id, comingih->ih_key.k_objectid);
+  
+  if (I_IS_DIRECT_ITEM (ih)) {
+    unfm_ptr = 0;
+    if (I_IS_INDIRECT_ITEM (comingih)) {
+      if (ih->ih_key.k_offset % g_sb.s_blocksize != 1)
+	die ("reiserfsck_overwrite_file: second part of tail can not be overwritten by indirect item");
+      /* use pointer from coming indirect item */
+      unfm_ptr = *(__u32 *)(item + *pos_in_coming_item * UNFM_P_SIZE);
+      if (unfm_ptr >= SB_BLOCK_COUNT (&g_sb) || is_block_used (unfm_ptr) || 
+	  !was_block_used (unfm_ptr) || is_block_uninsertable (unfm_ptr))
+	unfm_ptr = 0;
+    }
+    /* */
+    direct2indirect (unfm_ptr, path, keep_free_space = 1);
+  }
+
+  if (I_IS_DIRECT_ITEM (comingih)) {
+    written = overwrite_by_direct_item (comingih, item, path);
+  } else {
+    written = overwrite_by_indirect_item (comingih, (unsigned long *)item, path, pos_in_coming_item);
+  }
+
+  return written;
+}
+
+
+/*
+ */
+int reiserfsck_file_write (struct item_head * ih, char * item)
+{
+  struct path path;
+  struct item_head * path_ih;
+  int count, pos_in_coming_item;
+  int retval;
+  struct key key;
+  int written;
+
+  if (make_file_writeable (ih) == -1)
+    /* write was not completed. Skip that item. Maybe it should be
+       saved to lost_found */
+    return 0;
+
+  count = I_BYTES_NUMBER (ih, g_sb.s_blocksize);
+  pos_in_coming_item = 0;
+
+  copy_key (&key, &(ih->ih_key));
+  while (count) {
+    retval = usearch_by_position (&g_sb, &key, &path);
+    if (retval == DIRECTORY_FOUND) {
+      pathrelse (&path);
+      return 0;
+    }
+    if (retval == BYTE_FOUND) {
+      written = reiserfsck_overwrite_file (ih, item, &path, &pos_in_coming_item);
+      count -= written;
+      key.k_offset += written;
+    }
+    if (retval == FILE_NOT_FOUND) {
+      written = create_first_item_of_file (ih, item, &path, &pos_in_coming_item);
+      count -= written;
+      key.k_offset += written;
+    }
+    if (retval == BYTE_NOT_FOUND) {
+      path_ih = PATH_PITEM_HEAD (&path);
+      if (must_there_be_a_hole (ih, &path) == 1)
+	reiserfs_append_zero_unfm_ptr (&path);
+      else {
+	count -= reiserfsck_append_file (ih, item, pos_in_coming_item, &path);
+	key.k_offset += g_sb.s_blocksize;
+	pos_in_coming_item ++;
+      }
+    }
+    if (count < 0)
+      die ("reiserfsck_file_write: count < 0 (%d)", count);
+    pathrelse (&path);
+  }
+  
+  return I_BYTES_NUMBER (ih, g_sb.s_blocksize);
+}
+
+
+
diff -urN linux/fs/reiserfs/utils/fsck/uobjectid.c /tmp/linux/fs/reiserfs/utils/fsck/uobjectid.c
--- linux/fs/reiserfs/utils/fsck/uobjectid.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/fsck/uobjectid.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,236 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+#include "fsck.h"
+
+
+#if 0 /* original function */
+//
+// FIXME: we are going to turn reuse of objectids back
+//
+objectid_t get_unused_objectid (struct super_block * s)
+{
+    objectid_t * map, objectid;
+
+    map = (objectid_t *)(SB_DISK_SUPER_BLOCK (s) + 1);
+    objectid = map[1];
+    if (objectid == TYPE_INDIRECT)
+	die ("get_unused_objectid: no more objectids");
+
+    map[1] ++;
+    return objectid;
+}
+#endif
+
+/* stolen from reiserfs/objectid.c  --clm */
+objectid_t get_unused_objectid (struct super_block * s)
+{
+  objectid_t unused_objectid;
+  struct reiserfs_super_block * disk_sb;
+  objectid_t * objectid_map;
+
+
+  disk_sb = SB_DISK_SUPER_BLOCK (s);
+
+  /* The objectid map follows the superblock. */
+  objectid_map = (objectid_t *)(disk_sb + 1); 
+ 
+  unused_objectid = objectid_map[1]; /* commented more below */
+  if (unused_objectid == TYPE_INDIRECT) {
+    printf ("REISERFS: get_objectid: no more object ids\n");
+    return 0;
+  }
+
+  /* This incrementation allocates the first unused objectid. That is to say, 
+     the first entry on the objectid map is the first unused objectid, and 
+     by incrementing it we use it.  See below where we check to see if we 
+     eliminated a sequence of unused objectids.... */
+
+  objectid_map[1] ++;
+
+  /* Now we check to see if we eliminated the last remaining member of
+     the first even sequence (and can eliminate the sequence by
+     eliminating its last objectid from oids), and can collapse the
+     first two odd sequences into one sequence.  If so, then the net
+     result is to eliminate a pair of objectids from oids.  We do this
+     by shifting the entire map to the left. */
+  if (disk_sb->s_oid_cursize > 2 && objectid_map[1] == objectid_map[2]) {
+    memmove (objectid_map + 1, objectid_map + 3, (disk_sb->s_oid_cursize - 3) * sizeof(unsigned long));
+    disk_sb->s_oid_cursize -= 2;
+  }
+
+  return unused_objectid;
+}
+
+int is_objectid_used (unsigned long objectid)
+{
+  unsigned long * objectid_map;
+  int i = 0;
+
+  objectid_map = (unsigned long *)(SB_DISK_SUPER_BLOCK (&g_sb) + 1);
+
+  while (i < SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_cursize) {
+    if (objectid == objectid_map[i]) {
+      return 1;      /* objectid is used */
+    }
+    
+    if (objectid > objectid_map[i] && objectid < objectid_map[i+1]) {
+      return 1;	/* objectid is used */
+    }
+
+    if (objectid < objectid_map[i])
+      break;
+
+    i += 2;
+  }
+  
+  /* objectid is free */
+  return 0;
+}
+
+
+/* we mark objectid as used. Additionally, some unused objectids can
+   become used. It is ok. What is unacceptable, it is when used
+   objectids are marked as unused */
+void mark_objectid_as_used (unsigned long objectid)
+{
+  int i;
+  unsigned long * objectid_map;
+
+  if (is_objectid_used (objectid) == 1) {
+    
+    /*print_objectid_map (&g_sb);*/
+    /*printf ("mark_objectid_as_used: objectid %lu is used", objectid);*/
+    return;
+  }
+
+  objectid_map = (unsigned long *)(SB_DISK_SUPER_BLOCK (&g_sb) + 1);
+
+  for (i = 0; i < SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_cursize; i += 2) {
+    if (objectid >= objectid_map [i] && objectid < objectid_map [i + 1])
+      /* it is used */
+      return;
+
+    if (objectid + 1 == objectid_map[i]) {
+      /* size of objectid map is the same */
+      objectid_map[i] = objectid;
+      return;
+    }
+
+    if (objectid == objectid_map[i + 1]) {
+      /* size of objectid map is decreased */
+      objectid_map[i + 1] ++;
+      if (i + 2 < SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_cursize) {
+	if (objectid_map[i + 1] == objectid_map[i + 2]) {
+	  memmove (objectid_map + i + 1, objectid_map + i + 1 + 2, 
+		   (SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_cursize - (i + 2 + 2 - 1)) * sizeof (unsigned long));
+	  SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_cursize -= 2;
+	}
+      }
+      return;
+    }
+    
+    if (objectid < objectid_map[i]) {
+      /* size of objectid map must be increased */
+      if (SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_cursize == SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_maxsize) {
+	/* here all objectids between objectid and objectid_map[i] get used */
+	objectid_map[i] = objectid;
+	return;
+      } else {
+	memmove (objectid_map + i + 2, objectid_map + i, (SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_cursize - i) * sizeof (unsigned long));
+	SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_cursize += 2;
+      }
+      
+      objectid_map[i] = objectid;
+      objectid_map[i+1] = objectid + 1;
+      return;
+    }
+
+  }
+  
+  /* write out of current objectid map, if we have space */
+  if (i < SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_maxsize) {
+    objectid_map[i] = objectid;
+    objectid_map[i + 1] = objectid + 1;
+    SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_cursize += 2;
+  } else if (i == SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_maxsize) {
+    objectid_map[i - 1] = objectid + 1;
+  } else
+    die ("mark_objectid_as_used: objectid map corrupted");
+  
+  return;
+}
+
+
+#if 0 /* haven't checked this code carefully enough yet */
+void mark_objectid_as_free (unsigned long objectid)
+{
+  unsigned long * oids; /* pointer to objectid map */
+  int i = 0;
+
+  oids = (unsigned long *)(SB_DISK_SUPER_BLOCK (&g_sb) + 1);
+
+  while (i < SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_cursize)
+    {
+      if (objectid == oids[i])
+	{
+	  if (i == 0)
+	    die ("mark_objectid_as_free: trying to free root object id");
+	  oids[i]++;
+
+	  if (oids[i] == oids[i+1])
+	    {
+	      /* shrink objectid map */
+	      if (SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_cursize < i + 2)
+		die ("mark_objectid_as_free: bad cur size");
+
+	      memmove (oids + i, oids + i + 2, (SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_cursize - i - 2) * sizeof (unsigned long));
+	      SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_cursize -= 2;
+	      if (SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_cursize < 2 || SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_cursize > SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_maxsize)
+		die("mark_objectid_as_free: bad cur size");
+	    }
+	  return;
+	}
+
+      if (objectid > oids[i] && objectid < oids[i+1])
+	{
+	  /* size of objectid map is not changed */
+	  if (objectid + 1 == oids[i+1])
+	    {
+	      oids[i+1]--;
+	      return;
+	    }
+
+	  if (SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_cursize == SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_maxsize)
+	    /* objectid map must be expanded, but there is no space */
+	    return;
+
+	  /* expand the objectid map*/
+	  memmove (oids + i + 3, oids + i + 1, (SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_cursize - i - 1) * sizeof (unsigned long));
+	  oids[i+1] = objectid;
+	  oids[i+2] = objectid + 1;
+	  SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_cursize += 2;
+	  if (SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_cursize < 2 || SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_cursize > SB_DISK_SUPER_BLOCK (&g_sb)->s_oid_maxsize)
+	    die ("objectid_release: bad cur size");
+	  return;
+	}
+      i += 2;
+    }
+
+  die ("objectid_release: trying to free free object id (%lu)", objectid);
+}
+
+/* original function that does not use the objectid map */
+void mark_objectid_as_used (unsigned long objectid)
+{
+  unsigned long * objectid_map;
+  
+
+  objectid_map = (unsigned long *)(SB_DISK_SUPER_BLOCK (&g_sb) + 1);
+  if (objectid >= objectid_map[1]) {
+      objectid_map[1] = objectid + 1;
+  }
+
+}
+
+#endif 
diff -urN linux/fs/reiserfs/utils/fsck/ustree.c /tmp/linux/fs/reiserfs/utils/fsck/ustree.c
--- linux/fs/reiserfs/utils/fsck/ustree.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/fsck/ustree.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,443 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+/*#include <stdio.h>
+#include <string.h>*/
+/*#include <asm/bitops.h>
+#include "../include/reiserfs_fs.h"
+#include "../include/reiserfs_fs_sb.h"
+#include "../include/reiserfslib.h"*/
+#include "fsck.h"
+
+static inline int compare_keys (unsigned long * key1, unsigned long * key2, int length)
+{
+  for (; length--; ++key1, ++key2) {
+    if ( *key1 < *key2 )
+      return SECOND_GREATER;
+    if ( *key1 > *key2 )
+      return FIRST_GREATER;
+  }
+  
+  return KEYS_IDENTICAL;
+}
+
+
+/* compare 3 components of key */
+int comp_keys_3 (void * key1, void * key2)
+{
+  return compare_keys (key1, key2, 3);
+}
+
+
+/* compare 4 components of key */
+int comp_dir_entries (void * key1, void * key2)
+{
+  return compare_keys (key1, key2, 1);
+}
+
+void init_tb_struct (struct tree_balance * tb, struct super_block  * s, struct path * path, int size)
+{
+  memset (tb, '\0', sizeof(struct tree_balance));
+  tb->tb_sb = s;
+  tb->tb_path = path;
+  PATH_OFFSET_PBUFFER(path, ILLEGAL_PATH_ELEMENT_OFFSET) = NULL;
+  PATH_OFFSET_POSITION(path, ILLEGAL_PATH_ELEMENT_OFFSET) = 0;
+  tb->insert_size[0] = size;
+}
+
+struct tree_balance * cur_tb = 0;
+
+void reiserfsck_paste_into_item (struct path * path, const char * body, int size)
+{
+  struct tree_balance tb;
+  
+  init_tb_struct (&tb, &g_sb, path, size);
+  if (fix_nodes (0/*th*/, M_PASTE, &tb, path->pos_in_item, 0) != CARRY_ON)
+    die ("reiserfsck_paste_into_item: fix_nodes failed");
+
+  do_balance (0/*th*/, &tb, path->pos_in_item, 0, body, M_PASTE, REISERFS_KERNEL_MEM, 0);
+}
+
+
+void reiserfsck_insert_item (struct path * path, struct item_head * ih, const char * body)
+{
+  struct tree_balance tb;
+
+  init_tb_struct (&tb, &g_sb, path, IH_SIZE + ih->ih_item_len);
+  if (fix_nodes (0/*th*/, M_INSERT, &tb, 0, ih) != CARRY_ON)
+    die ("reiserfsck_insert_item: fix_nodes failed");
+
+  do_balance (0/*th*/, &tb, 0, ih, body, M_INSERT, REISERFS_KERNEL_MEM, 0);
+}
+
+
+static void free_unformatted_nodes (struct item_head * ih, struct buffer_head * bh)
+{
+  unsigned long * punfm = (unsigned long *)B_I_PITEM (bh, ih);
+  int i;
+
+  for (i = 0; i < I_UNFM_NUM (ih); i ++, punfm ++)
+    if (*punfm) {
+      struct buffer_head * to_be_forgotten;
+
+      to_be_forgotten = find_buffer (g_sb.s_dev, *punfm, g_sb.s_blocksize);
+      if (to_be_forgotten) {
+	to_be_forgotten->b_count ++;
+	bforget (to_be_forgotten);
+      }
+      reiserfs_free_block (0/*th*/, &g_sb, *punfm);
+/* this is for check only */
+      unmark_block_unformatted (*punfm);
+    }
+}
+
+
+void reiserfsck_delete_item (struct path * path)
+{
+  struct tree_balance tb;
+  struct item_head * ih = PATH_PITEM_HEAD (path);
+
+  if (I_IS_INDIRECT_ITEM (ih))
+    free_unformatted_nodes (ih, PATH_PLAST_BUFFER (path));
+
+  init_tb_struct (&tb, &g_sb, path, -(IH_SIZE + ih->ih_item_len));
+  if (fix_nodes (0/*th*/, M_DELETE, &tb, 0, 0) != CARRY_ON)
+    die ("reiserfsck_delete_item: fix_nodes failed");
+
+  do_balance (0/*th*/, &tb, 0, 0, 0, M_DELETE, REISERFS_KERNEL_MEM, 0);
+}
+
+
+void reiserfsck_cut_from_item (struct path * path, int cut_size)
+{
+  struct tree_balance tb;
+  struct item_head * ih;
+
+  if (cut_size >= 0)
+    die ("reiserfsck_cut_from_item: cut size == %d", cut_size);
+
+  if (I_IS_INDIRECT_ITEM (ih = PATH_PITEM_HEAD (path))) {
+    __u32 unfm_ptr = B_I_POS_UNFM_POINTER (PATH_PLAST_BUFFER (path), ih, I_UNFM_NUM (ih) - 1);
+    if (unfm_ptr) {
+      struct buffer_head * to_be_forgotten;
+
+      to_be_forgotten = find_buffer (g_sb.s_dev, unfm_ptr, g_sb.s_blocksize);
+      if (to_be_forgotten) {
+        to_be_forgotten->b_count ++;
+        bforget (to_be_forgotten);
+      }
+      reiserfs_free_block (0/*th*/, &g_sb, unfm_ptr);
+/* this is for check only */
+      unmark_block_unformatted (unfm_ptr);
+    }
+  }
+
+
+  init_tb_struct (&tb, &g_sb, path, cut_size);
+  if (fix_nodes (0/*th*/, M_CUT, &tb, path->pos_in_item, 0) != CARRY_ON)
+    die ("reiserfsck_cut_from_item: fix_nodes failed");
+
+  do_balance (0/*th*/, &tb, path->pos_in_item, 0, 0, M_CUT, REISERFS_KERNEL_MEM, 0);
+}
+
+
+/* uget_lkey is utils clone of stree.c/get_lkey */
+struct key * uget_lkey (struct path * path)
+{
+  int pos, offset = path->path_length;
+  struct buffer_head * bh;
+  
+  if (offset < FIRST_PATH_ELEMENT_OFFSET)
+    die ("uget_lkey: illegal offset in the path (%d)", offset);
+
+
+  /* While not higher in path than first element. */
+  while (offset-- > FIRST_PATH_ELEMENT_OFFSET) {
+    if (! buffer_uptodate (PATH_OFFSET_PBUFFER (path, offset)) )
+      die ("uget_lkey: parent is not uptodate");
+
+    /* Parent at the path is not in the tree now. */
+    if (! B_IS_IN_TREE (bh = PATH_OFFSET_PBUFFER (path, offset)))
+      die ("uget_lkey: buffer on the path is not in tree");
+
+    /* Check whether position in the parent is correct. */
+    if ((pos = PATH_OFFSET_POSITION (path, offset)) > B_NR_ITEMS (bh))
+      die ("uget_lkey: invalid position (%d) in the path", pos);
+
+    /* Check whether parent at the path really points to the child. */
+    if (B_N_CHILD_NUM (bh, pos) != PATH_OFFSET_PBUFFER (path, offset + 1)->b_blocknr)
+      die ("uget_lkey: invalid block number (%d). Must be %d",
+	   B_N_CHILD_NUM (bh, pos), PATH_OFFSET_PBUFFER (path, offset + 1)->b_blocknr);
+
+    /* Return delimiting key if position in the parent is not equal to zero. */
+    if (pos)
+      return B_N_PDELIM_KEY(bh, pos - 1);
+  }
+
+  /* we must be in the root */
+/*
+  if (PATH_OFFSET_PBUFFER (path, FIRST_PATH_ELEMENT_OFFSET)->b_blocknr != SB_ROOT_BLOCK (&g_sb))
+    die ("get_left_dkey: path does not start with the root");
+*/
+
+  /* there is no left delimiting key */
+  return 0;
+}
+
+
+/* uget_rkey is utils clone of stree.c/get_rkey */
+struct key * uget_rkey (struct path * path)
+{
+  int pos, offset = path->path_length;
+  struct buffer_head * bh;
+
+  if (offset < FIRST_PATH_ELEMENT_OFFSET)
+    die ("uget_rkey: illegal offset in the path (%d)", offset);
+
+  while (offset-- > FIRST_PATH_ELEMENT_OFFSET) {
+    if (! buffer_uptodate (PATH_OFFSET_PBUFFER (path, offset)))
+      die ("uget_rkey: parent is not uptodate");
+
+    /* Parent at the path is not in the tree now. */
+    if (! B_IS_IN_TREE (bh = PATH_OFFSET_PBUFFER (path, offset)))
+      die ("uget_rkey: buffer on the path is not in tree");
+
+    /* Check whether position in the parrent is correct. */
+    if ((pos = PATH_OFFSET_POSITION (path, offset)) > B_NR_ITEMS (bh))
+      die ("uget_rkey: invalid position (%d) in the path", pos);
+
+    /* Check whether parent at the path really points to the child. */
+    if (B_N_CHILD_NUM (bh, pos) != PATH_OFFSET_PBUFFER (path, offset + 1)->b_blocknr)
+      die ("uget_rkey: invalid block number (%d). Must be %d",
+	   B_N_CHILD_NUM (bh, pos), PATH_OFFSET_PBUFFER (path, offset + 1)->b_blocknr);
+
+    /* Return delimiting key if position in the parent is not the last one. */
+    if (pos != B_NR_ITEMS (bh))
+      return B_N_PDELIM_KEY(bh, pos);
+  }
+
+  /* we must be in the root */
+/*
+  if (PATH_OFFSET_PBUFFER (path, FIRST_PATH_ELEMENT_OFFSET)->b_blocknr != SB_ROOT_BLOCK (&g_sb))
+    die ("get_left_dkey: path does not start with the root");
+*/
+  /* there is no right delimiting key */
+  return 0;
+}
+
+
+static inline int ubin_search (void * key, void * base, int num, int width, int *ppos, comp_function_t comp_func)
+{
+  int   rbound, lbound, j;
+  
+  lbound = 0;
+  rbound = num - 1;
+  for (j = (rbound + lbound) / 2; lbound <= rbound; j = (rbound + lbound) / 2) {
+    switch (comp_func ((void *)((char *)base + j * width), key ) ) {
+    case SECOND_GREATER:
+      lbound = j + 1; 
+      continue;
+
+    case FIRST_GREATER:
+      rbound = j - 1;
+      continue;
+
+    case KEYS_IDENTICAL:
+      *ppos = j;
+      return KEY_FOUND;
+    }
+  }
+
+  *ppos = lbound;
+  return KEY_NOT_FOUND;
+}
+
+
+/* this searches in tree through items */
+int usearch_by_key (struct super_block * s, struct key * key, struct path * path, int * repeat, int stop_level, int bread_par, 
+		   comp_function_t comp_func)
+{
+  struct buffer_head * bh;
+  unsigned long block = s->u.reiserfs_sb.s_rs->s_root_block;
+  struct path_element * curr;
+
+  if (comp_func == 0)
+    comp_func = comp_keys;
+  if (repeat)
+    *repeat = CARRY_ON;
+
+  path->path_length = ILLEGAL_PATH_ELEMENT_OFFSET;
+  while (1) {
+    curr = PATH_OFFSET_PELEMENT (path, ++ path->path_length);
+    bh = curr->pe_buffer = bread (s->s_dev, block, s->s_blocksize);
+    if (ubin_search (key, B_N_PKEY (bh, 0), B_NR_ITEMS (bh),
+		    B_IS_ITEMS_LEVEL (bh) ? IH_SIZE : KEY_SIZE, &(curr->pe_position), comp_func) == KEY_FOUND) {
+      /* key found, return if this is leaf level */
+      if (B_BLK_HEAD (bh)->blk_level <= stop_level) {
+	path->pos_in_item = 0;
+	return KEY_FOUND;
+      }
+      curr->pe_position ++;
+    } else {
+      /* key not found in the node */
+      if (B_BLK_HEAD (bh)->blk_level <= stop_level)
+	return KEY_NOT_FOUND;
+    }
+    block = B_N_CHILD_NUM (bh, curr->pe_position);
+  }
+  die ("search_by_key: you can not get here");
+  return 0;
+}
+
+
+/* key is key of directory entry. This searches in tree through items
+   and in the found directory item as well */
+int usearch_by_entry_key (struct super_block * s, struct key * key, struct path * path)
+{
+  struct buffer_head * bh;
+  struct item_head * ih;
+  struct key tmpkey;
+
+  if (usearch_by_key (s, key, path, 0, DISK_LEAF_NODE_LEVEL, 0, comp_keys) == KEY_FOUND) {
+    path->pos_in_item = 0;
+    return ENTRY_FOUND;
+  }
+
+  bh = PATH_PLAST_BUFFER (path);
+  if (PATH_LAST_POSITION (path) == 0) {
+    /* previous item does not exist, that means we are in leftmost
+       leaf of the tree */
+    if (uget_lkey (path) != 0)
+      die ("search_by_entry_key: invalid position after search_by_key");
+    if (comp_short_keys ((unsigned long *)B_N_PKEY (bh, 0), (unsigned long *)key) == KEYS_IDENTICAL) {
+      path->pos_in_item = 0;
+      return ENTRY_NOT_FOUND;
+    }
+    path->pos_in_item = 0;
+    return DIRECTORY_NOT_FOUND;
+  }
+
+  /* take previous item */
+  PATH_LAST_POSITION (path) --;
+  ih = PATH_PITEM_HEAD (path);
+  if (comp_short_keys ((unsigned long *)ih, (unsigned long *)key) != KEYS_IDENTICAL || !I_IS_DIRECTORY_ITEM (ih)) {
+    struct key * next_key;
+
+    PATH_LAST_POSITION (path) ++;
+    /* previous item belongs to another object or is stat data, check next item */
+    if (PATH_LAST_POSITION (path) < B_NR_ITEMS (PATH_PLAST_BUFFER (path))) {
+      /* found item is not last item of the node */
+      next_key = B_N_PKEY (PATH_PLAST_BUFFER (path), PATH_LAST_POSITION (path));
+      if (comp_short_keys ((unsigned long *)next_key, (unsigned long *)key) != KEYS_IDENTICAL) {
+	path->pos_in_item = 0;
+	return DIRECTORY_NOT_FOUND;
+      }
+      if (!KEY_IS_DIRECTORY_KEY (next_key))
+	/* there is an item in the tree, but it is not a directory item */
+	return REGULAR_FILE_FOUND;
+    } else {
+      /* found item is last item of the node */
+      next_key = uget_rkey (path);
+      if (next_key == 0 || comp_short_keys ((unsigned long *)next_key, (unsigned long *)key) != KEYS_IDENTICAL) {
+	/* there is not any part of such directory in the tree */
+	path->pos_in_item = 0;
+	return DIRECTORY_NOT_FOUND;
+      }
+      if (!KEY_IS_DIRECTORY_KEY (next_key))
+	/* there is an item in the tree, but it is not a directory item */
+	return REGULAR_FILE_FOUND;    
+      
+      copy_key (&tmpkey, next_key);
+      pathrelse (path);
+      if (usearch_by_key (s, &tmpkey, path, 0, DISK_LEAF_NODE_LEVEL, 0, comp_keys) != KEY_FOUND || PATH_LAST_POSITION (path) != 0)
+	die ("search_by_entry_key: item not found by corresponding delimiting key");
+    }
+    /* next item is the part of this directory */
+    path->pos_in_item = 0;
+    return ENTRY_NOT_FOUND;
+  }
+
+  /* previous item is part of desired directory */
+  if (ubin_search (&(key->k_offset), B_I_DEH (bh, ih), I_ENTRY_COUNT (ih), DEH_SIZE, &(path->pos_in_item), comp_dir_entries) == KEY_FOUND)
+    return ENTRY_FOUND;
+  return ENTRY_NOT_FOUND;
+}
+
+
+/* key is key of byte in the regular file. This searches in tree
+   through items and in the found item as well */
+int usearch_by_position (struct super_block * s, struct key * key, struct path * path)
+{
+  struct buffer_head * bh;
+  struct item_head * ih;
+
+  if (usearch_by_key (s, key, path, 0, DISK_LEAF_NODE_LEVEL, 0, comp_keys_3) == KEY_FOUND) {
+    ih = PATH_PITEM_HEAD (path);
+    if (!I_IS_DIRECT_ITEM (ih) && !I_IS_INDIRECT_ITEM (ih))
+      return DIRECTORY_FOUND;
+    path->pos_in_item = 0;
+    return BYTE_FOUND;
+  }
+
+  bh = PATH_PLAST_BUFFER (path);
+  ih = PATH_PITEM_HEAD (path);
+  if (PATH_LAST_POSITION (path) == 0) {
+    /* previous item does not exist, that means we are in leftmost leaf of the tree */
+    if (comp_short_keys ((unsigned long *)B_N_PKEY (bh, 0), (unsigned long *)key) == KEYS_IDENTICAL) {
+      if (!I_IS_DIRECT_ITEM (ih) && !I_IS_INDIRECT_ITEM (ih))
+	return DIRECTORY_FOUND;
+      return BYTE_NOT_FOUND;
+    }
+    return FILE_NOT_FOUND;
+  }
+
+  /* take previous item */
+  PATH_LAST_POSITION (path) --;
+  ih = PATH_PITEM_HEAD (path);
+  if (comp_short_keys ((unsigned long *)&ih->ih_key, (unsigned long *)key) != KEYS_IDENTICAL ||
+      I_IS_STAT_DATA_ITEM (ih)) {
+    struct key * next_key;
+
+    /* previous item belongs to another object or is a stat data, check next item */
+    PATH_LAST_POSITION (path) ++;
+    if (PATH_LAST_POSITION (path) < B_NR_ITEMS (PATH_PLAST_BUFFER (path)))
+      /* next key is in the same node */
+      next_key = B_N_PKEY (PATH_PLAST_BUFFER (path), PATH_LAST_POSITION (path));
+    else
+      next_key = uget_rkey (path);
+    if (next_key == 0 || comp_short_keys ((unsigned long *)next_key, (unsigned long *)key) != KEYS_IDENTICAL) {
+      /* there is no any part of such file in the tree */
+      path->pos_in_item = 0;
+      return FILE_NOT_FOUND;
+    }
+
+    if (KEY_IS_DIRECTORY_KEY (next_key)) {
+      reiserfs_warning ("\ndirectory with the same key %d found\n", next_key);
+      return DIRECTORY_FOUND;
+    }
+    /* next item is the part of this file */
+    path->pos_in_item = 0;
+    return BYTE_NOT_FOUND;
+  }
+
+  if (I_IS_DIRECTORY_ITEM (ih)) {
+    return DIRECTORY_FOUND;
+  }
+  if (I_IS_STAT_DATA_ITEM (ih)) {
+    PATH_LAST_POSITION (path) ++;
+    return FILE_NOT_FOUND;
+  }
+
+  /* previous item is part of desired file */
+  if (I_K_KEY_IN_ITEM (ih, key, bh->b_size)) {
+    path->pos_in_item = key->k_offset - ih->ih_key.k_offset;
+    if ( I_IS_INDIRECT_ITEM (ih) )
+      path->pos_in_item /= bh->b_size;
+    return BYTE_FOUND;
+  }
+
+  path->pos_in_item = I_IS_INDIRECT_ITEM (ih) ? I_UNFM_NUM (ih) : ih->ih_item_len;
+  return BYTE_NOT_FOUND;
+}
+
+
diff -urN linux/fs/reiserfs/utils/include/fsck.h /tmp/linux/fs/reiserfs/utils/include/fsck.h
--- linux/fs/reiserfs/utils/include/fsck.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/include/fsck.h	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,267 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+#include <stdio.h>
+#include <string.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <asm/types.h>
+#include <sys/vfs.h>
+#include <errno.h>
+#include <unistd.h>
+#include <asm/byteorder.h>
+#include <asm/types.h>
+
+//#include "inode.h"
+#include "io.h"
+#include "sb.h"
+#include "misc.h"
+#include "reiserfs_fs.h"
+
+
+typedef __u32 blocknr_t;
+
+/* searches.c */
+#define KEY_FOUND 1
+#define KEY_NOT_FOUND 0
+
+#define DIRECTORY_NOT_FOUND -1
+
+#define FILE_NOT_FOUND -1
+
+
+#define reiserfsck_search_by_key(s,key,path,comp_func) search_by_key (s, key, path, 0, DISK_LEAF_NODE_LEVEL, READ_BLOCKS, comp_func)
+
+
+/* main.c */
+int main (int argc, char * argv []);
+
+
+//
+// options
+//
+extern int opt_verbose;
+extern int opt_fsck;
+
+#define FSCK_DEFAULT 0
+#define FSCK_REBUILD 1
+#define FSCK_FIND_ITEM 2
+extern int opt_fsck_mode;
+
+extern struct key key_to_find;
+
+#define STOP_DEFAULT 0
+#define STOP_AFTER_PASS1 1
+#define STOP_AFTER_PASS2 2
+#define STOP_AFTER_SEMANTIC 3
+#define STOP_AFTER_REPLAY 4
+extern int opt_stop_point;
+
+#define SCAN_USED_PART 0
+#define SCAN_WHOLE_PARTITION 1
+extern int opt_what_to_scan;
+
+#define NO_LOST_FOUND 0
+#define DO_LOST_FOUND 1
+extern int opt_lost_found;
+
+
+extern struct super_block g_sb;
+extern struct reiserfs_super_block * g_old_rs;
+extern char ** g_disk_bitmap;
+extern char ** g_new_bitmap;
+extern char ** g_uninsertable_leaf_bitmap;
+extern char ** g_formatted;
+extern char ** g_unformatted;
+extern int g_blocks_to_read;
+
+
+/* pass1.c */
+void build_the_tree (void);
+extern int g_unaccessed_items;
+int is_item_accessed (struct item_head * ih);
+void mark_item_accessed (struct item_head * ih, struct buffer_head * bh);
+void mark_item_unaccessed (struct item_head * ih);
+
+
+/* file.c */
+struct si {
+  struct item_head si_ih;
+  char * si_dnm_data;
+  struct si * si_next;
+
+  // changed by XB;
+  struct si * last_known;
+};
+void put_saved_items_into_tree (struct si *);
+int reiserfsck_file_write (struct item_head * ih, char * item);
+int are_file_items_correct (struct key * key, unsigned long * size, int mark_passed_items, struct path *, struct stat_data **);
+
+
+/* pass2.c */
+typedef	void (action_on_item_t)(struct si **, struct item_head *, char *);
+action_on_item_t save_item;
+action_on_item_t insert_item_separately;
+void for_all_items_in_node (action_on_item_t action, struct si ** si, struct buffer_head * bh);
+void take_bad_blocks_put_into_tree ();
+void insert_each_item_separately (struct buffer_head *);
+
+
+/* semantic.c */
+extern struct key g_root_directory_key;
+void semantic_pass (void);
+int check_semantic_tree (struct key * key, struct key * parent, int is_dot_dot);
+
+
+
+/* pass4.c */
+int check_unaccessed_items (void);
+void pass4 (struct super_block *);
+
+
+/* check.c */
+int check_file_system (void);
+void reiserfsck_check_pass1 (void);
+void reiserfsck_check_after_all (void);
+int is_leaf_bad (struct buffer_head * bh);
+int is_internal_bad (struct buffer_head * bh);
+
+void check_fs_tree (struct super_block * s);
+int is_internal_node (char * buf);
+int is_leaf_node (char * buf);
+
+
+
+/* noname.c */
+void get_max_buffer_key (struct buffer_head * bh, struct key * key);
+
+/* ustree.c */
+void init_tb_struct (struct tree_balance * tb, struct super_block  * s, struct path * path, int size);
+void reiserfsck_paste_into_item (struct path * path, const char * body, int size);
+void reiserfsck_insert_item (struct path * path, struct item_head * ih, const char * body);
+void reiserfsck_delete_item (struct path * path);
+void reiserfsck_cut_from_item (struct path * path, int cut_size);
+typedef	int (comp_function_t)(void * key1, void * key2);
+int usearch_by_key (struct super_block * s, struct key * key, struct path * path, int * repeat, int stop_level, int bread_par, 
+		   comp_function_t comp_func);
+int usearch_by_entry_key (struct super_block * s, struct key * key, struct path * path);
+int usearch_by_position (struct super_block * s, struct key * key, struct path * path);
+struct key * uget_lkey (struct path * path);
+struct key * uget_rkey (struct path * path);
+int comp_keys_3 (void * key1, void * key2);
+int comp_dir_entries (void * key1, void * key2);
+
+
+/* bitmap.c */
+extern int from_journal;
+int reiserfs_new_blocknrs (struct reiserfs_transaction_handle *th, struct super_block * s, unsigned long * free_blocknrs, unsigned long start, int amount_needed, int for_preserve_list);
+void reiserfs_free_block (struct reiserfs_transaction_handle *th, struct super_block * s, unsigned long block);
+void reiserfs_free_internal_block (struct super_block * s, unsigned long block);
+struct buffer_head * reiserfsck_get_new_buffer (unsigned long start);
+void force_freeing (void);
+int is_block_used (unsigned long block);
+int was_block_used (unsigned long block);
+void mark_block_used (unsigned long block);
+void mark_block_uninsertable (unsigned long block);
+int is_block_uninsertable (unsigned long block);
+void mark_block_unformatted (unsigned long block);
+void mark_block_formatted (unsigned long block);
+void unmark_block_unformatted (unsigned long block);
+void unmark_block_formatted (unsigned long block);
+
+/* objectid.c */
+int is_objectid_used (unsigned long objectid);
+void mark_objectid_as_used (unsigned long objectid);
+void mark_objectid_as_free (unsigned long objectid);
+objectid_t get_unused_objectid (struct super_block * s);
+
+
+
+/* segments.c */
+struct overwritten_unfm_segment {
+  int ous_begin;
+  int ous_end;
+  struct overwritten_unfm_segment * ous_next;  
+};
+struct overwritten_unfm * look_for_overwritten_unfm (__u32);
+struct overwritten_unfm_segment * find_overwritten_unfm (unsigned long unfm, int length, struct overwritten_unfm_segment * segment_to_init);
+int get_unoverwritten_segment (struct overwritten_unfm_segment * list_head, struct overwritten_unfm_segment * unoverwritten_segment);
+void save_unfm_overwriting (unsigned long unfm, struct item_head * direct_ih);
+void free_overwritten_unfms (void);
+void mark_formatted_pointed_by_indirect (__u32);
+int is_formatted_pointed_by_indirect (__u32);
+
+
+/* do_balan.c */
+/* lbalance.c */
+/* ibalance.c */	/* links to fs/reiser */
+/* fix_node.c */
+/* teahash3.c */
+
+
+/* info.c */
+struct fsck_stat {
+  /* pass 1,2 */
+  int fs_good_leaves;
+  int fs_uninsertable_leaves;
+  int fs_rewritten_files;
+  int fs_leaves_used_by_indirect_items;
+  int fs_unfm_overwriting_unfm;
+  int fs_indirect_to_direct;
+  /* pass 3 */
+  int fs_incorrect_regular_files;
+  int fs_fixed_size_directories;
+  int fs_fixed_size_files;
+  int fs_deleted_entries;
+  /* pass 4 */
+  int fs_unaccessed_items;
+  int fs_fixed_right_delim_key;
+  /* fs stat */
+  int fs_stat_data_items;
+  int fs_regular_files;
+  int fs_directories;
+  int fs_symlinks;
+  int fs_others;
+};
+  
+
+extern struct fsck_stat g_fsck_info;
+
+/* pass 1,2 */
+#define GOOD_LEAVES 0
+#define UNINSERTABLE_LEAVES 1
+#define REWRITTEN_FILES 2
+#define LEAVES_USED_BY_INDIRECT_ITEMS 3
+#define UNFM_OVERWRITING_UNFM 4		/* overwrite contents of unformatted node keeping what has been written there from direct items */
+
+/* pass 3 (semantic) */
+#define INCORRECT_REGULAR_FILES 5
+#define FIXED_SIZE_DIRECTORIES 6
+#define FIXED_SIZE_FILES 7
+#define DELETED_ENTRIES 8
+#define INDIRECT_TO_DIRECT 9
+
+/* pass 4 */
+#define UNACCESSED_ITEMS 10
+#define FIXED_RIGHT_DELIM_KEY 11
+
+/* fs stat */
+#define STAT_DATA_ITEMS 12
+#define REGULAR_FILES 13
+#define SYMLINKS 14
+#define OTHERS 15
+#define DIRECTORIES 16
+
+void add_event (int event);
+int get_event (int event);
+void output_information ();
+
+
+/* journal.c */
+void replay_all (struct super_block * s);
+/*int get_journal_size (struct super_block * s);
+int get_journal_start (struct super_block * s);*/
+void release_journal_blocks (struct super_block * s);
+void reset_journal (struct super_block * s);
+
diff -urN linux/fs/reiserfs/utils/include/io.h /tmp/linux/fs/reiserfs/utils/include/io.h
--- linux/fs/reiserfs/utils/include/io.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/include/io.h	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,66 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+ 
+struct buffer_head {
+  unsigned long b_blocknr;
+  unsigned short b_dev;
+  unsigned long b_size;
+  char * b_data;
+  unsigned long b_state;
+  unsigned int b_count;
+  unsigned int b_list ;
+  void (*b_end_io)(struct buffer_head *bh, int uptodate);
+
+  struct buffer_head * b_next;
+  struct buffer_head * b_prev;
+  struct buffer_head * b_hash_next;
+  struct buffer_head * b_hash_prev;
+};
+
+#define BH_Uptodate	0
+#define BH_Dirty	1
+#define BH_Lock		2
+#define BUF_DIRTY	1
+
+
+#define buffer_uptodate(bh) test_bit(BH_Uptodate, &(bh)->b_state)
+#define buffer_dirty(bh) test_bit(BH_Dirty, &(bh)->b_state)
+#define buffer_locked(bh) test_bit(BH_Lock, &(bh)->b_state)
+#define buffer_clean(bh) !test_bit(BH_Dirty, &(bh)->b_state)
+#define mark_buffer_dirty(bh,i) set_bit(BH_Dirty, &(bh)->b_state)
+#define mark_buffer_uptodate(bh,i) set_bit(BH_Uptodate, &(bh)->b_state)
+#define mark_buffer_clean(bh) clear_bit(BH_Dirty, &(bh)->b_state)
+
+
+//
+// ./include/linux/kdev_t.h>
+//
+typedef unsigned long long kdev_t;
+
+
+//void __wait_on_buffer (struct buffer_head * bh);
+struct buffer_head * getblk (int dev, int block, int size);
+struct buffer_head * find_buffer (int dev, int block, int size);
+struct buffer_head * get_hash_table(kdev_t dev, int block, int size);
+struct buffer_head * bread (int dev, unsigned long block, size_t size);
+int bwrite (struct buffer_head * bh);
+void brelse (struct buffer_head * bh);
+void bforget (struct buffer_head * bh);
+//void init_buffer_cache (void);
+//void refile_buffer (struct buffer_head * bh);
+//void file_buffer (struct buffer_head * bh, int list);
+//int fsync_dev (int dev);
+//void ll_rw_block (int rw, int nr, struct buffer_head * bh[]);
+void flush_buffers (void);
+void check_and_free_buffer_mem (void);
+
+#ifdef __alpha__
+
+#define reiserfs_llseek lseek
+
+#else
+
+loff_t reiserfs_llseek (unsigned int fd, loff_t offset, unsigned int origin);
+
+#endif /* __alpha__ */
diff -urN linux/fs/reiserfs/utils/include/misc.h /tmp/linux/fs/reiserfs/utils/include/misc.h
--- linux/fs/reiserfs/utils/include/misc.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/include/misc.h	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,37 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+/* nothing abount reiserfs here */
+
+void die (char * fmt, ...);
+void * getmem (int size);
+void freemem (void * p);
+void * expandmem (void * p, int size, int by);
+int is_mounted (char * device_name);
+void check_and_free_mem (void);
+char * kdevname (int dev);
+
+
+#ifdef __alpha__
+
+int set_bit (int nr, void * addr);
+int clear_bit (int nr, void * addr);
+int test_bit(int nr, const void * addr);
+int find_first_zero_bit (const void *vaddr, unsigned size);
+int find_next_zero_bit (const void *vaddr, unsigned size, unsigned offset);
+
+#else
+
+#include <asm/bitops.h>
+
+#endif
+
+void print_how_far (__u32 * passed, __u32 total);
+
+
+
+/*
+int test_and_set_bit (int nr, void * addr);
+int test_and_clear_bit (int nr, void * addr);
+*/
diff -urN linux/fs/reiserfs/utils/include/nokernel.h /tmp/linux/fs/reiserfs/utils/include/nokernel.h
--- linux/fs/reiserfs/utils/include/nokernel.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/include/nokernel.h	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,168 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+/*
+ * this is to be included by all kernel files if __KERNEL__ undefined
+ */
+#include <stdio.h>
+#include <string.h>
+#include <errno.h>
+#include <asm/types.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <malloc.h>
+#include <sys/vfs.h>
+#include <time.h>
+
+#ifndef __alpha__
+#include <asm/bitops.h>
+#endif
+
+/*#define kdev_t dev_t*/
+
+//#include "inode.h"
+#include "io.h"
+#include "sb.h"
+#include "misc.h"
+#include "reiserfs_fs.h"
+#include "reiserfs_fs_sb.h"
+
+#define printk printf
+#define le16_to_cpu(x) ((__u16)x)
+#define cpu_to_le16(x) ((__u16)x)
+#define le32_to_cpu(x) ((__u32)x)
+#define cpu_to_le32(x) ((__u32)x)
+#define kmalloc(a,b) getmem(a)
+#define kfree freemem
+#define GFP_ATOMIC 0
+#define GFP_BUFFER 0
+#define CURRENT_TIME (time(NULL))
+#define journal_mark_dirty(th,s,bh) mark_buffer_dirty (bh, 1)
+#define mark_buffer_journal_new(bh) mark_buffer_dirty (bh, 1)
+#define wait_buffer_until_released(bh) do {} while (0)
+#define schedule() do {} while (0)
+void unmark_block_formatted (unsigned long block);
+void mark_block_formatted (unsigned long block);
+
+
+#if 0 /////////////////////////////////
+
+#define make_bad_inode(i) {;}
+#define copy_from_user memcpy
+#define copy_to_user memcpy
+#define put_user(b,a) ((*(a))=b)
+#define GFP_KERNEL 0
+#define GFP_ATOMIC 0
+#define GFP_BUFFER 0
+#define BUF_CLEAN 0
+#define kmalloc(a,b) getmem(a)
+#define kfree freemem
+#define update_vm_cache(a,b,c,d)
+#define wait_on_buffer __wait_on_buffer
+#define NODEV 0
+extern struct inode_operations chrdev_inode_operations;
+extern struct inode_operations blkdev_inode_operations;
+
+
+#define page_address(page) 0
+void init_fifo(struct inode * inode);
+#define buffer_req(bh) 0
+//#define kdevname(a) "kdevname"
+#define kdev_t_to_nr(a) a
+#define is_read_only(dev) 0
+#define to_kdev_t(a) a
+#define clear_inode(a)
+#define PAGE_SIZE 4096
+#define d_add(a,b) a->d_inode = b
+#define d_instantiate(a,b) a->d_inode = b
+#define d_delete(a)
+#define d_move(a,b)
+#define down(a)
+#define up(a)
+#define MOD_DEC_USE_COUNT
+#define MOD_INC_USE_COUNT
+#define lock_super(a)
+#define unlock_super(a)
+#define set_blocksize(a,b)
+#define d_alloc_root(a,b) 0
+/* #define file_fsync 0 */
+#define set_writetime(a,b)
+
+#endif ////////////////////////////////
+
+#define reiserfs_bread(a,b,c,d) bread(a,b,c)
+
+#if 0 ////////////////////////////////
+/* #define reiserfs_getblk(a,b,c,d)  getblk(a,b,c) */
+extern struct inode_operations reiserfs_dir_inode_operations;
+extern struct inode_operations reiserfs_symlink_inode_operations;
+struct buffer_head * reiserfs_getblk (kdev_t n_dev, int n_block, int n_size, int * p_n_repeat);
+
+#ifdef REISERFS_FSCK
+#undef REISERFS_CHECK
+#endif
+
+/* fs.h */
+#define BLOCK_SIZE 1024
+#define READ 0
+#define WRITE 1
+#define MS_RDONLY	 1
+#define UPDATE_ATIME(inode)
+#define MAJOR(dev) ((dev)>>8)
+#define MINOR(dev) ((dev) & 0xff)
+
+
+void set_super(struct super_block *s) ;
+int file_fsync(struct file *filp, struct dentry *dentry) ;
+int reiserfs_file_release(struct inode *p_s_inode, struct file *p_s_filp) ;
+int preserve_trace_print_srs(struct super_block *s) ;
+
+
+//
+// fs/reiserfs/buffer.c
+//
+#define reiserfs_file_buffer(bh,state) do {} while (0)
+#define reiserfs_journal_end_io 0
+
+//
+// fs/reiserfs/journal.c
+//
+#define journal_mark_dirty(th,s,bh) mark_buffer_dirty (bh, 1)
+#define journal_mark_dirty_nolog(th,s,bh) mark_buffer_dirty (bh, 1)
+#define mark_buffer_journal_new(bh) mark_buffer_dirty (bh, 1)
+
+#define reiserfs_update_inode_transaction(i) do {} while (0)
+#define reiserfs_inode_in_this_transaction(i) 1
+#define reiserfs_commit_for_inode(i) do {} while(0)
+
+extern inline int flush_old_commits (struct super_block * s, int i)
+{
+  return 0;
+}
+
+#define journal_begin(th,s,n) do {int fu = n;fu++;} while (0)
+#define journal_release(th,s) do {} while (0)
+#define journal_release_error(th,s) do {} while (0)
+#define journal_init(s) 0
+#define journal_end(th,s,n) do {s=0;} while (0)
+#define buffer_journaled(bh) 0
+#define journal_lock_dobalance(s) do {} while (0)
+#define journal_unlock_dobalance(s) do {} while (0)
+#define journal_transaction_should_end(th,n) 1
+#define push_journal_writer(s) 1
+#define pop_journal_writer(n) do {} while (0)
+#define journal_end_sync(th,s,n) do {} while (0)
+#define journal_mark_freed(th,s,n) do {} while (0)
+#define reiserfs_in_journal(a,b,c,d,e,f) 0
+#define flush_async_commits(s,n) do {} while (0)
+
+//
+// fs/reiserfs/resize.c
+//
+#define reiserfs_resize(s,n) do {} while (0)
+#define simple_strtoul strtol
+
+
+#endif ///////////////
diff -urN linux/fs/reiserfs/utils/include/reiserfs.h /tmp/linux/fs/reiserfs/utils/include/reiserfs.h
--- linux/fs/reiserfs/utils/include/reiserfs.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/include/reiserfs.h	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,26 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+
+//
+// ./fs/reiserfs/utils/lib/reiserfs.c
+//
+int not_formatted_node (char * buf, int blocksize);
+int not_data_block (struct super_block * s, b_blocknr_t block);
+int uread_super_block (struct super_block * s);
+int uread_bitmaps (struct super_block * s);
+
+
+#define bh_desc(bh) ((struct reiserfs_journal_desc *)((bh)->b_data))
+#define bh_commit(bh) ((struct reiserfs_journal_commit *)((bh)->b_data))
+int get_journal_start (struct super_block * s);
+int get_journal_size (struct super_block * s);
+int is_desc_block (struct buffer_head * bh);
+int does_desc_match_commit (struct reiserfs_journal_desc * desc, 
+			    struct reiserfs_journal_commit * commit);
+
+void make_dir_stat_data (struct key * dir_key, struct item_head * ih,
+			 struct stat_data * sd);
+void make_empty_dir_item (char * body, objectid_t dirid, objectid_t objid,
+			  objectid_t par_dirid, objectid_t par_objid);
diff -urN linux/fs/reiserfs/utils/include/resize.h /tmp/linux/fs/reiserfs/utils/include/resize.h
--- linux/fs/reiserfs/utils/include/resize.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/include/resize.h	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,49 @@
+/* 
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+#define print_usage_and_exit()\
+ die ("Usage: %s  -s[+|-]#[M|K] [-fqv] device", argv[0])
+ 
+
+/* reiserfs_resize.c */
+extern struct buffer_head * g_sb_bh;
+
+extern int opt_force;
+extern int opt_verbose;
+extern int opt_nowrite;
+extern int opt_safe;
+
+int expand_fs(void);
+
+/* fe.c */
+int resize_fs_online(char * devname, unsigned long blocks);
+
+/* do_shrink.c */
+int shrink_fs(unsigned long blocks);
+
+/* bitmap.c */
+struct bitmap_head {
+	int bm_nr;
+	int bm_blocksize;
+	unsigned long bm_block_count; 
+	char ** bm_bmap;
+	struct buffer_head ** bm_bh_table;
+};
+
+struct bitmap_head * create_bitmap_from_sb (struct buffer_head * sb_bh);
+struct bitmap_head * create_bitmap (unsigned long size, int blocksize);
+void free_bitmap (struct bitmap_head * bmp);
+int sync_bitmap (struct bitmap_head * bmp);
+void truncate_bitmap (struct bitmap_head * bmp, unsigned long block);
+int is_block_used (struct bitmap_head * bmp, unsigned long block);
+
+#define is_block_free(bmp,block) (!is_block_used(bmp,block))			
+
+void mark_block_free (struct bitmap_head * bmp, unsigned long block);
+void mark_block_used (struct bitmap_head * bmp, unsigned long block);
+unsigned long find_1st_unused_block_right (struct bitmap_head * bmp,
+                                                  unsigned long start);
+unsigned long find_1st_unused_block_left (struct bitmap_head * bmp,
+                                                  unsigned long start);
+			
diff -urN linux/fs/reiserfs/utils/include/sb.h /tmp/linux/fs/reiserfs/utils/include/sb.h
--- linux/fs/reiserfs/utils/include/sb.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/include/sb.h	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,25 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+ 
+// list.h
+struct list_head {
+	struct list_head *next, *prev;
+};
+
+#include "reiserfs_fs_sb.h"
+
+struct super_block {
+  kdev_t s_dev;
+  unsigned long s_blocksize;
+  //int s_blocksize_bits;
+  int s_dirt;
+  //int s_flags;
+  //  struct dentry * s_root;
+  //struct super_operations * s_op;
+  union {
+    struct reiserfs_sb_info reiserfs_sb;
+  } u;
+};
+
+
diff -urN linux/fs/reiserfs/utils/lib/Makefile /tmp/linux/fs/reiserfs/utils/lib/Makefile
--- linux/fs/reiserfs/utils/lib/Makefile	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/lib/Makefile	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,25 @@
+OBJS = misc.o io.o  inode.o  version.o reiserfs.o
+#hlam.o
+
+MISC = $(TMPBINDIR)/libmisc.a
+
+all: $(MISC)
+
+.c.o:
+	$(CC) $(CFLAGS) $<
+
+$(MISC): $(OBJS)
+	ar -r $(MISC)  $(OBJS)
+
+clean:
+	rm -f *.o $(MISC) *~ TAGS .depend ../include/*~ ../include/TAGS
+
+dep:
+	gcc -MM $(IDIRS) *.c > .depend
+
+ifeq (.depend,$(wildcard .depend))
+include .depend
+endif
+
+
+
diff -urN linux/fs/reiserfs/utils/lib/inode.c /tmp/linux/fs/reiserfs/utils/lib/inode.c
--- linux/fs/reiserfs/utils/lib/inode.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/lib/inode.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,233 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+#include "nokernel.h"
+#include <limits.h>
+
+struct fs_struct fss = {0};
+struct task_struct cur_task = {0, 0, 0, 0, 0, 0, &fss, {{INT_MAX,INT_MAX},}};
+
+int generic_readpage(struct file * file, struct page * page)
+{
+  return 0;
+}
+
+int generic_file_mmap (struct file * file, struct vm_area_struct * vma)
+{
+  return 0;
+}
+
+int reiserfs_sync_file (struct file * p_s_filp, struct dentry * p_s_dentry)
+{
+  return 0;
+}
+
+void wait_buffer_until_released (struct buffer_head * bh)
+{
+}
+
+int schedule (void)
+{
+  return 0;
+}
+
+int fsuser (void)
+{
+  return 0;
+}
+
+int is_subdir (struct dentry * old, struct dentry * new)
+{
+  return 0;
+}
+
+void sleep_on (struct wait_queue ** w)
+{
+}
+
+void wake_up (struct wait_queue ** w)
+{
+}
+
+
+struct inode_operations chrdev_inode_operations = {0,};
+struct inode_operations blkdev_inode_operations = {0,};
+/*struct inode_operations reiserfs_dir_inode_operations = {0,};*/
+struct inode_operations reiserfs_symlink_inode_operations = {0,};
+struct super_operations reiserfs_sops = 
+{
+  reiserfs_read_inode,
+  reiserfs_write_inode,
+  NULL,				/* put_inode*/
+  reiserfs_delete_inode,
+  reiserfs_notify_change,
+  reiserfs_put_super,
+  reiserfs_write_super,
+  reiserfs_statfs,
+  reiserfs_remount,
+  NULL, 				/* clear_inode */
+  NULL				/* umount_begin */
+};
+
+
+
+void init_fifo(struct inode * inode)
+{
+}
+
+
+#define NR_INODES 10
+
+struct inode * first_inode;
+int inodes = 0;
+
+struct inode * find_inode (unsigned long ino)
+{
+  struct inode * inode;
+
+  inode = first_inode;
+  if (inode == 0)
+    return 0;
+
+  while (1) {
+    if (inode->i_ino == ino) {
+      inode->i_count ++;
+      return inode;
+    }
+    inode = inode->i_next;
+    if (inode == first_inode)
+      break;
+  }
+  return 0;
+}
+
+
+struct inode * get_empty_inode (void)
+{
+  struct inode * inode, * prev, * next;
+
+  if (inodes == NR_INODES) {
+    first_inode->i_sb->s_op->write_inode (first_inode);
+
+    /* set all but i_next and i_prev to 0 */
+    next = first_inode->i_next;
+    prev = first_inode->i_prev;
+    memset (first_inode, 0, sizeof (struct inode));
+    first_inode->i_next = next;
+    first_inode->i_prev = prev;
+    
+    /* move to end of list */
+    first_inode = first_inode->i_next;
+    return first_inode->i_prev;
+  }
+  /* allocate new inode */
+  inode = getmem (sizeof (struct inode));
+  if (!inode)
+    return 0;
+
+  /* add to end of list */
+  if (first_inode) {
+    inode->i_prev = first_inode->i_prev;
+    inode->i_next = first_inode;
+    first_inode->i_prev->i_next = inode;
+    first_inode->i_prev = inode;
+  } else {
+    first_inode = inode->i_next = inode->i_prev = inode;
+  }
+  inode->i_count = 1;
+  return inode;
+}
+
+
+void insert_inode_hash (struct inode * inode)
+{
+}
+
+struct inode * get_new_inode (struct super_block *sb, unsigned long ino)
+{
+  struct inode * inode;
+
+  inode = get_empty_inode ();
+  if (inode) {
+    inode->i_sb = sb;
+    inode->i_ino = ino;
+    //inode->i_count = 1;
+    sb->s_op->read_inode (inode);
+    return inode;
+  }
+  return 0;
+}
+
+
+struct inode * iget (struct super_block *sb, unsigned long ino)
+{
+  struct inode * inode;
+
+  inode = find_inode (ino);
+  if (inode)
+    return inode;
+  return get_new_inode (sb, ino);
+}
+
+
+void iput (struct inode * inode)
+{
+    if (inode) {
+	if (inode->i_count == 0)
+	    die ("iput: can not free free inode");
+
+	if (inode->i_op->default_file_ops->release)
+	  inode->i_op->default_file_ops->release (inode, 0);
+	if (inode->i_sb->s_op->put_inode)
+	  inode->i_sb->s_op->put_inode (inode);
+	inode->i_count --;
+
+	if (inode->i_nlink == 0) {
+	    inode->i_sb->s_op->delete_inode (inode);
+	    return;
+	}
+	if (inode->i_state & I_DIRTY) {
+	    inode->i_sb->s_op->write_inode (inode);
+	    inode->i_state &= ~I_DIRTY;
+	}
+    }
+}
+
+
+void sync_inodes (void)
+{
+  struct inode * inode, * tmp;
+
+  inode = first_inode;
+  if (inode == 0)
+    return;
+
+  while (1) {
+    if (inode->i_dirt)
+      inode->i_sb->s_op->write_inode (inode);
+
+    tmp = inode;
+    inode = inode->i_next;
+
+    inode->i_prev = tmp->i_prev;
+    tmp->i_prev->i_next = inode;
+    
+    freemem (tmp);
+    if (inode == tmp)
+      break;
+  }
+  return;
+}
+
+
+//
+// arch/i386/kernel/process.c
+//
+int kernel_thread(int (*fn)(void *), void * arg, unsigned long flags)
+{
+  return 0;
+}
+
+
+
diff -urN linux/fs/reiserfs/utils/lib/io.c /tmp/linux/fs/reiserfs/utils/lib/io.c
--- linux/fs/reiserfs/utils/lib/io.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/lib/io.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,493 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+#include <stdio.h>
+#include <unistd.h>
+#include <string.h>
+#include <errno.h>
+/*#include <mntent.h>*/
+#include <sys/types.h>
+#include <asm/types.h>
+#include <linux/unistd.h>
+
+#include "io.h"
+#include "misc.h"
+
+
+
+/* All buffers are in double linked cycled list. Buffers of tree are
+   hashed by their block number.  If getblk found buffer with wanted
+   block number in hash queue it moves buffer to the end of list */
+
+#define BLOCK_SIZE 1024
+#define MAX_NR_BUFFERS 16384
+static int g_nr_buffers;
+
+#define NR_HASH_QUEUES 20
+static struct buffer_head * g_a_hash_queues [NR_HASH_QUEUES];
+static struct buffer_head * g_buffer_list_head;
+static struct buffer_head * g_buffer_heads;
+
+
+
+//void unlock_buffer(struct buffer_head *bh) {;}
+
+static void show_buffers (int dev, int size)
+{
+    int all = 0;
+    int dirty = 0;
+    int in_use = 0; /* count != 0 */
+    int free = 0;
+    struct buffer_head * next = g_buffer_list_head;
+
+    for (;;) {
+	if (!next)
+	    die ("show_buffers: buffer list is corrupted");
+	if (next->b_dev == dev && next->b_size == size) {
+	    all ++;
+	    if (next->b_count != 0) {
+		in_use ++;
+	    }
+	    if (buffer_dirty (next)) {
+		dirty ++;
+	    }
+	    if (buffer_clean (next) && next->b_count == 0) {
+		free ++;
+	    }
+	}
+	next = next->b_next;
+	if (next == g_buffer_list_head)
+	    break;
+    }
+
+    printf ("show_buffers (dev %d, size %d): free %d, count != 0 %d, dirty %d, all %d\n", dev, size, free, in_use, dirty, all);
+}
+
+
+static void insert_into_hash_queue (struct buffer_head * bh)
+{
+    int index = bh->b_blocknr % NR_HASH_QUEUES;
+
+    if (bh->b_hash_prev || bh->b_hash_next)
+	die ("insert_into_hash_queue: hash queue corrupted");
+
+    if (g_a_hash_queues[index]) {
+	g_a_hash_queues[index]->b_hash_prev = bh;
+	bh->b_hash_next = g_a_hash_queues[index];
+    }
+    g_a_hash_queues[index] = bh;
+}
+
+
+static void remove_from_hash_queue (struct buffer_head * bh)
+{
+    if (bh->b_hash_next == 0 && bh->b_hash_prev == 0 && bh != g_a_hash_queues[bh->b_blocknr % NR_HASH_QUEUES])
+	/* (b_dev == 0) ? */
+	return;
+
+    if (bh == g_a_hash_queues[bh->b_blocknr % NR_HASH_QUEUES]) {
+	if (bh->b_hash_prev != 0)
+	    die ("remove_from_hash_queue: hash queue corrupted");
+	g_a_hash_queues[bh->b_blocknr % NR_HASH_QUEUES] = bh->b_hash_next;
+    }
+    if (bh->b_hash_next)
+	bh->b_hash_next->b_hash_prev = bh->b_hash_prev;
+
+    if (bh->b_hash_prev)
+	bh->b_hash_prev->b_hash_next = bh->b_hash_next;
+
+    bh->b_hash_prev = bh->b_hash_next = 0;
+}
+
+
+static void put_buffer_list_end (struct buffer_head * bh)
+{
+    struct buffer_head * last = 0;
+
+    if (bh->b_prev || bh->b_next)
+	die ("put_buffer_list_end: buffer list corrupted");
+
+    if (g_buffer_list_head == 0) {
+	bh->b_next = bh;
+	bh->b_prev = bh;
+	g_buffer_list_head = bh;
+    } else {
+	last = g_buffer_list_head->b_prev;
+    
+	bh->b_next = last->b_next;
+	bh->b_prev = last;
+	last->b_next->b_prev = bh;
+	last->b_next = bh;
+    }
+}
+
+
+static void remove_from_buffer_list (struct buffer_head * bh)
+{
+    if (bh == bh->b_next) {
+	g_buffer_list_head = 0;
+    } else {
+	bh->b_prev->b_next = bh->b_next;
+	bh->b_next->b_prev = bh->b_prev;
+	if (bh == g_buffer_list_head)
+	    g_buffer_list_head = bh->b_next;
+    }
+
+    bh->b_next = bh->b_prev = 0;
+}
+
+
+static void put_buffer_list_head (struct buffer_head * bh)
+{
+    put_buffer_list_end (bh);
+    g_buffer_list_head = bh;
+}
+
+
+#define GROW_BUFFERS__NEW_BUFERS_PER_CALL 10
+/* creates number of new buffers and insert them into head of buffer list 
+ */
+static int grow_buffers (int size)
+{
+    int i;
+    struct buffer_head * bh, * tmp;
+
+    if (g_nr_buffers + GROW_BUFFERS__NEW_BUFERS_PER_CALL > MAX_NR_BUFFERS)
+	return 0;
+
+    /* get memory for array of buffer heads */
+    bh = (struct buffer_head *)getmem (GROW_BUFFERS__NEW_BUFERS_PER_CALL * sizeof (struct buffer_head) + sizeof (struct buffer_head *));
+    if (g_buffer_heads == 0)
+	g_buffer_heads = bh;
+    else {
+	/* link new array to the end of array list */
+	tmp = g_buffer_heads;
+	while (*(struct buffer_head **)(tmp + GROW_BUFFERS__NEW_BUFERS_PER_CALL) != 0)
+	    tmp = *(struct buffer_head **)(tmp + GROW_BUFFERS__NEW_BUFERS_PER_CALL);
+	*(struct buffer_head **)(tmp + GROW_BUFFERS__NEW_BUFERS_PER_CALL) = bh;
+    }
+
+    for (i = 0; i < GROW_BUFFERS__NEW_BUFERS_PER_CALL; i ++) {
+
+	tmp = bh + i;
+	memset (tmp, 0, sizeof (struct buffer_head));
+	tmp->b_data = getmem (size);
+	if (tmp->b_data == 0)
+	    die ("grow_buffers: no memory for new buffer data");
+	tmp->b_dev = 0;
+	tmp->b_size = size;
+	put_buffer_list_head (tmp);
+
+	g_nr_buffers ++;
+    }
+    return GROW_BUFFERS__NEW_BUFERS_PER_CALL;
+}
+
+
+struct buffer_head * find_buffer (int dev, int block, int size)
+{		
+    struct buffer_head * next;
+
+    next = g_a_hash_queues[block % NR_HASH_QUEUES];
+    for (;;) {
+	struct buffer_head *tmp = next;
+	if (!next)
+	    break;
+	next = tmp->b_hash_next;
+	if (tmp->b_blocknr != block || tmp->b_size != size || tmp->b_dev != dev)
+	    continue;
+	next = tmp;
+	break;
+    }
+    return next;
+}
+
+void __wait_on_buffer (struct buffer_head * bh)
+{
+}
+
+struct buffer_head * get_hash_table(kdev_t dev, int block, int size)
+{
+    struct buffer_head * bh;
+
+    bh = find_buffer (dev, block, size);
+    if (bh) {
+	bh->b_count ++;
+    }
+    return bh;
+}
+
+
+static struct buffer_head * get_free_buffer (int size)
+{
+    struct buffer_head * next = g_buffer_list_head;
+
+    if (!next)
+	return 0;
+    for (;;) {
+	if (!next)
+	    die ("get_free_buffer: buffer list is corrupted");
+	if (next->b_count == 0 && buffer_clean (next) && next->b_size == size) {
+	    remove_from_hash_queue (next);
+	    remove_from_buffer_list (next);
+	    put_buffer_list_end (next);
+	    return next;
+	}
+	next = next->b_next;
+	if (next == g_buffer_list_head)
+	    break;
+    }
+    return 0;
+}
+
+
+static void sync_buffers (int size, int to_write)
+{
+    struct buffer_head * next = g_buffer_list_head;
+    int written = 0;
+
+    for (;;) {
+	if (!next)
+	    die ("flush_buffer: buffer list is corrupted");
+    
+	if (!size && !to_write) {
+	    // writing all buffers out
+	    bwrite (next);
+	} else {
+	    // writing free dirty buffers to make then reusable
+	    if (next->b_size == size && next->b_count == 0 && 
+		buffer_dirty (next) && buffer_uptodate (next)) {
+		written ++;
+		bwrite (next);
+		if (written == to_write)
+		    return;
+	    }
+	}
+
+	next = next->b_next;
+	if (next == g_buffer_list_head)
+	    break;
+    }
+}
+
+void flush_buffers (void)
+{
+    sync_buffers (0, 0);
+}
+
+
+struct buffer_head * getblk (int dev, int block, int size)
+{
+    struct buffer_head * bh;
+
+    bh = find_buffer (dev, block, size);
+    if (bh) {
+	if (0 && !buffer_uptodate (bh))
+	    die ("getblk: buffer must be uptodate");
+	bh->b_count ++;
+	return bh;
+    }
+
+    bh = get_free_buffer (size);
+    if (bh == 0) {
+	if (grow_buffers (size) == 0) {
+	    sync_buffers (size, 10);
+	}
+	bh = get_free_buffer (size);
+	if (bh == 0) {
+	    show_buffers (dev, size);
+	    die ("getblk: no free buffers after grow_buffers and refill (%d)", g_nr_buffers);
+	}
+    }
+
+    bh->b_count = 1;
+    bh->b_dev = dev;
+    bh->b_size = size;
+    bh->b_blocknr = block;
+    bh->b_end_io = NULL ;
+    memset (bh->b_data, 0, size);
+    clear_bit(BH_Dirty, &bh->b_state);
+    clear_bit(BH_Uptodate, &bh->b_state);
+
+    insert_into_hash_queue (bh);
+
+    return bh;
+}
+
+
+void brelse (struct buffer_head * bh)
+{
+    if (bh == 0)
+	return;
+    if (bh->b_count == 0) {
+	die ("brelse: can not free a free buffer %lu", bh->b_blocknr);
+    }
+    bh->b_count --;
+}
+
+
+void bforget (struct buffer_head * bh)
+{
+    if (bh) {
+	brelse (bh);
+	remove_from_hash_queue (bh);
+	remove_from_buffer_list (bh);
+	put_buffer_list_head (bh);
+    }
+}
+
+
+#ifndef __alpha__
+
+_syscall5 (int,  _llseek,  uint,  fd, ulong, hi, ulong, lo,
+	   loff_t *, res, uint, wh);
+
+loff_t reiserfs_llseek (unsigned int fd, loff_t offset, unsigned int origin)
+{
+    loff_t retval, result;
+  
+    retval = _llseek (fd, ((unsigned long long) offset) >> 32,
+		      ((unsigned long long) offset) & 0xffffffff,
+		      &result, origin);
+    return (retval != 0 ? (loff_t)-1 : result);
+  
+}
+
+#endif	/* ! __alpha__ */
+
+struct buffer_head * bread (int dev, unsigned long block, size_t size)
+{
+    struct buffer_head * bh;
+    loff_t offset;
+    ssize_t bytes;
+
+    bh = getblk (dev, block, size);
+    if (buffer_uptodate (bh))
+	return bh;
+
+    offset = (loff_t)size * (loff_t)block;
+    if (reiserfs_llseek (dev, offset, SEEK_SET) == (loff_t)-1)
+	die ("bread: _llseek to position %ld (block=%d, dev=%d): %s\n", offset, block, dev, strerror (errno));
+
+    bytes = read (bh->b_dev, bh->b_data, size);
+    if (bytes != (ssize_t)size) {
+	die ("bread: read %d bytes returned %d (block=%d, dev=%d)\n", size, bytes, block, dev);
+    }
+
+    mark_buffer_uptodate (bh, 0);
+    return bh;
+}
+
+
+//int aux_dev = 0;
+
+
+int bwrite (struct buffer_head * bh)
+{
+    loff_t offset;
+    ssize_t bytes;
+    size_t size;
+
+    if (!buffer_dirty (bh) || !buffer_uptodate (bh))
+	return 0;
+
+    size = bh->b_size;
+    offset = (loff_t)size * (loff_t)bh->b_blocknr;
+/*  off_hi = ((unsigned long long)offset) >> 32;
+    off_lo = offset & 0xffffffff;*/
+
+    if (reiserfs_llseek (bh->b_dev, offset, SEEK_SET) == (loff_t)-1)
+	die ("bwrite: lseek to position %ld (block=%d, dev=%d): %s\n", offset, bh->b_blocknr, bh->b_dev, strerror (errno));
+
+    bytes = write (bh->b_dev, bh->b_data, size);
+    if (bytes != (ssize_t)size) {
+	die ("bwrite: write %ld bytes returned %d (block=%ld, dev=%d): %s\n", size, bytes, bh->b_blocknr, bh->b_dev, strerror (errno));
+    }
+  
+    mark_buffer_clean (bh);
+    if (bh->b_end_io) {
+	bh->b_end_io(bh, 1) ;
+    }
+    return 0;
+}
+
+#if 0
+void ll_rw_block (int rw, int nr, struct buffer_head * bh[])
+{
+    int i;
+    long offset, res_lseek;
+    int res_read;
+
+    if (rw) {
+	for(i = 0 ; i < nr ; i++) {
+	    bwrite(bh[i]) ;
+	}
+	return;
+    }
+    for(i = 0; i < nr; i ++) {
+	offset = bh[i]->b_size * bh[i]->b_blocknr;
+				/* This might be a problem that it is
+                                   not lseek64 -Hans */
+	res_lseek = lseek (bh[i]->b_dev, offset, SEEK_SET);
+	if (res_lseek != offset) {      
+	    die ("bread: lseek to position %ld returned %ld (block=%d, dev=%d)\n", offset, res_lseek, bh[i]->b_blocknr, bh[i]->b_dev);
+	}
+
+	res_read = read (bh[i]->b_dev, bh[i]->b_data, bh[i]->b_size);
+	if (res_read - bh[i]->b_size) {
+	    die ("bread: read %d bytes returned %d (block=%d, dev=%d)\n", bh[i]->b_size, res_read, bh[i]->b_blocknr, bh[i]->b_dev);
+	}
+    
+	mark_buffer_uptodate (bh[i], 0);
+    }
+}
+#endif
+
+
+void check_and_free_buffer_mem (void)
+{
+    int i = 0;
+    struct buffer_head * next = g_buffer_list_head;
+
+    //sync_buffers (0, 0);
+    for (;;) {
+	if (!next)
+	    die ("check_and_free_buffer_mem: buffer list is corrupted");
+	if (next->b_count != 0)
+	    die ("check_and_free_buffer_mem: not free buffer (%d, %d, %d)",
+		 next->b_blocknr, next->b_size, next->b_count);
+
+	if (buffer_dirty (next) && buffer_uptodate (next))
+	    die ("check_and_free_buffer_mem: dirty buffer found");
+
+	freemem (next->b_data);
+	i ++;
+	next = next->b_next;
+	if (next == g_buffer_list_head)
+	    break;
+    }
+    if (i != g_nr_buffers)
+	die ("check_and_free_buffer_mem: found %d buffers, must be %d", i, g_nr_buffers);
+
+    /* free buffer heads */
+    while ((next = g_buffer_heads)) {
+	g_buffer_heads = *(struct buffer_head **)(next + GROW_BUFFERS__NEW_BUFERS_PER_CALL);
+	freemem (next);
+    }
+  
+    return;
+}
+
+
+
+
+
+
+
+
+
+
+
+
+
diff -urN linux/fs/reiserfs/utils/lib/misc.c /tmp/linux/fs/reiserfs/utils/lib/misc.c
--- linux/fs/reiserfs/utils/lib/misc.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/lib/misc.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,323 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+#include <stdio.h>
+#include <stdarg.h>
+#include <stdlib.h>
+#include <string.h>
+#include <mntent.h>
+#include <asm/types.h>
+#include <sys/vfs.h>
+
+
+//#include "inode.h"
+//#include "io.h"
+//#include "sb.h"
+//#include "misc.h"
+
+#ifdef __alpha__
+/*
+ * These have been stolen somewhere from linux. Anyone is welcome to write it better.
+ */
+int set_bit (int nr, void * addr)
+{
+    __u8 * p, mask;
+    int retval;
+
+    p = (__u8 *)addr;
+    p += nr >> 3;
+    mask = 1 << (nr & 0x7);
+    /*cli();*/
+    retval = (mask & *p) != 0;
+    *p |= mask;
+    /*sti();*/
+    return retval;
+}
+
+
+int clear_bit (int nr, void * addr)
+{
+    __u8 * p, mask;
+    int retval;
+
+    p = (__u8 *)addr;
+    p += nr >> 3;
+    mask = 1 << (nr & 0x7);
+    /*cli();*/
+    retval = (mask & *p) != 0;
+    *p &= ~mask;
+    /*sti();*/
+    return retval;
+}
+
+int test_bit(int nr, const void * addr)
+{
+    __u8 * p, mask;
+  
+    p = (__u8 *)addr;
+    p += nr >> 3;
+    mask = 1 << (nr & 0x7);
+    return ((mask & *p) != 0);
+}
+
+int find_first_zero_bit (const void *vaddr, unsigned size)
+{
+    const __u8 *p = vaddr, *addr = vaddr;
+    int res;
+
+    if (!size)
+	return 0;
+
+    size = (size >> 3) + ((size & 0x7) > 0);
+    while (*p++ == 255) {
+	if (--size == 0)
+	    return (p - addr) << 3;
+    }
+  
+    --p;
+    for (res = 0; res < 8; res++)
+	if (!test_bit (res, p))
+	    break;
+    return (p - addr) * 8 + res;
+}
+
+
+int find_next_zero_bit (const void *vaddr, unsigned size, unsigned offset)
+{
+    const __u8 *addr = vaddr;
+    const __u8 *p = addr + (offset >> 3);
+    int bit = offset & 7, res;
+  
+    if (offset >= size)
+	return size;
+  
+    if (bit) {
+	/* Look for zero in first char */
+	for (res = bit; res < 8; res++)
+	    if (!test_bit (res, p))
+		return (p - addr) * 8 + res;
+	p++;
+    }
+    /* No zero yet, search remaining full bytes for a zero */
+    res = find_first_zero_bit (p, size - 8 * (p - addr));
+    return (p - addr) * 8 + res;
+}
+#endif /* __alpha__ */
+
+
+
+/*int test_and_set_bit (int nr, void * addr)
+{
+  int oldbit = test_bit (nr, addr);
+  set_bit (nr, addr);
+  return oldbit;
+}
+
+
+int test_and_clear_bit (int nr, void * addr)
+{
+  int oldbit = test_bit (nr, addr);
+  clear_bit (nr, addr);
+  return oldbit;
+}*/
+
+
+void die (char * fmt, ...)
+{
+    static char buf[1024];
+    va_list args;
+
+    va_start (args, fmt);
+    vsprintf (buf, fmt, args);
+    va_end (args);
+
+    fprintf (stderr, "\n%s\n\n\n", buf);
+    exit (-1);
+}
+
+
+
+#define MEM_BEGIN "membegi"
+#define MEM_END "mem_end"
+#define MEM_FREED "__free_"
+#define CONTROL_SIZE (strlen (MEM_BEGIN) + 1 + sizeof (int) + strlen (MEM_END) + 1)
+
+
+static int get_mem_size (char * p)
+{
+    char * begin;
+
+    begin = p - strlen (MEM_BEGIN) - 1 - sizeof (int);
+    return *(int *)(begin + strlen (MEM_BEGIN) + 1);
+}
+
+
+static void checkmem (char * p, int size)
+{
+    char * begin;
+    char * end;
+  
+    begin = p - strlen (MEM_BEGIN) - 1 - sizeof (int);
+    if (strcmp (begin, MEM_BEGIN))
+	die ("checkmem: memory corrupted - invalid head sign");
+
+    if (*(int *)(begin + strlen (MEM_BEGIN) + 1) != size)
+	die ("checkmem: memory corrupted - invalid size");
+
+    end = begin + size + CONTROL_SIZE - strlen (MEM_END) - 1;
+    if (strcmp (end, MEM_END))
+	die ("checkmem: memory corrupted - invalid end sign");
+}
+
+
+void * getmem (int size)
+{
+    char * p;
+    char * mem;
+
+    p = (char *)malloc (CONTROL_SIZE + size);
+    if (!p)
+	die ("getmem: no more memory (%d)", size);
+
+    strcpy (p, MEM_BEGIN);
+    p += strlen (MEM_BEGIN) + 1;
+    *(int *)p = size;
+    p += sizeof (int);
+    mem = p;
+    memset (mem, 0, size);
+    p += size;
+    strcpy (p, MEM_END);
+
+    checkmem (mem, size);
+
+    return mem;
+}
+
+
+void * expandmem (void * vp, int size, int by)
+{
+    int allocated;
+    char * mem, * p = vp;
+    int expand_by = by;
+
+    if (p) {
+	checkmem (p, size);
+	allocated = CONTROL_SIZE + size;
+	p -= (strlen (MEM_BEGIN) + 1 + sizeof (int));
+    } else {
+	allocated = 0;
+	/* add control bytes to the new allocated area */
+	expand_by += CONTROL_SIZE;
+    }
+    p = realloc (p, allocated + expand_by);
+    if (!p)
+	die ("expandmem: no more memory (%d)", size);
+    if (!vp) {
+	strcpy (p, MEM_BEGIN);
+    }
+    mem = p + strlen (MEM_BEGIN) + 1 + sizeof (int);
+
+    *(int *)(p + strlen (MEM_BEGIN) + 1) = size + by;
+    /* fill new allocated area by 0s */
+    memset (mem + size, 0, by);
+    strcpy (mem + size + by, MEM_END);
+
+    checkmem (mem, size + by);
+
+    return mem;
+}
+
+
+void freemem (void * vp)
+{
+    char * p = vp;
+    int size;
+  
+    if (!p)
+	return;
+    size = get_mem_size (vp);
+    checkmem (p, size);
+
+    p -= (strlen (MEM_BEGIN) + 1 + sizeof (int));
+    strcpy (p, MEM_FREED);
+    strcpy (p + size + CONTROL_SIZE - strlen (MEM_END) - 1, MEM_FREED);
+    free (p);
+}
+
+
+int is_mounted (char * device_name)
+{
+    FILE *f;
+    struct mntent *mnt;
+
+    if ((f = setmntent (MOUNTED, "r")) == NULL)
+	return 0;
+
+    while ((mnt = getmntent (f)) != NULL)
+	if (strcmp (device_name, mnt->mnt_fsname) == 0)
+	    return 1;
+    endmntent (f);
+
+    return 0;
+}
+
+
+char buf[20];
+
+#include <linux/kdev_t.h>
+#include <sys/stat.h>
+
+char * kdevname (int dev)
+{
+    struct stat st;
+
+    if (fstat (dev, &st) != 0)
+	die ("stat failed");
+    sprintf (buf, "0x%x:0x%x", MAJOR((int)st.st_rdev), MINOR((int)st.st_rdev));
+    return buf;
+}
+
+
+static char * strs[] =
+{"0%",".",".",".",".","20%",".",".",".",".","40%",".",".",".",".","60%",".",".",".",".","80%",".",".",".",".","100%"};
+
+static char progress_to_be[1024];
+static char current_progress[1024];
+
+static void str_to_be (char * buf, int prosents)
+{
+    int i;
+    prosents -= prosents % 4;
+    buf[0] = 0;
+    for (i = 0; i <= prosents / 4; i ++)
+	strcat (buf, strs[i]);
+}
+
+
+void print_how_far (__u32 * passed, __u32 total)
+{
+    int n;
+
+    if (*passed == 0)
+	current_progress[0] = 0;
+
+    if (*passed >= total) {
+	fprintf/*die*/ (stderr, "\nprint_how_far: total %u has been reached already. cur=%u\n", total, ++(*passed));
+	return;
+    }
+
+    (*passed) ++;
+    n = ((double)((double)(*passed) / (double)total) * (double)100);
+
+    str_to_be (progress_to_be, n);
+
+    if (strlen (current_progress) != strlen (progress_to_be)) {
+	fprintf (stderr, "%s", progress_to_be + strlen (current_progress));
+    }
+
+    strcat (current_progress, progress_to_be + strlen (current_progress));
+
+
+    fflush (stdout);
+}
+
diff -urN linux/fs/reiserfs/utils/lib/reiserfs.c /tmp/linux/fs/reiserfs/utils/lib/reiserfs.c
--- linux/fs/reiserfs/utils/lib/reiserfs.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/lib/reiserfs.c	Fri Feb  2 19:05:42 2001
@@ -0,0 +1,358 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+#include <stdio.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <asm/types.h>
+#include <sys/vfs.h>
+#include <string.h>
+#include <asm/byteorder.h>
+#include <time.h>
+
+#include "io.h"
+#include "sb.h"
+#include "misc.h"
+//#include "inode.h"
+//#include "reiserfs_fs_sb.h"
+#include "reiserfs_fs.h"
+#include "reiserfs.h"
+
+
+#define reiserfs_sb(buf) ((struct reiserfs_super_block *)(buf))
+
+static int reiserfs_magic_string (char * buf)
+{
+    return (!strncmp (reiserfs_sb (buf)->s_magic, REISERFS_SUPER_MAGIC_STRING, 
+		      strlen ( REISERFS_SUPER_MAGIC_STRING)));
+    
+}
+
+
+
+/* returns 1 if buf looks like a leaf node, 0 otherwise */
+
+static int is_leaf (char * buf, int blocksize)
+{
+  struct block_head * blkh;
+  struct item_head * ih;
+  int used_space;
+  int prev_location;
+  int i;
+
+  blkh = (struct block_head *)buf;
+  if (blkh->blk_level != DISK_LEAF_NODE_LEVEL)
+      return 0;
+
+  if (blkh->blk_nr_item < 1 || 
+      blkh->blk_nr_item > ((blocksize - BLKH_SIZE) / (IH_SIZE + MIN_ITEM_LEN)))
+      /* item number is too big or too small */
+      return 0;
+
+  ih = (struct item_head *)(buf + BLKH_SIZE) + blkh->blk_nr_item - 1;
+  used_space = BLKH_SIZE + IH_SIZE * blkh->blk_nr_item + (blocksize - ih->ih_item_location);
+  if (used_space != blocksize - blkh->blk_free_space)
+      /* free space does not match to calculated amount of use space */
+      return 0;
+
+  /* check tables of item heads */
+  ih = (struct item_head *)(buf + BLKH_SIZE);
+  prev_location = blocksize;
+  for (i = 0; i < blkh->blk_nr_item; i ++, ih ++) {
+    if (ih->ih_item_location >= blocksize || ih->ih_item_location < IH_SIZE * blkh->blk_nr_item)
+      return 0;
+    if (ih->ih_item_len < 1 || ih->ih_item_len > MAX_ITEM_LEN (blocksize))
+      return 0;
+    if (prev_location - ih->ih_item_location != ih->ih_item_len)
+      return 0;
+    prev_location = ih->ih_item_location;
+  }
+
+  /* contents of buf looks like leaf so far */
+  return 1;
+}
+
+
+
+/* returns 1 if buf looks like an internal node, 0 otherwise */
+static int is_internal (char * buf, int blocksize)
+{
+    struct block_head * blkh;
+    int used_space;
+
+    blkh = (struct block_head *)buf;
+    if (blkh->blk_level < DISK_LEAF_NODE_LEVEL || blkh->blk_level > MAX_HEIGHT)
+	/* this level is not possible for internal nodes */
+	return 0;
+
+    if (blkh->blk_nr_item > (blocksize - BLKH_SIZE - DC_SIZE) / (KEY_SIZE + DC_SIZE))
+	/* for internal which is not root we might check min number of keys */
+	return 0;
+
+    used_space = BLKH_SIZE + KEY_SIZE * blkh->blk_nr_item + DC_SIZE * (blkh->blk_nr_item + 1);
+    if (used_space != blocksize - blkh->blk_free_space)
+	return 0;
+
+    return 1;
+}
+
+
+/* sometimes unfomatted node looks like formatted, if we check only
+   block_header. This is the reason, why it is so complicated. We
+   believe only when free space and item locations are ok 
+   */
+int not_formatted_node (char * buf, int blocksize)
+{
+    struct reiserfs_journal_desc * desc;
+
+    if (is_leaf (buf, blocksize))
+	return 0;
+
+    if (is_internal (buf, blocksize))
+	return 0;
+
+    /* super block? */
+    if (reiserfs_magic_string (buf))
+	return 0;
+
+    /* journal descriptor block? */
+    desc = (struct reiserfs_journal_desc *)buf;
+    if (!memcmp(desc->j_magic, JOURNAL_DESC_MAGIC, 8))
+	return 0;
+
+    /* contents of buf does not look like reiserfs metadata. Bitmaps
+       are possible here */
+    return 1;
+}
+
+
+/* is this block bitmap block or block from journal or skipped area or
+   super block? This works for both journal format only yet */
+int not_data_block (struct super_block * s, b_blocknr_t block)
+{
+    int i;
+
+    if (block < SB_JOURNAL_BLOCK (s) + JOURNAL_BLOCK_COUNT + 1)
+	return 1;
+    for (i = 0; i < SB_BMAP_NR (s); i ++)
+	if (block == SB_AP_BITMAP (s)[i]->b_blocknr)
+	    return 1;
+    return 0;
+}
+
+
+
+
+//////////////////////////////////////////////////////////
+//
+// in reiserfs version 0 (undistributed bitmap)
+//
+static int get_journal_old_start_must (struct reiserfs_super_block * s)
+{
+    return 3 + s->s_bmap_nr;
+}
+
+
+//
+// in reiserfs version 1 (distributed bitmap) journal starts at 18-th
+//
+static int get_journal_start_must (struct reiserfs_super_block * s)
+{
+    return REISERFS_DISK_OFFSET_IN_BYTES / s->s_blocksize + 2;
+}
+
+
+int get_journal_start (struct super_block * s)
+{
+    return s->u.reiserfs_sb.s_rs->s_journal_block;
+}
+
+
+int get_journal_size (struct super_block * s)
+{
+    return s->u.reiserfs_sb.s_rs->s_orig_journal_size;
+}
+
+
+int is_desc_block (struct buffer_head * bh)
+{
+    struct reiserfs_journal_desc * desc = bh_desc (bh);
+
+    if (!memcmp(desc->j_magic, JOURNAL_DESC_MAGIC, 8))
+	return 1;
+    return 0;
+}
+
+
+int does_desc_match_commit (struct reiserfs_journal_desc * desc, 
+			    struct reiserfs_journal_commit * commit)
+{
+    if (commit->j_trans_id != desc->j_trans_id || commit->j_len != desc->j_len || 
+	commit->j_len > JOURNAL_TRANS_MAX || commit->j_len <= 0 ) {
+	return 1 ;
+    }
+    return 0 ;
+}
+
+
+
+//struct super_operations reiserfs_sops = {0, };
+
+//
+// 4k only now ! 
+//
+
+int uread_super_block (struct super_block * s)
+{
+    struct buffer_head * bh;
+
+
+    bh = bread (s->s_dev, (REISERFS_DISK_OFFSET_IN_BYTES / 4096), 4096);
+    if (!bh)
+	goto not_found;
+
+    if (reiserfs_magic_string (bh->b_data) && 
+	reiserfs_sb (bh->b_data)->s_journal_block == get_journal_start_must (reiserfs_sb (bh->b_data)))
+	/* new super block found and correct journal start */
+	goto found;
+
+    /* new super block is not the correct one */
+    brelse (bh);
+
+    bh = bread (s->s_dev, 2, 4096);
+    if (!bh)
+	goto not_found;
+
+    if (reiserfs_magic_string (bh->b_data) && 
+	reiserfs_sb (bh->b_data)->s_journal_block == get_journal_old_start_must (reiserfs_sb (bh->b_data)))
+	goto found;
+
+    brelse (bh);
+
+ not_found:
+    printf ("uread_super_block: neither new nor old reiserfs format found on dev %s\n",
+	    kdevname (s->s_dev));
+    return 1;
+
+ found:
+
+    s->s_blocksize = __le16_to_cpu (reiserfs_sb (bh->b_data)->s_blocksize);
+    //s->s_blocksize_bits = 0;
+    //while ((1 << s->s_blocksize_bits) != s->s_blocksize)
+    //s->s_blocksize_bits ++;
+
+    SB_BUFFER_WITH_SB (s) = bh;
+    SB_DISK_SUPER_BLOCK (s) = reiserfs_sb (bh->b_data);
+    //s->s_op = &reiserfs_sops;
+    return 0;
+}
+
+
+static int new_format (struct super_block * s)
+{
+    return (SB_JOURNAL_BLOCK (s) == get_journal_start_must (SB_DISK_SUPER_BLOCK (s)));
+}
+
+
+
+int uread_bitmaps (struct super_block * s)
+{
+    int i, bmp ;
+    struct reiserfs_super_block * rs = SB_DISK_SUPER_BLOCK(s);
+
+    
+    SB_AP_BITMAP (s) = getmem (sizeof (struct buffer_head *) * __le16_to_cpu (rs->s_bmap_nr));
+    if (!SB_AP_BITMAP (s)) {
+	printf ("read_bitmaps: malloc failed\n");
+	return 1;
+    }
+
+    bmp = SB_BUFFER_WITH_SB (s)->b_blocknr + 1;
+
+    for (i = 0; i < __le16_to_cpu (rs->s_bmap_nr); i ++) {
+	SB_AP_BITMAP (s)[i] = bread (s->s_dev, bmp, s->s_blocksize);
+	if (!SB_AP_BITMAP (s)[i]) {
+	    printf ("read_bitmaps: bread failed\n");
+	    return 1;
+	}
+	if (new_format (s))
+	    bmp = (i + 1) * (s->s_blocksize * 8);
+	else
+	    bmp ++;
+    }
+    
+    return 0;
+}
+
+
+
+/* prepare stat data of new directory */
+void make_dir_stat_data (struct key * dir_key, struct item_head * ih,
+			 struct stat_data * sd)
+{
+    /* insert stat data item */
+    memcpy (&(ih->ih_key), dir_key, KEY_SIZE);
+    ih->ih_item_len = SD_SIZE;
+    ih->u.ih_free_space = MAX_US_INT;
+/*    mark_item_unaccessed (ih);*/
+    ih->ih_reserved = 0;
+
+    sd->sd_mode = S_IFDIR + 0755;
+    sd->sd_nlink = 0;
+    sd->sd_uid = 0;
+    sd->sd_gid = 0;
+    sd->sd_size = EMPTY_DIR_SIZE;
+    sd->sd_atime = sd->sd_ctime = sd->sd_mtime = time (NULL);
+    sd->u.sd_blocks = 0;
+    sd->sd_first_direct_byte = MAX_UL_INT;
+}
+
+
+/* compose directory item containing "." and ".." entries */
+void make_empty_dir_item (char * body, objectid_t dirid, objectid_t objid,
+			  objectid_t par_dirid, objectid_t par_objid)
+{
+    struct reiserfs_de_head * deh;
+    char * name;
+
+    deh = (struct reiserfs_de_head *)body;
+    
+    /* direntry header of "." */
+    deh[0].deh_offset = __cpu_to_le32 (DOT_OFFSET);
+    deh[0].deh_dir_id = __cpu_to_le32 (dirid);
+    deh[0].deh_objectid = __cpu_to_le32 (objid);
+    deh[0].deh_location = __cpu_to_le16 (EMPTY_DIR_SIZE - 1);
+    deh[0].deh_state = 0;
+    set_bit (DEH_Visible, &(deh[0].deh_state));
+  
+    /* direntry header of ".." */
+    deh[1].deh_offset = __cpu_to_le32 (DOT_DOT_OFFSET);
+    /* key of ".." for the root directory */
+    deh[1].deh_dir_id = __cpu_to_le32 (par_dirid);
+    deh[1].deh_objectid = __cpu_to_le32 (par_objid);
+    deh[1].deh_location = __cpu_to_le16 (__le16_to_cpu (deh[0].deh_location) - strlen (".."));
+    deh[1].deh_state = 0;
+    set_bit (DEH_Visible, &(deh[1].deh_state));
+
+    /* copy ".." and "." */
+    name = (char *)(deh + 2);
+    name[0] = name[1] = name[2] = '.';
+}
+
+
+void wait_buffer_until_released (struct buffer_head * bh)
+{
+}
+
+/*
+void if_in_ram_update_sd (struct reiserfs_transaction_handle * th, struct inode * inode)
+{
+}
+*/
+
+void schedule (void)
+{
+}
+
+
diff -urN linux/fs/reiserfs/utils/lib/version.c /tmp/linux/fs/reiserfs/utils/lib/version.c
--- linux/fs/reiserfs/utils/lib/version.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/lib/version.c	Fri Feb  2 19:05:43 2001
@@ -0,0 +1,7 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+char *reiserfs_get_version_string(void) {
+  return "ReiserFS version 3.5.29" ;
+}
diff -urN linux/fs/reiserfs/utils/mkreiserfs/HASHINFO /tmp/linux/fs/reiserfs/utils/mkreiserfs/HASHINFO
--- linux/fs/reiserfs/utils/mkreiserfs/HASHINFO	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/mkreiserfs/HASHINFO	Fri Feb  2 19:05:43 2001
@@ -0,0 +1,59 @@
+  Reiserfs file system uses a hash function plus a generation counter
+  in determining the key used in directory searching.
+
+  We do this for compatibility with NFS,  which needs to be able to
+  use a 32 (v2) or 64 bit (v3)  integer  to  specify  where  it  last
+  read  from in a  directory  (the  NFS  cookie)  because it
+  stupidly  assumes  that directories  are  implemented as files
+  with  byte  offsets  and directories are never shrunk.  It seems
+  V4 will be fixing that.  This  hash function determines the order
+  of insertions. That can have dramatic impact on performance for
+  large directories  because it can cause a random I/O per filename
+  created.  
+  
+  If you want certainty of avoiding hash collisions, which will cause 
+  a -EHASHCOLLISION error if you have more collisions on a 
+  given hash value  than  the maximum  generation counter (128), you 
+  should use the specify -h tea running mkreiserfs. 
+      
+  The Rupasov Hash  makes  an  attempt to preserve much of  the order 
+  that will be present in  alphabetically  or numerically consecutive 
+  names while adding just enough randomness for it to work as a hash.
+  Note that if it gets the order reversed,  the LRU algorithm 
+  will still work  better than if it fully randomizes.....
+  
+  TEA_HASH allows you to have fewer collisions, and this means that you
+  can have directories that are larger. As a practical matter, users 
+  never report r5 is insufficient for applications that are not deliberately 
+  designed to make r5 inadequate on current hardware.  r5 is faster 
+  for large directories, 30x faster for ones that exceed cache capacity.
+
+  Use r5, teahash is almost, but not quite always, the wrong answer.  
+  This hashing feature exists to cover over bad design of telldir and NFS 
+  cookies which assume that a byte offset into a directory has meaning and 
+  allocate a number of bytes appropriate for use of a byte offset not a 
+  filename for tracking position of a partial directory read in directories.
+
+  RUPASOV_HASH:
+  Invented by  Yuri Rupasov  while studying the problems of creating 
+  directories too large to fit into RAM.  Never slower than 
+  CRYPTO_SECURE_HASH, and for some applications involving directories 
+  too large for RAM it can be as much as 30 times faster.  For normal 
+  size  directories it makes reiserfs work with the same speed or 
+  just a bit faster than tea hash function.  
+  
+  Rupasov_hash is obsolete, please use r5_hash.
+    
+  R5_HASH:
+  Invented by Yuri Rupasov to solve collisions problem of rupasov_hash
+  in case of names with long identical tails. R5_hash gives the same or
+  better speed, so please use it to work with huge dirs.  
+  
+  The default hash is 'r5' hash.
+  
+  examples :
+  # mkreiserfs /dev/xxxx -h tea
+  # mkreiserfs /dev/xxxx -h r5
+
+
+
diff -urN linux/fs/reiserfs/utils/mkreiserfs/Makefile /tmp/linux/fs/reiserfs/utils/mkreiserfs/Makefile
--- linux/fs/reiserfs/utils/mkreiserfs/Makefile	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/mkreiserfs/Makefile	Tue Jun 12 22:38:37 2001
@@ -0,0 +1,83 @@
+VPATH = ../bin
+vpath %.c $(REISERFS_KERNEL_SOURCE) $(REISERFS_LIB)
+
+# files from utils's lib directory needed for mkreiserfs
+LIB_C = misc.c version.c io.c
+LIB_OBJS = misc.o version.o io.o
+
+MKFS_OBJS = mkreiserfs.o  $(LIB_OBJS)
+
+MKFS = $(TMPBINDIR)/mkreiserfs
+
+all: $(MKFS)
+
+.c.o:
+	$(CC) $(CFLAGS) $<
+
+$(MKFS): $(MKFS_OBJS)
+	$(CC) -static $(LFLAGS) -o $(MKFS) $(MKFS_OBJS)
+
+clean:
+	rm -f *.o $(MKFS) *~
+
+dep:
+	gcc -MM $(IDIRS) *.c > .depend
+	for i in $(LIB_C); do gcc -MM $(IDIRS) ../lib/$$i >> .depend ; done
+
+install:
+	cp -f $(MKFS) $(SBIN)
+	if [ -d $(MANDIR) ] ; then cp mkreiserfs.8 $(MANDIR) ; gzip -9 -f $(MANDIR)/mkreiserfs.8 ; fi
+
+
+uninstall:
+	rm -f $(MANDIR)/mkreiserfs.8.gz $(SBIN)/mkreiserfs
+
+
+ifeq (.depend,$(wildcard .depend))
+include .depend
+endif
+
+
+
+
+#VPATH = ../bin
+
+#OBJS = mkreiserfs.o
+
+#MKFS = $(TMPBINDIR)/mkreiserfs
+
+#all: $(MKFS)
+
+#.c.o:
+#	$(CC) $(CFLAGS) $<
+
+#$(MKFS): $(OBJS) libmisc.a
+#	$(CC) $(LFLAGS) -o $(MKFS) $(OBJS) -lmisc
+
+#clean:
+#	rm -f *.o $(MKFS) *~ TAGS .depend
+
+#dep:
+#	gcc -MM $(IDIRS) *.c > .depend
+
+#install:
+#	cp -f $(MKFS) $(SBIN)
+#	if [ -d $(MANDIR) ] ; then cp mkreiserfs.8 $(MANDIR) ; gzip -9 -f $(MANDIR)/mkreiserfs.8 ; fi
+
+
+#uninstall:
+#	rm -f $(MANDIR)/mkreiserfs.8.gz $(SBIN)/mkreiserfs
+
+
+#ifeq (.depend,$(wildcard .depend))
+#include .depend
+#endif
+
+
+
+
+
+
+
+
+
diff -urN linux/fs/reiserfs/utils/mkreiserfs/mkreiserfs.8 /tmp/linux/fs/reiserfs/utils/mkreiserfs/mkreiserfs.8
--- linux/fs/reiserfs/utils/mkreiserfs/mkreiserfs.8	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/mkreiserfs/mkreiserfs.8	Fri Feb  2 19:05:43 2001
@@ -0,0 +1,61 @@
+.\" -*- nroff -*-
+.\" Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+.\" 
+.TH MKREISERFS 8 "February 1999" "Reiserfs utilities"
+.SH NAME
+mkreiserfs \- create a Linux Reiserfs file system
+.SH SYNOPSIS
+.B mkreiserfs
+.\" [
+.\" .B \-b
+.\" .I block-size-in-1k-units
+.\" ]
+[
+.B \-h
+.I hash-name
+]
+.I device
+[
+.I size-in-blocks
+]
+.SH DESCRIPTION
+.B mkreiserfs
+utility is used to create a Linux Reiserfs file system on a device
+(usually a disk partition).
+.SH Non-optional argument
+.TP
+.I device
+is the special file corresponding to the device (e.g /dev/hdXX for
+IDE disk partition or /dev/sdXX for SCSI disk partition).
+.SH OPTIONS
+.\" .TP
+.\" .I -b block-size-in-1k-units
+.\" Specify the size of blocks in 1024b units.  In current version
+.\" .B mkreiserfs
+.\" accepts only values of 1, 2, or 4 (i.e. size of blocks can be 1024b,
+.\" 2048b, or 4096b correspondingly)
+.TP
+.I size-in-blocks
+is the number of blocks on the device.  If omitted, it will be
+determined by
+.B mkreiserfs
+automatically.
+.TP
+.I -h hash-name
+Specify the name hash to sort file names. In current version
+.B mkreiserfs
+accepts the next values : 'tea','r5'.
+
+(example : mkreiserfs -h tea)
+.\" .SH AUTHOR
+.\" This version of
+.\" .B mkreiserfs
+.\" has been written by Hans Reiser <reiser@idiom.com>.
+.SH BUGS
+Blocksize is 4k only for now.
+Please, report bugs to Hans Reiser <reiser@idiom.com>.
+.SH AVAILABILITY
+.B mkreiserfs
+sources are included into reiserfs patch and appear in the kernel source tree after applying under directory ./fs/reiserfs/utils/mkreiserfs
+.SH SEE ALSO
+.BR reiserfsck (8)
diff -urN linux/fs/reiserfs/utils/mkreiserfs/mkreiserfs.c /tmp/linux/fs/reiserfs/utils/mkreiserfs/mkreiserfs.c
--- linux/fs/reiserfs/utils/mkreiserfs/mkreiserfs.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/mkreiserfs/mkreiserfs.c	Fri Feb  2 19:05:43 2001
@@ -0,0 +1,596 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+/* mkreiserfs is very simple. It supports only 4 and 8K blocks. It skip
+   first REISERFS_DISK_OFFSET_IN_BYTES of device, and then writes the super
+   block, the needed amount of bitmap blocks (this amount is calculated
+   based on file system size), and root block. Bitmap policy is
+   primitive: it assumes, that device does not have unreadable blocks,
+   and it occupies first blocks for super, bitmap and root blocks.
+   bitmap blocks are interleaved across the disk, mainly to make
+   resizing faster. */
+
+#include <stdio.h>
+#include <string.h>
+#include <stdlib.h>
+#include <unistd.h>
+#include <sys/types.h>
+#include <asm/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <errno.h>
+#include <sys/vfs.h>
+#include <time.h>
+#include <sys/ioctl.h>
+#include <sys/mount.h>
+
+//#include "inode.h"
+#include "io.h"
+#include "sb.h"
+#include "misc.h"
+#include "reiserfs_fs.h"
+
+#define print_usage_and_exit() die ("Usage: %s [ -f ] [ -h tea | r5 ] device [block-count]\n\n", argv[0])
+
+#define DEFAULT_BLOCKSIZE 4096
+#define MIN_BLOCK_AMOUNT (100+JOURNAL_BLOCK_COUNT+RESERVED_FOR_PRESERVE_LIST)
+
+
+struct buffer_head * g_sb_bh;
+struct buffer_head * g_bitmap_bh;
+struct buffer_head * g_rb_bh;
+struct buffer_head * g_journal_bh ;
+
+
+int g_block_size = DEFAULT_BLOCKSIZE;
+unsigned long int g_block_number;
+int g_hash = DEFAULT_HASH;
+
+/* Given a file descriptor and an offset, check whether the offset is
+   a valid offset for the file - return 0 if it isn't valid or 1 if it
+   is */
+int valid_offset( int fd, loff_t offset )
+{
+  char ch;
+
+  if (reiserfs_llseek (fd, offset, 0) < 0)
+    return 0;
+
+  if (read (fd, &ch, 1) < 1)
+    return 0;
+
+  return 1;
+}
+
+
+
+/* calculates number of blocks on device 
+ */
+unsigned long count_blocks (char * filename, int blocksize)
+{
+  loff_t high, low;
+  int fd;
+
+  fd = open (filename, O_RDONLY);
+  if (fd < 0)
+    die ("count_blocks: open failed (%s)", strerror (errno));
+
+#ifdef BLKGETSIZE
+  {
+    unsigned long size;
+
+    if (ioctl (fd, BLKGETSIZE, &size) >= 0) {
+      close (fd);
+      return  size / (blocksize / 512);
+    }
+  }
+#endif
+
+  low = 0;
+  for( high = 1; valid_offset (fd, high); high *= 2 )
+    low = high;
+  while (low < high - 1) {
+    const loff_t mid = ( low + high ) / 2;
+      
+    if (valid_offset (fd, mid))
+      low = mid;
+    else
+      high = mid;
+  }
+  valid_offset (fd, 0);
+  close (fd);
+  
+  return (low + 1) / (blocksize);
+}
+
+
+
+/* form super block */
+void make_super_block (int dev)
+{
+  struct reiserfs_super_block * sb;
+  unsigned long * oids;
+
+
+  if (SB_SIZE > g_block_size)
+    die ("mkreiserfs: blocksize (%d) too small", g_block_size);
+    
+
+  /* get buffer for super block */
+  g_sb_bh = getblk (dev, REISERFS_DISK_OFFSET_IN_BYTES / g_block_size, g_block_size);
+
+/*  sb = (struct reiserfs_super_block *)g_cp_super_block;*/
+  sb = (struct reiserfs_super_block *)g_sb_bh->b_data;
+  sb->s_blocksize = g_block_size;   		/* block size in bytes */
+  sb->s_block_count = g_block_number;	/* how many block reiserfs must occupy */
+  sb->s_state = REISERFS_VALID_FS;
+  sb->s_tree_height = 2;
+  sb->s_journal_dev = 0 ;
+  sb->s_orig_journal_size = JOURNAL_BLOCK_COUNT ;
+  sb->s_journal_trans_max = 0 ;
+  sb->s_journal_block_count = 0 ;
+  sb->s_journal_max_batch = 0 ;
+  sb->s_journal_max_commit_age = 0 ;
+  sb->s_journal_max_trans_age = 0 ;
+
+  sb->s_bmap_nr = g_block_number / (g_block_size * 8) + ((g_block_number % (g_block_size * 8)) ? 1 : 0);
+  memcpy (sb->s_magic, REISERFS_SUPER_MAGIC_STRING, sizeof (REISERFS_SUPER_MAGIC_STRING));
+  sb->s_hash_function_code = g_hash; 
+
+
+  /* initialize object map */
+  oids = (unsigned long *)(sb + 1);
+  oids[0] = 1;
+  oids[1] = REISERFS_ROOT_OBJECTID + 1;	/* objectids > REISERFS_ROOT_OBJECTID are free */
+  sb->s_oid_cursize = 2;
+
+  /* max size must be even */
+  sb->s_oid_maxsize = (g_block_size - SB_SIZE) / sizeof(unsigned long) / 2 * 2;
+
+
+  mark_buffer_dirty (g_sb_bh, 0);
+  mark_buffer_uptodate (g_sb_bh, 0);
+  return;
+
+}
+
+
+void zero_journal_blocks(int dev, int start, int len) {
+  int i ;
+  struct buffer_head *bh ;
+  int done = 0;
+
+  printf ("Initializing journal - "); fflush (stdout);
+
+  for (i = 0 ; i < len ; i++) {
+    print_how_far (&done, len);
+    bh = getblk (dev, start + i, g_block_size) ;
+    memset(bh->b_data, 0, g_block_size) ;
+    mark_buffer_dirty(bh,0) ;
+    mark_buffer_uptodate(bh,0) ;
+    bwrite (bh);
+    brelse(bh) ;
+  }
+  printf ("\n"); fflush (stdout);
+}
+
+
+/* this only sets few first bits in bitmap block. Fills not initialized
+   fields of super block (root block and bitmap block numbers)
+   */
+void make_bitmap ()
+{
+  struct reiserfs_super_block * sb = (struct reiserfs_super_block *)g_sb_bh->b_data;
+  int i, j;
+  
+  /* get buffer for bitmap block */
+  g_bitmap_bh = getblk (g_sb_bh->b_dev, g_sb_bh->b_blocknr + 1, g_sb_bh->b_size);
+  
+  /* mark, that first 8K of device is busy */
+  for (i = 0; i < REISERFS_DISK_OFFSET_IN_BYTES / g_block_size; i ++)
+    set_bit (i, g_bitmap_bh->b_data);
+
+  /* mark that super block is busy */
+  set_bit (i++, g_bitmap_bh->b_data);
+
+  /* mark first bitmap block as busy */
+  set_bit (i ++, g_bitmap_bh->b_data);
+  
+  /* sb->s_journal_block = g_block_number - JOURNAL_BLOCK_COUNT ; */ /* journal goes at end of disk */
+  sb->s_journal_block = i;
+
+  /* mark journal blocks as busy  BUG! we need to check to make sure journal will fit in the first bitmap block */
+  for (j = 0 ; j < (JOURNAL_BLOCK_COUNT + 1); j++) /* the descriptor block goes after the journal */
+    set_bit (i ++, g_bitmap_bh->b_data);
+
+  /* and tree root is busy */
+  set_bit (i, g_bitmap_bh->b_data);
+  sb->s_root_block = i;
+  sb->s_free_blocks = sb->s_block_count - i - 1 ;
+
+  /* count bitmap blocks not resides in first s_blocksize blocks */
+  sb->s_free_blocks -= sb->s_bmap_nr - 1;
+
+  mark_buffer_dirty (g_bitmap_bh, 0);
+  mark_buffer_uptodate (g_bitmap_bh, 0);
+
+  mark_buffer_dirty (g_sb_bh, 0);
+  return;
+}
+
+
+/* form the root block of the tree (the block head, the item head, the
+   root directory) */
+void make_root_block ()
+{
+  struct reiserfs_super_block * sb = (struct reiserfs_super_block *)g_sb_bh->b_data;
+  char * rb;
+  struct block_head * blkh;
+  struct item_head * ih;
+
+  struct stat_data * sd;
+  struct reiserfs_de_head * deh;
+  struct key maxkey = {0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff};
+
+  /* get memory for root block */
+/*  g_cp_root_block = getmem (g_block_size);*/
+  /* no more cautious bitmap, kill the *2 */
+  /* g_rb_bh = getblk (g_sb_bh->b_dev, g_sb_bh->b_blocknr + sb->s_bmap_nr * 2 + 1, g_sb_bh->b_size); */
+  g_rb_bh = getblk (g_sb_bh->b_dev, sb->s_root_block, sb->s_blocksize);
+  rb = g_rb_bh->b_data;
+
+  /* block head */
+  blkh = (struct block_head *)rb;
+  blkh->blk_level = DISK_LEAF_NODE_LEVEL;
+  blkh->blk_nr_item = 0;
+  blkh->blk_free_space = sb->s_blocksize - BLKH_SIZE;
+  memcpy (&blkh->blk_right_delim_key, &maxkey, KEY_SIZE);
+
+  /* first item is stat data item of root directory */
+  ih = (struct item_head *)(blkh + 1);
+  ih->ih_key.k_dir_id = REISERFS_ROOT_PARENT_OBJECTID;
+  ih->ih_key.k_objectid = REISERFS_ROOT_OBJECTID;
+  ih->ih_key.k_offset = SD_OFFSET;
+  ih->ih_key.k_uniqueness = TYPE_STAT_DATA;
+  ih->ih_item_len = SD_SIZE;
+  ih->ih_item_location = sb->s_blocksize - ih->ih_item_len;
+  ih->u.ih_free_space = MAX_US_INT;
+  ih->ih_reserved = 0;
+
+  /* fill stat data */
+  sd = (struct stat_data *)(rb + ih->ih_item_location);
+  sd->sd_mode = S_IFDIR + 0755;
+  sd->sd_nlink = 3;
+  sd->sd_uid = 0;	/*??*/
+  sd->sd_gid = 0;	/*??*/
+  sd->sd_size = EMPTY_DIR_SIZE;
+  sd->sd_atime = sd->sd_ctime = sd->sd_mtime = time (NULL);
+  sd->u.sd_blocks = 0;	/*??*/
+  sd->sd_first_direct_byte = MAX_UL_INT;	/*??*/
+
+
+  blkh->blk_nr_item ++;
+  blkh->blk_free_space -= (IH_SIZE + ih->ih_item_len);
+
+  
+  /* second item is root directory item, containing "." and ".." */
+  ih ++;
+  ih->ih_key.k_dir_id = REISERFS_ROOT_PARENT_OBJECTID;
+  ih->ih_key.k_objectid = REISERFS_ROOT_OBJECTID;
+  ih->ih_key.k_offset = DOT_OFFSET;
+  ih->ih_key.k_uniqueness = DIRENTRY_UNIQUENESS/*DOT_UNIQUENESS*/;
+  ih->ih_item_len = DEH_SIZE * 2 + strlen (".") + strlen ("..")/* + sizeof (unsigned long)*/;
+  ih->ih_item_location = (ih-1)->ih_item_location - ih->ih_item_len;
+  ih->u.ih_entry_count = 2;
+  ih->ih_reserved = 0;
+  
+
+  deh = (struct reiserfs_de_head *)(rb + ih->ih_item_location);
+
+  /* "." */
+  deh[0].deh_offset = DOT_OFFSET;
+  /*  deh[0].deh_uniqueness = DOT_UNIQUENESS;*/
+  deh[0].deh_dir_id = ih->ih_key.k_dir_id;
+  deh[0].deh_objectid = ih->ih_key.k_objectid;
+  deh[0].deh_location = ih->ih_item_len - strlen (".");
+  /*mark_de_without_sd (&(deh[0]));*/
+  clear_bit (DEH_Statdata, &(deh[0].deh_state));
+
+  /*mark_de_with_directory_id (&(deh[0]));*/
+/*  clear_bit (DEH_AdditionalKeyComponent, &(deh[0].deh_state));*/
+
+  /*mark_de_visible (&(deh[0]));*/
+  set_bit (DEH_Visible, &(deh[0].deh_state));
+
+  /* ".." */
+  deh[1].deh_offset = DOT_DOT_OFFSET;
+  /*  deh[1].deh_uniqueness = DOT_DOT_UNIQUENESS;*/
+  deh[1].deh_dir_id = 0;
+  deh[1].deh_objectid = REISERFS_ROOT_PARENT_OBJECTID;	/* as key of root directory is [REISERFS_ROOT_PARENT_OBJECTID, 
+							                                REISERFS_ROOT_OBJECTID],
+							   so objectid of root directory
+							   parent direcotry is REISERFS_ROOT_PARENT_OBJECTID */
+  deh[1].deh_location = deh[0].deh_location - strlen ("..");
+
+  /*mark_de_without_sd (&(deh[1]));*/
+  clear_bit (DEH_Statdata, &(deh[1].deh_state));
+  
+  /*mark_de_with_directory_id (&(deh[1]));*/
+  /*set_bit (DEH_AdditionalKeyComponent, &(deh[1].deh_state));*/
+
+  /*mark_de_visible (&(deh[1]));*/
+  set_bit (DEH_Visible, &(deh[1].deh_state));
+
+
+  memcpy (rb + ih->ih_item_location + deh[0].deh_location, ".", strlen ("."));
+  memcpy (rb + ih->ih_item_location + deh[1].deh_location, "..", strlen (".."));
+  /* objectid of parent directory of object pointed by ".." */
+  /**(unsigned long *)(rb + ih->ih_item_location + deh[1].deh_location + strlen ("..")) = 0;*/
+  
+
+  blkh->blk_nr_item ++;
+  blkh->blk_free_space -= (IH_SIZE + ih->ih_item_len);
+  
+  mark_buffer_dirty (g_rb_bh, 0);
+  mark_buffer_uptodate (g_rb_bh, 0);
+  return;
+}
+
+
+/*
+ *  write the super block, the bitmap blocks and the root of the tree
+ */
+void write_super_and_root_blocks ()
+{
+  struct reiserfs_super_block * sb = (struct reiserfs_super_block *)g_sb_bh->b_data;
+  int i;
+
+  zero_journal_blocks(g_sb_bh->b_dev, sb->s_journal_block, JOURNAL_BLOCK_COUNT + 1) ;
+
+  /* super block */
+  bwrite (g_sb_bh);
+
+  /* bitmap blocks */
+  for (i = 0; i < sb->s_bmap_nr; i ++) {
+    if (i != 0) {
+      g_bitmap_bh->b_blocknr = i * sb->s_blocksize * 8;
+      memset (g_bitmap_bh->b_data, 0, g_bitmap_bh->b_size);
+      set_bit (0,g_bitmap_bh->b_data);
+    }
+    if (i == sb->s_bmap_nr - 1) {
+      int j;
+
+      /* fill unused part of last bitmap block with 1s */
+      if (sb->s_block_count % (sb->s_blocksize * 8))
+	for (j = sb->s_block_count % (sb->s_blocksize * 8); j < sb->s_blocksize * 8; j ++) {
+	  set_bit (j, g_bitmap_bh->b_data);
+      }
+    }
+    /* write true bitmap */
+    mark_buffer_dirty (g_bitmap_bh, 0);
+    bwrite (g_bitmap_bh);
+
+#if 0
+    /* write cautious bitmap */
+    g_bitmap_bh->b_blocknr += sb->s_bmap_nr;
+    mark_buffer_dirty (g_bitmap_bh, 0);
+    bwrite (g_bitmap_bh);
+    g_bitmap_bh->b_blocknr -= sb->s_bmap_nr;    
+#endif
+  }
+
+  /* root block */
+  bwrite (g_rb_bh);
+  brelse (g_rb_bh);
+  brelse (g_bitmap_bh);
+  brelse (g_sb_bh);
+}
+
+
+char buf[20];
+
+#include <linux/kdev_t.h>
+
+char * devname (int dev)
+{
+  struct stat st;
+
+  if (fstat (dev, &st) != 0)
+    die ("stat failed");
+  sprintf (buf, "0x%x:0x%x", MAJOR((int)st.st_rdev), MINOR((int)st.st_rdev));
+  return buf;
+}
+
+
+void report (void)
+{
+    struct reiserfs_super_block * sb = (struct reiserfs_super_block *)g_sb_bh->b_data;
+    unsigned int i;
+
+    printf ("Block size %u bytes\n", sb->s_blocksize);
+    printf ("Block count %u\n", sb->s_block_count);
+    printf ("First %lu blocks skipped\n", g_sb_bh->b_blocknr);
+    printf ("Super block is in %lu\n", g_sb_bh->b_blocknr);
+    printf ("Bitmap blocks are : \n\t%lu", g_bitmap_bh->b_blocknr);
+    for (i = 1; i < sb->s_bmap_nr; i ++) {
+	printf (", %u", i * sb->s_blocksize * 8);
+    }
+    printf ("\nJournal size %u (blocks %u-%u of device %s)\n",
+	    JOURNAL_BLOCK_COUNT, sb->s_journal_block, 
+	    sb->s_journal_block + JOURNAL_BLOCK_COUNT, devname (g_sb_bh->b_dev));
+    printf ("Root block %u\n", sb->s_root_block);
+    printf ("Used %u blocks\n", sb->s_block_count - sb->s_free_blocks);
+    printf ("Hash function \"%s\"\n", g_hash == TEA_HASH ? "tea" :
+	    ((g_hash == YURA_HASH) ? "rupasov" : "r5"));
+    fflush (stdout);
+}
+
+
+/* discard 1st 2k block partition. This should be enough to make
+   mount not see ext2 (and others) on mkreiserfs'd partition;
+   NOW it clear the first 2k block to avoid wrong vfat mounting 
+   (it search its "super block" in 1st 512 bytes) 
+
+   We also clear the original old journaled superblock (8k offset).
+   
+*/
+void invalidate_other_formats (int dev)
+{
+  struct buffer_head * bh;
+
+  bh = getblk (dev, 0, 2048);
+  mark_buffer_uptodate (bh, 1);
+  mark_buffer_dirty (bh, 1);
+  bwrite (bh);
+  brelse (bh);
+  bh = getblk(dev, REISERFS_OLD_DISK_OFFSET_IN_BYTES  / 1024, 1024) ;
+  if (!bh) {
+    printf("Unable to get block to clear the old reiserfs superblock\n") ;
+    return ;
+  }
+  mark_buffer_uptodate (bh, 1);
+  mark_buffer_dirty (bh, 1);
+  bwrite (bh);
+  brelse (bh);
+}
+
+
+static void set_hash_function (char * str)
+{
+    if (!strcmp (str, "tea"))
+	g_hash = TEA_HASH;
+    else if (!strcmp (str, "rupasov"))
+	g_hash = YURA_HASH;
+    else if (!strcmp (str, "r5"))
+	g_hash = R5_HASH;
+    else
+	printf ("mkreiserfs: wrong hash type specified. Using default\n");
+}
+
+
+int main (int argc, char **argv)
+{
+    char *tmp;
+    int dev;
+    int force = 0;
+    struct stat statbuf;
+    char * device_name;
+    char c;
+
+    printf ("\n\n<-----------MKREISERFS, 2000----------->\n%s\n", 
+	    reiserfs_get_version_string());
+  
+    if (argc < 2)
+	print_usage_and_exit ();
+
+
+    while ( ( c = getopt( argc, argv, "fh:" ) ) != EOF )
+	switch( c )
+	{
+	case 'f' :                 /* force if file is not a block device */
+	    force = 1;
+	    break;
+#if 0 /* -b is not supported with the journal code */
+	case 'b' :                  /* -k n - where n is 1,2 or 4 */
+	    g_block_size = (int) strtol (optarg, &tmp, 0);
+	    if ( *tmp || ( g_block_size != 1 && g_block_size != 2 && g_block_size != 4 ))
+		die ("mkreiserfs: bad block size : %s\n", optarg);
+	    g_block_size *= 1024;
+	    break;
+#endif /* -b */
+
+	case 'h':
+	    set_hash_function (optarg);
+	    break;
+
+	default :
+	    print_usage_and_exit ();
+	}
+    device_name = argv [optind];
+  
+
+    /* get block number for file system */
+    if (optind == argc - 2) {
+	g_block_number = strtol (argv[optind + 1], &tmp, 0);
+	if (*tmp == 0) {    /* The string is integer */
+	    if (g_block_number > count_blocks (device_name, g_block_size))
+		die ("mkreiserfs: specified block number (%d) is too high", g_block_number);
+/*      else if (g_block_number < MIN_BLOCK_AMOUNT)
+        die ("mkreiserfs: specified block number (%d) is too low", g_block_number); 
+*/
+	} else {
+	    die ("mkreiserfs: bad block count : %s\n", argv[optind + 1]);
+	}	
+    } else 
+	if (optind == argc - 1) {
+	    /* number of blocks is not specified */
+	    g_block_number = count_blocks (device_name, g_block_size);
+	    tmp = "";
+	} else
+	    print_usage_and_exit ();
+
+
+    g_block_number = g_block_number / 8 * 8;
+
+/*  if (*tmp || g_block_number % 8 || (g_block_number == 0))
+    / * block amount specified is not a valid integer * /
+    die ("mkreiserfs: bad block count : %s\n", argv[optind + 1]);
+*/	
+    if (g_block_number < MIN_BLOCK_AMOUNT)
+	die ("\nCan not mkreiserfs that small partition (block number %d).\n You need at least 32 Mb", g_block_number);
+
+    if (is_mounted (device_name))
+	die ("mkreiserfs: '%s' contains a mounted file system\n", device_name);
+
+    /* open_device will die if it could not open device */
+    dev = open (device_name, O_RDWR);
+    if (dev == -1)
+	die ("mkreiserfs: can not open '%s': %s", device_name, strerror (errno));
+  
+    if (fstat (dev, &statbuf) < 0)
+	die ("mkreiserfs: unable to stat %s", device_name);
+  
+    if (!force) { /* block device check if not "force" flag */
+	if (!S_ISBLK (statbuf.st_mode))
+	    die ("mkreiserfs: '%s (%o)' is not a block device", device_name, statbuf.st_mode);
+	else        /* Ignore any 'full' fixed disk devices */
+	    if ( statbuf.st_rdev == 0x0300 || statbuf.st_rdev == 0x0340 
+		 || statbuf.st_rdev == 0x0400 || statbuf.st_rdev == 0x0410
+		 || statbuf.st_rdev == 0x0420 || statbuf.st_rdev == 0x0430
+		 || statbuf.st_rdev == 0x0d00 || statbuf.st_rdev == 0x0d40 )
+		/* ???? */
+		die ("mkreiserfs: will not try to make filesystem on '%s'", device_name);
+    }
+  
+    /* these fill buffers (super block, first bitmap, root block) with
+       reiserfs structures */
+    make_super_block (dev);
+    make_bitmap ();
+    make_root_block ();
+  
+    report ();
+
+    printf ("ATTENTION: ALL DATA WILL BE LOST ON '%s'! (y/n)", device_name);
+    c = getchar ();
+    if (c != 'y' && c != 'Y')
+	die ("mkreiserfs: Disk was not formatted");
+
+    invalidate_other_formats (dev);
+    write_super_and_root_blocks ();
+
+    check_and_free_buffer_mem ();
+
+    printf ("Syncing.."); fflush (stdout);
+
+    close(dev) ;
+    sync ();
+ 
+    printf ("\n\nReiserFS core development sponsored by SuSE Labs (suse.com)\n\n"
+	    "Journaling sponsored by MP3.com.\n\n"
+	    //"Item handlers sponsored by Ecila.com\n\n
+	    "To learn about the programmers and ReiserFS, please go to\n"
+	    "http://www.devlinux.com/namesys\n\nHave fun.\n\n"); 
+    fflush (stdout);
+    return 0;
+}
diff -urN linux/fs/reiserfs/utils/resize_reiserfs/Makefile /tmp/linux/fs/reiserfs/utils/resize_reiserfs/Makefile
--- linux/fs/reiserfs/utils/resize_reiserfs/Makefile	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/resize_reiserfs/Makefile	Fri Feb  2 19:05:43 2001
@@ -0,0 +1,51 @@
+VPATH = ../bin
+vpath %.c $(REISERFS_KERNEL_SOURCE) $(REISERFS_LIB)
+
+# kernel files needed for resizer
+#KERNEL_C = prints.c
+#KERNEL_OBJS = prints.o
+
+# files from utils's lib directory needed for resizer
+LIB_C = misc.c io.c
+# version.c io.c reiserfs.c
+LIB_OBJS = misc.o io.o
+#version.o io.o reiserfs.o
+
+
+RESIZER_OBJS = resize_reiserfs.o fe.o do_shrink.o bitmap.o $(LIB_OBJS) $(KERNEL_OBJS)
+
+RESIZER = $(TMPBINDIR)/resize_reiserfs
+
+all: $(RESIZER)
+
+.c.o:
+	$(CC) $(CFLAGS) -Wall -g $<
+
+$(RESIZER): $(RESIZER_OBJS)
+	$(CC) $(LFLAGS) -o $(RESIZER) $(RESIZER_OBJS)
+
+clean:
+	rm -f *.o $(RESIZER) *~
+
+dep:
+	gcc -MM $(IDIRS) *.c > .depend
+
+install: all
+	cp -f $(RESIZER) $(SBIN)
+
+uninstall:
+	rm -f $(SBIN)/resize_reiserfs
+
+
+ifeq (.depend,$(wildcard .depend))
+include .depend
+endif
+
+
+
+
+
+
+
+
+
diff -urN linux/fs/reiserfs/utils/resize_reiserfs/bitmap.c /tmp/linux/fs/reiserfs/utils/resize_reiserfs/bitmap.c
--- linux/fs/reiserfs/utils/resize_reiserfs/bitmap.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/resize_reiserfs/bitmap.c	Fri Feb  2 19:05:43 2001
@@ -0,0 +1,182 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+/*
+ * Written by Alexander Zarochentcev <zam@programbank.ru>
+ * 
+ * This file contains small functons which operate with bitmaps:
+ * create, delete, copy, bit operations, etc.
+ * There are two bitmap types: general bitmap and bitmap with associated
+ * disk blocks from a reiserfs.
+ *
+ */
+
+#include <sys/vfs.h>
+#include <sys/types.h>
+#include <asm/types.h>
+ 
+//#include "inode.h"
+#include "io.h"
+#include "sb.h"
+#include "misc.h"
+#include "reiserfs_fs.h"
+
+#include "resize.h"
+
+/* sorry, I can't reuse code from fsck/ubitmap.c */
+
+struct bitmap_head * create_bitmap (unsigned long size, int blocksize) 
+{
+	struct bitmap_head * bmp;
+	int i, bmap_nr;
+	bmp = getmem(sizeof(struct bitmap_head));
+	bmp->bm_block_count = size;
+	bmp->bm_blocksize = blocksize;
+	bmp->bm_bh_table = NULL;
+	bmp->bm_nr = (size - 1) / blocksize + 1;
+	for (i = 0; i < bmp->bm_nr; i++)
+		bmp->bm_bmap [i] = getmem(blocksize);
+	return bmp;				
+}
+
+struct bitmap_head * create_bitmap_from_sb (struct buffer_head * sb_bh)
+{
+	struct bitmap_head * bmp;
+	struct reiserfs_super_block * sb;
+	int i;
+
+	sb = (struct reiserfs_super_block *) sb_bh->b_data;
+	
+	bmp = getmem(sizeof(struct bitmap_head));
+	bmp->bm_blocksize = sb->s_blocksize;
+	bmp->bm_block_count = sb->s_block_count;
+	bmp->bm_bh_table = NULL;
+	bmp->bm_nr = 0;
+
+	bmp->bm_bh_table = getmem(sizeof(struct buffer_head *) * sb->s_bmap_nr);
+	bmp->bm_bmap = getmem(sizeof(char *) * sb->s_bmap_nr);
+	
+	/* read first bitmap block */
+	bmp->bm_bh_table [0] = bread(sb_bh->b_dev,
+	 		   REISERFS_DISK_OFFSET_IN_BYTES / sb->s_blocksize + 1,
+			   sb->s_blocksize);
+	if (!bmp->bm_bh_table [0]) {
+		free_bitmap(bmp);
+		return NULL;
+	}
+	bmp->bm_bmap [0] = bmp->bm_bh_table [0] -> b_data;
+	bmp->bm_nr++;
+		
+	/* read others bitmap blocks */	
+	for (i=1; i < sb->s_bmap_nr; i++) {
+		bmp->bm_bh_table [i] = bread(sb_bh->b_dev, i * sb->s_blocksize * 8, sb->s_blocksize);
+		if (!bmp->bm_bh_table [i]) {
+			free_bitmap(bmp);
+			return NULL;
+		}
+		bmp->bm_bmap [i] = bmp->bm_bh_table [i] -> b_data;
+		bmp->bm_nr++;
+	}	
+	return bmp;	
+}
+
+void free_bitmap (struct bitmap_head * bmp)
+{
+	int i;
+	
+	if (bmp->bm_bh_table) {
+		for (i = 0; i < bmp->bm_nr; i++ )
+			brelse(bmp->bm_bh_table [i]);
+		freemem(bmp->bm_bh_table);
+	} else {
+		if(bmp->bm_bmap)
+			for (i = 0; i < bmp->bm_nr; i++)
+				freemem(bmp->bm_bmap [i]);
+	}
+	if(bmp->bm_bmap)
+		freemem(bmp->bm_bmap);
+	freemem(bmp);
+}
+
+int sync_bitmap (struct bitmap_head * bmp)
+{
+	int i;
+	if (bmp->bm_bh_table)
+		for (i = 0; i < bmp->bm_nr; i++) {
+			bwrite(bmp->bm_bh_table [i]); 
+		}	
+	return 0;
+}
+
+void truncate_bitmap (struct bitmap_head * bmp, unsigned long block)
+{
+	int i,j;
+
+	i = (block - 1) / (8 * bmp->bm_blocksize);
+	j = block - i * 8 * bmp->bm_blocksize;
+
+	while (j < (8 * bmp->bm_blocksize))
+		set_bit(j++, bmp->bm_bh_table [i] -> b_data);
+	mark_buffer_dirty(bmp->bm_bh_table [i], 1);	
+	
+	for (j = i + 1; j < bmp->bm_nr; j++)
+		brelse(bmp->bm_bh_table [j]);
+
+	bmp->bm_nr = i + 1;
+	bmp->bm_block_count = block;
+}
+
+
+int is_block_used (struct bitmap_head * bmp, unsigned long block)
+{
+	int i,j;
+
+	i = block / (8 * bmp->bm_blocksize);
+	j = block % (8 * bmp->bm_blocksize);
+
+	return test_bit(j, bmp->bm_bmap [i]);
+}
+
+void mark_block_free (struct bitmap_head * bmp, unsigned long block)
+{
+	int i,j;
+
+    i = block / (8 * bmp->bm_blocksize);
+    j = block % (8 * bmp->bm_blocksize);
+	
+	clear_bit(j, bmp->bm_bmap [i]);
+	if (bmp->bm_bh_table) 
+		mark_buffer_dirty(bmp->bm_bh_table [i], 1);
+}
+
+void mark_block_used (struct bitmap_head * bmp, unsigned long block)
+{
+	int i,j;
+	
+	i = block / (8 * bmp->bm_blocksize);
+	j = block % (8 * bmp->bm_blocksize);
+
+	set_bit(j, bmp->bm_bmap [i]);
+	if (bmp->bm_bh_table) 
+		mark_buffer_dirty(bmp->bm_bh_table [i], 1);
+}
+
+unsigned long find_1st_unused_block_right (struct bitmap_head * bmp,
+					 							  unsigned long start)
+{
+	for(;start < bmp->bm_block_count; start++)
+		if (is_block_free(bmp, start))
+			return start;
+	return 0;		
+}
+
+unsigned long find_1st_unused_block_left (struct bitmap_head * bmp,
+					 							  unsigned long start)
+{
+	while (--start)
+		if (is_block_free(bmp, start))
+			return start;
+	return 0;		
+}
+
diff -urN linux/fs/reiserfs/utils/resize_reiserfs/do_shrink.c /tmp/linux/fs/reiserfs/utils/resize_reiserfs/do_shrink.c
--- linux/fs/reiserfs/utils/resize_reiserfs/do_shrink.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/resize_reiserfs/do_shrink.c	Fri Feb  2 19:05:43 2001
@@ -0,0 +1,271 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+#include <stdio.h>
+#include <string.h>
+#include <stdlib.h>
+#include <unistd.h>
+#include <sys/types.h>
+#include <asm/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <errno.h>
+#include <sys/vfs.h>
+#include <time.h>
+#include <sys/ioctl.h>
+#include <sys/mount.h>
+
+//#include "inode.h"
+#include "io.h"
+#include "sb.h"
+#include "misc.h"
+#include "reiserfs_fs.h"
+#include "resize.h"
+
+static long int_node_cnt   = 0, int_moved_cnt   = 0;
+static long	leaf_node_cnt  = 0, leaf_moved_cnt  = 0;
+static long	unfm_node_cnt  = 0, unfm_moved_cnt  = 0;
+static long	total_node_cnt = 0, total_moved_cnt = 0;
+
+static unsigned long unused_block;
+static unsigned long blocks_used;
+static struct bitmap_head * bmp;
+
+/* abnornal exit from block reallocation process */
+static void quit_resizer()
+{
+	/* save changes to bitmap blocks */
+	sync_bitmap (bmp);
+	free_bitmap (bmp);
+	/* leave fs in ERROR state */
+	brelse(g_sb_bh);
+	die ("resize_reiserfs: fs shrinking was not completed successfully, run reiserfsck.\n");
+}
+
+/* block moving */
+static unsigned long move_generic_block(unsigned long block,
+										unsigned long bnd, int h)
+{
+	struct buffer_head * bh, * bh2;
+
+	/* primitive fsck */
+	if (block > ((struct reiserfs_super_block *)(g_sb_bh->b_data))->s_block_count) {
+		fprintf(stderr, "resize_reiserfs: invalid block number (%lu) found.\n", block);
+		quit_resizer();
+	}
+	/* progress bar, 3D style :) */
+	if (opt_verbose)	
+		print_how_far((__u32 *)&total_node_cnt, blocks_used);
+	else
+		total_node_cnt ++;
+	/* infinite loop check */
+	if( total_node_cnt > blocks_used) {
+		fputs("resize_reiserfs: block count exeeded\n",stderr);
+		quit_resizer();
+	}
+
+	if (block < bnd) /* block will not be moved */
+		return 0;
+	
+	/* move wrong block */ 
+	bh = bread(g_sb_bh->b_dev, block, g_sb_bh->b_size);
+
+	unused_block = find_1st_unused_block_right(bmp, unused_block);
+	if (unused_block == 0 || unused_block >= bnd) {
+		fputs ("resize_reiserfs: can\'t find free block\n", stderr);
+		quit_resizer();
+	}
+
+	/* blocknr changing */
+	bh2 = getblk(g_sb_bh->b_dev, unused_block, g_sb_bh->b_size);
+	memcpy(bh2->b_data, bh->b_data, bh2->b_size);
+	mark_block_free(bmp, block);
+	mark_block_used(bmp, unused_block);
+
+	brelse(bh);
+	mark_buffer_dirty(bh2,0);
+	mark_buffer_uptodate(bh2,0);
+	bwrite(bh2);
+	brelse(bh2);
+
+	total_moved_cnt++;
+	return unused_block;
+}
+
+static unsigned long move_unformatted_block(unsigned long block,
+											unsigned long bnd, int h)
+{
+	unsigned long b;
+	unfm_node_cnt++;
+	b = move_generic_block(block, bnd, h);
+	if (b)
+		unfm_moved_cnt++;
+	return b;		
+}
+
+
+/* recursive function processing all tree nodes */
+static unsigned long move_formatted_block(unsigned long block, unsigned long bnd, int h)
+{
+    struct buffer_head * bh;
+    struct item_head *ih;
+    unsigned long new_blocknr = 0;
+    int dev;
+    int i, j;
+	
+    dev = g_sb_bh -> b_dev;
+	
+    bh = bread(dev, block, g_sb_bh->b_size);
+
+    if (B_IS_ITEMS_LEVEL(bh)) { /* leaf node*/
+
+	leaf_node_cnt++;
+
+	for (i=0; i < B_NR_ITEMS(bh); i++) {
+	    ih = B_N_PITEM_HEAD(bh, i);
+	    if (I_IS_INDIRECT_ITEM(ih)) {
+		for (j = 0; j < I_UNFM_NUM(ih); j++) {
+		    unsigned long  unfm_block;
+		    if (B_I_POS_UNFM_POINTER(bh, ih, j) == 0) /* hole */
+			continue;
+		    unfm_block = move_unformatted_block(B_I_POS_UNFM_POINTER(bh, ih, j), bnd, h + 1);
+		    if (unfm_block) {
+			B_I_POS_UNFM_POINTER(bh,ih,j) = unfm_block;
+			mark_buffer_dirty(bh,0);
+		    }
+		}
+	    }	
+	}
+	mark_buffer_uptodate(bh,0);
+	bwrite(bh);
+	brelse(bh);
+	new_blocknr = move_generic_block(block, bnd, h);
+	if (new_blocknr)
+	    leaf_moved_cnt++;
+    } else if (B_IS_KEYS_LEVEL(bh)) { /* internal node */
+
+	int_node_cnt++;
+
+	for (i=0; i <= B_NR_ITEMS(bh); i++) {
+	    unsigned long moved_block;
+	    moved_block = move_formatted_block(B_N_CHILD_NUM(bh, i), bnd, h+1);
+	    if (moved_block) {
+		B_N_CHILD_NUM(bh, i) = moved_block;
+		mark_buffer_dirty(bh,0);
+	    }
+	}	
+	mark_buffer_uptodate(bh,0);
+	bwrite(bh);
+	brelse(bh);	
+	new_blocknr = move_generic_block(block, bnd, h);
+	if (new_blocknr)
+	    int_moved_cnt++;
+    } else {
+	die ("resize_reiserfs: block (%lu) have invalid format\n", block);
+    }
+
+    return new_blocknr;
+}
+
+static void sync_super_block()
+{
+    mark_buffer_dirty(g_sb_bh,0);
+    mark_buffer_uptodate(g_sb_bh,0);
+    bwrite(g_sb_bh);
+}
+
+int shrink_fs(unsigned long blocks)
+{
+	struct reiserfs_super_block *  sb;
+	unsigned long n_root_block;
+	int bmap_nr_new;
+
+	/* warn about alpha version */
+	{
+		int c;
+
+		printf(
+			"You are running ALPHA version of reiserfs shrinker.\n"
+			"This version is only for testing or VERY CAREFUL use.\n"
+			"Backup of you data is recommended.\n\n"
+			"Do you want to continue? [y/N]:"
+			);
+		c = getchar();
+		if (c != 'y' && c != 'Y')
+			exit(1);
+	}
+	sb = (struct reiserfs_super_block *) g_sb_bh->b_data;
+	bmap_nr_new = (blocks - 1) / (8 * sb->s_blocksize) + 1;
+	
+	/* is shrinking possible ? */
+	if (sb->s_block_count - blocks > 
+		sb->s_free_blocks + sb->s_bmap_nr - bmap_nr_new)
+		die ("resize_reiserfs: can\'t shrink fs; too many blocks already allocated\n"); 
+	/* calculate number of data blocks */		
+	blocks_used = 
+		sb->s_block_count
+		- sb->s_free_blocks
+		- sb->s_bmap_nr 
+		- sb->s_orig_journal_size
+		- REISERFS_DISK_OFFSET_IN_BYTES / sb->s_blocksize
+		- 2; /* superblock itself and 1 descriptor after the journal */
+
+	bmp = create_bitmap_from_sb(g_sb_bh);
+	if (!bmp) 
+		die ("resize_reiserfs: read bitmap failed\n");
+	unused_block = 1;
+
+	/* change fs state before shrinking */
+	sb->s_state = REISERFS_ERROR_FS;
+	sync_super_block();
+
+	if (opt_verbose) {
+		printf("Processing the tree: ");
+		fflush(stdout);
+	}
+
+	n_root_block = move_formatted_block(sb->s_root_block, blocks, 0);
+	if (n_root_block) {
+		sb->s_root_block = n_root_block;
+	}
+
+	if (opt_verbose)
+		printf ("\n\nnodes processed (moved):\n"
+				"int        %lu (%lu),\n"
+				"leaves     %lu (%lu),\n" 
+				"unfm       %lu (%lu),\n"
+				"total      %lu (%lu).\n\n",
+				int_node_cnt, int_moved_cnt,
+				leaf_node_cnt, leaf_moved_cnt, 
+				unfm_node_cnt, unfm_moved_cnt,
+				total_node_cnt, total_moved_cnt);
+	
+#if 0
+	printf("check for used blocks in truncated region\n");
+	{
+		long l;
+		for (l = blocks; l < sb->s_block_count; l++)
+			if (is_block_used(bmp,l))
+				printf("<%lu>", l);
+		printf("\n");
+	}
+#endif
+		
+	sb->s_free_blocks -= (sb->s_block_count - blocks) 
+							- (sb->s_bmap_nr - bmap_nr_new);
+	sb->s_block_count = blocks;
+	sb->s_bmap_nr = bmap_nr_new;
+	
+	truncate_bitmap(bmp, blocks);
+	sync_bitmap(bmp);
+	free_bitmap(bmp);
+
+	/* change fs state after shrinking */
+	sb->s_state = REISERFS_VALID_FS;
+
+	sync_super_block();
+
+	brelse(g_sb_bh);
+	return 0;
+}
diff -urN linux/fs/reiserfs/utils/resize_reiserfs/fe.c /tmp/linux/fs/reiserfs/utils/resize_reiserfs/fe.c
--- linux/fs/reiserfs/utils/resize_reiserfs/fe.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/resize_reiserfs/fe.c	Fri Feb  2 19:05:43 2001
@@ -0,0 +1,44 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+#include <sys/mount.h>
+#include <sys/types.h>
+#include <asm/types.h>
+#include <errno.h>
+#include <stdio.h>
+#include <mntent.h>
+#include <string.h>
+#include "misc.h"
+#include "resize.h"
+
+/* the front-end for kernel on-line resizer */
+int resize_fs_online(char * devname, unsigned long blocks)
+{
+	static char buf[40];
+	FILE * f;
+	struct mntent * mnt;
+	
+	if ((f = setmntent (MOUNTED, "r")) == NULL)
+		goto fail;
+
+    while ((mnt = getmntent (f)) != NULL)
+        if(strcmp(devname, mnt->mnt_fsname) == 0) {
+
+			if (strcmp(mnt->mnt_type,"reiserfs")) 			
+				die ("resize_reiserfs: can\'t resize fs other than reiserfs\n");
+				
+			sprintf(buf,"resize=%lu", blocks);
+
+			if (mount(mnt->mnt_fsname, mnt->mnt_dir, mnt->mnt_type,
+          			  (unsigned long)(MS_MGC_VAL | MS_REMOUNT), buf)) 
+				die ("resize_reiserfs: remount failed: %s\n", strerror(errno));
+			
+			endmntent(f);
+			return 0;
+		}
+fail:
+   die ("resize_reiserfs: can\t find mount entry\n");
+   return 1;
+}
+
diff -urN linux/fs/reiserfs/utils/resize_reiserfs/resize_reiserfs.c /tmp/linux/fs/reiserfs/utils/resize_reiserfs/resize_reiserfs.c
--- linux/fs/reiserfs/utils/resize_reiserfs/resize_reiserfs.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/utils/resize_reiserfs/resize_reiserfs.c	Fri Feb  2 19:05:43 2001
@@ -0,0 +1,315 @@
+/* 
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+ 
+/*  
+ * Written by Alexander Zarochentcev.
+ * 
+ * FS resize utility 
+ *
+ */
+
+#include <stdio.h>
+#include <string.h>
+#include <stdlib.h>
+#include <unistd.h>
+#include <sys/types.h>
+#include <asm/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <errno.h>
+#include <sys/vfs.h>
+#include <time.h>
+#include <sys/ioctl.h>
+#include <sys/mount.h>
+
+
+//#include "inode.h"
+#include "io.h"
+#include "sb.h"
+#include "misc.h"
+#include "reiserfs_fs.h"
+#include "resize.h"
+
+struct buffer_head * g_sb_bh;
+				/* use of a long is a 2.2 Linux VFS
+                                   limitation, review this decision for
+                                   2.3 and/or LFS patch. -Hans */
+unsigned long g_block_count_new;
+int g_bmap_nr_new;
+
+int opt_force = 0;
+int opt_verbose = 1;			/* now "verbose" option is default */
+int opt_nowrite = 0;
+int opt_safe = 0;
+
+/* Given a file descriptor and an offset, check whether the offset is
+   a valid offset for the file - return 0 if it isn't valid or 1 if it
+   is */
+int valid_offset( int fd, loff_t offset )
+{
+  char ch;
+
+  if (reiserfs_llseek (fd, offset, 0) < 0)
+    return 0;
+
+  if (read (fd, &ch, 1) < 1)
+    return 0;
+
+  return 1;
+}
+
+				/* A bunch of these functions look like
+                                   they could be shared with those in
+                                   super.c or the utils, can they?
+                                   If so, then do so.  -Hans */
+static void read_superblock(int dev) {
+	int bs;
+	struct reiserfs_super_block * sb;
+		
+	g_sb_bh = bread(dev, (REISERFS_DISK_OFFSET_IN_BYTES / 1024), 1024);
+	if (!g_sb_bh)
+		die ("resize_reiserfs: can\'t read superblock\n");
+	sb = (struct reiserfs_super_block *)g_sb_bh->b_data;
+
+	if(strncmp(sb->s_magic, REISERFS_SUPER_MAGIC_STRING, sizeof(REISERFS_SUPER_MAGIC_STRING) - 1) ) 
+        die ("resize_reiserfs: device doesn\'t contain valid reiserfs\n");
+							
+	bs = sb->s_blocksize;	
+	brelse(g_sb_bh);
+	
+	g_sb_bh = bread(dev, REISERFS_DISK_OFFSET_IN_BYTES / bs, bs);
+	if (!g_sb_bh)
+		die ("resize_reiserfs: can\'t read superblock\n");
+	if (g_sb_bh->b_blocknr >= sb->s_journal_block)
+		die ("resize_reiserfs: can\'t read superblock\n");
+}
+
+/* calculate the new fs size (in blocks) from old fs size and the string
+   representation of new size */
+static unsigned long calc_new_fs_size(unsigned long count, 
+								      int bs, char *bytes_str) {
+	long long int bytes;
+	unsigned long blocks;
+	int c;
+	
+	bytes = atoll(bytes_str);
+	c = bytes_str[strlen(bytes_str) - 1];
+
+	switch (c) {
+	case 'M':
+	case 'm':
+		bytes *= 1024;
+	case 'K':
+	case 'k':
+		bytes *= 1024;
+	}
+	
+	blocks = bytes / bs;
+
+	if (bytes_str[0] == '+' || bytes_str[0] == '-')
+		return (count + blocks);
+
+	return blocks;
+}
+
+/* print some fs parameters */
+static void sb_report(struct reiserfs_super_block * sb1,
+       			      struct reiserfs_super_block * sb2){
+	printf(
+		"ReiserFS report:\n"
+		"blocksize             %d\n"
+		"block count           %d (%d)\n"
+		"free blocks           %d (%d)\n"
+		"bitmap block count    %d (%d)\n", 
+		sb1->s_blocksize,
+		sb1->s_block_count, sb2->s_block_count,
+		sb1->s_free_blocks, sb2->s_free_blocks,
+		sb1->s_bmap_nr, sb2->s_bmap_nr);
+};
+
+/* read i-th bitmap block */
+static struct buffer_head * get_bm_blk (int dev, int ind, int bs) {
+	if (ind == 0) 
+		return bread(g_sb_bh->b_dev, REISERFS_DISK_OFFSET_IN_BYTES / bs + 1 ,bs);
+	return bread(dev, ind * bs * 8, bs);
+}
+
+/* conditional bwrite */
+static int bwrite_cond (struct buffer_head * bh) {
+	if(!opt_nowrite) { 
+		mark_buffer_uptodate(bh,0);
+		mark_buffer_dirty(bh,0);
+		return bwrite(bh);
+	}
+	return 0;
+}
+
+
+/* the first one of the mainest functions */
+int expand_fs(void) {
+	struct reiserfs_super_block *  sb;
+	struct buffer_head * bm_bh;
+	int block_r, block_r_new;
+	int i;
+	
+	sb = (struct reiserfs_super_block *) g_sb_bh->b_data;
+
+	/* count used bits in last bitmap block */
+	block_r = sb->s_block_count -
+		((sb->s_bmap_nr - 1) * sb->s_blocksize * 8);
+	
+	/* count bitmap blocks in new fs */
+	g_bmap_nr_new = g_block_count_new / (sb->s_blocksize * 8);
+	block_r_new = g_block_count_new -
+		g_bmap_nr_new * sb->s_blocksize	* 8;
+	if(block_r_new)
+		g_bmap_nr_new++;
+	else 
+		block_r_new = sb->s_blocksize * 8;
+
+	/* clear bits in last bitmap block (old layout) */
+	bm_bh = get_bm_blk(g_sb_bh->b_dev, sb->s_bmap_nr - 1, sb->s_blocksize);
+	for (i = block_r; i < sb->s_blocksize * 8; i++)
+		clear_bit(i, bm_bh->b_data);
+	bwrite_cond(bm_bh);
+	
+	/* add new bitmap blocks */
+	for (i = sb->s_bmap_nr; i < g_bmap_nr_new; i++) {
+		memset(bm_bh->b_data, 0, bm_bh->b_size);
+		set_bit(0, bm_bh->b_data);
+		bm_bh->b_blocknr =  			/* It is not a first BM block */
+			i * sb->s_blocksize * 8;	/* with special location */
+		bwrite_cond(bm_bh);
+	}
+	
+	/* set unused bits in last bitmap block (new layout) */
+	for (i = block_r_new; i < sb->s_blocksize * 8; i++)
+		set_bit(i, bm_bh->b_data);
+	bwrite_cond(bm_bh);
+
+	/* update super block buffer*/
+	sb->s_free_blocks += g_block_count_new - sb->s_block_count
+		- (g_bmap_nr_new - sb->s_bmap_nr);
+	sb->s_block_count = g_block_count_new;
+	sb->s_bmap_nr = g_bmap_nr_new;
+
+	/* commit changes */
+	bwrite_cond(g_sb_bh);
+
+	brelse(g_sb_bh);
+	brelse(bm_bh);
+	
+	return 0;
+}
+
+int main(int argc, char *argv[]) {
+	char * bytes_count_str = NULL;
+	char * devname;
+	struct stat statbuf;
+	int c;
+
+	int dev;
+	struct reiserfs_super_block *sb, *sb_old;
+	
+	while ((c = getopt(argc, argv, "fvcqs:")) != EOF) {
+		switch (c) {
+		case 's' :
+			  if (!optarg) 
+				  die("%s: Missing argument to -s option", argv[0]);		
+			  bytes_count_str = optarg;
+			  break;
+		case 'f':
+		    opt_force = 1;
+		    break;		 
+		case 'v':
+			opt_verbose++; 
+			break;
+		case 'n':
+			/* no nowrite option at this moment */
+			/* opt_nowrite = 1; */
+			break;
+		case 'c':
+			opt_safe = 1;
+			break;
+		case 'q':
+			opt_verbose = 0;
+			break;
+		default:
+			print_usage_and_exit ();
+		}
+	}
+
+	if (optind == argc || (!bytes_count_str))
+		print_usage_and_exit();
+	devname = argv[optind];
+
+	/* open_device will die if it could not open device */
+	dev = open (devname, O_RDWR);
+	if (dev == -1)
+		die ("%s: can not open '%s': %s", argv[0], devname, strerror (errno));
+
+	if (fstat (dev, &statbuf) < 0)
+		die ("%s: unable to stat %s", argv[0], devname);
+  
+	if (!S_ISBLK (statbuf.st_mode) && !opt_force )
+		die ("%s: '%s (%o)' is not a block device", 
+			 argv[0], devname, statbuf.st_mode);
+
+	read_superblock(dev);
+	
+	sb = (struct reiserfs_super_block *) g_sb_bh->b_data;
+	g_block_count_new = calc_new_fs_size(sb->s_block_count,
+					     sb->s_blocksize, bytes_count_str);
+	if (is_mounted (devname)) {
+		close(dev);
+		if (!opt_force) 
+	    	die ("%s: '%s' contains a mounted file system,\n"
+			     "\tspecify -f option to resize the fs online\n", 
+				 argv[0], devname);
+		resize_fs_online(devname, g_block_count_new);
+		return 0;
+	}	
+
+	if (sb->s_state != REISERFS_VALID_FS) 
+		die ("%s: the file system isn't in valid state\n", argv[0]);
+		
+	if(!valid_offset(dev, (loff_t) g_block_count_new * sb->s_blocksize - 1))
+		die ("%s: %s too small", argv[0], devname);
+
+	sb_old = 0;		/* Needed to keep idiot compiler from issuing false warning */
+	/* save SB for reporting */
+	if(opt_verbose) {
+		sb_old = getmem(sizeof(struct reiserfs_super_block));
+		memcpy(sb_old, sb, sizeof(struct reiserfs_super_block));
+    }
+
+	if (g_block_count_new == sb->s_block_count) 
+		die ("%s: Calculated fs size is the same as the previous one.",
+			 argv[0]);
+	if (g_block_count_new > sb->s_block_count) 
+		expand_fs();
+	else
+		shrink_fs(g_block_count_new);
+
+	if(opt_verbose) {
+		sb_report(sb, sb_old);
+		freemem(sb_old);
+	}
+
+	flush_buffers ();
+	check_and_free_buffer_mem ();		
+	
+	if (opt_verbose) {
+		printf("\nSyncing..");
+		fflush(stdout);
+	}
+	fsync (dev);
+	if (opt_verbose)
+		printf("done\n");
+	
+
+	close(dev);
+	
+	return 0;
+}
diff -urN linux/fs/reiserfs/version.c /tmp/linux/fs/reiserfs/version.c
--- linux/fs/reiserfs/version.c	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/fs/reiserfs/version.c	Fri Feb  2 19:05:43 2001
@@ -0,0 +1,7 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+char *reiserfs_get_version_string(void) {
+  return "ReiserFS version 3.5.29" ;
+}
diff -urN linux/include/asm-alpha/md.h /tmp/linux/include/asm-alpha/md.h
--- linux/include/asm-alpha/md.h	Fri May  8 01:17:13 1998
+++ /tmp/linux/include/asm-alpha/md.h	Wed Dec 31 17:00:00 1969
@@ -1,13 +0,0 @@
-/* $Id: md.h,v 1.1 1997/12/15 15:11:48 jj Exp $
- * md.h: High speed xor_block operation for RAID4/5 
- *
- */
- 
-#ifndef __ASM_MD_H
-#define __ASM_MD_H
-
-/* #define HAVE_ARCH_XORBLOCK */
-
-#define MD_XORBLOCK_ALIGNMENT	sizeof(long)
-
-#endif /* __ASM_MD_H */
diff -urN linux/include/asm-i386/errno.h /tmp/linux/include/asm-i386/errno.h
--- linux/include/asm-i386/errno.h	Mon Apr 14 17:28:18 1997
+++ /tmp/linux/include/asm-i386/errno.h	Fri Feb  2 19:05:43 2001
@@ -128,5 +128,6 @@
 
 #define	ENOMEDIUM	123	/* No medium found */
 #define	EMEDIUMTYPE	124	/* Wrong medium type */
+#define	EHASHCOLLISION	125	/* Number of hash collisons exceeds maximum generation counter value.  */
 
 #endif
diff -urN linux/include/asm-i386/md.h /tmp/linux/include/asm-i386/md.h
--- linux/include/asm-i386/md.h	Fri May  8 01:17:13 1998
+++ /tmp/linux/include/asm-i386/md.h	Wed Dec 31 17:00:00 1969
@@ -1,13 +0,0 @@
-/* $Id: md.h,v 1.1 1997/12/15 15:11:57 jj Exp $
- * md.h: High speed xor_block operation for RAID4/5 
- *
- */
- 
-#ifndef __ASM_MD_H
-#define __ASM_MD_H
-
-/* #define HAVE_ARCH_XORBLOCK */
-
-#define MD_XORBLOCK_ALIGNMENT	sizeof(long)
-
-#endif /* __ASM_MD_H */
diff -urN linux/include/asm-m68k/md.h /tmp/linux/include/asm-m68k/md.h
--- linux/include/asm-m68k/md.h	Fri May  8 01:15:22 1998
+++ /tmp/linux/include/asm-m68k/md.h	Wed Dec 31 17:00:00 1969
@@ -1,13 +0,0 @@
-/* $Id: md.h,v 1.1 1997/12/15 15:12:04 jj Exp $
- * md.h: High speed xor_block operation for RAID4/5 
- *
- */
- 
-#ifndef __ASM_MD_H
-#define __ASM_MD_H
-
-/* #define HAVE_ARCH_XORBLOCK */
-
-#define MD_XORBLOCK_ALIGNMENT	sizeof(long)
-
-#endif /* __ASM_MD_H */
diff -urN linux/include/asm-ppc/md.h /tmp/linux/include/asm-ppc/md.h
--- linux/include/asm-ppc/md.h	Tue Oct 26 18:53:42 1999
+++ /tmp/linux/include/asm-ppc/md.h	Wed Dec 31 17:00:00 1969
@@ -1,13 +0,0 @@
-/* $Id: md.h,v 1.1.4.1 1999/08/13 18:30:41 davem dead $
- * md.h: High speed xor_block operation for RAID4/5 
- *
- */
- 
-#ifndef __ASM_MD_H
-#define __ASM_MD_H
-
-/* #define HAVE_ARCH_XORBLOCK */
-
-#define MD_XORBLOCK_ALIGNMENT	sizeof(long)
-
-#endif /* __ASM_MD_H */
diff -urN linux/include/asm-sparc/md.h /tmp/linux/include/asm-sparc/md.h
--- linux/include/asm-sparc/md.h	Mon Jan 12 16:15:54 1998
+++ /tmp/linux/include/asm-sparc/md.h	Wed Dec 31 17:00:00 1969
@@ -1,13 +0,0 @@
-/* $Id: md.h,v 1.1 1997/12/15 15:12:39 jj Exp $
- * md.h: High speed xor_block operation for RAID4/5 
- *
- */
- 
-#ifndef __ASM_MD_H
-#define __ASM_MD_H
-
-/* #define HAVE_ARCH_XORBLOCK */
-
-#define MD_XORBLOCK_ALIGNMENT	sizeof(long)
-
-#endif /* __ASM_MD_H */
diff -urN linux/include/asm-sparc64/md.h /tmp/linux/include/asm-sparc64/md.h
--- linux/include/asm-sparc64/md.h	Mon Jan 12 16:15:58 1998
+++ /tmp/linux/include/asm-sparc64/md.h	Wed Dec 31 17:00:00 1969
@@ -1,91 +0,0 @@
-/* $Id: md.h,v 1.2 1997/12/27 16:28:38 jj Exp $
- * md.h: High speed xor_block operation for RAID4/5 
- *            utilizing the UltraSparc Visual Instruction Set.
- *
- * Copyright (C) 1997 Jakub Jelinek (jj@sunsite.mff.cuni.cz)
- */
- 
-#ifndef __ASM_MD_H
-#define __ASM_MD_H
-
-#include <asm/head.h>
-#include <asm/asi.h>
-
-#define HAVE_ARCH_XORBLOCK
-
-#define MD_XORBLOCK_ALIGNMENT	64
-
-/*	void __xor_block (char *dest, char *src, long len)
- *	{
- *		while (len--) *dest++ ^= *src++;
- * 	}
- *
- *	Requirements:
- *	!(((long)dest | (long)src) & (MD_XORBLOCK_ALIGNMENT - 1)) &&
- *	!(len & 127) && len >= 256
- */
-
-static inline void __xor_block (char *dest, char *src, long len)
-{
-	__asm__ __volatile__ ("
-	wr	%%g0, %3, %%fprs
-	wr	%%g0, %4, %%asi
-	membar	#LoadStore|#StoreLoad|#StoreStore
-	sub	%2, 128, %2
-	ldda	[%0] %4, %%f0
-	ldda	[%1] %4, %%f16
-1:	ldda	[%0 + 64] %%asi, %%f32
-	fxor	%%f0, %%f16, %%f16
-	fxor	%%f2, %%f18, %%f18
-	fxor	%%f4, %%f20, %%f20
-	fxor	%%f6, %%f22, %%f22
-	fxor	%%f8, %%f24, %%f24
-	fxor	%%f10, %%f26, %%f26
-	fxor	%%f12, %%f28, %%f28
-	fxor	%%f14, %%f30, %%f30
-	stda	%%f16, [%0] %4
-	ldda	[%1 + 64] %%asi, %%f48
-	ldda	[%0 + 128] %%asi, %%f0
-	fxor	%%f32, %%f48, %%f48
-	fxor	%%f34, %%f50, %%f50
-	add	%0, 128, %0
-	fxor	%%f36, %%f52, %%f52
-	add	%1, 128, %1
-	fxor	%%f38, %%f54, %%f54
-	subcc	%2, 128, %2
-	fxor	%%f40, %%f56, %%f56
-	fxor	%%f42, %%f58, %%f58
-	fxor	%%f44, %%f60, %%f60
-	fxor	%%f46, %%f62, %%f62
-	stda	%%f48, [%0 - 64] %%asi
-	bne,pt	%%xcc, 1b
-	 ldda	[%1] %4, %%f16
-	ldda	[%0 + 64] %%asi, %%f32
-	fxor	%%f0, %%f16, %%f16
-	fxor	%%f2, %%f18, %%f18
-	fxor	%%f4, %%f20, %%f20
-	fxor	%%f6, %%f22, %%f22
-	fxor	%%f8, %%f24, %%f24
-	fxor	%%f10, %%f26, %%f26
-	fxor	%%f12, %%f28, %%f28
-	fxor	%%f14, %%f30, %%f30
-	stda	%%f16, [%0] %4
-	ldda	[%1 + 64] %%asi, %%f48
-	membar	#Sync
-	fxor	%%f32, %%f48, %%f48
-	fxor	%%f34, %%f50, %%f50
-	fxor	%%f36, %%f52, %%f52
-	fxor	%%f38, %%f54, %%f54
-	fxor	%%f40, %%f56, %%f56
-	fxor	%%f42, %%f58, %%f58
-	fxor	%%f44, %%f60, %%f60
-	fxor	%%f46, %%f62, %%f62
-	stda	%%f48, [%0 + 64] %%asi
-	membar	#Sync|#StoreStore|#StoreLoad
-	wr	%%g0, 0, %%fprs
-	" : :
-	"r" (dest), "r" (src), "r" (len), "i" (FPRS_FEF), "i" (ASI_BLK_P) :
-	"cc", "memory");
-}
-
-#endif /* __ASM_MD_H */
diff -urN linux/include/linux/blkdev.h /tmp/linux/include/linux/blkdev.h
--- linux/include/linux/blkdev.h	Sun Dec 10 17:49:44 2000
+++ /tmp/linux/include/linux/blkdev.h	Mon Jul  9 20:33:42 2001
@@ -91,8 +91,9 @@
 extern void make_request(int major,int rw, struct buffer_head * bh);
 
 /* md needs this function to remap requests */
-extern int md_map (int minor, kdev_t *rdev, unsigned long *rsector, unsigned long size);
-extern int md_make_request (int minor, int rw, struct buffer_head * bh);
+extern int md_map (kdev_t dev, kdev_t *rdev,
+				 unsigned long *rsector, unsigned long size);
+extern int md_make_request (struct buffer_head * bh, int rw);
 extern int md_error (kdev_t mddev, kdev_t rdev);
 
 extern int * blk_size[MAX_BLKDEV];
diff -urN linux/include/linux/fs.h /tmp/linux/include/linux/fs.h
--- linux/include/linux/fs.h	Sun Dec 10 17:49:44 2000
+++ /tmp/linux/include/linux/fs.h	Mon Jul  9 20:33:32 2001
@@ -185,6 +185,8 @@
 #define BH_Lock		2	/* 1 if the buffer is locked */
 #define BH_Req		3	/* 0 if the buffer has been invalidated */
 #define BH_Protected	6	/* 1 if the buffer is protected */
+#define BH_LowPrio	7	/* 1 if the buffer is lowprio */
+#define BH_Wait_IO      7       /* 1 if we should throttle on this buffer */
 
 /*
  * Try to keep the most commonly used fields in single cache lines (16
@@ -279,6 +281,7 @@
 #include <linux/hfs_fs_i.h>
 #include <linux/adfs_fs_i.h>
 #include <linux/qnx4_fs_i.h>
+#include <linux/reiserfs_fs_i.h>
 #include <linux/usbdev_fs_i.h>
 
 /*
@@ -394,6 +397,7 @@
 		struct hfs_inode_info		hfs_i;
 		struct adfs_inode_info		adfs_i;
 		struct qnx4_inode_info		qnx4_i;
+ 	        struct reiserfs_inode_info	reiserfs_i;
 		struct usbdev_inode_info	usbdev_i;
 		struct socket			socket_i;
 		void				*generic_ip;
@@ -519,6 +523,7 @@
 #include <linux/hfs_fs_sb.h>
 #include <linux/adfs_fs_sb.h>
 #include <linux/qnx4_fs_sb.h>
+#include <linux/reiserfs_fs_sb.h>
 #include <linux/usbdev_fs_sb.h>
 
 extern struct list_head super_blocks;
@@ -562,7 +567,8 @@
 		struct smb_sb_info	smbfs_sb;
 		struct hfs_sb_info	hfs_sb;
 		struct adfs_sb_info	adfs_sb;
-		struct qnx4_sb_info	qnx4_sb;	   
+		struct qnx4_sb_info	qnx4_sb;
+ 	        struct reiserfs_sb_info	reiserfs_sb;	   	   
 		struct usbdev_sb_info	usbdevfs_sb;
 		void			*generic_sbp;
 	} u;
@@ -777,7 +783,8 @@
 
 extern void refile_buffer(struct buffer_head * buf);
 extern void set_writetime(struct buffer_head * buf, int flag);
-extern int try_to_free_buffers(struct page *, int wait);
+extern int try_to_free_buffers(struct page *, int);
+extern void cache_drop_behind(struct buffer_head *bh);
 
 extern int nr_buffers;
 extern long buffermem;
@@ -786,18 +793,47 @@
 #define BUF_CLEAN	0
 #define BUF_LOCKED	1	/* Buffers scheduled for write */
 #define BUF_DIRTY	2	/* Dirty buffers, not yet scheduled for write */
-#define NR_LIST		3
+#define BUF_PROTECTED	3	/* Ramdisk persistent storage */
+#define NR_LIST		4
 
 void mark_buffer_uptodate(struct buffer_head * bh, int on);
 
+extern inline void mark_buffer_protected(struct buffer_head * bh)
+{
+	if (!test_and_set_bit(BH_Protected, &bh->b_state)) {
+		if (bh->b_list != BUF_PROTECTED)
+			refile_buffer(bh);
+	}
+}
+
 extern inline void mark_buffer_clean(struct buffer_head * bh)
 {
 	if (test_and_clear_bit(BH_Dirty, &bh->b_state)) {
 		if (bh->b_list == BUF_DIRTY)
 			refile_buffer(bh);
+		clear_bit(BH_Wait_IO, &bh->b_state);
 	}
 }
 
+extern inline void mark_buffer_highprio(struct buffer_head * bh)
+{
+	clear_bit(BH_LowPrio, &bh->b_state);
+}
+
+extern inline void mark_buffer_lowprio(struct buffer_head * bh)
+{
+	/*
+	 * dirty buffers cannot be marked lowprio.
+	 */
+	if (!buffer_dirty(bh))
+		set_bit(BH_LowPrio, &bh->b_state);
+}
+
+static inline int buffer_lowprio(struct buffer_head * bh)
+{
+	return test_bit(BH_LowPrio, &bh->b_state);
+}
+
 extern inline void mark_buffer_dirty(struct buffer_head * bh, int flag)
 {
 	if (!test_and_set_bit(BH_Dirty, &bh->b_state)) {
@@ -805,6 +841,23 @@
 		if (bh->b_list != BUF_DIRTY)
 			refile_buffer(bh);
 	}
+	/*
+	 * if a buffer gets marked dirty then it has to lose
+	 * it's lowprio state.
+	 */
+	mark_buffer_highprio(bh);
+}
+
+extern inline void mark_buffer_dirty_lowprio(struct buffer_head * bh)
+{
+	if (!test_and_set_bit(BH_Dirty, &bh->b_state)) {
+		if (bh->b_list != BUF_DIRTY)
+			refile_buffer(bh);
+		/*
+		 * Mark it lowprio only if it was not dirty before!
+		 */
+		set_bit(BH_LowPrio, &bh->b_state);
+	}
 }
 
 extern int check_disk_change(kdev_t dev);
@@ -882,6 +935,7 @@
 extern struct buffer_head * find_buffer(kdev_t dev, int block, int size);
 extern void ll_rw_block(int, int, struct buffer_head * bh[]);
 extern int is_read_only(kdev_t);
+extern int is_device_idle(kdev_t);
 extern void __brelse(struct buffer_head *);
 extern inline void brelse(struct buffer_head *buf)
 {
@@ -897,8 +951,12 @@
 extern void set_blocksize(kdev_t dev, int size);
 extern unsigned int get_hardblocksize(kdev_t dev);
 extern struct buffer_head * bread(kdev_t dev, int block, int size);
+extern struct buffer_head * buffer_ready (kdev_t dev, int block, int size);
+extern void bread_ahead (kdev_t dev, int block, int size);
 extern struct buffer_head * breada(kdev_t dev,int block, int size, 
 				   unsigned int pos, unsigned int filesize);
+extern struct buffer_head * breada_blocks(kdev_t dev,int block,
+						int size, int blocks);
 
 extern int brw_page(int, struct page *, kdev_t, int [], int, int);
 
@@ -935,6 +993,9 @@
 extern void inode_setattr(struct inode *, struct iattr *);
 
 extern __u32 inode_generation_count;
+
+#define fs_down(sem)	do { current->fs_locks++; down(sem); } while (0)
+#define fs_up(sem)	do { up(sem); current->fs_locks--; } while (0)
 
 #endif /* __KERNEL__ */
 
diff -urN linux/include/linux/locks.h /tmp/linux/include/linux/locks.h
--- linux/include/linux/locks.h	Tue May 11 11:36:15 1999
+++ /tmp/linux/include/linux/locks.h	Mon Jul  9 20:33:42 2001
@@ -50,10 +50,12 @@
 	if (sb->s_lock)
 		__wait_on_super(sb);
 	sb->s_lock = 1;
+	current->fs_locks++;
 }
 
 extern inline void unlock_super(struct super_block * sb)
 {
+	current->fs_locks--;
 	sb->s_lock = 0;
 	wake_up(&sb->s_wait);
 }
diff -urN linux/include/linux/md.h /tmp/linux/include/linux/md.h
--- linux/include/linux/md.h	Fri May  8 01:17:13 1998
+++ /tmp/linux/include/linux/md.h	Wed Dec 31 17:00:00 1969
@@ -1,300 +0,0 @@
-/*
-   md.h : Multiple Devices driver for Linux
-          Copyright (C) 1994-96 Marc ZYNGIER
-	  <zyngier@ufr-info-p7.ibp.fr> or
-	  <maz@gloups.fdn.fr>
-	  
-   This program is free software; you can redistribute it and/or modify
-   it under the terms of the GNU General Public License as published by
-   the Free Software Foundation; either version 2, or (at your option)
-   any later version.
-   
-   You should have received a copy of the GNU General Public License
-   (for example /usr/src/linux/COPYING); if not, write to the Free
-   Software Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.  
-*/
-
-#ifndef _MD_H
-#define _MD_H
-
-#include <linux/major.h>
-#include <linux/ioctl.h>
-#include <linux/types.h>
-
-/*
- * Different major versions are not compatible.
- * Different minor versions are only downward compatible.
- * Different patchlevel versions are downward and upward compatible.
- */
-#define MD_MAJOR_VERSION		0
-#define MD_MINOR_VERSION		36
-#define MD_PATCHLEVEL_VERSION		6
-
-#define MD_DEFAULT_DISK_READAHEAD	(256 * 1024)
-
-/* ioctls */
-#define REGISTER_DEV 		_IO (MD_MAJOR, 1)
-#define START_MD     		_IO (MD_MAJOR, 2)
-#define STOP_MD      		_IO (MD_MAJOR, 3)
-#define REGISTER_DEV_NEW	_IO (MD_MAJOR, 4)
-
-/*
-   personalities :
-   Byte 0 : Chunk size factor
-   Byte 1 : Fault tolerance count for each physical device
-            (   0 means no fault tolerance,
-             0xFF means always tolerate faults), not used by now.
-   Byte 2 : Personality
-   Byte 3 : Reserved.
- */
-
-#define FAULT_SHIFT       8
-#define PERSONALITY_SHIFT 16
-
-#define FACTOR_MASK       0x000000FFUL
-#define FAULT_MASK        0x0000FF00UL
-#define PERSONALITY_MASK  0x00FF0000UL
-
-#define MD_RESERVED       0	/* Not used by now */
-#define LINEAR            (1UL << PERSONALITY_SHIFT)
-#define STRIPED           (2UL << PERSONALITY_SHIFT)
-#define RAID0             STRIPED
-#define RAID1             (3UL << PERSONALITY_SHIFT)
-#define RAID5             (4UL << PERSONALITY_SHIFT)
-#define MAX_PERSONALITY   5
-
-/*
- * MD superblock.
- *
- * The MD superblock maintains some statistics on each MD configuration.
- * Each real device in the MD set contains it near the end of the device.
- * Some of the ideas are copied from the ext2fs implementation.
- *
- * We currently use 4096 bytes as follows:
- *
- *	word offset	function
- *
- *	   0  -    31	Constant generic MD device information.
- *        32  -    63   Generic state information.
- *	  64  -   127	Personality specific information.
- *	 128  -   511	12 32-words descriptors of the disks in the raid set.
- *	 512  -   911	Reserved.
- *	 912  -  1023	Disk specific descriptor.
- */
-
-/*
- * If x is the real device size in bytes, we return an apparent size of:
- *
- *	y = (x & ~(MD_RESERVED_BYTES - 1)) - MD_RESERVED_BYTES
- *
- * and place the 4kB superblock at offset y.
- */
-#define MD_RESERVED_BYTES		(64 * 1024)
-#define MD_RESERVED_SECTORS		(MD_RESERVED_BYTES / 512)
-#define MD_RESERVED_BLOCKS		(MD_RESERVED_BYTES / BLOCK_SIZE)
-
-#define MD_NEW_SIZE_SECTORS(x)		((x & ~(MD_RESERVED_SECTORS - 1)) - MD_RESERVED_SECTORS)
-#define MD_NEW_SIZE_BLOCKS(x)		((x & ~(MD_RESERVED_BLOCKS - 1)) - MD_RESERVED_BLOCKS)
-
-#define MD_SB_BYTES			4096
-#define MD_SB_WORDS			(MD_SB_BYTES / 4)
-#define MD_SB_BLOCKS			(MD_SB_BYTES / BLOCK_SIZE)
-#define MD_SB_SECTORS			(MD_SB_BYTES / 512)
-
-/*
- * The following are counted in 32-bit words
- */
-#define	MD_SB_GENERIC_OFFSET		0
-#define MD_SB_PERSONALITY_OFFSET	64
-#define MD_SB_DISKS_OFFSET		128
-#define MD_SB_DESCRIPTOR_OFFSET		992
-
-#define MD_SB_GENERIC_CONSTANT_WORDS	32
-#define MD_SB_GENERIC_STATE_WORDS	32
-#define MD_SB_GENERIC_WORDS		(MD_SB_GENERIC_CONSTANT_WORDS + MD_SB_GENERIC_STATE_WORDS)
-#define MD_SB_PERSONALITY_WORDS		64
-#define MD_SB_DISKS_WORDS		384
-#define MD_SB_DESCRIPTOR_WORDS		32
-#define MD_SB_RESERVED_WORDS		(1024 - MD_SB_GENERIC_WORDS - MD_SB_PERSONALITY_WORDS - MD_SB_DISKS_WORDS - MD_SB_DESCRIPTOR_WORDS)
-#define MD_SB_EQUAL_WORDS		(MD_SB_GENERIC_WORDS + MD_SB_PERSONALITY_WORDS + MD_SB_DISKS_WORDS)
-#define MD_SB_DISKS			(MD_SB_DISKS_WORDS / MD_SB_DESCRIPTOR_WORDS)
-
-/*
- * Device "operational" state bits
- */
-#define MD_FAULTY_DEVICE		0	/* Device is faulty / operational */
-#define MD_ACTIVE_DEVICE		1	/* Device is a part or the raid set / spare disk */
-#define MD_SYNC_DEVICE			2	/* Device is in sync with the raid set */
-
-typedef struct md_device_descriptor_s {
-	__u32 number;		/* 0 Device number in the entire set */
-	__u32 major;		/* 1 Device major number */
-	__u32 minor;		/* 2 Device minor number */
-	__u32 raid_disk;	/* 3 The role of the device in the raid set */
-	__u32 state;		/* 4 Operational state */
-	__u32 reserved[MD_SB_DESCRIPTOR_WORDS - 5];
-} md_descriptor_t;
-
-#define MD_SB_MAGIC		0xa92b4efc
-
-/*
- * Superblock state bits
- */
-#define MD_SB_CLEAN		0
-#define MD_SB_ERRORS		1
-
-typedef struct md_superblock_s {
-
-	/*
-	 * Constant generic information
-	 */
-	__u32 md_magic;		/*  0 MD identifier */
-	__u32 major_version;	/*  1 major version to which the set conforms */
-	__u32 minor_version;	/*  2 minor version to which the set conforms */
-	__u32 patch_version;	/*  3 patchlevel version to which the set conforms */
-	__u32 gvalid_words;	/*  4 Number of non-reserved words in this section */
-	__u32 set_magic;	/*  5 Raid set identifier */
-	__u32 ctime;		/*  6 Creation time */
-	__u32 level;		/*  7 Raid personality (mirroring, raid5, ...) */
-	__u32 size;		/*  8 Apparent size of each individual disk, in kB */
-	__u32 nr_disks;		/*  9 Number of total disks in the raid set */
-	__u32 raid_disks;	/* 10 Number of disks in a fully functional raid set */
-	__u32 gstate_creserved[MD_SB_GENERIC_CONSTANT_WORDS - 11];
-
-	/*
-	 * Generic state information
-	 */
-	__u32 utime;		/*  0 Superblock update time */
-	__u32 state;		/*  1 State bits (clean, ...) */
-	__u32 active_disks;	/*  2 Number of currently active disks (some non-faulty disks might not be in sync) */
-	__u32 working_disks;	/*  3 Number of working disks */
-	__u32 failed_disks;	/*  4 Number of failed disks */
-	__u32 spare_disks;	/*  5 Number of spare disks */
-	__u32 gstate_sreserved[MD_SB_GENERIC_STATE_WORDS - 6];
-
-	/*
-	 * Personality information
-	 */
-	__u32 parity_algorithm;
-	__u32 chunk_size;
-	__u32 pstate_reserved[MD_SB_PERSONALITY_WORDS - 2];
-
-	/*
-	 * Disks information
-	 */
-	md_descriptor_t disks[MD_SB_DISKS];
-
-	/*
-	 * Reserved
-	 */
-	__u32 reserved[MD_SB_RESERVED_WORDS];
-
-	/*
-	 * Active descriptor
-	 */
-	md_descriptor_t descriptor;
-} md_superblock_t;
-
-#ifdef __KERNEL__
-
-#include <linux/mm.h>
-#include <linux/fs.h>
-#include <linux/blkdev.h>
-#include <asm/semaphore.h>
-
-/*
- * Kernel-based reconstruction is mostly working, but still requires
- * some additional work.
- */
-#define SUPPORT_RECONSTRUCTION	0
-
-#define MAX_REAL     8		/* Max number of physical dev per md dev */
-#define MAX_MD_DEV   4		/* Max number of md dev */
-
-#define FACTOR(a)         ((a)->repartition & FACTOR_MASK)
-#define MAX_FAULT(a)      (((a)->repartition & FAULT_MASK)>>8)
-#define PERSONALITY(a)    ((a)->repartition & PERSONALITY_MASK)
-
-#define FACTOR_SHIFT(a) (PAGE_SHIFT + (a) - 10)
-
-struct real_dev
-{
-  kdev_t dev;			/* Device number */
-  int size;			/* Device size (in blocks) */
-  int offset;			/* Real device offset (in blocks) in md dev
-				   (only used in linear mode) */
-  struct inode *inode;		/* Lock inode */
-  md_superblock_t *sb;
-  u32 sb_offset;
-};
-
-struct md_dev;
-
-#define SPARE_INACTIVE	0
-#define SPARE_WRITE	1
-#define SPARE_ACTIVE	2
-
-struct md_personality
-{
-  char *name;
-  int (*map)(struct md_dev *mddev, kdev_t *rdev,
-	              unsigned long *rsector, unsigned long size);
-  int (*make_request)(struct md_dev *mddev, int rw, struct buffer_head * bh);
-  void (*end_request)(struct buffer_head * bh, int uptodate);
-  int (*run)(int minor, struct md_dev *mddev);
-  int (*stop)(int minor, struct md_dev *mddev);
-  int (*status)(char *page, int minor, struct md_dev *mddev);
-  int (*ioctl)(struct inode *inode, struct file *file,
-	       unsigned int cmd, unsigned long arg);
-  int max_invalid_dev;
-  int (*error_handler)(struct md_dev *mddev, kdev_t dev);
-
-/*
- * Some personalities (RAID-1, RAID-5) can get disks hot-added and
- * hot-removed. Hot removal is different from failure. (failure marks
- * a disk inactive, but the disk is still part of the array)
- */
-  int (*hot_add_disk) (struct md_dev *mddev, kdev_t dev);
-  int (*hot_remove_disk) (struct md_dev *mddev, kdev_t dev);
-  int (*mark_spare) (struct md_dev *mddev, md_descriptor_t *descriptor, int state);
-};
-
-struct md_dev
-{
-  struct real_dev	devices[MAX_REAL];
-  struct md_personality	*pers;
-  md_superblock_t	*sb;
-  int			sb_dirty;
-  int			repartition;
-  int			busy;
-  int			nb_dev;
-  void			*private;
-};
-
-struct md_thread {
-	void			(*run) (void *data);
-	void			*data;
-	struct wait_queue	*wqueue;
-	unsigned long           flags;
-	struct semaphore	*sem;
-	struct task_struct	*tsk;
-};
-
-#define THREAD_WAKEUP  0
-
-extern struct md_dev md_dev[MAX_MD_DEV];
-extern int md_size[MAX_MD_DEV];
-extern int md_maxreadahead[MAX_MD_DEV];
-
-extern char *partition_name (kdev_t dev);
-
-extern int register_md_personality (int p_num, struct md_personality *p);
-extern int unregister_md_personality (int p_num);
-extern struct md_thread *md_register_thread (void (*run) (void *data), void *data);
-extern void md_unregister_thread (struct md_thread *thread);
-extern void md_wakeup_thread(struct md_thread *thread);
-extern int md_update_sb (int minor);
-extern int md_do_sync(struct md_dev *mddev);
-
-#endif __KERNEL__
-#endif _MD_H
diff -urN linux/include/linux/raid/hsm.h /tmp/linux/include/linux/raid/hsm.h
--- linux/include/linux/raid/hsm.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/include/linux/raid/hsm.h	Fri Feb  2 19:06:00 2001
@@ -0,0 +1,65 @@
+#ifndef _HSM_H
+#define _HSM_H
+
+#include <linux/raid/md.h>
+
+#if __alpha__
+#error fix cpu_addr on Alpha first
+#endif
+
+#include <linux/raid/hsm_p.h>
+
+#define index_pv(lv,index) ((lv)->vg->pv_array+(index)->data.phys_nr)
+#define index_dev(lv,index) index_pv((lv),(index))->dev
+#define index_block(lv,index) (index)->data.phys_block
+#define index_child(index) ((lv_lptr_t *)((index)->cpu_addr))
+
+#define ptr_to_cpuaddr(ptr) ((__u32) (ptr))
+
+
+typedef struct pv_bg_desc_s {
+	unsigned int		free_blocks;
+	pv_block_group_t 	*bg;
+} pv_bg_desc_t;
+
+typedef struct pv_s pv_t;
+typedef struct vg_s vg_t;
+typedef struct lv_s lv_t;
+
+struct pv_s
+{
+	int			phys_nr;
+	kdev_t			dev;
+	pv_sb_t			*pv_sb;
+	pv_bg_desc_t	 	*bg_array;
+};
+
+struct lv_s
+{
+	int		log_id;
+	vg_t		*vg;
+
+	unsigned int	max_indices;
+	unsigned int	free_indices;
+	lv_lptr_t	root_index;
+
+	kdev_t		dev;
+};
+
+struct vg_s
+{
+	int		nr_pv;
+	pv_t		pv_array [MD_SB_DISKS];
+
+	int		nr_lv;
+	lv_t		lv_array [HSM_MAX_LVS_PER_VG];
+
+	vg_sb_t		*vg_sb;
+	mddev_t		*mddev;
+};
+
+#define kdev_to_lv(dev) ((lv_t *) mddev_map[MINOR(dev)].data)
+#define mddev_to_vg(mddev) ((vg_t *) mddev->private)
+
+#endif
+
diff -urN linux/include/linux/raid/hsm_p.h /tmp/linux/include/linux/raid/hsm_p.h
--- linux/include/linux/raid/hsm_p.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/include/linux/raid/hsm_p.h	Fri Feb  2 19:06:00 2001
@@ -0,0 +1,237 @@
+#ifndef _HSM_P_H
+#define _HSM_P_H
+
+#define HSM_BLOCKSIZE 4096
+#define HSM_BLOCKSIZE_WORDS (HSM_BLOCKSIZE/4)
+#define PACKED __attribute__ ((packed))
+
+/*
+ * Identifies a block in physical space
+ */
+typedef struct phys_idx_s {
+	__u16 phys_nr;
+	__u32 phys_block;
+
+} PACKED phys_idx_t;
+
+/*
+ * Identifies a block in logical space
+ */
+typedef struct log_idx_s {
+	__u16 log_id;
+	__u32 log_index;
+
+} PACKED log_idx_t;
+
+/*
+ * Describes one PV
+ */
+#define HSM_PV_SB_MAGIC          0xf091ae9fU
+
+#define HSM_PV_SB_GENERIC_WORDS 32
+#define HSM_PV_SB_RESERVED_WORDS \
+		(HSM_BLOCKSIZE_WORDS - HSM_PV_SB_GENERIC_WORDS)
+
+/*
+ * On-disk PV identification data, on block 0 in any PV.
+ */
+typedef struct pv_sb_s
+{
+	__u32 pv_magic;		/*  0 		 			    */
+
+	__u32 pv_uuid0;		/*  1 					    */
+	__u32 pv_uuid1;		/*  2		 			    */
+	__u32 pv_uuid2;		/*  3 					    */
+	__u32 pv_uuid3;		/*  4 					    */
+
+	__u32 pv_major;		/*  5  					    */
+	__u32 pv_minor;		/*  6  					    */
+	__u32 pv_patch;		/*  7 					    */
+
+	__u32 pv_ctime;		/*  8 Creation time			    */
+
+	__u32 pv_total_size;	/*  9 size of this PV, in blocks	    */
+	__u32 pv_first_free;	/*  10 first free block			    */
+	__u32 pv_first_used;	/*  11 first used block			    */
+	__u32 pv_blocks_left;	/*  12 unallocated blocks		    */
+	__u32 pv_bg_size;	/*  13 size of a block group, in blocks	    */
+	__u32 pv_block_size;	/*  14 size of blocks, in bytes		    */
+	__u32 pv_pptr_size;	/*  15 size of block descriptor, in bytes   */
+	__u32 pv_block_groups;	/*  16 number of block groups		    */
+
+	__u32 __reserved1[HSM_PV_SB_GENERIC_WORDS - 17];
+
+	/*
+	 * Reserved
+	 */
+	__u32 __reserved2[HSM_PV_SB_RESERVED_WORDS];
+
+} PACKED pv_sb_t;
+
+/*
+ * this is pretty much arbitrary, but has to be less than ~64
+ */
+#define HSM_MAX_LVS_PER_VG 32
+
+#define HSM_VG_SB_GENERIC_WORDS 32
+
+#define LV_DESCRIPTOR_WORDS 8
+#define HSM_VG_SB_RESERVED_WORDS (HSM_BLOCKSIZE_WORDS - \
+	LV_DESCRIPTOR_WORDS*HSM_MAX_LVS_PER_VG - HSM_VG_SB_GENERIC_WORDS)
+
+#if (HSM_PV_SB_RESERVED_WORDS < 0)
+#error you messed this one up dude ...
+#endif
+
+typedef struct lv_descriptor_s
+{
+	__u32 lv_id;		/*  0 					    */
+	phys_idx_t lv_root_idx; /*  1					    */
+	__u16 __reserved;	/*  2 					    */
+	__u32 lv_max_indices;	/*  3 					    */
+	__u32 lv_free_indices;	/*  4 					    */
+	__u32 md_id;		/*  5 					    */
+
+	__u32 reserved[LV_DESCRIPTOR_WORDS - 6];
+
+} PACKED lv_descriptor_t;
+
+#define HSM_VG_SB_MAGIC          0x98320d7aU
+/*
+ * On-disk VG identification data, in block 1 on all PVs
+ */
+typedef struct vg_sb_s
+{
+	__u32 vg_magic;		/*  0 		 			    */
+	__u32 nr_lvs;		/*  1					    */
+
+	__u32 __reserved1[HSM_VG_SB_GENERIC_WORDS - 2];
+
+	lv_descriptor_t lv_array [HSM_MAX_LVS_PER_VG];
+	/*
+	 * Reserved
+	 */
+	__u32 __reserved2[HSM_VG_SB_RESERVED_WORDS];
+
+} PACKED vg_sb_t;
+
+/*
+ * Describes one LV
+ */
+
+#define HSM_LV_SB_MAGIC          0xe182bd8aU
+
+/* do we need lv_sb_t? */
+
+typedef struct lv_sb_s
+{
+	/*
+	 * On-disk LV identifier
+	 */
+	__u32 lv_magic;		/*  0 LV identifier 			    */
+	__u32 lv_uuid0;		/*  1 					    */
+	__u32 lv_uuid1;		/*  2		 			    */
+	__u32 lv_uuid2;		/*  3 					    */
+	__u32 lv_uuid3;		/*  4 					    */
+
+	__u32 lv_major;		/*  5 PV identifier 			    */
+	__u32 lv_minor;		/*  6 PV identifier 			    */
+	__u32 lv_patch;		/*  7 PV identifier 			    */
+
+	__u32 ctime;		/*  8 Creation time			    */
+	__u32 size;		/*  9 size of this LV, in blocks	    */
+	phys_idx_t start;	/*  10 position of root index block	    */
+	log_idx_t first_free;	/*  11-12 first free index		    */
+
+	/*
+	 * Reserved
+	 */
+	__u32 reserved[HSM_BLOCKSIZE_WORDS-13];
+
+} PACKED lv_sb_t;
+
+/*
+ * Pointer pointing from the physical space, points to
+ * the LV owning this block. It also contains various
+ * statistics about the physical block.
+ */
+typedef struct pv_pptr_s
+{
+	union {
+	/* case 1 */
+		struct {
+			log_idx_t owner;
+			log_idx_t predicted;
+			__u32 last_referenced;
+		} used;
+	/* case 2 */
+		struct {
+			__u16 log_id;
+			__u16 __unused1;
+			__u32 next_free;
+			__u32 __unused2;
+			__u32 __unused3;
+		} free;
+	} u;
+} PACKED pv_pptr_t;
+
+static __inline__ int pv_pptr_free (const pv_pptr_t * pptr)
+{
+	return !pptr->u.free.log_id;
+}
+
+
+#define DATA_BLOCKS_PER_BG ((HSM_BLOCKSIZE*8)/(8*sizeof(pv_pptr_t)+1))
+
+#define TOTAL_BLOCKS_PER_BG (DATA_BLOCKS_PER_BG+1)
+/*
+ * A table of pointers filling up a single block, managing
+ * the next DATA_BLOCKS_PER_BG physical blocks. Such block
+ * groups form the physical space of blocks.
+ */
+typedef struct pv_block_group_s
+{
+	__u8 used_bitmap[(DATA_BLOCKS_PER_BG+7)/8];
+
+	pv_pptr_t blocks[DATA_BLOCKS_PER_BG];
+
+} PACKED pv_block_group_t;
+
+/*
+ * Pointer from the logical space, points to
+ * the (PV,block) containing this logical block
+ */
+typedef struct lv_lptr_s
+{
+	phys_idx_t data;
+	__u16 __reserved;
+	__u32 cpu_addr;
+	__u32 __reserved2;
+
+} PACKED lv_lptr_t;
+
+static __inline__ int index_free (const lv_lptr_t * index)
+{
+	return !index->data.phys_block;
+}
+
+static __inline__ int index_present (const lv_lptr_t * index)
+{
+	return index->cpu_addr;
+}
+
+
+#define HSM_LPTRS_PER_BLOCK (HSM_BLOCKSIZE/sizeof(lv_lptr_t))
+/*
+ * A table of pointers filling up a single block, managing
+ * HSM_LPTRS_PER_BLOCK logical blocks. Such block groups form
+ * the logical space of blocks.
+ */
+typedef struct lv_index_block_s
+{
+	lv_lptr_t blocks[HSM_LPTRS_PER_BLOCK];
+
+} PACKED lv_index_block_t;
+
+#endif
+
diff -urN linux/include/linux/raid/linear.h /tmp/linux/include/linux/raid/linear.h
--- linux/include/linux/raid/linear.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/include/linux/raid/linear.h	Mon Jul  9 20:34:06 2001
@@ -0,0 +1,32 @@
+#ifndef _LINEAR_H
+#define _LINEAR_H
+
+#include <linux/raid/md.h>
+
+struct dev_info {
+	kdev_t		dev;
+	int		size;
+	unsigned int	offset;
+};
+
+typedef struct dev_info dev_info_t;
+
+struct linear_hash
+{
+	dev_info_t *dev0, *dev1;
+};
+
+struct linear_private_data
+{
+	struct linear_hash	*hash_table;
+	dev_info_t		disks[MD_SB_DISKS];
+	dev_info_t		*smallest;
+	int			nr_zones;
+};
+
+
+typedef struct linear_private_data linear_conf_t;
+
+#define mddev_to_conf(mddev) ((linear_conf_t *) mddev->private)
+
+#endif
diff -urN linux/include/linux/raid/md.h /tmp/linux/include/linux/raid/md.h
--- linux/include/linux/raid/md.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/include/linux/raid/md.h	Mon Jul  9 20:33:42 2001
@@ -0,0 +1,96 @@
+/*
+   md.h : Multiple Devices driver for Linux
+          Copyright (C) 1996-98 Ingo Molnar, Gadi Oxman
+          Copyright (C) 1994-96 Marc ZYNGIER
+	  <zyngier@ufr-info-p7.ibp.fr> or
+	  <maz@gloups.fdn.fr>
+	  
+   This program is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 2, or (at your option)
+   any later version.
+   
+   You should have received a copy of the GNU General Public License
+   (for example /usr/src/linux/COPYING); if not, write to the Free
+   Software Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.  
+*/
+
+#ifndef _MD_H
+#define _MD_H
+
+#include <linux/mm.h>
+#include <linux/fs.h>
+#include <linux/blkdev.h>
+#include <asm/semaphore.h>
+#include <linux/major.h>
+#include <linux/ioctl.h>
+#include <linux/types.h>
+#include <asm/bitops.h>
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <linux/hdreg.h>
+#include <linux/sysctl.h>
+#include <linux/fs.h>
+#include <linux/proc_fs.h>
+#include <linux/smp_lock.h>
+#include <linux/delay.h>
+#include <net/checksum.h>
+#include <linux/random.h>
+#include <linux/locks.h>
+#include <asm/io.h>
+
+#include <linux/raid/md_compatible.h>
+/*
+ * 'md_p.h' holds the 'physical' layout of RAID devices
+ * 'md_u.h' holds the user <=> kernel API
+ *
+ * 'md_k.h' holds kernel internal definitions
+ */
+
+#include <linux/raid/md_p.h>
+#include <linux/raid/md_u.h>
+#include <linux/raid/md_k.h>
+
+/*
+ * Different major versions are not compatible.
+ * Different minor versions are only downward compatible.
+ * Different patchlevel versions are downward and upward compatible.
+ */
+#define MD_MAJOR_VERSION                0
+#define MD_MINOR_VERSION                90
+#define MD_PATCHLEVEL_VERSION           0
+
+extern int md_size[MAX_MD_DEVS];
+extern struct hd_struct md_hd_struct[MAX_MD_DEVS];
+
+extern void add_mddev_mapping (mddev_t *mddev, kdev_t dev, void *data);
+extern void del_mddev_mapping (mddev_t *mddev, kdev_t dev);
+extern char * partition_name (kdev_t dev);
+extern int register_md_personality (int p_num, mdk_personality_t *p);
+extern int unregister_md_personality (int p_num);
+extern mdk_thread_t * md_register_thread (void (*run) (void *data),
+				void *data, const char *name);
+extern void md_unregister_thread (mdk_thread_t *thread);
+extern void md_wakeup_thread(mdk_thread_t *thread);
+extern void md_interrupt_thread (mdk_thread_t *thread);
+extern int md_update_sb (mddev_t *mddev);
+extern int md_do_sync(mddev_t *mddev, mdp_disk_t *spare);
+extern void md_recover_arrays (void);
+extern int md_check_ordering (mddev_t *mddev);
+extern void autodetect_raid(void);
+extern struct gendisk * find_gendisk (kdev_t dev);
+extern int md_notify_reboot(struct notifier_block *this,
+					unsigned long code, void *x);
+#if CONFIG_BLK_DEV_MD
+extern void raid_setup(char *str,int *ints) md__init;
+#endif
+#ifdef CONFIG_MD_BOOT
+extern void md_setup(char *str,int *ints) md__init;
+#endif
+
+extern void md_print_devices (void);
+
+#define MD_BUG(x...) { printk("md: bug in file %s, line %d\n", __FILE__, __LINE__); md_print_devices(); }
+
+#endif _MD_H
+
diff -urN linux/include/linux/raid/md_compatible.h /tmp/linux/include/linux/raid/md_compatible.h
--- linux/include/linux/raid/md_compatible.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/include/linux/raid/md_compatible.h	Mon Jul  9 20:33:42 2001
@@ -0,0 +1,387 @@
+
+/*
+   md.h : Multiple Devices driver compatibility layer for Linux 2.0/2.2
+          Copyright (C) 1998 Ingo Molnar
+	  
+   This program is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 2, or (at your option)
+   any later version.
+   
+   You should have received a copy of the GNU General Public License
+   (for example /usr/src/linux/COPYING); if not, write to the Free
+   Software Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.  
+*/
+
+#include <linux/version.h>
+
+#ifndef _MD_COMPATIBLE_H
+#define _MD_COMPATIBLE_H
+
+#define LinuxVersionCode(v, p, s) (((v)<<16)+((p)<<8)+(s))
+
+#if LINUX_VERSION_CODE < LinuxVersionCode(2,1,0)
+
+/* 000 */
+#define md__get_free_pages(x,y) __get_free_pages(x,y,GFP_KERNEL)
+
+#ifdef __i386__
+/* 001 */
+extern __inline__ int md_cpu_has_mmx(void)
+{
+	return x86_capability & 0x00800000;
+}
+#endif
+
+/* 002 */
+#define md_clear_page(page)        memset((void *)(page), 0, PAGE_SIZE)
+
+/* 003 */
+/*
+ * someone please suggest a sane compatibility layer for modules
+ */
+#define MD_EXPORT_SYMBOL(x)
+
+/* 004 */
+static inline unsigned long
+md_copy_from_user(void *to, const void *from, unsigned long n)
+{
+	int err;
+
+	err = verify_area(VERIFY_READ,from,n);
+	if (!err)
+		memcpy_fromfs(to, from, n);
+	return err; 
+}
+
+/* 005 */
+extern inline unsigned long
+md_copy_to_user(void *to, const void *from, unsigned long n)
+{
+	int err;
+
+	err = verify_area(VERIFY_WRITE,to,n);
+	if (!err)
+		memcpy_tofs(to, from, n);
+	return err; 
+}
+
+/* 006 */
+#define md_put_user(x,ptr)						\
+({									\
+	int __err;							\
+									\
+	__err = verify_area(VERIFY_WRITE,ptr,sizeof(*ptr));		\
+	if (!__err)							\
+		put_user(x,ptr);					\
+	__err;								\
+})
+
+/* 007 */
+extern inline int md_capable_admin(void)
+{
+	return suser();
+}
+ 
+/* 008 */
+#define MD_FILE_TO_INODE(file) ((file)->f_inode)
+
+/* 009 */
+extern inline void md_flush_signals (void)
+{
+	current->signal = 0;
+}
+ 
+/* 010 */
+#define __S(nr) (1<<((nr)-1))
+extern inline void md_init_signals (void)
+{
+        current->exit_signal = SIGCHLD;
+        current->blocked = ~(__S(SIGKILL));
+}
+#undef __S
+
+/* 011 */
+extern inline unsigned long md_signal_pending (struct task_struct * tsk)
+{
+	return (tsk->signal & ~tsk->blocked);
+}
+
+/* 012 */
+#define md_set_global_readahead(x) read_ahead[MD_MAJOR] = MD_READAHEAD
+
+/* 013 */
+#define md_mdelay(n) (\
+	{unsigned long msec=(n); while (msec--) udelay(1000);})
+
+/* 014 */
+#define MD_SYS_DOWN 0
+#define MD_SYS_HALT 0
+#define MD_SYS_POWER_OFF 0
+
+/* 015 */
+#define md_register_reboot_notifier(x)
+
+/* 016 */
+extern __inline__ unsigned long
+md_test_and_set_bit(int nr, void * addr)
+{
+	unsigned long flags;
+	unsigned long oldbit;
+
+	save_flags(flags);
+	cli();
+	oldbit = test_bit(nr,addr);
+	set_bit(nr,addr);
+	restore_flags(flags);
+	return oldbit;
+}
+
+/* 017 */
+extern __inline__ unsigned long
+md_test_and_clear_bit(int nr, void * addr)
+{
+	unsigned long flags;
+	unsigned long oldbit;
+
+	save_flags(flags);
+	cli();
+	oldbit = test_bit(nr,addr);
+	clear_bit(nr,addr);
+	restore_flags(flags);
+	return oldbit;
+}
+
+/* 018 */
+#define md_atomic_read(x) (*(volatile int *)(x))
+#define md_atomic_set(x,y) (*(volatile int *)(x) = (y))
+
+/* 019 */
+extern __inline__ void md_lock_kernel (void)
+{
+#if __SMP__
+	lock_kernel();
+	syscall_count++;
+#endif
+}
+
+extern __inline__ void md_unlock_kernel (void)
+{
+#if __SMP__
+	syscall_count--;
+	unlock_kernel();
+#endif
+}
+/* 020 */
+
+#define md__init
+#define md__initdata
+#define md__initfunc(__arginit) __arginit
+
+/* 021 */
+
+/* 022 */
+
+struct md_list_head {
+	struct md_list_head *next, *prev;
+};
+
+#define MD_LIST_HEAD(name) \
+	struct md_list_head name = { &name, &name }
+
+#define MD_INIT_LIST_HEAD(ptr) do { \
+	(ptr)->next = (ptr); (ptr)->prev = (ptr); \
+} while (0)
+
+static __inline__ void md__list_add(struct md_list_head * new,
+	struct md_list_head * prev,
+	struct md_list_head * next)
+{
+	next->prev = new;
+	new->next = next;
+	new->prev = prev;
+	prev->next = new;
+}
+
+static __inline__ void md_list_add(struct md_list_head *new,
+						struct md_list_head *head)
+{
+	md__list_add(new, head, head->next);
+}
+
+static __inline__ void md__list_del(struct md_list_head * prev,
+					struct md_list_head * next)
+{
+	next->prev = prev;
+	prev->next = next;
+}
+
+static __inline__ void md_list_del(struct md_list_head *entry)
+{
+	md__list_del(entry->prev, entry->next);
+}
+
+static __inline__ int md_list_empty(struct md_list_head *head)
+{
+	return head->next == head;
+}
+
+#define md_list_entry(ptr, type, member) \
+	((type *)((char *)(ptr)-(unsigned long)(&((type *)0)->member)))
+
+/* 023 */
+
+static __inline__ signed long md_schedule_timeout(signed long timeout)
+{
+	current->timeout = jiffies + timeout;
+	schedule();
+	return 0;
+}
+
+/* 024 */
+#define md_need_resched(tsk) (need_resched)
+
+/* 025 */
+typedef struct { int gcc_is_buggy; } md_spinlock_t;
+#define MD_SPIN_LOCK_UNLOCKED (md_spinlock_t) { 0 }
+
+#define md_spin_lock_irq cli
+#define md_spin_unlock_irq sti
+#define md_spin_unlock_irqrestore(x,flags) restore_flags(flags)
+#define md_spin_lock_irqsave(x,flags) do { save_flags(flags); cli(); } while (0)
+
+/* END */
+
+#else
+
+#include <linux/reboot.h>
+#include <linux/vmalloc.h>
+
+/* 000 */
+#define md__get_free_pages(x,y) __get_free_pages(x,y)
+
+#ifdef __i386__
+/* 001 */
+extern __inline__ int md_cpu_has_mmx(void)
+{
+	return boot_cpu_data.x86_capability & X86_FEATURE_MMX;
+}
+#endif
+
+/* 002 */
+#define md_clear_page(page)        clear_page(page)
+
+/* 003 */
+#define MD_EXPORT_SYMBOL(x) EXPORT_SYMBOL(x)
+
+/* 004 */
+#define md_copy_to_user(x,y,z) copy_to_user(x,y,z)
+
+/* 005 */
+#define md_copy_from_user(x,y,z) copy_from_user(x,y,z)
+
+/* 006 */
+#define md_put_user put_user
+
+/* 007 */
+extern inline int md_capable_admin(void)
+{
+	return capable(CAP_SYS_ADMIN);
+}
+
+/* 008 */
+#define MD_FILE_TO_INODE(file) ((file)->f_dentry->d_inode)
+
+/* 009 */
+extern inline void md_flush_signals (void)
+{
+	spin_lock(&current->sigmask_lock);
+	flush_signals(current);
+	spin_unlock(&current->sigmask_lock);
+}
+ 
+/* 010 */
+extern inline void md_init_signals (void)
+{
+        current->exit_signal = SIGCHLD;
+        siginitsetinv(&current->blocked, sigmask(SIGKILL));
+}
+
+/* 011 */
+#define md_signal_pending signal_pending
+
+/* 012 */
+extern inline void md_set_global_readahead(int * table)
+{
+	max_readahead[MD_MAJOR] = table;
+}
+
+/* 013 */
+#define md_mdelay(x) mdelay(x)
+
+/* 014 */
+#define MD_SYS_DOWN SYS_DOWN
+#define MD_SYS_HALT SYS_HALT
+#define MD_SYS_POWER_OFF SYS_POWER_OFF
+
+/* 015 */
+#define md_register_reboot_notifier register_reboot_notifier
+
+/* 016 */
+#define md_test_and_set_bit test_and_set_bit
+
+/* 017 */
+#define md_test_and_clear_bit test_and_clear_bit
+
+/* 018 */
+#define md_atomic_read atomic_read
+#define md_atomic_set atomic_set
+
+/* 019 */
+#define md_lock_kernel lock_kernel
+#define md_unlock_kernel unlock_kernel
+
+/* 020 */
+
+#include <linux/init.h>
+
+#define md__init __init
+#define md__initdata __initdata
+#define md__initfunc(__arginit) __initfunc(__arginit)
+
+/* 021 */
+
+
+/* 022 */
+
+#define md_list_head list_head
+#define MD_LIST_HEAD(name) LIST_HEAD(name)
+#define MD_INIT_LIST_HEAD(ptr) INIT_LIST_HEAD(ptr)
+#define md_list_add list_add
+#define md_list_del list_del
+#define md_list_empty list_empty
+
+#define md_list_entry(ptr, type, member) list_entry(ptr, type, member)
+
+/* 023 */
+
+#define md_schedule_timeout schedule_timeout
+
+/* 024 */
+#define md_need_resched(tsk) ((tsk)->need_resched)
+
+/* 025 */
+#define md_spinlock_t spinlock_t
+#define MD_SPIN_LOCK_UNLOCKED SPIN_LOCK_UNLOCKED
+
+#define md_spin_lock_irq spin_lock_irq
+#define md_spin_unlock_irq spin_unlock_irq
+#define md_spin_unlock_irqrestore spin_unlock_irqrestore
+#define md_spin_lock_irqsave spin_lock_irqsave
+
+/* END */
+
+#endif
+
+#endif _MD_COMPATIBLE_H
+
diff -urN linux/include/linux/raid/md_k.h /tmp/linux/include/linux/raid/md_k.h
--- linux/include/linux/raid/md_k.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/include/linux/raid/md_k.h	Fri Feb  2 19:06:00 2001
@@ -0,0 +1,338 @@
+/*
+   md_k.h : kernel internal structure of the Linux MD driver
+          Copyright (C) 1996-98 Ingo Molnar, Gadi Oxman
+	  
+   This program is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 2, or (at your option)
+   any later version.
+   
+   You should have received a copy of the GNU General Public License
+   (for example /usr/src/linux/COPYING); if not, write to the Free
+   Software Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.  
+*/
+
+#ifndef _MD_K_H
+#define _MD_K_H
+
+#define MD_RESERVED       0UL
+#define LINEAR            1UL
+#define STRIPED           2UL
+#define RAID0             STRIPED
+#define RAID1             3UL
+#define RAID5             4UL
+#define TRANSLUCENT       5UL
+#define HSM               6UL
+#define MAX_PERSONALITY   7UL
+
+extern inline int pers_to_level (int pers)
+{
+	switch (pers) {
+		case HSM:		return -3;
+		case TRANSLUCENT:	return -2;
+		case LINEAR:		return -1;
+		case RAID0:		return 0;
+		case RAID1:		return 1;
+		case RAID5:		return 5;
+	}
+	panic("pers_to_level()");
+}
+
+extern inline int level_to_pers (int level)
+{
+	switch (level) {
+		case -3: return HSM;
+		case -2: return TRANSLUCENT;
+		case -1: return LINEAR;
+		case 0: return RAID0;
+		case 1: return RAID1;
+		case 4:
+		case 5: return RAID5;
+	}
+	return MD_RESERVED;
+}
+
+typedef struct mddev_s mddev_t;
+typedef struct mdk_rdev_s mdk_rdev_t;
+
+#if (MINORBITS != 8)
+#error MD doesnt handle bigger kdev yet
+#endif
+
+#define MAX_REAL     12			/* Max number of disks per md dev */
+#define MAX_MD_DEVS  (1<<MINORBITS)	/* Max number of md dev */
+
+/*
+ * Maps a kdev to an mddev/subdev. How 'data' is handled is up to
+ * the personality. (eg. HSM uses this to identify individual LVs)
+ */
+typedef struct dev_mapping_s {
+	mddev_t *mddev;
+	void *data;
+} dev_mapping_t;
+
+extern dev_mapping_t mddev_map [MAX_MD_DEVS];
+
+extern inline mddev_t * kdev_to_mddev (kdev_t dev)
+{
+        return mddev_map[MINOR(dev)].mddev;
+}
+
+/*
+ * options passed in raidrun:
+ */
+
+#define MAX_CHUNK_SIZE (4096*1024)
+
+/*
+ * default readahead
+ */
+#define MD_READAHEAD	(256 * 512)
+
+extern inline int disk_faulty(mdp_disk_t * d)
+{
+	return d->state & (1 << MD_DISK_FAULTY);
+}
+
+extern inline int disk_active(mdp_disk_t * d)
+{
+	return d->state & (1 << MD_DISK_ACTIVE);
+}
+
+extern inline int disk_sync(mdp_disk_t * d)
+{
+	return d->state & (1 << MD_DISK_SYNC);
+}
+
+extern inline int disk_spare(mdp_disk_t * d)
+{
+	return !disk_sync(d) && !disk_active(d) && !disk_faulty(d);
+}
+
+extern inline int disk_removed(mdp_disk_t * d)
+{
+	return d->state & (1 << MD_DISK_REMOVED);
+}
+
+extern inline void mark_disk_faulty(mdp_disk_t * d)
+{
+	d->state |= (1 << MD_DISK_FAULTY);
+}
+
+extern inline void mark_disk_active(mdp_disk_t * d)
+{
+	d->state |= (1 << MD_DISK_ACTIVE);
+}
+
+extern inline void mark_disk_sync(mdp_disk_t * d)
+{
+	d->state |= (1 << MD_DISK_SYNC);
+}
+
+extern inline void mark_disk_spare(mdp_disk_t * d)
+{
+	d->state = 0;
+}
+
+extern inline void mark_disk_removed(mdp_disk_t * d)
+{
+	d->state = (1 << MD_DISK_FAULTY) | (1 << MD_DISK_REMOVED);
+}
+
+extern inline void mark_disk_inactive(mdp_disk_t * d)
+{
+	d->state &= ~(1 << MD_DISK_ACTIVE);
+}
+
+extern inline void mark_disk_nonsync(mdp_disk_t * d)
+{
+	d->state &= ~(1 << MD_DISK_SYNC);
+}
+
+/*
+ * MD's 'extended' device
+ */
+struct mdk_rdev_s
+{
+	struct md_list_head same_set;	/* RAID devices within the same set */
+	struct md_list_head all;	/* all RAID devices */
+	struct md_list_head pending;	/* undetected RAID devices */
+
+	kdev_t dev;			/* Device number */
+	kdev_t old_dev;			/*  "" when it was last imported */
+	int size;			/* Device size (in blocks) */
+	mddev_t *mddev;			/* RAID array if running */
+	unsigned long last_events;	/* IO event timestamp */
+
+	struct inode *inode;		/* Lock inode */
+	struct file filp;		/* Lock file */
+
+	mdp_super_t *sb;
+	int sb_offset;
+
+	int faulty;			/* if faulty do not issue IO requests */
+	int desc_nr;			/* descriptor index in the superblock */
+};
+
+
+/*
+ * disk operations in a working array:
+ */
+#define DISKOP_SPARE_INACTIVE	0
+#define DISKOP_SPARE_WRITE	1
+#define DISKOP_SPARE_ACTIVE	2
+#define DISKOP_HOT_REMOVE_DISK	3
+#define DISKOP_HOT_ADD_DISK	4
+
+typedef struct mdk_personality_s mdk_personality_t;
+
+struct mddev_s
+{
+	void				*private;
+	mdk_personality_t		*pers;
+	int				__minor;
+	mdp_super_t			*sb;
+	int				nb_dev;
+	struct md_list_head 		disks;
+	int				sb_dirty;
+	mdu_param_t			param;
+	int				ro;
+	unsigned int			curr_resync;
+	unsigned long			resync_start;
+	char				*name;
+	int				recovery_running;
+	struct semaphore		reconfig_sem;
+	struct semaphore		recovery_sem;
+	struct semaphore		resync_sem;
+	struct md_list_head		all_mddevs;
+};
+
+struct mdk_personality_s
+{
+	char *name;
+	int (*map)(mddev_t *mddev, kdev_t dev, kdev_t *rdev,
+		unsigned long *rsector, unsigned long size);
+	int (*make_request)(mddev_t *mddev, int rw, struct buffer_head * bh);
+	void (*end_request)(struct buffer_head * bh, int uptodate);
+	int (*run)(mddev_t *mddev);
+	int (*stop)(mddev_t *mddev);
+	int (*status)(char *page, mddev_t *mddev);
+	int (*ioctl)(struct inode *inode, struct file *file,
+		unsigned int cmd, unsigned long arg);
+	int max_invalid_dev;
+	int (*error_handler)(mddev_t *mddev, kdev_t dev);
+
+/*
+ * Some personalities (RAID-1, RAID-5) can have disks hot-added and
+ * hot-removed. Hot removal is different from failure. (failure marks
+ * a disk inactive, but the disk is still part of the array) The interface
+ * to such operations is the 'pers->diskop()' function, can be NULL.
+ *
+ * the diskop function can change the pointer pointing to the incoming
+ * descriptor, but must do so very carefully. (currently only
+ * SPARE_ACTIVE expects such a change)
+ */
+	int (*diskop) (mddev_t *mddev, mdp_disk_t **descriptor, int state);
+
+	int (*stop_resync)(mddev_t *mddev);
+	int (*restart_resync)(mddev_t *mddev);
+};
+
+
+/*
+ * Currently we index md_array directly, based on the minor
+ * number. This will have to change to dynamic allocation
+ * once we start supporting partitioning of md devices.
+ */
+extern inline int mdidx (mddev_t * mddev)
+{
+	return mddev->__minor;
+}
+
+extern inline kdev_t mddev_to_kdev(mddev_t * mddev)
+{
+	return MKDEV(MD_MAJOR, mdidx(mddev));
+}
+
+extern mdk_rdev_t * find_rdev(mddev_t * mddev, kdev_t dev);
+extern mdk_rdev_t * find_rdev_nr(mddev_t *mddev, int nr);
+
+/*
+ * iterates through some rdev ringlist. It's safe to remove the
+ * current 'rdev'. Dont touch 'tmp' though.
+ */
+#define ITERATE_RDEV_GENERIC(head,field,rdev,tmp)			\
+									\
+	for (tmp = head.next;						\
+		rdev = md_list_entry(tmp, mdk_rdev_t, field),		\
+			tmp = tmp->next, tmp->prev != &head		\
+		; )
+/*
+ * iterates through the 'same array disks' ringlist
+ */
+#define ITERATE_RDEV(mddev,rdev,tmp)					\
+	ITERATE_RDEV_GENERIC((mddev)->disks,same_set,rdev,tmp)
+
+/*
+ * Same as above, but assumes that the device has rdev->desc_nr numbered
+ * from 0 to mddev->nb_dev, and iterates through rdevs in ascending order.
+ */
+#define ITERATE_RDEV_ORDERED(mddev,rdev,i)				\
+	for (i = 0; rdev = find_rdev_nr(mddev, i), i < mddev->nb_dev; i++)
+
+
+/*
+ * Iterates through all 'RAID managed disks'
+ */
+#define ITERATE_RDEV_ALL(rdev,tmp)					\
+	ITERATE_RDEV_GENERIC(all_raid_disks,all,rdev,tmp)
+
+/*
+ * Iterates through 'pending RAID disks'
+ */
+#define ITERATE_RDEV_PENDING(rdev,tmp)					\
+	ITERATE_RDEV_GENERIC(pending_raid_disks,pending,rdev,tmp)
+
+/*
+ * iterates through all used mddevs in the system.
+ */
+#define ITERATE_MDDEV(mddev,tmp)					\
+									\
+	for (tmp = all_mddevs.next;					\
+		mddev = md_list_entry(tmp, mddev_t, all_mddevs),	\
+			tmp = tmp->next, tmp->prev != &all_mddevs	\
+		; )
+
+extern inline int lock_mddev (mddev_t * mddev)
+{
+	return down_interruptible(&mddev->reconfig_sem);
+}
+
+extern inline void unlock_mddev (mddev_t * mddev)
+{
+	up(&mddev->reconfig_sem);
+}
+
+#define xchg_values(x,y) do { __typeof__(x) __tmp = x; \
+				x = y; y = __tmp; } while (0)
+
+typedef struct mdk_thread_s {
+	void			(*run) (void *data);
+	void			*data;
+	struct wait_queue	*wqueue;
+	unsigned long           flags;
+	struct semaphore	*sem;
+	struct task_struct	*tsk;
+	const char		*name;
+} mdk_thread_t;
+
+#define THREAD_WAKEUP  0
+
+typedef struct dev_name_s {
+	struct md_list_head list;
+	kdev_t dev;
+	char name [MAX_DISKNAME_LEN];
+} dev_name_t;
+
+#endif _MD_K_H
+
diff -urN linux/include/linux/raid/md_p.h /tmp/linux/include/linux/raid/md_p.h
--- linux/include/linux/raid/md_p.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/include/linux/raid/md_p.h	Fri Feb  2 19:06:00 2001
@@ -0,0 +1,161 @@
+/*
+   md_p.h : physical layout of Linux RAID devices
+          Copyright (C) 1996-98 Ingo Molnar, Gadi Oxman
+	  
+   This program is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 2, or (at your option)
+   any later version.
+   
+   You should have received a copy of the GNU General Public License
+   (for example /usr/src/linux/COPYING); if not, write to the Free
+   Software Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.  
+*/
+
+#ifndef _MD_P_H
+#define _MD_P_H
+
+/*
+ * RAID superblock.
+ *
+ * The RAID superblock maintains some statistics on each RAID configuration.
+ * Each real device in the RAID set contains it near the end of the device.
+ * Some of the ideas are copied from the ext2fs implementation.
+ *
+ * We currently use 4096 bytes as follows:
+ *
+ *	word offset	function
+ *
+ *	   0  -    31	Constant generic RAID device information.
+ *        32  -    63   Generic state information.
+ *	  64  -   127	Personality specific information.
+ *	 128  -   511	12 32-words descriptors of the disks in the raid set.
+ *	 512  -   911	Reserved.
+ *	 912  -  1023	Disk specific descriptor.
+ */
+
+/*
+ * If x is the real device size in bytes, we return an apparent size of:
+ *
+ *	y = (x & ~(MD_RESERVED_BYTES - 1)) - MD_RESERVED_BYTES
+ *
+ * and place the 4kB superblock at offset y.
+ */
+#define MD_RESERVED_BYTES		(64 * 1024)
+#define MD_RESERVED_SECTORS		(MD_RESERVED_BYTES / 512)
+#define MD_RESERVED_BLOCKS		(MD_RESERVED_BYTES / BLOCK_SIZE)
+
+#define MD_NEW_SIZE_SECTORS(x)		((x & ~(MD_RESERVED_SECTORS - 1)) - MD_RESERVED_SECTORS)
+#define MD_NEW_SIZE_BLOCKS(x)		((x & ~(MD_RESERVED_BLOCKS - 1)) - MD_RESERVED_BLOCKS)
+
+#define MD_SB_BYTES			4096
+#define MD_SB_WORDS			(MD_SB_BYTES / 4)
+#define MD_SB_BLOCKS			(MD_SB_BYTES / BLOCK_SIZE)
+#define MD_SB_SECTORS			(MD_SB_BYTES / 512)
+
+/*
+ * The following are counted in 32-bit words
+ */
+#define	MD_SB_GENERIC_OFFSET		0
+#define MD_SB_PERSONALITY_OFFSET	64
+#define MD_SB_DISKS_OFFSET		128
+#define MD_SB_DESCRIPTOR_OFFSET		992
+
+#define MD_SB_GENERIC_CONSTANT_WORDS	32
+#define MD_SB_GENERIC_STATE_WORDS	32
+#define MD_SB_GENERIC_WORDS		(MD_SB_GENERIC_CONSTANT_WORDS + MD_SB_GENERIC_STATE_WORDS)
+#define MD_SB_PERSONALITY_WORDS		64
+#define MD_SB_DISKS_WORDS		384
+#define MD_SB_DESCRIPTOR_WORDS		32
+#define MD_SB_RESERVED_WORDS		(1024 - MD_SB_GENERIC_WORDS - MD_SB_PERSONALITY_WORDS - MD_SB_DISKS_WORDS - MD_SB_DESCRIPTOR_WORDS)
+#define MD_SB_EQUAL_WORDS		(MD_SB_GENERIC_WORDS + MD_SB_PERSONALITY_WORDS + MD_SB_DISKS_WORDS)
+#define MD_SB_DISKS			(MD_SB_DISKS_WORDS / MD_SB_DESCRIPTOR_WORDS)
+
+/*
+ * Device "operational" state bits
+ */
+#define MD_DISK_FAULTY		0 /* disk is faulty / operational */
+#define MD_DISK_ACTIVE		1 /* disk is running or spare disk */
+#define MD_DISK_SYNC		2 /* disk is in sync with the raid set */
+#define MD_DISK_REMOVED		3 /* disk is in sync with the raid set */
+
+typedef struct mdp_device_descriptor_s {
+	__u32 number;		/* 0 Device number in the entire set	      */
+	__u32 major;		/* 1 Device major number		      */
+	__u32 minor;		/* 2 Device minor number		      */
+	__u32 raid_disk;	/* 3 The role of the device in the raid set   */
+	__u32 state;		/* 4 Operational state			      */
+	__u32 reserved[MD_SB_DESCRIPTOR_WORDS - 5];
+} mdp_disk_t;
+
+#define MD_SB_MAGIC		0xa92b4efc
+
+/*
+ * Superblock state bits
+ */
+#define MD_SB_CLEAN		0
+#define MD_SB_ERRORS		1
+
+typedef struct mdp_superblock_s {
+	/*
+	 * Constant generic information
+	 */
+	__u32 md_magic;		/*  0 MD identifier 			      */
+	__u32 major_version;	/*  1 major version to which the set conforms */
+	__u32 minor_version;	/*  2 minor version ...			      */
+	__u32 patch_version;	/*  3 patchlevel version ...		      */
+	__u32 gvalid_words;	/*  4 Number of used words in this section    */
+	__u32 set_uuid0;	/*  5 Raid set identifier		      */
+	__u32 ctime;		/*  6 Creation time			      */
+	__u32 level;		/*  7 Raid personality			      */
+	__u32 size;		/*  8 Apparent size of each individual disk   */
+	__u32 nr_disks;		/*  9 total disks in the raid set	      */
+	__u32 raid_disks;	/* 10 disks in a fully functional raid set    */
+	__u32 md_minor;		/* 11 preferred MD minor device number	      */
+	__u32 not_persistent;	/* 12 does it have a persistent superblock    */
+	__u32 set_uuid1;	/* 13 Raid set identifier #2		      */
+	__u32 set_uuid2;	/* 14 Raid set identifier #3		      */
+	__u32 set_uuid3;	/* 14 Raid set identifier #4		      */
+	__u32 gstate_creserved[MD_SB_GENERIC_CONSTANT_WORDS - 16];
+
+	/*
+	 * Generic state information
+	 */
+	__u32 utime;		/*  0 Superblock update time		      */
+	__u32 state;		/*  1 State bits (clean, ...)		      */
+	__u32 active_disks;	/*  2 Number of currently active disks	      */
+	__u32 working_disks;	/*  3 Number of working disks		      */
+	__u32 failed_disks;	/*  4 Number of failed disks		      */
+	__u32 spare_disks;	/*  5 Number of spare disks		      */
+	__u32 sb_csum;		/*  6 checksum of the whole superblock        */
+	__u64 events;		/*  7 number of superblock updates (64-bit!)  */
+	__u32 gstate_sreserved[MD_SB_GENERIC_STATE_WORDS - 9];
+
+	/*
+	 * Personality information
+	 */
+	__u32 layout;		/*  0 the array's physical layout	      */
+	__u32 chunk_size;	/*  1 chunk size in bytes		      */
+	__u32 root_pv;		/*  2 LV root PV */
+	__u32 root_block;	/*  3 LV root block */
+	__u32 pstate_reserved[MD_SB_PERSONALITY_WORDS - 4];
+
+	/*
+	 * Disks information
+	 */
+	mdp_disk_t disks[MD_SB_DISKS];
+
+	/*
+	 * Reserved
+	 */
+	__u32 reserved[MD_SB_RESERVED_WORDS];
+
+	/*
+	 * Active descriptor
+	 */
+	mdp_disk_t this_disk;
+
+} mdp_super_t;
+
+#endif _MD_P_H
+
diff -urN linux/include/linux/raid/md_u.h /tmp/linux/include/linux/raid/md_u.h
--- linux/include/linux/raid/md_u.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/include/linux/raid/md_u.h	Fri Feb  2 19:06:00 2001
@@ -0,0 +1,115 @@
+/*
+   md_u.h : user <=> kernel API between Linux raidtools and RAID drivers
+          Copyright (C) 1998 Ingo Molnar
+	  
+   This program is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 2, or (at your option)
+   any later version.
+   
+   You should have received a copy of the GNU General Public License
+   (for example /usr/src/linux/COPYING); if not, write to the Free
+   Software Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.  
+*/
+
+#ifndef _MD_U_H
+#define _MD_U_H
+
+/* ioctls */
+
+/* status */
+#define RAID_VERSION		_IOR (MD_MAJOR, 0x10, mdu_version_t)
+#define GET_ARRAY_INFO		_IOR (MD_MAJOR, 0x11, mdu_array_info_t)
+#define GET_DISK_INFO		_IOR (MD_MAJOR, 0x12, mdu_disk_info_t)
+#define PRINT_RAID_DEBUG	_IO (MD_MAJOR, 0x13)
+
+/* configuration */
+#define CLEAR_ARRAY		_IO (MD_MAJOR, 0x20)
+#define ADD_NEW_DISK		_IOW (MD_MAJOR, 0x21, mdu_disk_info_t)
+#define HOT_REMOVE_DISK		_IO (MD_MAJOR, 0x22)
+#define SET_ARRAY_INFO		_IOW (MD_MAJOR, 0x23, mdu_array_info_t)
+#define SET_DISK_INFO		_IO (MD_MAJOR, 0x24)
+#define WRITE_RAID_INFO		_IO (MD_MAJOR, 0x25)
+#define UNPROTECT_ARRAY		_IO (MD_MAJOR, 0x26)
+#define PROTECT_ARRAY		_IO (MD_MAJOR, 0x27)
+#define HOT_ADD_DISK		_IO (MD_MAJOR, 0x28)
+#define SET_DISK_FAULTY		_IO (MD_MAJOR, 0x29)
+
+/* usage */
+#define RUN_ARRAY		_IOW (MD_MAJOR, 0x30, mdu_param_t)
+#define START_ARRAY		_IO (MD_MAJOR, 0x31)
+#define STOP_ARRAY		_IO (MD_MAJOR, 0x32)
+#define STOP_ARRAY_RO		_IO (MD_MAJOR, 0x33)
+#define RESTART_ARRAY_RW	_IO (MD_MAJOR, 0x34)
+
+typedef struct mdu_version_s {
+	int major;
+	int minor;
+	int patchlevel;
+} mdu_version_t;
+
+typedef struct mdu_array_info_s {
+	/*
+	 * Generic constant information
+	 */
+	int major_version;
+	int minor_version;
+	int patch_version;
+	int ctime;
+	int level;
+	int size;
+	int nr_disks;
+	int raid_disks;
+	int md_minor;
+	int not_persistent;
+
+	/*
+	 * Generic state information
+	 */
+	int utime;		/*  0 Superblock update time		      */
+	int state;		/*  1 State bits (clean, ...)		      */
+	int active_disks;	/*  2 Number of currently active disks	      */
+	int working_disks;	/*  3 Number of working disks		      */
+	int failed_disks;	/*  4 Number of failed disks		      */
+	int spare_disks;	/*  5 Number of spare disks		      */
+
+	/*
+	 * Personality information
+	 */
+	int layout;		/*  0 the array's physical layout	      */
+	int chunk_size;	/*  1 chunk size in bytes		      */
+
+} mdu_array_info_t;
+
+typedef struct mdu_disk_info_s {
+	/*
+	 * configuration/status of one particular disk
+	 */
+	int number;
+	int major;
+	int minor;
+	int raid_disk;
+	int state;
+
+} mdu_disk_info_t;
+
+typedef struct mdu_start_info_s {
+	/*
+	 * configuration/status of one particular disk
+	 */
+	int major;
+	int minor;
+	int raid_disk;
+	int state;
+
+} mdu_start_info_t;
+
+typedef struct mdu_param_s
+{
+	int			personality;	/* 1,2,3,4 */
+	int			chunk_size;	/* in bytes */
+	int			max_fault;	/* unused for now */
+} mdu_param_t;
+
+#endif _MD_U_H
+
diff -urN linux/include/linux/raid/raid0.h /tmp/linux/include/linux/raid/raid0.h
--- linux/include/linux/raid/raid0.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/include/linux/raid/raid0.h	Mon Jul  9 20:34:06 2001
@@ -0,0 +1,33 @@
+#ifndef _RAID0_H
+#define _RAID0_H
+
+#include <linux/raid/md.h>
+
+struct strip_zone
+{
+	int zone_offset;		/* Zone offset in md_dev */
+	int dev_offset;			/* Zone offset in real dev */
+	int size;			/* Zone size */
+	int nb_dev;			/* # of devices attached to the zone */
+	mdk_rdev_t *dev[MAX_REAL]; /* Devices attached to the zone */
+};
+
+struct raid0_hash
+{
+	struct strip_zone *zone0, *zone1;
+};
+
+struct raid0_private_data
+{
+	struct raid0_hash *hash_table; /* Dynamically allocated */
+	struct strip_zone *strip_zone; /* This one too */
+	int nr_strip_zones;
+	struct strip_zone *smallest;
+	int nr_zones;
+};
+
+typedef struct raid0_private_data raid0_conf_t;
+
+#define mddev_to_conf(mddev) ((raid0_conf_t *) mddev->private)
+
+#endif
diff -urN linux/include/linux/raid/raid1.h /tmp/linux/include/linux/raid/raid1.h
--- linux/include/linux/raid/raid1.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/include/linux/raid/raid1.h	Mon Jul  9 20:34:07 2001
@@ -0,0 +1,64 @@
+#ifndef _RAID1_H
+#define _RAID1_H
+
+#include <linux/raid/md.h>
+
+struct mirror_info {
+	int		number;
+	int		raid_disk;
+	kdev_t		dev;
+	int		next;
+	int		sect_limit;
+
+	/*
+	 * State bits:
+	 */
+	int		operational;
+	int		write_only;
+	int		spare;
+
+	int		used_slot;
+};
+
+struct raid1_private_data {
+	mddev_t			*mddev;
+	struct mirror_info	mirrors[MD_SB_DISKS];
+	int			nr_disks;
+	int			raid_disks;
+	int			working_disks;
+	int			last_used;
+	unsigned long		next_sect;
+	int			sect_count;
+	mdk_thread_t		*thread, *resync_thread;
+	int			resync_mirrors;
+	struct mirror_info	*spare;
+};
+
+typedef struct raid1_private_data raid1_conf_t;
+
+/*
+ * this is the only point in the RAID code where we violate
+ * C type safety. mddev->private is an 'opaque' pointer.
+ */
+#define mddev_to_conf(mddev) ((raid1_conf_t *) mddev->private)
+
+/*
+ * this is our 'private' 'collective' RAID1 buffer head.
+ * it contains information about what kind of IO operations were started
+ * for this RAID1 operation, and about their status:
+ */
+
+struct raid1_bh {
+	atomic_t		remaining; /* 'have we finished' count,
+					    * used from IRQ handlers
+					    */
+	int			cmd;
+	unsigned long		state;
+	mddev_t			*mddev;
+	struct buffer_head	*master_bh;
+	struct buffer_head	*mirror_bh [MD_SB_DISKS];
+	struct buffer_head	bh_req;
+	struct buffer_head	*next_retry;
+};
+
+#endif
diff -urN linux/include/linux/raid/raid5.h /tmp/linux/include/linux/raid/raid5.h
--- linux/include/linux/raid/raid5.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/include/linux/raid/raid5.h	Mon Jul  9 20:34:08 2001
@@ -0,0 +1,113 @@
+#ifndef _RAID5_H
+#define _RAID5_H
+
+#include <linux/raid/md.h>
+#include <linux/raid/xor.h>
+
+struct disk_info {
+	kdev_t	dev;
+	int	operational;
+	int	number;
+	int	raid_disk;
+	int	write_only;
+	int	spare;
+	int	used_slot;
+};
+
+struct stripe_head {
+	md_spinlock_t		stripe_lock;
+	struct stripe_head	*hash_next, **hash_pprev; /* hash pointers */
+	struct stripe_head	*free_next;		/* pool of free sh's */
+	struct buffer_head	*buffer_pool;		/* pool of free buffers */
+	struct buffer_head	*bh_pool;		/* pool of free bh's */
+	struct raid5_private_data	*raid_conf;
+	struct buffer_head	*bh_old[MD_SB_DISKS];	/* disk image */
+	struct buffer_head	*bh_new[MD_SB_DISKS];	/* buffers of the MD device (present in buffer cache) */
+	struct buffer_head	*bh_copy[MD_SB_DISKS];	/* copy on write of bh_new (bh_new can change from under us) */
+	struct buffer_head	*bh_req[MD_SB_DISKS];	/* copy of bh_new (only the buffer heads), queued to the lower levels */
+	int			cmd_new[MD_SB_DISKS];	/* READ/WRITE for new */
+	int			new[MD_SB_DISKS];	/* buffer added since the last handle_stripe() */
+	unsigned long		sector;			/* sector of this row */
+	int			size;			/* buffers size */
+	int			pd_idx;			/* parity disk index */
+	atomic_t		nr_pending;		/* nr of pending cmds */
+	unsigned long		state;			/* state flags */
+	int			cmd;			/* stripe cmd */
+	int			count;			/* nr of waiters */
+	int			write_method;		/* reconstruct-write / read-modify-write */
+	int			phase;			/* PHASE_BEGIN, ..., PHASE_COMPLETE */
+	struct wait_queue	*wait;			/* processes waiting for this stripe */
+};
+
+/*
+ * Phase
+ */
+#define PHASE_BEGIN		0
+#define PHASE_READ_OLD		1
+#define PHASE_WRITE		2
+#define PHASE_READ		3
+#define PHASE_COMPLETE		4
+
+/*
+ * Write method
+ */
+#define METHOD_NONE		0
+#define RECONSTRUCT_WRITE	1
+#define READ_MODIFY_WRITE	2
+
+/*
+ * Stripe state
+ */
+#define STRIPE_LOCKED		0
+#define STRIPE_ERROR		1
+
+/*
+ * Stripe commands
+ */
+#define STRIPE_NONE		0
+#define	STRIPE_WRITE		1
+#define STRIPE_READ		2
+
+struct raid5_private_data {
+	struct stripe_head	**stripe_hashtbl;
+	mddev_t			*mddev;
+	mdk_thread_t		*thread, *resync_thread;
+	struct disk_info	disks[MD_SB_DISKS];
+	struct disk_info	*spare;
+	int			buffer_size;
+	int			chunk_size, level, algorithm;
+	int			raid_disks, working_disks, failed_disks;
+	int			sector_count;
+	unsigned long		next_sector;
+	atomic_t		nr_handle;
+	struct stripe_head	*next_free_stripe;
+	int			nr_stripes;
+	int			resync_parity;
+	int			max_nr_stripes;
+	int			clock;
+	int			nr_hashed_stripes;
+	int			nr_locked_stripes;
+	int			nr_pending_stripes;
+	int			nr_cached_stripes;
+
+	/*
+	 * Free stripes pool
+	 */
+	int			nr_free_sh;
+	struct stripe_head	*free_sh_list;
+	struct wait_queue	*wait_for_stripe;
+};
+
+typedef struct raid5_private_data raid5_conf_t;
+
+#define mddev_to_conf(mddev) ((raid5_conf_t *) mddev->private)
+
+/*
+ * Our supported algorithms
+ */
+#define ALGORITHM_LEFT_ASYMMETRIC	0
+#define ALGORITHM_RIGHT_ASYMMETRIC	1
+#define ALGORITHM_LEFT_SYMMETRIC	2
+#define ALGORITHM_RIGHT_SYMMETRIC	3
+
+#endif
diff -urN linux/include/linux/raid/translucent.h /tmp/linux/include/linux/raid/translucent.h
--- linux/include/linux/raid/translucent.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/include/linux/raid/translucent.h	Fri Feb  2 19:06:00 2001
@@ -0,0 +1,23 @@
+#ifndef _TRANSLUCENT_H
+#define _TRANSLUCENT_H
+
+#include <linux/raid/md.h>
+
+typedef struct dev_info dev_info_t;
+
+struct dev_info {
+	kdev_t		dev;
+	int		size;
+};
+
+struct translucent_private_data
+{
+	dev_info_t		disks[MD_SB_DISKS];
+};
+
+
+typedef struct translucent_private_data translucent_conf_t;
+
+#define mddev_to_conf(mddev) ((translucent_conf_t *) mddev->private)
+
+#endif
diff -urN linux/include/linux/raid/xor.h /tmp/linux/include/linux/raid/xor.h
--- linux/include/linux/raid/xor.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/include/linux/raid/xor.h	Mon Jul  9 20:33:50 2001
@@ -0,0 +1,12 @@
+#ifndef _XOR_H
+#define _XOR_H
+
+#include <linux/raid/md.h>
+
+#define MAX_XOR_BLOCKS 5
+
+extern void calibrate_xor_block(void);
+extern void (*xor_block)(unsigned int count,
+                         struct buffer_head **bh_ptr);
+
+#endif
diff -urN linux/include/linux/raid0.h /tmp/linux/include/linux/raid0.h
--- linux/include/linux/raid0.h	Tue Oct 29 06:20:24 1996
+++ /tmp/linux/include/linux/raid0.h	Wed Dec 31 17:00:00 1969
@@ -1,27 +0,0 @@
-#ifndef _RAID0_H
-#define _RAID0_H
-
-struct strip_zone
-{
-  int zone_offset;		/* Zone offset in md_dev */
-  int dev_offset;		/* Zone offset in real dev */
-  int size;			/* Zone size */
-  int nb_dev;			/* Number of devices attached to the zone */
-  struct real_dev *dev[MAX_REAL]; /* Devices attached to the zone */
-};
-
-struct raid0_hash
-{
-  struct strip_zone *zone0, *zone1;
-};
-
-struct raid0_data
-{
-  struct raid0_hash *hash_table; /* Dynamically allocated */
-  struct strip_zone *strip_zone; /* This one too */
-  int nr_strip_zones;
-  struct strip_zone *smallest;
-  int nr_zones;
-};
-
-#endif
diff -urN linux/include/linux/raid1.h /tmp/linux/include/linux/raid1.h
--- linux/include/linux/raid1.h	Fri May  8 01:17:13 1998
+++ /tmp/linux/include/linux/raid1.h	Wed Dec 31 17:00:00 1969
@@ -1,49 +0,0 @@
-#ifndef _RAID1_H
-#define _RAID1_H
-
-#include <linux/md.h>
-
-struct mirror_info {
-	int		number;
-	int		raid_disk;
-	kdev_t		dev;
-	int		next;
-	int		sect_limit;
-
-	/*
-	 * State bits:
-	 */
-	int		operational;
-	int		write_only;
-	int		spare;
-};
-
-struct raid1_data {
-	struct md_dev *mddev;
-	struct mirror_info mirrors[MD_SB_DISKS];  	/* RAID1 devices, 2 to MD_SB_DISKS */
-	int raid_disks;
-	int working_disks;			/* Number of working disks */
-	int last_used;
-	unsigned long	next_sect;
-	int		sect_count;
-	int resync_running;
-};
-
-/*
- * this is our 'private' 'collective' RAID1 buffer head.
- * it contains information about what kind of IO operations were started
- * for this RAID5 operation, and about their status:
- */
-
-struct raid1_bh {
-	unsigned int		remaining;
-	int			cmd;
-	unsigned long		state;
-	struct md_dev		*mddev;
-	struct buffer_head	*master_bh;
-	struct buffer_head	*mirror_bh [MD_SB_DISKS];
-	struct buffer_head	bh_req;
-	struct buffer_head	*next_retry;
-};
-
-#endif
diff -urN linux/include/linux/raid5.h /tmp/linux/include/linux/raid5.h
--- linux/include/linux/raid5.h	Fri May  8 01:17:13 1998
+++ /tmp/linux/include/linux/raid5.h	Wed Dec 31 17:00:00 1969
@@ -1,110 +0,0 @@
-#ifndef _RAID5_H
-#define _RAID5_H
-
-#ifdef __KERNEL__
-#include <linux/md.h>
-#include <asm/atomic.h>
-
-struct disk_info {
-	kdev_t	dev;
-	int	operational;
-	int	number;
-	int	raid_disk;
-	int	write_only;
-	int	spare;
-};
-
-struct stripe_head {
-	struct stripe_head	*hash_next, **hash_pprev; /* hash pointers */
-	struct stripe_head	*free_next;		/* pool of free sh's */
-	struct buffer_head	*buffer_pool;		/* pool of free buffers */
-	struct buffer_head	*bh_pool;		/* pool of free bh's */
-	struct raid5_data	*raid_conf;
-	struct buffer_head	*bh_old[MD_SB_DISKS];	/* disk image */
-	struct buffer_head	*bh_new[MD_SB_DISKS];	/* buffers of the MD device (present in buffer cache) */
-	struct buffer_head	*bh_copy[MD_SB_DISKS];	/* copy on write of bh_new (bh_new can change from under us) */
-	struct buffer_head	*bh_req[MD_SB_DISKS];	/* copy of bh_new (only the buffer heads), queued to the lower levels */
-	int			cmd_new[MD_SB_DISKS];	/* READ/WRITE for new */
-	int			new[MD_SB_DISKS];	/* buffer added since the last handle_stripe() */
-	unsigned long		sector;			/* sector of this row */
-	int			size;			/* buffers size */
-	int			pd_idx;			/* parity disk index */
-	int			nr_pending;		/* nr of pending cmds */
-	unsigned long		state;			/* state flags */
-	int			cmd;			/* stripe cmd */
-	int			count;			/* nr of waiters */
-	int			write_method;		/* reconstruct-write / read-modify-write */
-	int			phase;			/* PHASE_BEGIN, ..., PHASE_COMPLETE */
-	struct wait_queue	*wait;			/* processes waiting for this stripe */
-};
-
-/*
- * Phase
- */
-#define PHASE_BEGIN		0
-#define PHASE_READ_OLD		1
-#define PHASE_WRITE		2
-#define PHASE_READ		3
-#define PHASE_COMPLETE		4
-
-/*
- * Write method
- */
-#define METHOD_NONE		0
-#define RECONSTRUCT_WRITE	1
-#define READ_MODIFY_WRITE	2
-
-/*
- * Stripe state
- */
-#define STRIPE_LOCKED		0
-#define STRIPE_ERROR		1
-
-/*
- * Stripe commands
- */
-#define STRIPE_NONE		0
-#define	STRIPE_WRITE		1
-#define STRIPE_READ		2
-
-struct raid5_data {
-	struct stripe_head	**stripe_hashtbl;
-	struct md_dev		*mddev;
-	struct md_thread	*thread, *resync_thread;
-	struct disk_info	disks[MD_SB_DISKS];
-	struct disk_info	*spare;
-	int			buffer_size;
-	int			chunk_size, level, algorithm;
-	int			raid_disks, working_disks, failed_disks;
-	int			sector_count;
-	unsigned long		next_sector;
-	atomic_t		nr_handle;
-	struct stripe_head	*next_free_stripe;
-	int			nr_stripes;
-	int			resync_parity;
-	int			max_nr_stripes;
-	int			clock;
-	int			nr_hashed_stripes;
-	int			nr_locked_stripes;
-	int			nr_pending_stripes;
-	int			nr_cached_stripes;
-
-	/*
-	 * Free stripes pool
-	 */
-	int			nr_free_sh;
-	struct stripe_head	*free_sh_list;
-	struct wait_queue	*wait_for_stripe;
-};
-
-#endif
-
-/*
- * Our supported algorithms
- */
-#define ALGORITHM_LEFT_ASYMMETRIC	0
-#define ALGORITHM_RIGHT_ASYMMETRIC	1
-#define ALGORITHM_LEFT_SYMMETRIC	2
-#define ALGORITHM_RIGHT_SYMMETRIC	3
-
-#endif
diff -urN linux/include/linux/reiserfs_fs.h /tmp/linux/include/linux/reiserfs_fs.h
--- linux/include/linux/reiserfs_fs.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/include/linux/reiserfs_fs.h	Mon Jul  9 20:35:51 2001
@@ -0,0 +1,1511 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+#ifndef _LINUX_REISER_FS_H
+#define _LINUX_REISER_FS_H
+
+
+#include <linux/types.h>
+#ifdef __KERNEL__
+#include <linux/malloc.h>
+#include <linux/tqueue.h>
+#endif
+
+/*
+ *  include/linux/reiser_fs.h
+ *
+ *  Reiser File System constants and structures
+ *
+ */
+
+/* in reading the #defines, it may help to understand that they employ
+ the following abbreviations:
+
+B = Buffer
+I = Item header
+H = Height within the tree (should be changed to LEV)
+N = Number of the item in the node
+DNM = DyNaMic data
+STAT = stat data
+DEH = Directory Entry Header
+EC = Entry Count
+E = Entry number
+UL = Unsigned Long
+BLKH = BLocK Header
+UNFM = UNForMatted node
+DC = Disk Child
+P = Path
+
+These #defines are named by concatenating these abbreviations, where
+first comes the arguments, and last comes the return value, of the
+macro.
+
+*/
+
+#define REISERFS_CHECK 
+
+/* NEW_GET_NEW_BUFFER will try to allocate new blocks better */
+/*#define NEW_GET_NEW_BUFFER*/
+#define OLD_GET_NEW_BUFFER
+
+/* if this is undefined, all inode changes get into stat data immediately, if it can be found in RAM */
+#define DIRTY_LATER
+
+
+/* these are used by reiserfs_file_read when no genericread mount option specified */
+#define REISERFS_NBUF 32
+
+#define REISERFS_OBJECT_READ_AHEAD
+
+#define PACKING_LOCALITY_READ_AHEAD
+
+/* Should be used for single disk file systems. */
+/* #define READ_LOCK_REISERFS */
+
+
+
+/* obsolete defines */
+/*#define REISERFS_CHECK_ONE_PROCESS*/
+/*#define REISERFS_INFO*/
+
+
+
+
+
+/*
+ * Disk Data Structures
+ */
+
+/***************************************************************************/
+/*                             SUPER BLOCK                                 */
+/***************************************************************************/
+
+/*
+ * Structure of super block on disk, a version of which in RAM is often accessed as s->u.reiserfs_sb.s_rs
+ * the version in RAM is part of a larger structure containing fields never written to disk.
+ */
+
+				/* used by gcc */
+#define REISERFS_SUPER_MAGIC 0x52654973
+				/* used by file system utilities that
+                                   look at the superblock, etc. */
+#define REISERFS_SUPER_MAGIC_STRING "ReIsErFs"
+
+				/* ReiserFS leaves the first 64k unused,
+                                   so that partition labels have enough
+                                   space.  If someone wants to write a
+                                   fancy bootloader that needs more than
+                                   64k, let us know, and this will be
+                                   increased in size.  This number must
+                                   be larger than than the largest block
+                                   size on any platform, or code will
+                                   break.  -Hans */
+#define REISERFS_DISK_OFFSET_IN_BYTES (64 * 1024)
+#define REISERFS_FIRST_BLOCK unused_define
+
+/* the spot for the super in versions 3.5 - 3.5.11 (inclusive) */
+#define REISERFS_OLD_DISK_OFFSET_IN_BYTES (8 * 1024)
+
+#define READ_BLOCKS  1
+#define DONT_READ_BLOCKS 2
+
+#define CARRY_ON          	0
+#define SCHEDULE_OCCURRED  	1
+#define PATH_INCORRECT    	2
+#define IO_ERROR		3
+
+#define NO_DISK_SPACE        (-1)
+#define NO_BALANCING_NEEDED  (-2)
+
+
+struct buffer_and_id {
+  struct buffer_head  * bi_buf;
+  unsigned long  bi_id;
+};
+
+typedef unsigned long b_blocknr_t;
+typedef __u32 unp_t;
+
+struct unfm_nodeinfo {
+  unsigned long	 unfm_nodenum;
+  unsigned short unfm_freespace;
+};
+
+/* when reiserfs_file_write is called with a byte count >= MIN_PACK_ON_CLOSE,
+** it sets the inode to pack on close, and when extending the file, will only
+** use unformatted nodes.
+**
+** This is a big speed up for the journal, which is badly hurt by direct->indirect
+** conversions (they must be logged).
+*/
+#define MIN_PACK_ON_CLOSE		512
+
+/* the defines below say, that if file size is >=
+   DIRECT_TAIL_SUPPRESSION_SIZE * blocksize, then if tail is longer
+   than MAX_BYTES_SUPPRESS_DIRECT_TAIL, it will be stored in
+   unformatted node */
+#define DIRECT_TAIL_SUPPRESSION_SIZE      1024
+#define MAX_BYTES_SUPPRESS_DIRECT_TAIL    1024
+
+/* Check whether byte is placed in a direct item. */
+#define INODE_OFFSET_IN_DIRECT(p_s_inode, n_offset) \
+( (n_offset) >= (p_s_inode)->u.reiserfs_i.i_first_direct_byte )
+
+/* this is used for i_first_direct_byte field of inode */
+#define NO_BYTES_IN_DIRECT_ITEM MAX_KEY_OFFSET
+
+/* We store tail in unformatted node if it is too big to fit into a
+   formatted node or if DIRECT_TAIL_SUPPRESSION_SIZE,
+   MAX_BYTES_SUPPRESS_DIRECT_TAIL and file size say that. */
+/* #define STORE_TAIL_IN_UNFM(n_file_size,n_tail_size,n_block_size) \ */
+/* ( ((n_tail_size) > MAX_DIRECT_ITEM_LEN(n_block_size)) || \ */
+/*   ( ( (n_file_size) >= (n_block_size) * DIRECT_TAIL_SUPPRESSION_SIZE ) && \ */
+/*    ( (n_tail_size) >= MAX_BYTES_SUPPRESS_DIRECT_TAIL ) ) ) */
+
+  /* This is an aggressive tail suppression policy, I am hoping it
+     improves our benchmarks. The principle behind it is that
+     percentage space saving is what matters, not absolute space
+     saving.  This is non-intuitive, but it helps to understand it if
+     you consider that the cost to access 4 blocks is not much more
+     than the cost to access 1 block, if you have to do a seek and
+     rotate.  A tail risks a non-linear disk access that is
+     significant as a percentage of total time cost for a 4 block file
+     and saves an amount of space that is less significant as a
+     percentage of space, or so goes the hypothesis.  -Hans */
+#define STORE_TAIL_IN_UNFM(n_file_size,n_tail_size,n_block_size) \
+\
+( ((n_tail_size) > MAX_DIRECT_ITEM_LEN(n_block_size)) || \
+  ( (n_file_size) >= (n_block_size) * 4 ) || \
+   ( ( (n_file_size) >= (n_block_size) * 3 ) && \
+   ( (n_tail_size) >=   (MAX_DIRECT_ITEM_LEN(n_block_size))/4) ) || \
+   ( ( (n_file_size) >= (n_block_size) * 2 ) && \
+   ( (n_tail_size) >=   (MAX_DIRECT_ITEM_LEN(n_block_size))/2) ) || \
+   ( ( (n_file_size) >= (n_block_size) ) && \
+   ( (n_tail_size) >=   (MAX_DIRECT_ITEM_LEN(n_block_size) * 3)/4) ) )
+
+
+/*
+ * values for s_state field
+ */
+#define REISERFS_VALID_FS    1
+#define REISERFS_ERROR_FS    2
+
+
+
+/***************************************************************************/
+/*                       KEY & ITEM HEAD                                   */
+/***************************************************************************/
+typedef __u32 objectid_t;
+
+/* Key of the object drop determines its location in the S+tree, and is composed of 4 components */
+struct key {
+  __u32 k_dir_id;   	    /* packing locality: by default parent directory object id */
+  __u32 k_objectid;          /* object identifier */
+  __u32 k_offset;	    /* for regular files this is the offset to the first byte of the body, */
+	                            /* contained in the object-item, */
+       		                    /* as measured from the start of the entire body of the object. */
+
+				       /* for directory entries, k_offset consists of hash derived from
+					  hashing the name and using few bits (23 or more) of the resulting
+					  hash, and generation number that allows distinguishing names with
+					  hash collisions. If number of collisions overflows generation number, we return EEXIST. 
+					  High order bit is 0 always */
+  __u32 k_uniqueness;	       /* uniqueness field used for storing flags about the item's type and
+					  mergeability.  Key size is performance critical.  These flags should
+					  not be stored here in the key, and the key could be reduced to three
+					  components by pushing the uniqueness field into the offset for
+					  directories.  Grrr. -Hans */
+};
+
+ /* Our function for comparing keys can compare keys of different
+    lengths.  It takes as a parameter the length of the keys it is to
+    compare.  These defines are used in determining what is to be
+    passed to it as that parameter. */
+#define REISERFS_FULL_KEY_LEN     4
+
+#define REISERFS_SHORT_KEY_LEN    2
+
+/* The result of the key compare */
+#define FIRST_GREATER 1
+#define SECOND_GREATER -1
+#define KEYS_IDENTICAL 0
+#define KEY_FOUND 1
+#define KEY_NOT_FOUND 0
+
+
+#define KEY_SIZE (sizeof(struct key))
+#define SHORT_KEY_SIZE (sizeof (unsigned long) + sizeof (unsigned long))
+
+/* return values for search_by_key and clones */
+#define ITEM_FOUND 1
+#define ITEM_NOT_FOUND 0
+#define ENTRY_FOUND 1
+#define ENTRY_NOT_FOUND 0
+#define DIRECTORY_NOT_FOUND -1
+#define REGULAR_FILE_FOUND -2
+#define DIRECTORY_FOUND -3
+#define BYTE_FOUND 1
+#define BYTE_NOT_FOUND 0
+#define FILE_NOT_FOUND -1
+
+#define POSITION_FOUND 1
+#define POSITION_NOT_FOUND 0
+#define GOTO_PREVIOUS_ITEM 2
+#define POSITION_FOUND_INVISIBLE 3
+
+
+/*  Everything in the filesystem is stored as a set of items.  The item head contains the key of the item, its
+   free space (for indirect items) and specifies the location of the item itself within the block.  */
+
+struct item_head
+{
+  struct key ih_key; 	/* Everything in the tree is found by searching for it based on its key.*/
+
+  union {
+    __u16 ih_free_space; /* The free space in the last unformatted node of an indirect item if this
+				     is an indirect item.  This equals 0xFFFF iff this is a direct item or
+				     stat data item. Note that the key, not this field, is used to determine
+				     the item type, and thus which field this union contains. */
+    __u16 ih_entry_count; /* Iff this is a directory item, this field equals the number of directory
+				      entries in the directory item. */
+  } u;
+  __u16 ih_item_len;           /* total size of the item body                  */
+  __u16 ih_item_location;      /* an offset to the item body within the block  */
+  __u16 ih_reserved;		/* used by reiserfsck */
+};
+/* size of item header     */
+#define IH_SIZE (sizeof(struct item_head))
+
+#define ih_item_len(ih) (__le16_to_cpu((ih)->ih_item_len))
+
+
+#define I_K_KEY_IN_ITEM(p_s_ih, p_s_key, n_blocksize) \
+    ( ! COMP_SHORT_KEYS(p_s_ih, p_s_key) && \
+          I_OFF_BYTE_IN_ITEM(p_s_ih, (p_s_key)->k_offset, n_blocksize) )
+
+/* maximal length of item */ 
+#define MAX_ITEM_LEN(block_size) (block_size - BLKH_SIZE - IH_SIZE)
+#define MIN_ITEM_LEN 1
+
+
+/* object identifier for root dir */
+#define REISERFS_ROOT_OBJECTID 2
+#define REISERFS_ROOT_PARENT_OBJECTID 1
+extern struct key root_key;
+
+
+
+
+/* 
+ * Picture represents a leaf of the S+tree
+ *  ______________________________________________________
+ * |      |  Array of     |                   |           |
+ * |Block |  Object-Item  |      F r e e      |  Objects- |
+ * | head |  Headers      |     S p a c e     |   Items   |
+ * |______|_______________|___________________|___________|
+ */
+
+/* Header of a disk block.  More precisely, header of a formatted leaf
+   or internal node, and not the header of an unformatted node. */
+struct block_head {       
+  __u16 blk_level;        /* Level of a block in the tree. */
+  __u16 blk_nr_item;      /* Number of keys/items in a block. */
+  __u16 blk_free_space;   /* Block free space in bytes. */
+  struct key  blk_right_delim_key; /* Right delimiting key for this block (supported for leaf level nodes
+				      only) */
+};
+
+#define BLKH_SIZE (sizeof(struct block_head))
+
+/*
+ * values for blk_type field
+ */
+
+#define FREE_LEVEL        0 /* Node of this level is out of the tree. */
+
+#define DISK_LEAF_NODE_LEVEL  1 /* Leaf node level.                       */
+
+/* Given the buffer head of a formatted node, resolve to the block head of that node. */
+#define B_BLK_HEAD(p_s_bh)  ((struct block_head *)((p_s_bh)->b_data))
+/* Number of items that are in buffer. */
+#define B_NR_ITEMS(p_s_bh)	  	( B_BLK_HEAD(p_s_bh)->blk_nr_item )
+#define B_LEVEL(bh)			( B_BLK_HEAD(bh)->blk_level )
+#define B_FREE_SPACE(bh)		( B_BLK_HEAD(bh)->blk_free_space )
+/* Get right delimiting key. */
+#define B_PRIGHT_DELIM_KEY(p_s_bh)	( &(B_BLK_HEAD(p_s_bh)->blk_right_delim_key) )
+
+/* Does the buffer contain a disk leaf. */
+#define B_IS_ITEMS_LEVEL(p_s_bh)   	( B_BLK_HEAD(p_s_bh)->blk_level == DISK_LEAF_NODE_LEVEL )
+
+/* Does the buffer contain a disk internal node */
+#define B_IS_KEYS_LEVEL(p_s_bh) 	( B_BLK_HEAD(p_s_bh)->blk_level > DISK_LEAF_NODE_LEVEL &&\
+					  B_BLK_HEAD(p_s_bh)->blk_level <= MAX_HEIGHT )
+
+
+
+
+/***************************************************************************/
+/*                             STAT DATA                                   */
+/***************************************************************************/
+
+/* Stat Data on disk (reiserfs version of UFS disk inode minus the address blocks) */
+
+/*
+  The sense of adding union to stat data is to keep a value of real number of blocks used by file.
+  The necessity of adding such information is caused by existing of files with holes.
+  Reiserfs should keep number of used blocks for file, but not calculate it from file size
+  (that is not correct for holed files). Thus we have to add additional information to stat data.
+  When we have a device special file, there is no need to get number of used blocks for them,
+  and, accordingly, we doesn't need to keep major and minor numbers for regular files, which
+  might have holes. So this field is being overloaded.
+*/
+
+struct stat_data {
+  __u16 sd_mode;	/* file type, permissions */
+  __u16 sd_nlink;	/* number of hard links */
+  __u16 sd_uid;		/* owner */
+  __u16 sd_gid;		/* group */
+  __u32 sd_size;	/* file size */
+  __u32 sd_atime;	/* time of last access */
+  __u32 sd_mtime;	/* time file was last modified  */
+  __u32 sd_ctime;	/* time inode (stat data) was last changed (except changes to sd_atime and sd_mtime) */
+  union {
+	 __u32 sd_rdev;
+	 __u32 sd_blocks;	/* number of blocks file uses */
+  } u;
+  __u32 sd_first_direct_byte; /* first byte of file which is stored in a direct item: except that if it equals 1 it is a
+     symlink and if it equals MAX_KEY_OFFSET there is no direct item.  The existence of this
+     field really grates on me. Let's replace it with a macro based on sd_size and our tail
+     suppression policy.  Someday.  -Hans */
+};
+#define SD_SIZE (sizeof(struct stat_data))
+
+
+/***************************************************************************/
+/*                      DIRECTORY STRUCTURE                                */
+/***************************************************************************/
+/* 
+   Picture represents the structure of directory items
+   ________________________________________________
+   |  Array of     |   |     |        |       |   |
+   | directory     |N-1| N-2 | ....   |   1st |0th|
+   | entry headers |   |     |        |       |   |
+   |_______________|___|_____|________|_______|___|
+                    <----   directory entries         ------>
+
+ First directory item has k_offset component 1. We store "." and ".."
+ in one item, always, we never split "." and ".." into differing
+ items.  This makes, among other things, the code for removing
+ directories simpler. */
+#define SD_OFFSET  0
+#define SD_UNIQUENESS 0
+#define DOT_OFFSET 1
+#define DOT_DOT_OFFSET 2
+#define DIRENTRY_UNIQUENESS 500
+
+/* */
+#define FIRST_ITEM_OFFSET 1
+
+/*
+   Q: How to get key of object pointed to by entry from entry?  
+
+   A: Each directory entry has its header. This header has deh_dir_id and deh_objectid fields, those are key
+      of object, entry points to */
+
+/* NOT IMPLEMENTED:   
+   Directory will someday contain stat data of object */
+
+
+
+struct reiserfs_de_head
+{
+  __u32 deh_offset;  /* third component of the directory entry key */
+  __u32 deh_dir_id;  /* objectid of the parent directory of the
+			object, that is referenced by directory entry */
+  __u32 deh_objectid;/* objectid of the object, that is referenced by
+                        directory entry */
+  __u16 deh_location;/* offset of name in the whole item */
+  __u16 deh_state;   /* whether 1) entry contains stat data (for
+			future), and 2) whether entry is hidden
+			(unlinked) */
+};
+#define DEH_SIZE sizeof(struct reiserfs_de_head)
+
+#define deh_offset(deh) (__le32_to_cpu ((deh)->deh_offset))
+#define deh_dir_id(deh) (__le32_to_cpu ((deh)->deh_dir_id))
+#define deh_objectid(deh) (__le32_to_cpu ((deh)->deh_objectid))
+#define deh_location(deh) (__le16_to_cpu ((deh)->deh_location))
+
+#define DEH_Statdata 0			/* not used now */
+#define DEH_Visible 2
+
+#define mark_de_with_sd(deh)        set_bit (DEH_Statdata, &((deh)->deh_state))
+#define mark_de_without_sd(deh)     clear_bit (DEH_Statdata, &((deh)->deh_state))
+#define mark_de_visible(deh)	    set_bit (DEH_Visible, &((deh)->deh_state))
+#define mark_de_hidden(deh)	    clear_bit (DEH_Visible, &((deh)->deh_state))
+
+#define de_with_sd(deh)		    test_bit (DEH_Statdata, &((deh)->deh_state))
+#define de_visible(deh)	    	    test_bit (DEH_Visible, &((deh)->deh_state))
+#define de_hidden(deh)	    	    !test_bit (DEH_Visible, &((deh)->deh_state))
+
+/* length of the directory entry in directory item. This define calculates length of i-th directory entry
+   using directory entry locations from dir entry head. When it calculates length of 0-th directory entry, it
+   uses length of whole item in place of entry location of the non-existent following entry in the
+   calculation.  See picture above.*/
+#define I_DEH_N_ENTRY_LENGTH(ih,deh,i) \
+((i) ? (((deh)-1)->deh_location - (deh)->deh_location) : ((ih)->ih_item_len) - (deh)->deh_location)
+
+/* empty directory contains two entries "." and ".." and their headers */
+#define EMPTY_DIR_SIZE  (2 * DEH_SIZE + /*sizeof (unsigned long) +*/ 3)
+
+/* number of entries in the directory item, depends on ENTRY_COUNT being at the start of directory dynamic data. */
+#define I_ENTRY_COUNT(ih) ((ih)->u.ih_entry_count)
+
+/* array of the entry headers */
+#define B_I_DEH(bh,ih) ((struct reiserfs_de_head *)(B_I_PITEM(bh,ih)))
+
+/* name by bh, ih and entry_num */
+#define B_I_E_NAME(entry_num,bh,ih) ((char *)(bh->b_data + ih->ih_item_location + (B_I_DEH(bh,ih)+(entry_num))->deh_location))
+
+#define REISERFS_MAX_NAME_LEN(block_size) (block_size - BLKH_SIZE - IH_SIZE - DEH_SIZE)	/* -SD_SIZE when entry will contain stat data */
+
+/* this structure is used for operations on directory entries. It is not a disk structure. */
+/* When reiserfs_find_entry or search_by_entry_key find directory entry, they return filled reiserfs_dir_entry structure */
+struct reiserfs_dir_entry
+{
+  struct buffer_head * de_bh;
+  int de_item_num;
+  struct item_head * de_ih;
+  int de_entry_num;
+  struct reiserfs_de_head * de_deh;
+  int de_entrylen;
+  int de_namelen;
+  char * de_name;
+  char * de_gen_number_bit_string;
+
+  __u32 de_dir_id;
+  __u32 de_objectid;
+
+  struct key de_entry_key;
+};
+   
+/* these defines are useful when a particular member of a reiserfs_dir_entry is needed */
+
+/* pointer to file name, stored in entry */
+#define B_I_DEH_ENTRY_FILE_NAME(bh,ih,deh) (B_I_PITEM (bh, ih) + (deh)->deh_location)
+
+/* length of name */
+#define I_DEH_N_ENTRY_FILE_NAME_LENGTH(ih,deh,entry_num) \
+(I_DEH_N_ENTRY_LENGTH (ih, deh, entry_num) - (de_with_sd (deh) ? SD_SIZE : 0))
+
+#define DEH_OBJECTID(deh) ((deh)->deh_objectid)
+
+/* hash value occupies 24 bits starting from 7 up to 30 */
+#define GET_HASH_VALUE(offset) ((offset) & 0x7fffff80)
+/* generation number occupies 7 bits starting from 0 up to 6 */
+#define GET_GENERATION_NUMBER(offset) ((offset) & 0x0000007f)
+
+
+/*
+ * Picture represents an internal node of the reiserfs tree
+ *  ______________________________________________________
+ * |      |  Array of     |  Array of         |  Free     |
+ * |block |    keys       |  pointers         | space     |
+ * | head |      N        |      N+1          |           |
+ * |______|_______________|___________________|___________|
+ */
+
+/***************************************************************************/
+/*                      DISK CHILD                                         */
+/***************************************************************************/
+/* Disk child pointer: The pointer from an internal node of the tree
+   to a node that is on disk. */
+struct disk_child {
+  unsigned long       dc_block_number;              /* Disk child's block number. */
+  unsigned short      dc_size;		            /* Disk child's used space.   */
+};
+
+#define DC_SIZE (sizeof(struct disk_child))
+
+/* Get disk child by buffer header and position in the tree node. */
+#define B_N_CHILD(p_s_bh,n_pos)  ((struct disk_child *)\
+((p_s_bh)->b_data+BLKH_SIZE+B_NR_ITEMS(p_s_bh)*KEY_SIZE+DC_SIZE*(n_pos)))
+
+/* Get disk child number by buffer header and position in the tree node. */
+#define B_N_CHILD_NUM(p_s_bh,n_pos) (B_N_CHILD(p_s_bh,n_pos)->dc_block_number)
+
+ /* maximal value of field child_size in structure disk_child */ 
+ /* child size is the combined size of all items and their headers */
+#define MAX_CHILD_SIZE(bh) ((int)( (bh)->b_size - BLKH_SIZE ))
+
+/* amount of used space in buffer (not including block head) */
+#define B_CHILD_SIZE(cur) (MAX_CHILD_SIZE(cur)-(B_FREE_SPACE(cur)))
+
+/* max and min number of keys in internal node */
+#define MAX_NR_KEY(bh) ( (MAX_CHILD_SIZE(bh)-DC_SIZE)/(KEY_SIZE+DC_SIZE) )
+#define MIN_NR_KEY(bh)    (MAX_NR_KEY(bh)/2)
+
+/***************************************************************************/
+/*                      PATH STRUCTURES AND DEFINES                        */
+/***************************************************************************/
+
+
+/* Search_by_key fills up the path from the root to the leaf as it descends the tree looking for the
+   key.  It uses reiserfs_bread to try to find buffers in the cache given their block number.  If it
+   does not find them in the cache it reads them from disk.  For each node search_by_key finds using
+   reiserfs_bread it then uses bin_search to look through that node.  bin_search will find the
+   position of the block_number of the next node if it is looking through an internal node.  If it
+   is looking through a leaf node bin_search will find the position of the item which has key either
+   equal to given key, or which is the maximal key less than the given key. */
+
+struct  path_element  {
+  struct buffer_head *	pe_buffer;    /* Pointer to the buffer at the path in the tree. */
+  int         		pe_position;  /* Position in the tree node which is placed in the */
+                                      /* buffer above.                                  */
+};
+
+#define MAX_HEIGHT 5 /* maximal height of a tree. don't change this without changing JOURNAL_PER_BALANCE_CNT */
+#define EXTENDED_MAX_HEIGHT         7 /* Must be equals MAX_HEIGHT + FIRST_PATH_ELEMENT_OFFSET */
+#define FIRST_PATH_ELEMENT_OFFSET   2 /* Must be equal to at least 2. */
+
+#define ILLEGAL_PATH_ELEMENT_OFFSET 1 /* Must be equal to FIRST_PATH_ELEMENT_OFFSET - 1 */
+#define MAX_FEB_SIZE 6   /* this MUST be MAX_HEIGHT + 1. See about FEB below */
+
+
+
+/* We need to keep track of who the ancestors of nodes are.  When we
+   perform a search we record which nodes were visited while
+   descending the tree looking for the node we searched for. This list
+   of nodes is called the path.  This information is used while
+   performing balancing.  Note that this path information may become
+   invalid, and this means we must check it when using it to see if it
+   is still valid. You'll need to read search_by_key and the comments
+   in it, especially about decrement_counters_in_path(), to understand
+   this structure. */
+struct  path {
+  struct  path_element  path_elements[EXTENDED_MAX_HEIGHT];	/* Array of the path elements.  */
+  int                   path_length;                      	/* Length of the array above.   */
+  int			pos_in_item;
+};
+
+/* Get path element by path and path position. */
+#define PATH_OFFSET_PELEMENT(p_s_path,n_offset)  ((p_s_path)->path_elements +(n_offset))
+
+/* Get buffer header at the path by path and path position. */
+#define PATH_OFFSET_PBUFFER(p_s_path,n_offset)   (PATH_OFFSET_PELEMENT(p_s_path,n_offset)->pe_buffer)
+
+/* Get position in the element at the path by path and path position. */
+#define PATH_OFFSET_POSITION(p_s_path,n_offset) (PATH_OFFSET_PELEMENT(p_s_path,n_offset)->pe_position)
+
+
+#define PATH_PLAST_BUFFER(p_s_path) (PATH_OFFSET_PBUFFER((p_s_path), (p_s_path)->path_length))
+#define PATH_LAST_POSITION(p_s_path) (PATH_OFFSET_POSITION((p_s_path), (p_s_path)->path_length))
+
+
+#define PATH_PITEM_HEAD(p_s_path)    B_N_PITEM_HEAD(PATH_PLAST_BUFFER(p_s_path),PATH_LAST_POSITION(p_s_path))
+
+/* in do_balance leaf has h == 0 in contrast with path structure,
+   where root has level == 0. That is why we need these defines */
+#define PATH_H_PBUFFER(p_s_path, h) PATH_OFFSET_PBUFFER (p_s_path, p_s_path->path_length - (h))	/* tb->S[h] */
+#define PATH_H_PPARENT(path, h) PATH_H_PBUFFER (path, (h) + 1)			/* tb->F[h] or tb->S[0]->b_parent */
+#define PATH_H_POSITION(path, h) PATH_OFFSET_POSITION (path, path->path_length - (h))	
+#define PATH_H_B_ITEM_ORDER(path, h) PATH_H_POSITION(path, h + 1)		/* tb->S[h]->b_item_order */
+
+#define PATH_H_PATH_OFFSET(p_s_path, n_h) ((p_s_path)->path_length - (n_h))
+
+
+/***************************************************************************/
+/*                       MISC                                              */
+/***************************************************************************/
+
+
+/* Size of pointer to the unformatted node. */
+#define UNFM_P_SIZE (sizeof(unsigned long))
+
+#define INODE_PKEY(inode) ((struct key *)((inode)->u.reiserfs_i.i_key))
+
+/* these say to reiserfs_file_read about desired kind of read ahead */
+
+#ifdef REISERFS_CHECK
+extern int g_kmalloc_count;
+#endif
+
+
+
+
+/***************************************************************************/
+/*                PRESERVE LIST STUFF                                      */
+/***************************************************************************/
+/* This flag tracks whether a buffer might contain data that has been shifted to it from another
+   node which is on disk and might be obliterated if power fails and this buffer is not written to
+   disk before that other node is. This flag is cleared in unlock buffers..  Note that this assumes
+   that write_caching is turned off for the disk.  Linux drivers turn write_caching off by default,
+   so this should be correct.  I need this flag to ensure that we don't pass a block, from which
+   items were shifted, to the scsi controller, free the preserve list, use a freed block, and then
+   have the scsi controller reorder the writes so that the supposedly preserved block is overwritten
+   before the block containing the shifted items reaches disk, and then risk the system crashing
+   before the shifted items reach disk.  See preserve.c for a discussion of the preserve list and
+   its role. -Hans */
+#define BH_Suspected_Recipient	12
+
+/* 1 if the node that this buffer has assigned to it has not been written to disk since it was
+   assigned to the buffer.  If a buffer is BH_Unwritten then there is no need to preserve the
+   contents of its block after balancing shifts data from it.  See preserve.c for a discussion of
+   the preserve list and its role. */
+#define BH_Unwritten	13
+
+/* 1 if the block that this buffer was last read from or written to has been placed on the preserved
+   list.  See preserve.c for a discussion of the preserve list and its role. */
+#define BH_Preserved	14
+
+
+/* modes of preserving */
+#define PRESERVE_DIRECT_TO_INDIRECT 1
+#define PRESERVE_INDIRECT_TO_DIRECT 2
+#define PRESERVE_RENAMING 3
+#define NOTHING_SPECIAL 4
+
+
+/* return value for get_space_from_preserve_list */
+#define PRESERVE_LIST_WAS_EMPTY 0
+#define FEW_BLOCKS_ARE_FREED 1
+
+
+#define MAX_UL_INT 0xffffffff
+#define MAX_INT    0x7ffffff
+#define MAX_US_INT 0xffff
+
+#define MAX_KEY_OFFSET		MAX_UL_INT
+#define MAX_KEY_UNIQUENESS	MAX_UL_INT
+#define MAX_KEY_OBJECTID	MAX_UL_INT
+
+#define MAX_B_NUM  MAX_UL_INT
+#define MAX_FC_NUM MAX_US_INT
+
+
+/* the purpose is to detect overflow of an unsigned short */
+#define REISERFS_LINK_MAX (MAX_US_INT - 1000)
+
+
+/* The following defines are used in reiserfs_insert_item and reiserfs_append_item  */
+#define REISERFS_KERNEL_MEM		0	/* reiserfs kernel memory mode	*/
+#define REISERFS_USER_MEM		1	/* reiserfs user memory mode		*/
+
+
+/***************************************************************************/
+/*                  FIXATE NODES                                           */
+/***************************************************************************/
+
+#define VI_TYPE_STAT_DATA 1
+#define VI_TYPE_DIRECT 2
+#define VI_TYPE_INDIRECT 4
+#define VI_TYPE_DIRECTORY 8
+#define VI_TYPE_FIRST_DIRECTORY_ITEM 16
+#define VI_TYPE_INSERTED_DIRECTORY_ITEM 32
+
+#define VI_TYPE_LEFT_MERGEABLE 64
+#define VI_TYPE_RIGHT_MERGEABLE 128
+
+/* To make any changes in the tree we always first find node, that contains item to be changed/deleted or
+   place to insert a new item. We call this node S. To do balancing we need to decide what we will shift to
+   left/right neighbor, or to a new node, where new item will be etc. To make this analysis simpler we build
+   virtual node. Virtual node is an array of items, that will replace items of node S. (For instance if we are
+   going to delete an item, virtual node does not contain it). Virtual node keeps information about item sizes
+   and types, mergeability of first and last items, sizes of all entries in directory item. We use this array
+   of items when calculating what we can shift to neighbors and how many nodes we have to have if we do not
+   any shiftings, if we shift to left/right neighbor or to both. */
+struct virtual_item
+{
+  unsigned short vi_type;		/* item type, mergeability */
+  unsigned short vi_item_len;           /* length of item that it will have after balancing */
+  
+  short vi_entry_count;			/* number of entries in directory item (including the new one if any,
+					   or excluding entry if it must be cut) */
+  unsigned short * vi_entry_sizes;	/* array of entry lengths for directory item */
+};
+
+struct virtual_node
+{
+  char * vn_free_ptr;		/* this is a pointer to the free space in the buffer */
+  unsigned short vn_nr_item;	/* number of items in virtual node */
+  short vn_size;        	/* size of node , that node would have if it has unlimited size and no balancing is performed */
+  short vn_mode;		/* mode of balancing (paste, insert, delete, cut) */
+  short vn_affected_item_num; 
+  short vn_pos_in_item;
+  struct item_head * vn_ins_ih;	/* item header of inserted item, 0 for other modes */
+  struct virtual_item * vn_vi;	/* array of items (including a new one, excluding item to be deleted) */
+};
+
+
+/***************************************************************************/
+/*                  TREE BALANCE                                           */
+/***************************************************************************/
+
+/* This temporary structure is used in tree balance algorithms, and
+   constructed as we go to the extent that its various parts are
+   needed.  It contains arrays of nodes that can potentially be
+   involved in the balancing of node S, and parameters that define how
+   each of the nodes must be balanced.  Note that in these algorithms
+   for balancing the worst case is to need to balance the current node
+   S and the left and right neighbors and all of their parents plus
+   create a new node.  We implement S1 balancing for the leaf nodes
+   and S0 balancing for the internal nodes (S1 and S0 are defined in
+   our papers.)*/
+
+#define MAX_FREE_BLOCK 7	/* size of the array of buffers to free at end of do_balance */
+
+/*#define MAX_DIRTIABLE 3*/		/* L, S, R */
+#define MAX_PRESERVE_NODES 2
+
+/* maximum number of FEB blocknrs on a single level */
+#define MAX_AMOUNT_NEEDED 2
+
+/* someday somebody will prefix every field in this struct with tb_ */
+struct tree_balance
+{
+  struct super_block * tb_sb;
+  struct path * tb_path;
+  struct buffer_head * L[MAX_HEIGHT];        /* array of left neighbors of nodes in the path */
+  struct buffer_head * R[MAX_HEIGHT];        /* array of right neighbors of nodes in the path*/
+  struct buffer_head * FL[MAX_HEIGHT];       /* array of fathers of the left  neighbors      */
+  struct buffer_head * FR[MAX_HEIGHT];       /* array of fathers of the right neighbors      */
+  struct buffer_head * CFL[MAX_HEIGHT];      /* array of common parents of center node and its left neighbor  */
+  struct buffer_head * CFR[MAX_HEIGHT];      /* array of common parents of center node and its right neighbor */
+
+  /* array of blocknr's that are free and are the nearest to the left node that are usable
+     for writing dirty formatted leaves, using the write_next_to algorithm. */
+  /*unsigned long free_and_near[MAX_DIRTIABLE];*/
+
+  /* nodes will be used in preserving */
+                                /* so we don't just get blocks, we have to get buffers which we likely won't use.  Seems
+                                   like it is not optimal. -Hans */
+  struct buffer_head * tb_nodes_for_preserving[MAX_PRESERVE_NODES];
+  struct buffer_head * preserved[MAX_PRESERVE_NODES];
+  struct buffer_head * FEB[MAX_FEB_SIZE]; /* array of empty buffers. Number of buffers in array equals
+					     cur_blknum. */
+  struct buffer_head * used[MAX_FEB_SIZE];
+  struct buffer_head * thrown[MAX_FEB_SIZE];
+  short int lnum[MAX_HEIGHT];	/* array of number of items which must be shifted to the left in
+				   order to balance the current node; for leaves includes item
+				   that will be partially shifted; for internal nodes, it is
+				   the number of child pointers rather than items. It includes
+				   the new item being created.  For preserve_shifted() purposes
+				   the code sometimes subtracts one from this number to get the
+				   number of currently existing items being shifted, and even
+				   more often for leaves it subtracts one to get the number of
+				   wholly shifted items for other purposes. */
+  short int rnum[MAX_HEIGHT];	/* substitute right for left in comment above */
+  short int lkey[MAX_HEIGHT];               /* array indexed by height h mapping the key delimiting L[h] and
+					       S[h] to its item number within the node CFL[h] */
+  short int rkey[MAX_HEIGHT];               /* substitute r for l in comment above */
+  short int insert_size[MAX_HEIGHT];        /* the number of bytes by we are trying to add or remove from
+					       S[h]. A negative value means removing.  */
+  short int blknum[MAX_HEIGHT];             /* number of nodes that will replace node S[h] after
+					       balancing on the level h of the tree.  If 0 then S is
+					       being deleted, if 1 then S is remaining and no new nodes
+					       are being created, if 2 or 3 then 1 or 2 new nodes is
+					       being created */
+
+  /* fields that are used only for balancing leaves of the tree */
+  short int cur_blknum;	/* number of empty blocks having been already allocated			*/
+  short int s0num;             /* number of items that fall into left most  node when S[0] splits	*/
+  short int s1num;             /* number of items that fall into first  new node when S[0] splits	*/
+  short int s2num;             /* number of items that fall into second new node when S[0] splits	*/
+  short int lbytes;            /* number of bytes which can flow to the left neighbor from the	left	*/
+  /* most liquid item that cannot be shifted from S[0] entirely		*/
+  /* if -1 then nothing will be partially shifted */
+  short int rbytes;            /* number of bytes which will flow to the right neighbor from the right	*/
+  /* most liquid item that cannot be shifted from S[0] entirely		*/
+  /* if -1 then nothing will be partially shifted                           */
+  short int s1bytes;		/* number of bytes which flow to the first  new node when S[0] splits	*/
+            			/* note: if S[0] splits into 3 nodes, then items do not need to be cut	*/
+  short int s2bytes;
+  struct buffer_head * buf_to_free[MAX_FREE_BLOCK]; /* buffers which are to be freed after do_balance finishes by unfix_nodes */
+  char * vn_buf;		/* kmalloced memory. Used to create
+				   virtual node and keep map of
+				   dirtied bitmap blocks */
+  int vn_buf_size;		/* size of the vn_buf */
+  struct virtual_node * tb_vn;	/* VN starts after bitmap of bitmap blocks */
+  char preserve_mode;		/* indicates that the deletion that will be done is part of a conversion of
+				   a tail to an unformatted node, and the buffer with the item should be
+				   preserve_shifted(). see preserve.c */
+} ;
+#define DIRTY_BITMAP_MAP(tb) ((tb)->vn_buf)
+
+#if 0
+				/* when balancing we potentially affect a 3 node wide column of nodes
+                                   in the tree (the top of the column may be tapered). C is the nodes
+                                   at the center of this column, and L and R are the nodes to the
+                                   left and right.  */
+  struct seal * L_path_seals[MAX_HEIGHT];
+  struct seal * C_path_seals[MAX_HEIGHT];
+  struct seal * R_path_seals[MAX_HEIGHT];
+  char L_path_lock_types[MAX_HEIGHT];   /* 'r', 'w', or 'n' for read, write, or none */
+  char C_path_lock_types[MAX_HEIGHT];
+  char R_path_lock_types[MAX_HEIGHT];
+
+
+  struct seal_list_elem * C_seal[MAX_HEIGHT];        /* array of seals on nodes in the path */
+  struct seal_list_elem * L_seal[MAX_HEIGHT];        /* array of seals on left neighbors of nodes in the path */
+  struct seal_list_elem * R_seal[MAX_HEIGHT];        /* array of seals on right neighbors of nodes in the path*/
+  struct seal_list_elem * FL_seal[MAX_HEIGHT];       /* array of seals on fathers of the left  neighbors      */
+  struct seal_list_elem * FR_seal[MAX_HEIGHT];       /* array of seals on fathers of the right neighbors      */
+  struct seal_list_elem * CFL_seal[MAX_HEIGHT];      /* array of seals on common parents of center node and its left neighbor  */
+  struct seal_list_elem * CFR_seal[MAX_HEIGHT];      /* array of seals on common parents of center node and its right neighbor */
+ 
+  struct char C_desired_lock_type[MAX_HEIGHT]; /* 'r', 'w', or 'n' for read, write, or none */
+  struct char L_desired_lock_type[MAX_HEIGHT];        
+  struct char R_desired_lock_type[MAX_HEIGHT];        
+  struct char FL_desired_lock_type[MAX_HEIGHT];       
+  struct char FR_desired_lock_type[MAX_HEIGHT];       
+  struct char CFL_desired_lock_type[MAX_HEIGHT];      
+  struct char CFR_desired_lock_type[MAX_HEIGHT];      
+#endif
+
+
+
+
+
+/* These are modes of balancing */
+
+/* When inserting an item. */
+#define M_INSERT	'i'
+/* When inserting into (directories only) or appending onto an already
+   existant item. */
+#define M_PASTE		'p'
+/* When deleting an item. */
+#define M_DELETE	'd'
+/* When truncating an item or removing an entry from a (directory) item. */
+#define M_CUT 		'c'
+
+/* used when balancing on leaf level skipped (in reiserfsck) */
+#define M_INTERNAL	'n'
+
+/* When further balancing is not needed, then do_balance does not need
+   to be called. */
+#define M_SKIP_BALANCING 		's'
+#define M_CONVERT	'v'
+
+/* modes of leaf_move_items */
+#define LEAF_FROM_S_TO_L 0
+#define LEAF_FROM_S_TO_R 1
+#define LEAF_FROM_R_TO_L 2
+#define LEAF_FROM_L_TO_R 3
+#define LEAF_FROM_S_TO_SNEW 4
+
+#define FIRST_TO_LAST 0
+#define LAST_TO_FIRST 1
+
+/* used in do_balance for passing parent of node information that has
+   been gotten from tb struct */
+struct buffer_info {
+	struct buffer_head * bi_bh;
+	struct buffer_head * bi_parent;
+	int bi_position;
+};
+
+
+/* there are 4 types of items: stat data, directory item, indirect, direct.
++-------------------+------------+--------------+------------+
+|	            |  k_offset  | k_uniqueness | mergeable? |
++-------------------+------------+--------------+------------+
+|     stat data     |	0        |      0       |   no       |
++-------------------+------------+--------------+------------+
+| 1st directory item| DOT_OFFSET |DIRENTRY_UNIQUENESS|   no       | 
+| non 1st directory | hash value |              |   yes      |
+|     item          |            |              |            |
++-------------------+------------+--------------+------------+
+| indirect item     | offset + 1 |TYPE_INDIRECT |   if this is not the first indirect item of the object
++-------------------+------------+--------------+------------+
+| direct item       | offset + 1 |TYPE_DIRECT   | if not this is not the first direct item of the object
++-------------------+------------+--------------+------------+
+*/
+#define TYPE_STAT_DATA 0x0
+#define TYPE_DIRECT 0xffffffff
+#define TYPE_INDIRECT 0xfffffffe
+#define TYPE_DIRECTORY_MAX 0xfffffffd
+
+
+#define KEY_IS_STAT_DATA_KEY(p_s_key) 	( (p_s_key)->k_uniqueness == TYPE_STAT_DATA )
+#define KEY_IS_DIRECTORY_KEY(p_s_key)	( (p_s_key)->k_uniqueness == DIRENTRY_UNIQUENESS )
+#define KEY_IS_DIRECT_KEY(p_s_key) 	( (p_s_key)->k_uniqueness == TYPE_DIRECT )
+#define KEY_IS_INDIRECT_KEY(p_s_key)	( (p_s_key)->k_uniqueness == TYPE_INDIRECT )
+
+#define I_IS_STAT_DATA_ITEM(p_s_ih) 	KEY_IS_STAT_DATA_KEY(&((p_s_ih)->ih_key))
+#define I_IS_DIRECTORY_ITEM(p_s_ih) 	KEY_IS_DIRECTORY_KEY(&((p_s_ih)->ih_key))
+#define I_IS_DIRECT_ITEM(p_s_ih) 	KEY_IS_DIRECT_KEY(&((p_s_ih)->ih_key))
+#define I_IS_INDIRECT_ITEM(p_s_ih) 	KEY_IS_INDIRECT_KEY(&((p_s_ih)->ih_key))
+
+/*
+#ifdef __KERNEL__
+
+extern inline int is_left_mergeable (struct item_head * ih, unsigned long bsize)
+{
+  if (I_IS_DIRECT_ITEM (ih))
+    return (ih->ih_key.k_offset % bsize != 1);
+
+  if (I_IS_INDIRECT_ITEM (ih))
+    return (ih->ih_key.k_offset != 1);
+
+  if (I_IS_DIRECTORY_ITEM (ih))
+   return ((ih)->ih_key.k_offset != DOT_OFFSET);
+
+#ifdef REISERFS_CHECK
+  if ( ! I_IS_STAT_DATA_ITEM (ih))
+    reiserfs_panic (0, "is_left_mergeable: 1020: item [%lu %lu %lu %lu] must be a stat data",
+		    ih->ih_key.k_dir_id, ih->ih_key.k_objectid, ih->ih_key.k_offset, ih->ih_key.k_uniqueness);
+#endif
+  return 0;
+}
+
+
+
+#endif*/	/* __KERNEL__ */
+
+#define COMP_KEYS comp_keys
+#define COMP_SHORT_KEYS comp_short_keys
+/*#define COMP_KEYS(p_s_key1, p_s_key2)		comp_keys((unsigned long *)(p_s_key1), (unsigned long *)(p_s_key2))
+#define COMP_SHORT_KEYS(p_s_key1, p_s_key2)	comp_short_keys((unsigned long *)(p_s_key1), (unsigned long *)(p_s_key2))*/
+
+
+/* number of blocks pointed to by the indirect item */
+#define I_UNFM_NUM(p_s_ih)	( (p_s_ih)->ih_item_len / UNFM_P_SIZE )
+
+/* the used space within the unformatted node corresponding to pos within the item pointed to by ih */
+#define I_POS_UNFM_SIZE(ih,pos,size) (((pos) == I_UNFM_NUM(ih) - 1 ) ? (size) - (ih)->u.ih_free_space : (size))
+
+/* number of bytes contained by the direct item or the unformatted nodes the indirect item points to */
+
+#define I_BYTES_NUMBER(ih,size) (( I_IS_INDIRECT_ITEM(ih) ) ?\
+				 I_UNFM_NUM(ih)*size - (ih)->u.ih_free_space :\
+				 (( I_IS_DIRECT_ITEM(ih) ) ? (ih)->ih_item_len : 0 /* stat data */)) 
+
+/* check whether byte number 'offset' is in this item */
+#define I_OFF_BYTE_IN_ITEM(p_s_ih, n_offset, n_blocksize) \
+                  ( (p_s_ih)->ih_key.k_offset <= (n_offset) && \
+                    (p_s_ih)->ih_key.k_offset + I_BYTES_NUMBER(p_s_ih,n_blocksize) > (n_offset) )
+
+/* get the item header */ 
+#define B_N_PITEM_HEAD(bh,item_num) ( (struct item_head * )((bh)->b_data + BLKH_SIZE) + (item_num) )
+
+/* get key */
+#define B_N_PDELIM_KEY(bh,item_num) ( (struct key * )((bh)->b_data + BLKH_SIZE) + (item_num) )
+
+/* get the key */
+#define B_N_PKEY(bh,item_num) ( &(B_N_PITEM_HEAD(bh,item_num)->ih_key) )
+
+/* get item body */
+#define B_N_PITEM(bh,item_num) ( (bh)->b_data + B_N_PITEM_HEAD((bh),(item_num))->ih_item_location)
+
+/* get the stat data by the buffer header and the item order */
+#define B_N_STAT_DATA(bh,nr) \
+( (struct stat_data *)((bh)->b_data+B_N_PITEM_HEAD((bh),(nr))->ih_item_location ) )
+
+                 /* following defines use reiserfs buffer header and item header */
+ /* get item body */
+#define B_I_PITEM(bh,ih) ( (bh)->b_data + (ih)->ih_item_location )
+
+/* get stat-data */
+#define B_I_STAT_DATA(bh, ih) ( (struct stat_data * )((bh)->b_data + (ih)->ih_item_location) )
+
+#define MAX_DIRECT_ITEM_LEN(size) ((size) - BLKH_SIZE - 2*IH_SIZE - SD_SIZE - UNFM_P_SIZE)
+
+/* indirect items consist of entries which contain blocknrs, pos
+   indicates which entry, and B_I_POS_UNFM_POINTER resolves to the
+   blocknr contained by the entry pos points to */
+#define B_I_POS_UNFM_POINTER(bh,ih,pos) (*(((unsigned long *)B_I_PITEM(bh,ih)) + (pos)))
+
+/* Reiserfs buffer cache statistics. */
+#ifdef REISERFS_CACHE_STAT
+ struct reiserfs_cache_stat
+	{
+  	int nr_reiserfs_ll_r_block; 		/* Number of block reads. */
+  	int nr_reiserfs_ll_w_block; 		/* Number of block writes. */
+	int nr_reiserfs_schedule; 		/* Number of locked buffers waits. */
+	unsigned long nr_reiserfs_bread;	/* Number of calls to reiserfs_bread function */
+	unsigned long nr_returns; /* Number of breads of buffers that were hoped to contain a key but did not after bread completed
+				     (usually due to object shifting while bread was executing.)
+				     In the code this manifests as the number
+				     of times that the repeat variable is nonzero in search_by_key.*/
+	unsigned long nr_fixed;		/* number of calls of fix_nodes function */
+	unsigned long nr_failed;	/* number of calls of fix_nodes in which schedule occurred while the function worked */
+	unsigned long nr_find1;		/* How many times we access a child buffer using its direct pointer from an internal node.*/
+	unsigned long nr_find2;	        /* Number of times there is neither a direct pointer to
+					   nor any entry in the child list pointing to the buffer. */
+	unsigned long nr_find3;	        /* When parent is locked (meaning that there are no direct pointers)
+					   or parent is leaf and buffer to be found is an unformatted node. */
+	}  cache_stat;
+#endif
+
+
+
+/***************************************************************************/
+/*                    FUNCTION DECLARATIONS                                */
+/***************************************************************************/
+
+/*#ifdef __KERNEL__*/
+
+/* journal.c see journal.c for all the comments here */
+
+#define JOURNAL_TRANS_HALF 1018   /* must be correct to keep the desc and commit structs at 4k */
+
+/* first block written in a commit.  BUG, not 64bit safe */
+struct reiserfs_journal_desc {
+  unsigned long j_trans_id ;			/* id of commit */
+  unsigned long j_len ;			/* length of commit. len +1 is the commit block */
+  unsigned long j_mount_id ;				/* mount id of this trans*/
+  unsigned long j_realblock[JOURNAL_TRANS_HALF] ; /* real locations for each block */
+  char j_magic[12] ;
+} ;
+
+/* last block written in a commit BUG, not 64bit safe */
+struct reiserfs_journal_commit {
+  unsigned long j_trans_id ;			/* must match j_trans_id from the desc block */
+  unsigned long j_len ;			/* ditto */
+  unsigned long j_realblock[JOURNAL_TRANS_HALF] ; /* real locations for each block */
+  char j_digest[16] ;			/* md5 sum of all the blocks involved, including desc and commit. not used, kill it */
+} ;
+
+/* this header block gets written whenever a transaction is considered fully flushed, and is more recent than the
+** last fully flushed transaction.  fully flushed means all the log blocks and all the real blocks are on disk,
+** and this transaction does not need to be replayed.
+*/
+struct reiserfs_journal_header {
+  unsigned long j_last_flush_trans_id ;		/* id of last fully flushed transaction */
+  unsigned long j_first_unflushed_offset ;      /* offset in the log of where to start replay after a crash */
+  unsigned long j_mount_id ;
+} ;
+
+
+#define JOURNAL_BLOCK_COUNT 8192 /* number of blocks in the journal */
+
+#ifdef __KERNEL__
+
+/* biggest tunable defines are right here */
+#define JOURNAL_MAX_BATCH   900 /* max blocks to batch into one transaction, don't make this any bigger than 900 */
+#define JOURNAL_MAX_COMMIT_AGE 30 
+#define JOURNAL_MAX_TRANS_AGE 30
+#define JOURNAL_PER_BALANCE_CNT 12   /* must be >= (5 + 2 * (MAX_HEIGHT-2) + 1) */
+
+/* both of these can be as low as 1, or as high as you want.  The min is the
+** number of 4k bitmap nodes preallocated on mount. New nodes are allocated
+** as needed, and released when transactions are committed.  On release, if 
+** the current number of nodes is > max, the node is freed, otherwise, 
+** it is put on a free list for faster use later.
+*/
+#define REISERFS_MIN_BITMAP_NODES 10 
+#define REISERFS_MAX_BITMAP_NODES 100 
+
+/* state bits for the per FS journal struct */
+#define JOURNAL_UNMOUNTING 1 /* tells the commit thread he's done */
+
+/* hash funcs more or less stolen from buffer cache.  t is a pointer to the hash table */
+#define JHASHDEV(d) ((unsigned int) (d))
+#define _jhashfn(dev,block)  (((unsigned)(JHASHDEV(dev)^(block))) % JOURNAL_HASH_SIZE)
+#define journal_hash(t,dev,block) ((t)[_jhashfn((dev),(block))])
+
+/* finds n'th buffer with 0 being the start of this commit.  Needs to go away, j_ap_blocks has changed
+** since I created this.  One chunk of code in journal.c needs changing before deleting it
+*/
+#define JOURNAL_BUFFER(j,n) ((j)->j_ap_blocks[((j)->j_start + (n)) % JOURNAL_BLOCK_COUNT])
+
+int journal_init(struct super_block *) ;
+int journal_release(struct reiserfs_transaction_handle*, struct super_block *) ;
+int journal_release_error(struct reiserfs_transaction_handle*, struct super_block *) ;
+int journal_end(struct reiserfs_transaction_handle *, struct super_block *, unsigned long) ;
+int journal_end_sync(struct reiserfs_transaction_handle *, struct super_block *, unsigned long) ;
+int journal_mark_dirty_nolog(struct reiserfs_transaction_handle *, struct super_block *, struct buffer_head *bh) ;
+int journal_mark_freed(struct reiserfs_transaction_handle *, struct super_block *, unsigned long blocknr) ;
+int push_journal_writer(char *w) ;
+int pop_journal_writer(int windex) ;
+int journal_lock_dobalance(struct super_block *p_s_sb) ;
+int journal_unlock_dobalance(struct super_block *p_s_sb) ;
+int journal_transaction_should_end(struct reiserfs_transaction_handle *, int) ;
+int reiserfs_in_journal(struct super_block *p_s_sb, kdev_t dev, unsigned long bl, int size, int searchall, unsigned long *next) ;
+int journal_begin(struct reiserfs_transaction_handle *, struct super_block *p_s_sb, unsigned long) ;
+int journal_join(struct reiserfs_transaction_handle *, struct super_block *p_s_sb, unsigned long) ;
+struct super_block *reiserfs_get_super(kdev_t dev) ;
+void flush_async_commits(struct super_block *p_s_sb, int *repeat) ;
+
+int remove_from_transaction(struct super_block *p_s_sb, unsigned long blocknr, int already_cleaned) ;
+int remove_from_journal_list(struct super_block *s, struct reiserfs_journal_list *jl, struct buffer_head *bh, int remove_freed) ;
+
+int buffer_journaled(struct buffer_head *bh) ;
+int mark_buffer_journal_new(struct buffer_head *bh) ;
+int reiserfs_sync_all_buffers(kdev_t dev, int wait) ;
+int reiserfs_sync_buffers(kdev_t dev, int wait) ;
+void reiserfs_file_buffer(struct buffer_head *bh, int list) ;
+void reiserfs_update_inode_transaction(struct inode *) ;
+int reiserfs_inode_in_this_transaction(struct inode *) ;
+void reiserfs_commit_for_inode(struct inode *) ;
+int reiserfs_allocate_list_bitmaps(struct super_block *, struct reiserfs_list_bitmap *, int) ;
+
+				/* Why is this kerplunked right here? -Hans */
+/* buffer was journaled, waiting to get to disk */
+static inline int buffer_journal_dirty(struct buffer_head *bh) {
+  if (bh)
+    return test_bit(BH_JDirty_wait, &bh->b_state) ;
+  else
+    return 0 ;
+}
+static inline int mark_buffer_notjournal_dirty(struct buffer_head *bh) {
+  if (bh)
+    clear_bit(BH_JDirty_wait, &bh->b_state) ;
+  return 0 ;
+}
+static inline int mark_buffer_notjournal_new(struct buffer_head *bh) {
+  if (bh) {
+    clear_bit(BH_JNew, &bh->b_state) ;
+  }
+  return 0 ;
+}
+
+#endif // __KERNEL__
+
+
+/* objectid.c */
+unsigned long reiserfs_get_unused_objectid (struct reiserfs_transaction_handle *th, struct super_block * s);
+void reiserfs_release_objectid (struct reiserfs_transaction_handle *th, unsigned long objectid_to_release, struct super_block * s);
+
+
+/* stree.c */
+int B_IS_IN_TREE(struct buffer_head *);
+extern inline void copy_key (void * to, void * from);
+extern inline void copy_short_key (void * to, void * from);
+extern inline void copy_item_head(void * p_v_to, void * p_v_from);
+extern inline int comp_keys (void * p_s_key1, void * p_s_key2);
+extern inline int  comp_short_keys (void * p_s_key1, void * p_s_key2);
+int comp_items (struct item_head  * p_s_ih, struct path * p_s_path);
+struct key * get_rkey (struct path * p_s_chk_path, struct super_block  * p_s_sb);
+inline int bin_search (void * p_v_key, void * p_v_base, int p_n_num, int p_n_width, int * p_n_pos);
+int search_by_key (struct super_block *, struct key *, struct path *, int * , int, int);
+int search_by_objectid (struct super_block *, struct key *, struct path *, int * , int, int);
+int search_for_position_by_key (struct super_block * p_s_sb, struct key * p_s_key, struct path * p_s_search_path, int * p_n_pos_in_item, int * p_n_repeat);
+extern inline void decrement_bcount (struct buffer_head * p_s_bh);
+void decrement_counters_in_path (struct path * p_s_search_path);
+void pathrelse (struct path * p_s_search_path);
+
+#ifdef __KERNEL__
+
+int reiserfs_insert_item (struct reiserfs_transaction_handle *th, struct super_block * sb, struct path * path, 
+                          struct item_head * ih, const char * body, int memmode, int zeros_number, int preserve_mode);
+int reiserfs_paste_into_item (struct reiserfs_transaction_handle *th, struct super_block * sb, 
+                              struct path * path, int * pos_in_item, struct key * key,	
+			      const char * body, int paste_size, int memmode, int zeros_number);	/* returns -1 if failed and paste_size otherwise */
+int reiserfs_cut_from_item (struct reiserfs_transaction_handle *th, struct inode * inode, 
+			    struct super_block * sb, struct path * path, int * pos_in_item, struct key * key,
+			    unsigned long new_file_size, int preserve_mode);
+int reiserfs_delete_item (struct reiserfs_transaction_handle *th, struct inode * inode, struct path * p_s_path, 
+                          int * p_n_pos_in_item, struct key * p_s_item_key,
+			  struct buffer_head  * p_s_un_bh, int preserve_mode);
+void reiserfs_delete_object (struct reiserfs_transaction_handle *th, struct inode * p_s_inode);
+void reiserfs_truncate_file (struct  inode * p_s_inode);
+int reiserfs_file_release(struct inode *p_s_inode, struct file *p_s_filp) ;
+int lock_inode_to_convert (struct inode * p_s_inode);
+void unlock_inode_after_convert (struct inode * p_s_inode);
+int increment_i_read_sync_counter (struct inode * p_s_inode);
+void decrement_i_read_sync_counter (struct inode * p_s_inode);
+
+#endif // __KERNEL__
+
+#ifndef REISERFS_FSCK
+
+inline int is_left_mergeable (struct item_head * ih, unsigned long bsize);
+
+#else
+
+int is_left_mergeable (struct super_block * s, struct path * path);
+int is_right_mergeable (struct super_block * s, struct path * path);
+int are_items_mergeable (struct item_head * left, struct item_head * right, int bsize);
+
+#endif
+
+
+#ifdef __KERNEL__
+
+/* inode.c */
+void store_key (struct super_block *s, struct key * key);
+void forget_key (struct super_block *s, struct key * key);
+struct inode * reiserfs_iget (struct super_block * s, struct key * key);
+void reiserfs_read_inode (struct inode * inode);
+void reiserfs_delete_inode (struct inode * inode);
+extern int reiserfs_notify_change(struct dentry * dentry, struct iattr * attr);
+int reiserfs_bmap (struct inode * inode, int block);
+void reiserfs_write_inode (struct inode * inode);
+struct inode * reiserfs_new_inode (struct reiserfs_transaction_handle *th, 
+			           const struct inode * dir, int mode, const char * symname,
+				   struct dentry *dentry, struct inode *inode, int * err);
+int reiserfs_sync_inode (struct reiserfs_transaction_handle *th, struct inode * inode);
+void if_in_ram_update_sd (struct reiserfs_transaction_handle *th, struct inode * inode);
+void reiserfs_inode_setattr(struct reiserfs_transaction_handle *th, struct inode * inode,  struct iattr * attr);
+void __wait_on_inode(struct inode * inode);
+
+/* namei.c */
+int bin_search_in_dir_item (struct item_head * ih, struct reiserfs_de_head * deh, struct key * key, int * pos_in_item);
+int search_by_entry_key (struct super_block * sb, struct key * key, struct path * path, int * pos_in_item, int * repeat);
+struct dentry * reiserfs_lookup (struct inode * dir, struct dentry *dentry);
+int reiserfs_create (struct inode * dir, struct dentry *dentry,	int mode);
+int reiserfs_mknod (struct inode * dir_inode, struct dentry *dentry, int mode, int rdev);
+int reiserfs_mkdir (struct inode * dir, struct dentry *dentry, int mode);
+int reiserfs_rmdir (struct inode * dir,	struct dentry *dentry);
+int reiserfs_unlink (struct inode * dir, struct dentry *dentry);
+int reiserfs_symlink (struct inode * dir, struct dentry *dentry, const char * symname);
+int reiserfs_link (struct dentry * old_dentry, struct inode * dir, struct dentry *dentry);
+int reiserfs_rename (struct inode * old_dir, struct dentry *old_dentry, struct inode * new_dir, struct dentry *new_dentry);
+
+/* super.c */
+inline void reiserfs_mark_buffer_dirty (struct buffer_head * bh, int flag);
+inline void reiserfs_mark_buffer_clean (struct buffer_head * bh);
+void reiserfs_panic (struct super_block * s, const char * fmt, ...);
+void reiserfs_write_super (struct super_block * s);
+void reiserfs_put_super (struct super_block * s);
+int reiserfs_remount (struct super_block * s, int * flags, char * data);
+/*int read_super_block (struct super_block * s, int size);
+int read_bitmaps (struct super_block * s);
+int read_old_bitmaps (struct super_block * s);
+int read_old_super_block (struct super_block * s, int size);*/
+struct super_block * reiserfs_read_super (struct super_block * s, void * data, int silent);
+int reiserfs_statfs (struct super_block * s, struct statfs * buf, int bufsiz);
+int init_reiserfs_fs (void);
+
+/* dir.c */
+extern struct inode_operations reiserfs_dir_inode_operations;
+
+/* file.c */
+extern struct inode_operations reiserfs_file_inode_operations;
+int  direct_to_indirect(struct reiserfs_transaction_handle *,
+		        struct super_block *, struct inode *, 
+			struct path *, int n_item_zeros_to_add,
+			const char * p_c_buf, int n_item_bytes_to_write,
+		        struct buffer_head  * p_s_un_bh) ;
+
+int get_new_buffer(struct reiserfs_transaction_handle *,
+                   struct super_block *, struct buffer_head *,
+		   struct buffer_head **, struct path *,
+		   struct inode *, unsigned long) ;
+/* symlink.c */
+extern struct inode_operations reiserfs_symlink_inode_operations;
+
+/* buffer2.c */
+void wait_buffer_until_released (struct buffer_head * bh);
+struct buffer_head * reiserfs_bread (kdev_t n_dev, int n_block, int n_size, int * p_n_repeat);
+struct buffer_head * reiserfs_get_hash_table (kdev_t  n_dev, int n_block, int n_size, int * p_n_repeat);
+int reiserfs_sync_file (struct file * p_s_filp, struct dentry * p_s_dentry);
+void reiserfs_show_buffers (kdev_t dev);
+
+#endif // __KERNEL__
+
+
+/* buffer.c */
+inline int test_and_wait_on_buffer (struct buffer_head * p_s_bh);
+struct buffer_head * reiserfs_getblk (kdev_t n_dev, int n_block, int n_size, int * p_n_repeat);
+void reiserfs_rehash_buffer (struct buffer_head * bh, unsigned long block);
+void mark_suspected_recipients_dirty (struct reiserfs_transaction_handle *th, kdev_t dev);
+void fixup_reiserfs_buffers (kdev_t dev);
+
+
+
+
+
+/*#endif  __KERNEL__ */
+
+/* fix_nodes.c */
+void * reiserfs_kmalloc (size_t size, int flags, struct super_block * s);
+void reiserfs_kfree (/*const*/ void * vp, size_t size, struct super_block * s);
+int fix_nodes (struct reiserfs_transaction_handle *th, int n_op_mode, struct tree_balance * p_s_tb, 
+               int n_pos_in_item, struct item_head * p_s_ins_ih);
+void unfix_nodes (struct reiserfs_transaction_handle *th, struct tree_balance *);
+void free_buffers_in_tb (struct tree_balance * p_s_tb);
+void init_path (struct path *);
+
+/* prints.c */
+void reiserfs_panic (struct super_block * s, const char * fmt, ...);
+void reiserfs_warning (const char * fmt, ...);
+void print_virtual_node (struct virtual_node * vn);
+void print_indirect_item (struct buffer_head * bh, int item_num);
+void print_tb (int mode, int item_pos, int pos_in_item, struct tree_balance * tb, char * mes);
+void print_de (struct reiserfs_dir_entry * de);
+void print_bi (struct buffer_info * bi, char * mes);
+#define PRINT_LEAF_ITEMS 1   /* print all items */
+#define PRINT_DIRECTORY_ITEMS 2 /* print directory items */
+#define PRINT_DIRECT_ITEMS 4 /* print contents of direct items */
+void print_block (struct buffer_head * bh, ...);//int print_mode, int first, int last);
+void print_path (struct tree_balance * tb, struct path * path);
+void print_bmap (struct super_block * s, int silent);
+void print_objectid_map (struct super_block * s);
+void print_block_head (struct buffer_head * bh, char * mes);
+void check_leaf (struct buffer_head * bh);
+void print_statistics (struct super_block * s);
+
+/* lbalance.c */
+int leaf_move_items (struct reiserfs_transaction_handle *th, int shift_mode, struct tree_balance * tb, 
+                     int mov_num, int mov_bytes, struct buffer_head * Snew);
+int leaf_shift_left (struct reiserfs_transaction_handle *th, struct tree_balance * tb, int shift_num, int shift_bytes);
+int leaf_shift_right (struct reiserfs_transaction_handle *th, struct tree_balance * tb, int shift_num, int shift_bytes);
+void leaf_delete_items (struct reiserfs_transaction_handle *th, struct buffer_info * cur_bi, 
+                        int last_first, int first, int del_num, int del_bytes);
+void leaf_insert_into_buf (struct reiserfs_transaction_handle *th, struct buffer_info * bi, 
+			   int before, struct item_head * inserted_item_ih, const char * inserted_item_body, 
+			   int mem_mode, int zeros_number);
+void leaf_paste_in_buffer (struct reiserfs_transaction_handle *th, struct buffer_info * bi, int pasted_item_num, 
+			   int pos_in_item, int paste_size, const char * body, int mem_mode, int zeros_number);
+void leaf_cut_from_buffer (struct reiserfs_transaction_handle *th, struct buffer_info * bi, int cut_item_num, 
+                           int pos_in_item, int cut_size);
+void leaf_paste_entries (struct buffer_head * bh, int item_num, int before, int new_entry_count, struct reiserfs_de_head * new_dehs, const char * records,
+			 int paste_size);
+
+/*#ifdef __KERNEL__*/
+/* preserve.c */
+int ready_preserve_list (struct tree_balance *, struct buffer_head * bh);
+void preserve_shifted (struct tree_balance *, struct buffer_head **, struct buffer_head *, int, struct buffer_head *);
+void add_to_preserve (unsigned long blocknr, struct super_block * sb);
+int maybe_free_preserve_list (struct super_block * sb);
+int get_space_from_preserve_list (struct super_block * s);
+inline void unpreserve (struct super_block * s, struct buffer_head * bh);
+void preserve_invalidate (struct reiserfs_transaction_handle *,
+                          struct tree_balance * tb, struct buffer_head * bh, struct buffer_head *);
+unsigned long free_and_near_block (struct tree_balance * tb);
+inline void mark_suspected_recipient (struct super_block * sb, struct buffer_head * bh);
+inline void unmark_suspected_recipient (struct super_block * sb, struct buffer_head * bh);
+int is_buffer_unwritten (struct buffer_head * bh);
+int is_buffer_preserved (struct buffer_head * bh);
+int is_buffer_suspected_recipient (struct super_block * s, struct buffer_head * bh);
+inline void mark_buffer_unwritten (struct buffer_head * bh);
+void unpreserve (struct super_block * s, struct buffer_head * bh);
+#ifdef REISERFS_CHECK
+void preserve_trace_init_bitmap (struct super_block * s);
+void preserve_trace_release_bitmap (struct super_block * s);
+void preserve_trace_reset_suspected_recipients (struct super_block * s);
+#endif
+/*#endif*/ /* __KERNEL__ */
+
+/* ibalance.c */
+int balance_internal (struct reiserfs_transaction_handle *th, struct tree_balance * , int, int, struct item_head * , 
+                      struct buffer_head **);
+
+/* do_balance.c */
+void do_balance (struct reiserfs_transaction_handle *th, struct tree_balance * tb, int pos_in_item, 
+                 struct item_head * ih, const char * body, int flag, int mem_mode, int zeros_num);
+void reiserfs_invalidate_buffer (struct reiserfs_transaction_handle *th, struct tree_balance * tb, struct buffer_head * bh, int);
+int get_left_neighbor_position (struct tree_balance * tb, int h);
+int get_right_neighbor_position (struct tree_balance * tb, int h);
+void replace_key (struct reiserfs_transaction_handle *th, struct buffer_head *, int, struct buffer_head *, int);
+void replace_lkey (struct reiserfs_transaction_handle *th, struct tree_balance *, int, struct item_head *);
+void replace_rkey (struct reiserfs_transaction_handle *th, struct tree_balance *, int, struct item_head *);
+void make_empty_node (struct buffer_info *);
+struct buffer_head * get_FEB (struct tree_balance *);
+
+/* bitmap.c */
+int is_reusable (struct super_block * s, unsigned long block, int bit_value);
+void reiserfs_free_block (struct reiserfs_transaction_handle *th, struct super_block *, unsigned long);
+int reiserfs_new_blocknrs (struct reiserfs_transaction_handle *th, struct super_block *, unsigned long *, unsigned long, int, int);
+int reiserfs_new_unf_blocknrs (struct reiserfs_transaction_handle *th,struct super_block *,unsigned long *,unsigned long,int,int);
+
+/* reiserfs_new_blocknrs and reiserfs_free_block use
+   reiserfs_mark_buffer_dirty. To complete marking buffer dirty we use
+   brelse. But first we do b_count++ instead of getblk, as bitmaps and
+   super block were acquired once by getblk at mount time. */
+#define COMPLETE_BITMAP_DIRTING_AFTER_ALLOCATING(s,bitmapnr) \
+	SB_AP_BITMAP(s)[bitmapnr]->b_count ++;\
+	brelse (SB_AP_BITMAP(s)[bitmapnr]);\
+    	SB_BUFFER_WITH_SB (s)->b_count ++;\
+	brelse (SB_BUFFER_WITH_SB (s));\
+
+#define COMPLETE_BITMAP_DIRTING_AFTER_FREEING(s,bitmapnr) \
+	SB_AP_BITMAP(s)[bitmapnr]->b_count ++;\
+	brelse (SB_AP_BITMAP(s)[bitmapnr]);\
+    	SB_BUFFER_WITH_SB (s)->b_count ++;\
+	brelse (SB_BUFFER_WITH_SB (s));\
+
+/* hashes.c */
+__u32 keyed_hash (const char *msg, int len);
+__u32 yura_hash (const char *msg, int len);
+__u32 r5_hash (const char *msg, int len);
+
+/* version.c */
+char *reiserfs_get_version_string(void) ;
+
+#ifdef __i386__
+
+extern __inline__ int 
+find_first_nonzero_bit(void * addr, unsigned size) {
+  int res;
+  int __d0;
+  void *__d1;
+
+
+  if (!size) {
+    return (0);
+  }
+  __asm__ __volatile__ (
+	  "cld\n\t"
+	  "xorl %%eax,%%eax\n\t"
+	  "repe; scasl\n\t"
+	  "je 1f\n\t"
+	  "movl -4(%%edi),%%eax\n\t"
+	  "subl $4, %%edi\n\t"
+	  "bsfl %%eax,%%eax\n\t"
+	  "1:\tsubl %%edx,%%edi\n\t"
+	  "shll $3,%%edi\n\t"
+	  "addl %%edi,%%eax"
+	  :"=a" (res),
+	  "=c"(__d0), "=D"(__d1)
+	  :"1" ((size + 31) >> 5), "d" (addr), "2" (addr));
+  return (res);
+}
+
+#else /* __i386__ */
+
+extern __inline__ int find_next_nonzero_bit(void * addr, unsigned size, unsigned offset)
+{
+	unsigned int * p = ((unsigned int *) addr) + (offset >> 5);
+	unsigned int result = offset & ~31UL;
+	unsigned int tmp;
+
+	if (offset >= size)
+		return size;
+	size -= result;
+	offset &= 31UL;
+	if (offset) {
+		tmp = *p++;
+		/* set to zero first offset bits */
+		tmp &= ~(~0UL >> (32-offset));
+		if (size < 32)
+			goto found_first;
+		if (tmp != 0U)
+			goto found_middle;
+		size -= 32;
+		result += 32;
+	}
+	while (size >= 32) {
+		if ((tmp = *p++) != 0U)
+			goto found_middle;
+		result += 32;
+		size -= 32;
+	}
+	if (!size)
+		return result;
+	tmp = *p;
+found_first:
+found_middle:
+	return result + ffs(tmp);
+}
+
+#define find_first_nonzero_bit(addr,size) find_next_nonzero_bit((addr), (size), 0)
+
+#endif /* 0 */
+
+#ifdef __KERNEL__
+/* prototypes from ioctl.c */
+int reiserfs_ioctl (struct inode * inode, struct file * filp, 
+		    unsigned int cmd, unsigned long arg);
+int reiserfs_unpack (struct inode * inode, struct file * filp);
+#endif /* __KERNEL__ */ 
+
+/* ioctl's command */
+#define REISERFS_IOC_UNPACK		_IOW(0xCD,1,long)
+
+
+#endif /* _LINUX_REISER_FS_H */
+
+
diff -urN linux/include/linux/reiserfs_fs_i.h /tmp/linux/include/linux/reiserfs_fs_i.h
--- linux/include/linux/reiserfs_fs_i.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/include/linux/reiserfs_fs_i.h	Fri Feb  2 19:05:43 2001
@@ -0,0 +1,28 @@
+/*
+ * Copiright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+#ifndef _REISER_FS_I
+#define _REISER_FS_I
+
+#define REISERFS_N_BLOCKS 10
+
+struct reiserfs_inode_info {
+  struct pipe_inode_info reserved;
+  __u32 i_key [4];
+  __u32 i_first_direct_byte;
+
+  int i_data_length;
+  __u32 i_data [REISERFS_N_BLOCKS];
+  int i_is_being_converted;
+  int i_read_sync_counter;
+  int i_pack_on_close ;
+  int i_transaction_index ;
+  int i_transaction_id ;
+
+  //nopack-attribute
+  int nopack;
+};
+
+
+#endif
diff -urN linux/include/linux/reiserfs_fs_sb.h /tmp/linux/include/linux/reiserfs_fs_sb.h
--- linux/include/linux/reiserfs_fs_sb.h	Wed Dec 31 17:00:00 1969
+++ /tmp/linux/include/linux/reiserfs_fs_sb.h	Mon Jul  9 20:33:32 2001
@@ -0,0 +1,298 @@
+/*
+ * Copyright 2000 by Hans Reiser, licensing governed by reiserfs/README
+ */
+
+#ifndef _LINUX_REISER_FS_SB
+#define _LINUX_REISER_FS_SB
+
+#ifdef __KERNEL__
+#include <linux/tqueue.h>
+#endif
+
+#define UNSET_HASH 0 // read_super will guess about, what hash names
+                     // in directories were sorted with
+#define TEA_HASH  1
+#define YURA_HASH 2
+#define R5_HASH   3
+#define DEFAULT_HASH R5_HASH
+
+struct reiserfs_super_block
+{
+  __u32 s_block_count;			/* blocks count         */
+  __u32 s_free_blocks;                  /* free blocks count    */
+  __u32 s_root_block;           	/* root block number    */
+  __u32 s_journal_block;           	/* journal block number    */
+  __u32 s_journal_dev;           	/* journal device number  */
+  __u32 s_orig_journal_size; 		/* size of the journal on FS creation.  used to make sure they don't overflow it */
+  __u32 s_journal_trans_max ;           /* max number of blocks in a transaction.  */
+  __u32 s_journal_block_count ;         /* total size of the journal. can change over time  */
+  __u32 s_journal_max_batch ;           /* max number of blocks to batch into a trans */
+  __u32 s_journal_max_commit_age ;      /* in seconds, how old can an async commit be */
+  __u32 s_journal_max_trans_age ;       /* in seconds, how old can a transaction be */
+  __u16 s_blocksize;                   	/* block size           */
+  __u16 s_oid_maxsize;			/* max size of object id array, see get_objectid() commentary  */
+  __u16 s_oid_cursize;			/* current size of object id array */
+  __u16 s_state;                       	/* valid or error       */
+  char s_magic[12];                     /* reiserfs magic string indicates that file system is reiserfs */
+  __u32 s_hash_function_code;		/* indicate, what hash fuction is being use to sort names in a directory*/
+  __u16 s_tree_height;                  /* height of disk tree */
+  __u16 s_bmap_nr;                      /* amount of bitmap blocks needed to address each block of file system */
+  __u16 s_reserved;
+};
+
+#define SB_SIZE (sizeof(struct reiserfs_super_block))
+
+/* LOGGING -- */
+
+/* These all interelate for performance.  
+**
+** If the journal block count is smaller than n transactions, you lose speed. I don't know what n is yet, I'm guessing 8-16 
+** 
+** If your journal fills faster than dirty buffers get flushed to disk, it must flush them before allowing the journal
+** to wrap, which slows things down.  If you need high speed meta data updates, the journal should be big enough
+** to prevent wrapping before dirty meta blocks get to disk.
+**
+** If the batch max is smaller than the transaction max, you'll waste space at the end of the journal
+** because journal_end sets the next transaction to start at 0 if the next transaction has any chance of wrapping.
+**
+** The large the batch max age, the better the speed, and the more meta data changes you'll lose after a crash.
+**
+*/
+
+/*
+** transaction handle which is passed around for all journal calls
+*/
+struct reiserfs_transaction_handle {
+  char *t_caller ;              /* debugging use */
+  int t_blocks_logged ;         /* number of blocks this writer has actually logged */
+  int t_blocks_allocated ;      /* number of blocks this writer allocated */
+  unsigned long t_trans_id ;    /* sanity check, should equal the current trans id */
+  struct super_block *t_super ; /* super for this FS when journal_begin was called. saves calls to reiserfs_get_super */
+} ;
+
+struct reiserfs_bitmap_node {
+  int id ;
+  char *data ;
+  struct list_head list ;
+} ;
+
+struct reiserfs_list_bitmap {
+  struct reiserfs_journal_list *journal_list ;
+  struct reiserfs_bitmap_node **bitmaps ;
+} ;
+
+#define JOURNAL_TRANS_MAX 1024   /* biggest possible single transaction, don't change for now (8/3/99) */
+
+#ifdef __KERNEL__
+
+
+/* don't mess with these for a while */
+#define JOURNAL_BLOCK_SIZE  4096 /* BUG gotta get rid of this */
+#define JOURNAL_MAX_CNODE   1500 /* max cnodes to allocate. */
+#define JOURNAL_HASH_SIZE 2048   /* size of journal hash table, used to index lookups in current commit  */
+#define JOURNAL_LIST_HASH_SIZE 16384   
+#define JOURNAL_NUM_BITMAPS 5 /* number of copies of the bitmaps to have floating.  Must be >= 2 */
+#define JOURNAL_LIST_COUNT 128
+#define BH_JDirty       16      /* journal data needs to be written before buffer can be marked dirty */
+#define BH_JDirty_wait 18	/* commit is done, buffer marked dirty */
+#define BH_JNew 19		/* buffer allocated during this transaction, no need to write if freed during this trans too */
+
+/* One of these for every block in every transaction
+** Each one is in two hash tables.  First, a hash of the current transaction, and after journal_end, a
+** hash of all the in memory transactions.
+** next and prev are used by the current transaction (journal_hash).
+** hnext and hprev are used by journal_list_hash.  If a block is in more than one transaction, the journal_list_hash
+** links it in multiple times.  This allows the end_io handler, and flush_journal_list to remove just the cnode belonging
+** to a given transaction.
+*/
+struct reiserfs_journal_cnode {
+  struct buffer_head *bh ;		 /* real buffer head */
+  kdev_t dev ;				 /* dev of real buffer head */
+  unsigned long blocknr ;		 /* block number of real buffer head, == 0 when buffer on disk */		 
+  int state ;
+  struct reiserfs_journal_list *jlist ;  /* journal list this cnode lives in */
+  struct reiserfs_journal_cnode *next ;  /* next in transaction list */
+  struct reiserfs_journal_cnode *prev ;  /* prev in transaction list */
+  struct reiserfs_journal_cnode *hprev ; /* prev in hash list */
+  struct reiserfs_journal_cnode *hnext ; /* next in hash list */
+};
+
+
+/*
+** one of these for each transaction.  The most important part here is the j_realblock.
+** this list of cnodes is used to hash all the blocks in all the commits, to mark all the
+** real buffer heads dirty once all the commits hit the disk,
+** and to make sure every real block in a transaction is on disk before allowing the log area
+** to be overwritten
+*/
+struct reiserfs_journal_list {
+  unsigned long j_start ;
+  unsigned long j_len ;
+  atomic_t j_nonzerolen ;
+  atomic_t j_commit_left ;
+  atomic_t j_flushing ;
+  atomic_t j_commit_flushing ;
+  atomic_t j_older_commits_done ;      /* all commits older than this on disk*/
+  unsigned long j_trans_id ;
+  time_t j_timestamp ;
+  struct reiserfs_list_bitmap *j_list_bitmap ;
+  struct buffer_head *j_commit_bh ; /* commit buffer head */
+  struct reiserfs_journal_cnode *j_realblock  ;
+  struct reiserfs_journal_cnode *j_freedlist ; /* list of buffers that were freed during this trans.  free each of these on flush */
+  struct wait_queue *j_commit_wait ; /* wait for all the commit blocks to be flushed */
+  struct wait_queue *j_flush_wait ; /* wait for all the real blocks to be flushed */
+} ;
+
+
+struct reiserfs_journal {
+  struct buffer_head ** j_ap_blocks ; /* journal blocks on disk */
+  struct reiserfs_journal_cnode *j_last ; /* newest journal block */
+  struct reiserfs_journal_cnode *j_first ; /*  oldest journal block.  start here for traverse */
+  int j_state ;			
+  unsigned long j_trans_id ;
+  unsigned long j_mount_id ;
+  unsigned long j_start ;             /* start of current waiting commit (index into j_ap_blocks) */
+  unsigned long j_len ;               /* lenght of current waiting commit */
+  unsigned long j_len_alloc ;         /* number of buffers requested by journal_begin() */
+  atomic_t j_wcount ;            /* count of writers for current commit */
+  unsigned long j_bcount ;            /* batch count. allows turning X transactions into 1 */
+  unsigned long j_first_unflushed_offset ;  /* first unflushed transactions offset */
+  unsigned long j_last_flush_trans_id ;    /* last fully flushed journal timestamp */
+  struct buffer_head *j_header_bh ;   
+  time_t j_trans_start_time ;         /* time this transaction started */
+  struct wait_queue *j_wait ;         /* wait  journal_end to finish I/O */
+  atomic_t j_wlock ;                       /* lock for j_wait */
+  struct wait_queue *j_join_wait ;    /* wait for current transaction to finish before starting new one */
+  atomic_t j_jlock ;                       /* lock for j_join_wait */
+  struct wait_queue *j_dobalance_wait ; /* call this before going into do_balance */
+  struct wait_queue *j_commit_thread_wait ; /* commit thread waits on this between commits */
+  struct wait_queue *j_commit_thread_done ; /* commit thread wakes this up when it has run its task queue */
+  task_queue j_commit_thread_tq ;        /* task queue for this FS's commit thread */
+  int j_dobalance_lock ;		/* lock for the above */
+  int j_journal_list_index ;	      /* journal list number of the current trans */
+  int j_list_bitmap_index ;	      /* number of next list bitmap to use */
+  int j_must_wait ;		       /* no more journal begins allowed. MUST sleep on j_join_wait */
+  int j_next_full_flush ;             /* next journal_end will flush all journal list */
+  int j_next_async_flush ;             /* next journal_end will flush all async commits */
+
+  int j_cnode_used ;	      /* number of cnodes on the used list */
+  int j_cnode_free ;          /* number of cnodes on the free list */
+
+  struct reiserfs_journal_cnode *j_cnode_free_list ;
+  struct reiserfs_journal_cnode *j_cnode_free_orig ; /* orig pointer returned from vmalloc */
+
+  int j_free_bitmap_nodes ;
+  int j_used_bitmap_nodes ;
+  struct list_head j_bitmap_nodes ;
+  struct reiserfs_list_bitmap j_list_bitmap[JOURNAL_NUM_BITMAPS] ;	/* array of bitmaps to record the deleted blocks */
+  struct reiserfs_journal_list j_journal_list[JOURNAL_LIST_COUNT] ;	    /* array of all the journal lists */
+  struct reiserfs_journal_cnode *j_hash_table[JOURNAL_HASH_SIZE] ; 	    /* hash table for real buffer heads in current trans */ 
+  struct reiserfs_journal_cnode *j_list_hash_table[JOURNAL_LIST_HASH_SIZE] ; /* hash table for all the real buffer heads in all 
+  										the transactions */
+};
+
+#endif // __KERNEL__
+
+#define JOURNAL_DESC_MAGIC "ReIsErLB" /* ick.  magic string to find desc blocks in the journal */
+
+
+
+/*
+ * reiserfs super block data in memory
+ */
+
+typedef __u32 (*hashf_t) (const char *, int);
+
+struct reiserfs_sb_info
+{
+  struct buffer_head * s_sbh;                   /* Buffer containing the super block */
+  struct reiserfs_super_block * s_rs;           /* Pointer to the super block in the buffer */
+  struct buffer_head ** s_ap_true_bitmap;       /* array of buffers, holding block bitmap */
+  struct reiserfs_journal *s_journal ;		/* pointer to journal information */
+  unsigned short s_mount_state;                 /* reiserfs state (valid, invalid) */
+  
+  struct wait_queue * read_wait;                /* wait queue for reiserfs read lock */
+  hashf_t s_hash_function;	/* pointer to function which is used
+                                   to sort names in directory. Set on
+                                   mount */
+  unsigned long s_mount_opt;
+
+  /* session statistics */
+  int s_kmallocs;
+  int s_disk_reads;
+  int s_disk_writes;
+  int s_fix_nodes;
+  int s_do_balance;
+  int s_unneeded_left_neighbor;
+  int s_good_search_by_key_reada;
+  int s_bmaps;
+  int s_bmaps_without_search;
+  int s_direct2indirect;
+  int s_indirect2direct;
+
+};
+
+
+#ifdef __KERNEL__
+
+#define NOTAIL 0  /* mount option -o notail */
+#define GENERICREAD 2 /*          -o genericread */
+#define REPLAYONLY 3 /* replay journal and return 0. Use by fsck */
+#define NOLOG 4 /* don't log metadata at all.  */
+#define FORCE_TEA_HASH 5
+#define FORCE_RUPASOV_HASH 6
+#define FORCE_R5_HASH 7
+#define FORCE_HASH_DETECT 8
+
+
+#define dont_have_tails(s) ((s)->u.reiserfs_sb.s_mount_opt & (1 << NOTAIL))
+#define use_genericread(s) ((s)->u.reiserfs_sb.s_mount_opt & (1 << GENERICREAD))
+#define replay_only(s) ((s)->u.reiserfs_sb.s_mount_opt & (1 << REPLAYONLY))
+#define reiserfs_dont_log(s) ((s)->u.reiserfs_sb.s_mount_opt & (1 << NOLOG))
+#define reiserfs_rupasov_hash(s) ((s)->u.reiserfs_sb.s_mount_opt & (1 << FORCE_RUPASOV_HASH))
+#define reiserfs_tea_hash(s) ((s)->u.reiserfs_sb.s_mount_opt & (1 << FORCE_TEA_HASH))
+#define reiserfs_r5_hash(s) ((s)->u.reiserfs_sb.s_mount_opt & (1 << FORCE_R5_HASH))
+#define reiserfs_hash_detect(s) ((s)->u.reiserfs_sb.s_mount_opt & (1 << FORCE_HASH_DETECT))
+
+
+
+void __wait_on_inode (struct inode * inode);
+extern struct list_head inode_in_use;
+extern spinlock_t inode_lock;
+inline int  test_and_wait_on_buffer(struct buffer_head * p_s_bh);
+struct buffer_head * reiserfs_get_hash_table (kdev_t  n_dev, int n_block, int n_size, int * p_n_repeat);
+struct buffer_head * reiserfs_getblk (kdev_t  n_dev, int n_block, int n_size, int * p_n_repeat);
+void reiserfs_show_buffers (kdev_t dev);
+void fixup_reiserfs_buffers (kdev_t dev);
+void reiserfs_refile_buffer (struct buffer_head * bh);
+void reiserfs_file_buffer (struct buffer_head * bh, int list);
+int reiserfs_is_super(struct super_block *s)  ;
+int journal_mark_dirty(struct reiserfs_transaction_handle *, struct super_block *, struct buffer_head *bh) ;
+int flush_old_commits(struct super_block *s, int) ;
+int show_reiserfs_locks(void) ;
+void reiserfs_end_buffer_io_sync (struct buffer_head *bh, int uptodate) ; 
+void reiserfs_journal_end_io(struct buffer_head *bh, int uptodate) ;
+int reiserfs_resize(struct super_block *, unsigned long) ;
+
+#endif
+
+
+
+#define SB_BUFFER_WITH_SB(s) ((s)->u.reiserfs_sb.s_sbh)
+#define SB_DISK_SUPER_BLOCK(s) ((s)->u.reiserfs_sb.s_rs)
+#define SB_JOURNAL(s) ((s)->u.reiserfs_sb.s_journal)
+#define SB_JOURNAL_BLOCK(s) (SB_DISK_SUPER_BLOCK(s)->s_journal_block)
+#define SB_JOURNAL_LIST(s) (SB_JOURNAL(s)->j_journal_list)
+#define SB_JOURNAL_LIST_INDEX(s) (SB_JOURNAL(s)->j_journal_list_index) 
+#define SB_JOURNAL_LEN_FREE(s) (SB_JOURNAL(s)->j_journal_len_free) 
+#define SB_BLOCK_COUNT(s) (SB_DISK_SUPER_BLOCK(s)->s_block_count)
+#define RESERVED_FOR_PRESERVE_LIST 500
+#define SB_FREE_BLOCKS(s) (SB_DISK_SUPER_BLOCK(s)->s_free_blocks)
+#define SB_REISERFS_MAGIC(s) (SB_DISK_SUPER_BLOCK(s)->s_magic)
+#define SB_ROOT_BLOCK(s) (SB_DISK_SUPER_BLOCK(s)->s_root_block)
+#define SB_TREE_HEIGHT(s) (SB_DISK_SUPER_BLOCK(s)->s_tree_height)
+#define SB_REISERFS_STATE(s) (SB_DISK_SUPER_BLOCK(s)->s_state)
+#define SB_BMAP_NR(s) (SB_DISK_SUPER_BLOCK(s)->s_bmap_nr)
+#define SB_AP_BITMAP(s) ((s)->u.reiserfs_sb.s_ap_true_bitmap)
+
+#endif	/* _LINUX_REISER_FS_SB */
diff -urN linux/include/linux/sched.h /tmp/linux/include/linux/sched.h
--- linux/include/linux/sched.h	Sun Dec 10 17:49:44 2000
+++ /tmp/linux/include/linux/sched.h	Mon Jul  9 20:33:32 2001
@@ -317,6 +317,8 @@
 	struct files_struct *files;
 /* memory management info */
 	struct mm_struct *mm;
+	struct list_head local_pages; int allocation_order, nr_local_pages;
+	int fs_locks;
 
 /* signal handlers */
 	spinlock_t sigmask_lock;	/* Protects signal and blocked */
@@ -349,6 +351,7 @@
 #define PF_SIGNALED	0x00000400	/* killed by a signal */
 #define PF_MEMALLOC	0x00000800	/* Allocating memory */
 #define PF_VFORK	0x00001000	/* Wake up parent in mm_release */
+#define PF_FREE_PAGES	0x00002000	/* The current-> */
 
 #define PF_USEDFPU	0x00100000	/* task used FPU this quantum (SMP) */
 #define PF_DTRACE	0x00200000	/* delayed trace (used on m68k, i386) */
@@ -397,7 +400,7 @@
 /* tss */	INIT_TSS, \
 /* fs */	&init_fs, \
 /* files */	&init_files, \
-/* mm */	&init_mm, \
+/* mm */	&init_mm, { &init_task.local_pages, &init_task.local_pages}, 0, 0, 0, \
 /* signals */	SPIN_LOCK_UNLOCKED, &init_signals, {{0}}, {{0}}, NULL, &init_task.sigqueue, 0, 0, \
 /* exec cts */	0,0, \
 /* oom */	0, \
diff -urN linux/include/linux/sysctl.h /tmp/linux/include/linux/sysctl.h
--- linux/include/linux/sysctl.h	Sun Dec 10 17:49:44 2000
+++ /tmp/linux/include/linux/sysctl.h	Fri Feb  2 19:06:00 2001
@@ -435,6 +435,7 @@
 enum {
 	DEV_CDROM=1,
 	DEV_HWMON=2,
+	DEV_MD=3,
 	DEV_MAC_HID=5
 };
 
@@ -446,6 +447,11 @@
 	DEV_CDROM_DEBUG=4,
 	DEV_CDROM_LOCK=5,
 	DEV_CDROM_CHECK_MEDIA=6
+};
+
+/* /proc/sys/dev/md */
+enum {
+	DEV_MD_SPEED_LIMIT=1
 };
 
 /* /proc/sys/dev/mac_hid */
diff -urN linux/init/main.c /tmp/linux/init/main.c
--- linux/init/main.c	Sun Dec 10 17:49:44 2000
+++ /tmp/linux/init/main.c	Fri Feb  2 19:06:42 2001
@@ -19,6 +19,7 @@
 #include <linux/utsname.h>
 #include <linux/ioport.h>
 #include <linux/init.h>
+#include <linux/raid/md.h>
 #include <linux/smp_lock.h>
 #include <linux/blk.h>
 #include <linux/hdreg.h>
@@ -80,7 +81,6 @@
 extern int bdflush(void *);
 extern int kupdate(void *);
 extern int kswapd(void *);
-extern int kpiod(void *);
 extern void kswapd_setup(void);
 extern unsigned long init_IRQ( unsigned long);
 extern void init_modules(void);
@@ -549,7 +549,7 @@
 #ifdef CONFIG_BLK_DEV_FD
 	{ "fd",      0x0200 },
 #endif
-#ifdef CONFIG_MD_BOOT
+#if CONFIG_MD_BOOT || CONFIG_AUTODETECT_RAID
 	{ "md",      0x0900 },	     
 #endif     
 #ifdef CONFIG_BLK_DEV_XD
@@ -1057,6 +1057,9 @@
 #ifdef CONFIG_MD_BOOT
 	{ "md=", md_setup},
 #endif
+#if CONFIG_BLK_DEV_MD
+	{ "raid=", raid_setup},
+#endif
 #ifdef CONFIG_ADBMOUSE
 	{ "adb_buttons=", adb_mouse_setup },
 #endif
@@ -1580,7 +1583,6 @@
 	kernel_thread(kupdate, NULL, CLONE_FS | CLONE_FILES | CLONE_SIGHAND);
 	/* Start the background pageout daemon. */
 	kswapd_setup();
-	kernel_thread(kpiod, NULL, CLONE_FS | CLONE_FILES | CLONE_SIGHAND);
 	kernel_thread(kswapd, NULL, CLONE_FS | CLONE_FILES | CLONE_SIGHAND);
 
 #if CONFIG_AP1000
@@ -1635,6 +1637,9 @@
 			while (pid != wait(&i));
 		if (MAJOR(real_root_dev) != RAMDISK_MAJOR
 		     || MINOR(real_root_dev) != 0) {
+#ifdef CONFIG_BLK_DEV_MD
+			autodetect_raid();
+#endif
 			error = change_root(real_root_dev,"/initrd");
 			if (error)
 				printk(KERN_ERR "Change root to /initrd: "
diff -urN linux/ipc/shm.c /tmp/linux/ipc/shm.c
--- linux/ipc/shm.c	Wed Jun  7 15:26:44 2000
+++ /tmp/linux/ipc/shm.c	Fri Feb  2 19:06:42 2001
@@ -679,7 +679,7 @@
 }
 
 /*
- * Goes through counter = (shm_rss >> prio) present shm pages.
+ * Goes through counter = (shm_rss / prio) present shm pages.
  */
 static unsigned long swap_id = 0; /* currently being swapped */
 static unsigned long swap_idx = 0; /* next to swap */
@@ -693,7 +693,7 @@
 	int loop = 0;
 	int counter;
 	
-	counter = shm_rss >> prio;
+	counter = shm_rss / prio;
 	if (!counter || !(swap_nr = get_swap_page()))
 		return 0;
 
diff -urN linux/kernel/fork.c /tmp/linux/kernel/fork.c
--- linux/kernel/fork.c	Tue Oct 26 18:53:42 1999
+++ /tmp/linux/kernel/fork.c	Fri Feb  2 19:06:42 2001
@@ -665,6 +665,8 @@
 	p->lock_depth = -1;		/* -1 = no lock */
 	p->start_time = jiffies;
 
+	INIT_LIST_HEAD(&p->local_pages);
+
 	retval = -ENOMEM;
 	/* copy all the process information */
 	if (copy_files(clone_flags, p))
diff -urN linux/kernel/ksyms.c /tmp/linux/kernel/ksyms.c
--- linux/kernel/ksyms.c	Sun Dec 10 17:49:44 2000
+++ /tmp/linux/kernel/ksyms.c	Fri Feb  2 19:05:43 2001
@@ -210,6 +210,20 @@
 EXPORT_SYMBOL(page_hash_bits);
 EXPORT_SYMBOL(__wait_on_page);
 
+#if !defined(CONFIG_REISERFS_FS) && defined(CONFIG_REISERFS_FS_MODULE)
+EXPORT_SYMBOL(__wait_on_inode);
+EXPORT_SYMBOL(inode_in_use);
+EXPORT_SYMBOL(inode_lock);
+EXPORT_SYMBOL(test_and_wait_on_buffer);
+EXPORT_SYMBOL(reiserfs_get_hash_table);
+EXPORT_SYMBOL(reiserfs_getblk);
+EXPORT_SYMBOL(fixup_reiserfs_buffers);
+EXPORT_SYMBOL(reiserfs_refile_buffer);
+EXPORT_SYMBOL(reiserfs_file_buffer) ;
+EXPORT_SYMBOL(reiserfs_journal_end_io) ;
+EXPORT_SYMBOL(reiserfs_end_buffer_io_sync) ;
+#endif
+
 #if !defined(CONFIG_NFSD) && defined(CONFIG_NFSD_MODULE)
 EXPORT_SYMBOL(do_nfsservctl);
 #endif
diff -urN linux/mm/filemap.c /tmp/linux/mm/filemap.c
--- linux/mm/filemap.c	Sun Dec 10 17:49:44 2000
+++ /tmp/linux/mm/filemap.c	Fri Feb  2 19:06:42 2001
@@ -19,7 +19,6 @@
 #include <linux/blkdev.h>
 #include <linux/file.h>
 #include <linux/swapctl.h>
-#include <linux/slab.h>
 #include <linux/init.h>
 
 #include <asm/pgtable.h>
@@ -36,25 +35,6 @@
 unsigned int page_hash_bits, page_hash_mask;
 struct page **page_hash_table;
 
-/* 
- * Define a request structure for outstanding page write requests
- * to the background page io daemon
- */
-
-struct pio_request 
-{
-	struct pio_request *	next;
-	struct file *		file;
-	unsigned long		offset;
-	unsigned long		page;
-};
-static struct pio_request *pio_first = NULL, **pio_last = &pio_first;
-static kmem_cache_t *pio_request_cache;
-static struct wait_queue *pio_wait = NULL;
-
-static inline void 
-make_pio_request(struct file *, unsigned long, unsigned long);
-
 static inline int sync_page(struct page *page)
 {
 	struct inode *inode = page->inode;
@@ -150,16 +130,21 @@
 	unsigned long limit = num_physpages;
 	struct page * page;
 	int count;
-	int nr_dirty = 0;
-	
+
 	/* Make sure we scan all pages twice at priority 0. */
-	count = (limit << 1) >> priority;
+	count = limit / priority;
 
  refresh_clock:
 	page = mem_map + clock;
 	do {
 		int referenced;
 
+		if (current->need_resched) {
+			current->state = TASK_RUNNING;
+			schedule();
+			goto refresh_clock;
+		}
+		
 		/* This works even in the presence of PageSkip because
 		 * the first two entries at the beginning of a hole will
 		 * be marked, not just the first.
@@ -176,6 +161,8 @@
 			clock = page - mem_map;
 		}
 		
+		count--;
+
 		/* We can't free pages unless there's just one user */
 		if (atomic_read(&page->count) != 1)
 			continue;
@@ -185,8 +172,10 @@
 		if (PageLocked(page))
 			continue;
 
-		if ((gfp_mask & __GFP_DMA) && !PageDMA(page))
+		if ((gfp_mask & __GFP_DMA) && !PageDMA(page)) {
+			count++;
 			continue;
+		}
 
 		/*
 		 * Is it a page swap page? If so, we want to
@@ -205,14 +194,6 @@
 
 		/* Is it a buffer page? */
 		if (page->buffers) {
-			/*
-			 * Wait for async IO to complete
-			 * at each 64 buffers
-			 */ 
-
-			int wait = ((gfp_mask & __GFP_IO) 
-				&& (!(nr_dirty++ % 64)));
-
 			if (buffer_under_min())
 				continue;
 			/*
@@ -220,10 +201,8 @@
 			 * throttling.
 			 */
 
-			if (!try_to_free_buffers(page, wait)) { 
-				if(--count < 0) break;
+			if (!try_to_free_buffers(page, gfp_mask))
 				goto refresh_clock;
-			}
 			return 1;
 		}
 
@@ -234,8 +213,7 @@
 			remove_inode_page(page);
 			return 1;
 		}
-
-	} while (--count > 0);
+	} while (count > 0);
 	return 0;
 }
 
@@ -302,7 +280,7 @@
 	struct page **hash)
 {
 	atomic_inc(&page->count);
-	page->flags = (page->flags & ~((1 << PG_uptodate) | (1 << PG_error))) | (1 << PG_referenced);
+	page->flags = page->flags & ~((1 << PG_uptodate) | (1 << PG_error) | (1 << PG_referenced));
 	page->offset = offset;
 	add_page_to_inode_queue(inode, page);
 	__add_page_to_hash_queue(page, hash);
@@ -881,12 +859,12 @@
 
 	if (size > count)
 		size = count;
-	down(&inode->i_sem);
+	fs_down(&inode->i_sem);
 	old_fs = get_fs();
 	set_fs(KERNEL_DS);
 	written = file->f_op->write(file, area, size, &file->f_pos);
 	set_fs(old_fs);
-	up(&inode->i_sem);
+	fs_up(&inode->i_sem);
 	if (written < 0) {
 		desc->error = written;
 		written = 0;
@@ -1163,8 +1141,7 @@
 
 static int filemap_write_page(struct vm_area_struct * vma,
 			      unsigned long offset,
-			      unsigned long page,
-			      int wait)
+			      unsigned long page)
 {
 	int result;
 	struct file * file;
@@ -1182,20 +1159,9 @@
 	 * and file could be released ... increment the count to be safe.
 	 */
 	file->f_count++;
-
-	/* 
-	 * If this is a swapping operation rather than msync(), then
-	 * leave the actual IO, and the restoration of the file count,
-	 * to the kpiod thread.  Just queue the request for now.
-	 */
-	if (!wait) {
-		make_pio_request(file, offset, page);
-		return 0;
-	}
-	
-	down(&inode->i_sem);
+	fs_down(&inode->i_sem);
 	result = do_write_page(inode, file, (const char *) page, offset);
-	up(&inode->i_sem);
+	fs_up(&inode->i_sem);
 	fput(file);
 	return result;
 }
@@ -1208,7 +1174,7 @@
  */
 int filemap_swapout(struct vm_area_struct * vma, struct page * page)
 {
-	return filemap_write_page(vma, page->offset, page_address(page), 0);
+	return filemap_write_page(vma, page->offset, page_address(page));
 }
 
 static inline int filemap_sync_pte(pte_t * ptep, struct vm_area_struct *vma,
@@ -1245,7 +1211,7 @@
 			return 0;
 		}
 	}
-	error = filemap_write_page(vma, address - vma->vm_start + vma->vm_offset, page, 1);
+	error = filemap_write_page(vma, address - vma->vm_start + vma->vm_offset, page);
 	page_cache_free(page);
 	return error;
 }
@@ -1417,9 +1383,9 @@
 			if (file) {
 				struct dentry * dentry = file->f_dentry;
 				struct inode * inode = dentry->d_inode;
-				down(&inode->i_sem);
+				fs_down(&inode->i_sem);
 				error = file_fsync(file, dentry);
-				up(&inode->i_sem);
+				fs_up(&inode->i_sem);
 			}
 		}
 		return error;
@@ -1746,130 +1712,6 @@
 	clear_bit(PG_locked, &page->flags);
 	wake_up(&page->wait);
 	page_cache_release(page);
-}
-
-
-/* Add request for page IO to the queue */
-
-static inline void put_pio_request(struct pio_request *p)
-{
-	*pio_last = p;
-	p->next = NULL;
-	pio_last = &p->next;
-}
-
-/* Take the first page IO request off the queue */
-
-static inline struct pio_request * get_pio_request(void)
-{
-	struct pio_request * p = pio_first;
-	pio_first = p->next;
-	if (!pio_first)
-		pio_last = &pio_first;
-	return p;
-}
-
-/* Make a new page IO request and queue it to the kpiod thread */
-
-static inline void make_pio_request(struct file *file,
-				    unsigned long offset,
-				    unsigned long page)
-{
-	struct pio_request *p;
-
-	atomic_inc(&page_cache_entry(page)->count);
-
-	/* 
-	 * We need to allocate without causing any recursive IO in the
-	 * current thread's context.  We might currently be swapping out
-	 * as a result of an allocation made while holding a critical
-	 * filesystem lock.  To avoid deadlock, we *MUST* not reenter
-	 * the filesystem in this thread.
-	 *
-	 * We can wait for kswapd to free memory, or we can try to free
-	 * pages without actually performing further IO, without fear of
-	 * deadlock.  --sct
-	 */
-
-	while ((p = kmem_cache_alloc(pio_request_cache, GFP_BUFFER)) == NULL) {
-		if (try_to_free_pages(__GFP_WAIT))
-			continue;
-		current->state = TASK_INTERRUPTIBLE;
-		schedule_timeout(HZ/10);
-	}
-	
-	p->file   = file;
-	p->offset = offset;
-	p->page   = page;
-
-	put_pio_request(p);
-	wake_up(&pio_wait);
-}
-
-
-/*
- * This is the only thread which is allowed to write out filemap pages
- * while swapping.
- * 
- * To avoid deadlock, it is important that we never reenter this thread.
- * Although recursive memory allocations within this thread may result
- * in more page swapping, that swapping will always be done by queuing
- * another IO request to the same thread: we will never actually start
- * that IO request until we have finished with the current one, and so
- * we will not deadlock.  
- */
-
-int kpiod(void * unused)
-{
-	struct task_struct *tsk = current;
-	struct wait_queue wait = { tsk, };
-	struct inode * inode;
-	struct dentry * dentry;
-	struct pio_request * p;
-	
-	tsk->session = 1;
-	tsk->pgrp = 1;
-	strcpy(tsk->comm, "kpiod");
-	sigfillset(&tsk->blocked);
-	init_waitqueue(&pio_wait);
-	/*
-	 * Mark this task as a memory allocator - we don't want to get caught
-	 * up in the regular mm freeing frenzy if we have to allocate memory
-	 * in order to write stuff out.
-	 */
-	tsk->flags |= PF_MEMALLOC;
-
-	lock_kernel();
-	
-	pio_request_cache = kmem_cache_create("pio_request", 
-					      sizeof(struct pio_request),
-					      0, SLAB_HWCACHE_ALIGN, 
-					      NULL, NULL);
-	if (!pio_request_cache)
-		panic ("Could not create pio_request slab cache");
-
-	while (1) {
-		tsk->state = TASK_INTERRUPTIBLE;
-		add_wait_queue(&pio_wait, &wait);
-		if (!pio_first)
-			schedule();
-		remove_wait_queue(&pio_wait, &wait);
-		tsk->state = TASK_RUNNING;
-
-		while (pio_first) {
-			p = get_pio_request();
-			dentry = p->file->f_dentry;
-			inode = dentry->d_inode;
-			
-			down(&inode->i_sem);
-			do_write_page(inode, p->file,
-				      (const char *) p->page, p->offset);
-			up(&inode->i_sem);
-			fput(p->file);
-			page_cache_free(p->page);
-			kmem_cache_free(pio_request_cache, p);
-		}
-	}
 }
 
 void __init page_cache_init(unsigned long memory_size)
diff -urN linux/mm/page_alloc.c /tmp/linux/mm/page_alloc.c
--- linux/mm/page_alloc.c	Mon Sep  4 11:39:28 2000
+++ /tmp/linux/mm/page_alloc.c	Fri Feb  2 19:06:42 2001
@@ -93,34 +93,69 @@
  */
 spinlock_t page_alloc_lock = SPIN_LOCK_UNLOCKED;
 
+#define list(x) (mem_map+(x))
+#define __free_pages_ok(map_nr, mask, area, index)		\
+	nr_free_pages -= (mask);				\
+	while ((mask) + (1 << (NR_MEM_LISTS-1))) {		\
+		if (!test_and_change_bit((index), (area)->map))	\
+			break;					\
+		(area)->count--;				\
+		remove_mem_queue(list((map_nr) ^ -(mask)));	\
+		(mask) <<= 1;					\
+		(area)++;					\
+		(index) >>= 1;					\
+		(map_nr) &= (mask);				\
+	}							\
+	add_mem_queue(area, list(map_nr));
+
+static void free_local_pages(struct page * page) {
+	unsigned long order = page->offset;
+	unsigned int type = PageDMA(page) ? 1 : 0;
+	struct free_area_struct *area;
+	unsigned long map_nr = page - mem_map;
+	unsigned long mask = (~0UL) << order;
+	unsigned long index = map_nr >> (1 + order);
+
+	area = free_area[type] + order;
+	__free_pages_ok(map_nr, mask, area, index);
+}
+
 static inline void free_pages_ok(unsigned long map_nr, unsigned long order, unsigned type)
 {
-	struct free_area_struct *area = free_area[type] + order;
-	unsigned long index = map_nr >> (1 + order);
-	unsigned long mask = (~0UL) << order;
+	struct free_area_struct *area;
+	unsigned long index;
+	unsigned long mask;
 	unsigned long flags;
+	struct page * page;
 
-	spin_lock_irqsave(&page_alloc_lock, flags);
-
-#define list(x) (mem_map+(x))
+	if (current->flags & PF_FREE_PAGES)
+		goto local_freelist;
+ back_local_freelist:
 
+	index = map_nr >> (1 + order);
+	mask = (~0UL) << order;
 	map_nr &= mask;
-	nr_free_pages -= mask;
-	while (mask + (1 << (NR_MEM_LISTS-1))) {
-		if (!test_and_change_bit(index, area->map))
-			break;
-		area->count--;
-		remove_mem_queue(list(map_nr ^ -mask));
-		mask <<= 1;
-		area++;
-		index >>= 1;
-		map_nr &= mask;
-	}
-	add_mem_queue(area, list(map_nr));
-
-#undef list
 
+	spin_lock_irqsave(&page_alloc_lock, flags);
+	area = free_area[type] + order;
+	__free_pages_ok(map_nr, mask, area, index);
 	spin_unlock_irqrestore(&page_alloc_lock, flags);
+	return;
+
+ local_freelist:
+	/*
+	 * This is a little subtle: if the allocation order
+	 * wanted is major than zero we'd better take all the pages
+	 * local since we must deal with fragmentation too and we
+	 * can't rely on the nr_local_pages information.
+	 */
+	if (current->nr_local_pages && !current->allocation_order)
+		goto back_local_freelist;
+
+	page = mem_map + map_nr;
+	list_add((struct list_head *) page, &current->local_pages);
+	page->offset = order;
+	current->nr_local_pages++;
 }
 
 void __free_pages(struct page *page, unsigned long order)
@@ -179,13 +214,32 @@
 	atomic_set(&map->count, 1); \
 } while (0)
 
+static void refile_local_pages(void)
+{
+	if (current->nr_local_pages) {
+		struct page * page;
+		struct list_head * entry;
+		int nr_pages = current->nr_local_pages;
+
+		while ((entry = current->local_pages.next) != &current->local_pages) {
+			list_del(entry);
+			page = (struct page *) entry;
+			free_local_pages(page);
+			if (!nr_pages--)
+				panic("__get_free_pages local_pages list corrupted I");
+		}
+		if (nr_pages)
+			panic("__get_free_pages local_pages list corrupted II");
+		current->nr_local_pages = 0;
+	}
+}
+
 unsigned long __get_free_pages(int gfp_mask, unsigned long order)
 {
 	unsigned long flags;
-	static atomic_t free_before_allocate = ATOMIC_INIT(0);
 
 	if (order >= NR_MEM_LISTS)
-		goto nopage;
+		goto out;
 
 #ifdef ATOMIC_MEMORY_DEBUGGING
 	if ((gfp_mask & __GFP_WAIT) && in_interrupt()) {
@@ -194,26 +248,24 @@
 			printk("gfp called nonatomically from interrupt %p\n",
 				__builtin_return_address(0));
 		}
-		goto nopage;
+		goto out;
 	}
 #endif
 
 	/*
+	 * Acquire lock before reading nr_free_pages to make sure it
+	 * won't change from under us.
+	 */
+	spin_lock_irqsave(&page_alloc_lock, flags);
+
+	/*
 	 * If this is a recursive call, we'd better
 	 * do our best to just allocate things without
 	 * further thought.
 	 */
 	if (!(current->flags & PF_MEMALLOC)) {
-		int freed;
 		extern struct wait_queue * kswapd_wait;
 
-		/* Somebody needs to free pages so we free some of our own. */
-		if (atomic_read(&free_before_allocate)) {
-			current->flags |= PF_MEMALLOC;
-			try_to_free_pages(gfp_mask);
-			current->flags &= ~PF_MEMALLOC;
-		}
-
 		if (nr_free_pages > freepages.low)
 			goto ok_to_allocate;
 
@@ -223,35 +275,44 @@
 		/* Do we have to block or can we proceed? */
 		if (nr_free_pages > freepages.min)
 			goto ok_to_allocate;
+		if (gfp_mask & __GFP_WAIT) {
+			int freed;
+			/*
+			 * If the task is ok to sleep it's fine also
+			 * if we release irq here.
+			 */
+			spin_unlock_irq(&page_alloc_lock);
+
+			current->flags |= PF_MEMALLOC|PF_FREE_PAGES;
+			current->allocation_order = order;
+			freed = try_to_free_pages(gfp_mask);
+			current->flags &= ~(PF_MEMALLOC|PF_FREE_PAGES);
+
+			spin_lock_irq(&page_alloc_lock);
+			refile_local_pages();
+
+			/*
+			 * Re-check we're still low on memory after we blocked
+			 * for some time. Somebody may have released lots of
+			 * memory from under us while we was trying to free
+			 * the pages. We check against pages_high to be sure
+			 * to succeed only if lots of memory is been released.
+			 */
+			if (nr_free_pages > freepages.high)
+				goto ok_to_allocate;
 
-		current->flags |= PF_MEMALLOC;
-		atomic_inc(&free_before_allocate);
-		freed = try_to_free_pages(gfp_mask);
-		atomic_dec(&free_before_allocate);
-		current->flags &= ~PF_MEMALLOC;
-
-		/*
-		 * Re-check we're still low on memory after we blocked
-		 * for some time. Somebody may have released lots of
-		 * memory from under us while we was trying to free
-		 * the pages. We check against pages_high to be sure
-		 * to succeed only if lots of memory is been released.
-		 */
-		if (nr_free_pages > freepages.high)
-			goto ok_to_allocate;
-
-		if (!freed && !(gfp_mask & (__GFP_MED | __GFP_HIGH)))
-			goto nopage;
+			if (!freed && !(gfp_mask & (__GFP_MED | __GFP_HIGH)))
+				goto nopage;
+		}
 	}
 ok_to_allocate:
-	spin_lock_irqsave(&page_alloc_lock, flags);
 	/* if it's not a dma request, try non-dma first */
 	if (!(gfp_mask & __GFP_DMA))
 		RMQUEUE_TYPE(order, 0);
 	RMQUEUE_TYPE(order, 1);
+ nopage:
 	spin_unlock_irqrestore(&page_alloc_lock, flags);
-
-nopage:
+ out:
 	return 0;
 }
 
diff -urN linux/mm/swap_state.c /tmp/linux/mm/swap_state.c
--- linux/mm/swap_state.c	Wed Jan 13 10:54:50 1999
+++ /tmp/linux/mm/swap_state.c	Fri Feb  2 19:06:42 2001
@@ -63,6 +63,7 @@
 		return 0;
 	}
 	atomic_inc(&page->count);
+	page->flags = page->flags & ~((1 << PG_uptodate) | (1 << PG_error) | (1 << PG_referenced));
 	page->inode = &swapper_inode;
 	page->offset = entry;
 	add_page_to_hash_queue(page, &swapper_inode, entry);
diff -urN linux/mm/vmscan.c /tmp/linux/mm/vmscan.c
--- linux/mm/vmscan.c	Mon Sep  4 11:39:28 2000
+++ /tmp/linux/mm/vmscan.c	Fri Feb  2 19:06:42 2001
@@ -96,6 +96,9 @@
 	 * some real work in the future in "shrink_mmap()".
 	 */
 	if (!pte_dirty(pte)) {
+		if (page_map->inode && pgcache_under_min())
+			/* unmapping this page would be useless */
+			return 0;
 		flush_cache_page(vma, address);
 		pte_clear(page_table);
 		goto drop_pte;
@@ -106,7 +109,7 @@
 	 * we cannot do I/O! Avoid recursing on FS
 	 * locks etc.
 	 */
-	if (!(gfp_mask & __GFP_IO))
+	if (!(gfp_mask & __GFP_IO) || current->fs_locks)
 		return 0;
 
 	/*
@@ -208,6 +211,8 @@
 		result = try_to_swap_out(tsk, vma, address, pte, gfp_mask);
 		if (result)
 			return result;
+		if (current->need_resched)
+			return 2;
 		address += PAGE_SIZE;
 		pte++;
 	} while (address < end);
@@ -327,7 +332,7 @@
 	 * Think of swap_cnt as a "shadow rss" - it tells us which process
 	 * we want to page out (always try largest first).
 	 */
-	counter = nr_tasks / (priority+1);
+	counter = nr_tasks / priority;
 	if (counter < 1)
 		counter = 1;
 
@@ -361,8 +366,13 @@
 			goto out;
 		}
 
-		if (swap_out_process(pbest, gfp_mask))
+		switch (swap_out_process(pbest, gfp_mask)) {
+		case 1:
 			return 1;
+		case 2:
+			current->state = TASK_RUNNING;
+			schedule();
+		}
 	}
 out:
 	return 0;
@@ -377,11 +387,9 @@
  * cluster them so that we get good swap-out behaviour. See
  * the "free_memory()" macro for details.
  */
-static int do_try_to_free_pages(unsigned int gfp_mask)
+int try_to_free_pages(unsigned int gfp_mask)
 {
 	int priority;
-	int ret = 0;
-	int swapcount;
 	int count = SWAP_CLUSTER_MAX;
 
 	lock_kernel();
@@ -389,41 +397,34 @@
 	/* Always trim SLAB caches when memory gets low. */
 	kmem_cache_reap(gfp_mask);
 
-	priority = 6;
+	priority = 5;
 	do {
 		while (shrink_mmap(priority, gfp_mask)) {
-			ret = 1;
 			if (!--count)
 				goto done;
 		}
 
 		/* Try to get rid of some shared memory pages.. */
-		if (gfp_mask & __GFP_IO) {
+		if (gfp_mask & __GFP_IO && !current->fs_locks) {
 			while (shm_swap(priority, gfp_mask)) {
-				ret = 1;
 				if (!--count)
 					goto done;
 			}
 		}
 
 		/* Then, try to page stuff out.. */
-		swapcount = count;
 		while (swap_out(priority, gfp_mask)) {
-			ret = 1;
-			if (!--swapcount)
-				break;
+			if (!--count)
+				goto done;
 		}
 
 		shrink_dcache_memory(priority, gfp_mask);
-	} while (--priority >= 0);
+	} while (--priority > 0);
 done:
 	unlock_kernel();
 
-	if (!ret)
-		printk("VM: do_try_to_free_pages failed for %s...\n",
-				current->comm);
 	/* Return success if we freed a page. */
-	return ret;
+	return priority > 0;
 }
 
 /*
@@ -499,7 +500,7 @@
 
 		while (nr_free_pages < freepages.high)
 		{
-			if (do_try_to_free_pages(GFP_KSWAPD))
+			if (try_to_free_pages(GFP_KSWAPD))
 			{
 				if (tsk->need_resched)
 					schedule();
@@ -510,17 +511,3 @@
 		}
 	}
 }
-
-/*
- * Called by non-kswapd processes when kswapd really cannot
- * keep up with the demand for free memory.
- */
-int try_to_free_pages(unsigned int gfp_mask)
-{
-	int retval = 1;
-
-	if (gfp_mask & __GFP_WAIT)
-		retval = do_try_to_free_pages(gfp_mask);
-	return retval;
-}
-	
